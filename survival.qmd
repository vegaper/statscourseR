---
title: "Survival Analisys"
format: html
editor: visual
---
```{r}
#| echo: false
library(dplyr)
library(tidyverse)
#library(here)
library(readxl)
#library(easystats)
#library(infer)
#library(kableExtra)
#library(plotly)
library(ggplot2)
#library(patchwork)
#library(BSDA) 
#library(MASS)
#library(rafalib)
#library(UsingR) #datasets
library(ISLR2) #datasets
#library(gam)
#library(scatterplot3d)
#library(gridExtra)
library(caret) #confusionMatrix
#library(pROC)
#library(class)
library(boot) #crossvalidation
#library(leaps) #best subset selection
#library(glmnet) #ridge regression and lasso
library(survival) #survival 
library(survminer) #survival ggplots
#library(splines) #splines 
theme_set(theme_minimal())
options(scipen= 999)
```


Survival analysis concerns a special kind of outcome variable: the time until an event occurs. For example, suppose that we have conducted a five-year medical study, in which patients have been treated for cancer. We would like to fit a model to predict patient survival time, using features such a baseline measurements or type of treatment. Sounds like a regression problem, but there is an important complication: some of the patients have survived until the end of the study. Such a patient's survival time is said to be *censored*. We do not want to discard this subset of surviving patients, since the fact that they survived at least 5 years amounts to valuable information.

The applications of survival analysis extend far beyond medicine. For example, consider a company that wishes to model churn, the event when customers cancel subscription to a service. The company might collect data on customers over some period of time, in order to predict each customer's time to cancellation.

For each individual, we suppose that there is a true *failure* or *event* time T, as well as a true censoring time C. The survival time represents the time at which the event of interest occurs. The censoring is the time at which censoring occurs, for example the time at which the patient drops out of the study or the study ends. For each observation we compute the min of those times, either the event or the censoring time. If the event occurs before censoring, then we observe the true survival time T, if censoring occurs before the event, then we observe C. We observe a status indicator $\delta = 1\ if\  T\leq C$ $\delta = 0\ if\  T > C$ so in our dataset we observe n pairs (Y,$\delta$) where Y is the time to T or C.

Suppose that a number of patients drop out of a cancer study early because they are very sick. An Analysis that does not take into consideration the reason why the patients dropped out will likely overestimate the true average survival time. Similarly, suppose that males are more likely to drop out of the study than females, then a comparison of male and female survival times may wrongly suggest that males survive longer than females.

In general, we need to assume that, conditional on the features, the event time T is independent of the censoring time C, and the above examples violate that assumption. There is no way of statistically checking if this assumption is right, you will have to think about those variables yourself and find out the reasons why observations are censored.

The **survival curve** is the probability that a true survival time $T$ is higher than a specified time $t$. $$
S(t)=Pr(T>t)
$$ {#eq-survivalCurve}

This decreasing function quantifies the probability of surviving past time $t$ For example, suppose that a company is interested in modeling customer churn. Let $T$ represent the time that a customer cancels a subscription to the service, then $S(t)$ represents the probability that a customer cancels later than time $t$. The larger the value of $S(t)$, the less likely that the customer will cancel before time $t$.

Let's consider the `BrainCancer` dataset, which contains the survival times for patients with primary brain tumors undergoing treatments. The predictors are `gtv` (gross tumor volume), `sex`, `diagnosis` (type of cancer), `loc` (location of the tumor), `ki` (Karnofsky index), and `stereo` (stereotactic method). 53 of the 88 patients were still alive at the end of the study.

Suppose we'd like to estimate the probability of surviving past 20 months. $S(20)=Pr(T>20)$. The first approach that would come to our minds would be to just count the proportion of patients that survived past 20 months, and that is 48/88 = 55%. However, this is not right, 17 of the 40 patients who did not survive to 20 months were actually censored, and this way of doing the analysis implicitly assumes they died, hence this probability is an underestimate.

# The Kaplan-Meier Estimate

Suppose we have 5 observations (patients). number 2 and 4 censored and we have the time to event for the rest:

```{r ,echo=FALSE, fig.align='center'}
# Sample data
time <- c(2, 3, 4, 5, 6)
status <- c(1, 0, 1, 0, 1)  # 1 = event (e.g., death), 0 = censored
patient <- c(1, 2, 3, 4, 5)

# Create a data frame
data <- data.frame(time, status, patient)
# Plot the time vs patient
plot(data$time, 1:nrow(data), type = "n", xlab = "Time", ylab = "Patient", yaxt = "n", xlim=c(0,6), ylim=c(0,6))

# Add patient labels
axis(2, at = 1:nrow(data), labels = data$patient)

# Add points for events and censored data
points(data$time[data$status == 1], data$patient[data$status == 1], pch = 19, col = "red")  # Events
points(data$time[data$status == 0], data$patient[data$status == 0], pch = 1, col = "blue")  # Censored

for (i in 1:nrow(data)) { lines(c(0, data$time[i]), c(data$patient[i], data$patient[i]), col = "gray", lty = 2) }
```

when we are observing the first patient's T, 4 out of 5 patients survived past that point in time, so the probability of surviving past t(2) is 4/5

Then we move to the next T (skip the censored number 2) and we have that 2 out of the 3 patients remaining survived past that point. So the probability of surviving past t(4) is $\frac{4}{5} \times \frac{2}{3}$ so the probability of surviving past t(4) is the probability of surviving t(4) given that you have survived pass the previous T.

The last point, patient 5 in t(6) is the last failure, and no one has survived pass that point, so we have $\frac{4}{5} \times \frac{2}{3} \times 0$

So we see how the role of the censored observations is to participate in the denominator up to the point that they dropped out of the study.

The kaplan-Meier curve for this simple experiment would look like this:

```{r, fig.align='center', fig.width=6}
fit <- survival::survfit(Surv(time, status) ~ 1, data = data)
survminer::ggsurvplot(fit, data = data, xlab = "Time", ylab = "Survival Probability", title = "Kaplan-Meier Survival Estimate", censor.shape = 124, censor.size = 4, risk.table = TRUE, risk.table.col = "strata", ggtheme = theme_minimal())
```

Now that we have seen this with a simple example, let's run it over the BrainCancer dataset. Each point in the solid step-like curve shows the estimated probability of surviving past the time indicated on the horizontal axis.

Before begining the analysis, it is important to know how the `status` variable has been coded. Most software, including R, uses the convention that `status =1` indicates an uncensored observation, and `status =0` indicates a censored observations. But some scientist might use the opposite coding.

To begin the analysis we recreate the Kaplan-Meier survival curve using the `survfit()` function from `survival` library. By including `~1` we indicate that the survival analysis is performed without considering any covariates or grouping. The model will estimate a single survival curve for all subjects combined. This allows us to understand the general survival pattern across the entire dataset.

```{r, fig.align='center', fig.width=6}
library(survival)

BrainCancer= ISLR2::BrainCancer
fit.surv <- survfit(Surv(time, status) ~ 1, data = BrainCancer)
plot(fit.surv, xlab="Months", ylab= "Estimated Pr survival")
```

The dotted lines indicates the confidence interval for 2 standard errors. The estimated probability of survival past 20 months is 71%, which is higher than the 55% that we estimated just but counting the amount of survivors.

# Proportional Hazards Model: Log-Rank Test

Now we are going to see how to compare two samples. For example, for our BrainCancer dataset, let's compare females and males. If we plot them we can appreciate a difference until month 50 but is this difference significant?

```{r, fig.align='center', fig.width=6}
fit.sex <- survfit(Surv(time, status) ~ sex, data= BrainCancer)

plot(fit.sex, xlab = "Months",
    ylab = "Estimated Probability of Survival", col = c(2,4))

legend("bottomleft", levels(BrainCancer$sex), col = c(2,4), lty = 1)

```

The proportional Hazard Model allows us to compare two survival samples. It is the equivalent of a t.test for survival data. Being $d_1<d_2<\dots,d_k$ unique death times among non-censored patients, $r_k$ is the number of patients at risk at time $d_k$ and $q_k$ is the number of patients who died at time $d_k$. We divide our patients in two groups males and females, and we will call their variables $q_{1k}$ and $q_{2k}$ for the number of male and female patients who died and $r_{1k}$ and $r_{2k}$ are the number of patients at risk in each group. So at a specific time $d_k$ we have:

```{r echo=FALSE}
library(kableExtra)
data <- data.frame(
  Group1 = c("$q_{1k}$", "$r_{1k} - q_{1k}$", "$r_{1k}$"),
  Group2 = c("$q_{2k}$", "$r_{2k} - q_{2k}$", "$r_{2k}$"),
  Total = c("$q_{k}$", "$r_{k} - q_{k}$", "$r_{k}$") )
row.names(data) <- c("Died", "Survived", "Total")
kable(data, format = "html", escape = FALSE)%>% kable_styling(full_width = FALSE, position = "center")
```

To test $H_0$, our null hypothesis that the survival rates are the same for both genders, we construct a test statistic of the form:

$$
W=\frac{X-E(X)}{\sqrt{Var(X)}}
$$ where E(X) is the expectation and Var(X) is the variance of X under the null hypothesis. $X=\sum^K_{k=1}q_{1k}$

The resulting formula for the log-rank test statistic is:

$$
W =\frac{\sum^K_{k=1}(q_{1k}-E(q_{1k}))}{\sqrt{\sum^K_{k=1}Var(q_{1k})}}
$$ {#eq-logrankTestStatistic}

We are not going to develop this formula further in this page, but the full definition can be found on the book. When the sample size is large, the *log-rank test statistic W* has approximately a standard normal distribution.

This can be used to compute a `p-value` for the null hypothesis that there is no difference between survival curves in the two groups.

Instead of calculating it manually we will use R with the function `survdiff()`:

```{r}
logrank.test <- survdiff(Surv(time, status) ~ sex, data =BrainCancer)
logrank.test
```

We use a two sided test because we want to know if the male survival rate is less or more than those of females, we are not interested only in if they are higher. We see that our $p$-value for a two-sided test is 0.2, which does not allow us to reject the null hypothesis that there is a difference. This could be just because the dataset is too small. Even though the number of observations are 88, the survival information is extracted from the number of failures, so the censored are not really adding so much information.

The `survminer` package draws survival curves using ggplot, and can provide log-rank $p$-values:

```{r, fig.align='center', fig.width= 8, fig.height=6}
library(survminer)
fit.sex <- survfit(Surv(time, status) ~ sex, data= BrainCancer)
survminer::ggsurvplot(fit.sex, data = BrainCancer,
           pval = TRUE,
           conf.int = TRUE,
           risk.table = TRUE,
           legend.title = "Sex",
           legend.labs = c("Female", "Male"))

```

# Regression Models with a Survival Response Cox Model.

We now consider the task of fitting a regression model to survival data. We wish to predict the true survival time T. Because of the censoring, we cannot really just fit a linear or log model. To overcome this difficulty, we instead make use of a sequential construction, similar to the idea used for the Kaplan-Meier survival curve.

**Proportional Hazards Model (Cox Model)** This is a statistical technique used to explore the relationship between the survival time of subjects and one or more predictor variables.

We start by defining the *Hazard function* this is the risk of the event happening at a particular time, given the fact that the subject has survived up to that time. Why do we care about the hazard function? It turns out that a key approach for modeling survival data as a function of covariates relies heavily on the hazard function.

The model assumes that the hazard rate for an individual is a baseline hazard, shared by all individuals, modified by a set of covariates (predictors). The hazard at time t for an individual is given by $$
h(t | X) = h_0(t) \exp\left( \sum^p_{j=1}x_{ij}\beta_j \right)=  h_0(t) \exp(\beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p)
$$ {#eq-hazardFunction}

Where $h_0(t)$ is an unspecified function, known as the *baseline hazard* The quantity $exp\left( \sum^p_{j=1}x_{ij}\beta_j \right)$ is called the *relative risk*

There is a deeper explanation of this method on the book that we skip here.

There is no intercept in the proportional hazards model because the intercept is absorbed into the baseline hazard. We assume that there are no tied failure times (only one individual Fails at a given time). In the case this is not correct and there are ties, the model is more complicated to calculate and a number of computational approximations must be used. Apart from estimating the coefficients, we may also wish to estimate the baseline hazard, so we can estimate the survival curve. These are implemented in the `survival` package in R.

If we have a single predictor (sex) with a binary outcome (0,1) or (male,female). To test whether there is a difference between the survival tiems of the observations in the two groups we can consider two different approaches:

1- Fit a Cox proportional hazards model and test the null hypothesis that $\beta =0$ 2- Perform a log-rank test to compare the two groups. When taking approach 1, there are a number of possible ways to test $H_0$. One way is known as *score test* and it turn out that in the case of a single binary covariate, the score test for $H_0:\beta=0$ in Cox's proportional hazards model is exactly equal to the log-rank test.

Let's fix Cox proportional hazards models using the `coxph()` function. To begin, we consider a model that uses `sex` as the only predictor in the BrainCancer dataset.

```{r}
fit.cox<- survival::coxph(Surv(time, status) ~ sex, data=BrainCancer)
summary(fit.cox)
```

As we saw with the log-rank test, there is no evidence to suggest that there is a survival difference by sex.

Now we fit all the predictors:

```{r}
fit.cox<- survival::coxph(Surv(time, status) ~ sex + diagnosis + loc + ki + gtv +
   stereo, data = BrainCancer)
fit.cox
```

**Interpreting the results:**

As we mentioned before, there is no intersect because it has been absorbed into the baseline hazard function $h_0(t)$.

-   Call: This shows the formula used for the model.

-   n: Number of observations used in the analysis (87).

-   number of events: Number of observed events (35), such as deaths or failures.

-   coef: The regression coefficients for each predictor.

-   exp(coef): The hazard ratios (HR), which indicate the effect size of each predictor.

-   se(coef): Standard errors of the coefficients.

-   z: z-statistic for testing the null hypothesis that the coefficient is zero.

-   Pr(\>\|z\|): $p$-values for the z-tests, indicating whether the coefficients are significantly different from zero.

-   Likelihood ratio test, Wald test, Score (log-rank) test: These tests assess the overall significance of the model. The low $p$-values (\< 0.001) indicate that the predictors collectively have a significant effect on survival.

In our specific result, the `diagnosis` variable has been coded so that the baseline corresponds to meningioma. Results indicate that the risk associated with HG glioma is more than eight times (i.e. $e^{2.15} = 8.62$) the risk associated with meningioma. In addition, larger values of the Karnofsky index (ki) are associated with lower risk. (negative coefficient).

We need to remember, that like in linear models, if some of the variables are correlated, a variable could show as non-significant even if it is, because the other correlated variables are absorbing part of its influence.

It is possible to plot estimated survival curves for each diagnosis category, adjusting for the other predictors. To make these plots, set the values of the other predictors equal to the mean for quantitative variables, and the modal value for factors.

```{r, fig.align='center', fig.width=7}

with(BrainCancer,{
  modaldata <<- data.frame(
       diagnosis = levels(diagnosis),
       sex = rep("Female", 4),
       loc = rep("Supratentorial", 4),
       ki = rep(mean(ki), 4),
       gtv = rep(mean(gtv), 4),
       stereo = rep("SRT", 4)
       )})
modaldata

survplots <- survfit(fit.cox, newdata = modaldata)
  plot(survplots, xlab = "Months",
      ylab = "Survival Probability", col = 2:5)
  legend("bottomleft", levels(BrainCancer$diagnosis), col = 2:5, lty = 1)

```

::: exercise-box
We are using now the Publication dataset from ISLR2. The time is time until publication and we are interested in knowing if having a positive or negative result versus a null result resulted in faster publication time. A censored time here would be that either the paper was never published or was published after this studied finished. We will start by plotting the survival curve.

```{r, fig.align='center', fig.width=7}

fit.posres <- survfit(Surv(time, status) ~ posres, data = Publication)
plot(fit.posres, xlab ="Months", ylab= "Pr. of not being published", col =3:4)
legend("topright", c("Negative Result", "Positive Result"), col=3:4, lty =1)
```

we can fit a cox model:

```{r}
(fit.pub <- coxph(Surv(time,status) ~ posres, data=Publication))

```

The $p$-value is not significant. We can do a log-rank test:

```{r}
(logrank.test <- survdiff(Surv(time,status) ~ posres, data=Publication))
```

and again we see not significant difference.

However, the results change dramatically when we include other predictors in the model. Here we have excluded only the funding mechanism variable

```{r}
(fit.pub <- coxph(Surv(time,status)~ . -mech, data=Publication))
```

We have now some significant variables, including `posres` result. This often happens when we include other confounding factors.
:::

::: exercise-box
In this section we will simulate survival data using the `sim.survdata()` function, which is part of the `coxed` library. Our simulated data will represent the observed wait times (in seconds) for 2000 customers who have phoned a call center. In this context, censoring occurs if a customers hangs up before his or her call is anwered.

There are three covariates: `operators` (the number of call center operators available) `center` and `time` of day (morning, afternoon, evening). We generate data for these covariates so that all possibilities are equally likely, for instance, morning, afternoon and evening calls are equally likely, and the number of operators (from 5 to 15) is equally likely.

```{r}
set.seed(4)
N <-2000
Operators <- sample(5:15, N, replace=T)
Center <-sample(c("A","B","C"), N, replace=T )
Time <- sample(c("Morn.","After.","Even."), N, replace=T)
X <- model.matrix( ~ Operators + Center + Time)[,-1]

X[1:5,]

```

Next, we specify the coefficients and the hazard function:

```{r}
true.beta <- c(0.04, -0.3,0,0.2,-0.2)
h.fn <- function(x) return(0.00001 * x) #function to create the HAzard.
```

Here, we have set the coefficient associated with `Operators` to equal $0.04$; in other words, each additional operator leads to a $e^{0.04}=1.041$-fold increase in the "risk" that the call will be answered, given the `Center` and `Time` covariates. This makes sense: the greater the number of operators at hand, the shorter the wait time.

The coefficient associated with `Center = B` is $-0.3$, and `Center = A` is treated as the baseline. This means that the risk of a call being answered at Center B is $0.74$ times the risk that it will be answered at Center A; in other words, the wait times are a bit longer at Center B.

We are now ready to generate data under the Cox proportional hazards model. The `sim.survdata()` function allows us to specify the maximum possible failure time, which in this case corresponds to the longest possible wait time for a customer; we set this to equal $1{,}000$ seconds.

```{r chunk19}
library(coxed)
queuing <- sim.survdata(N = N, T = 1000, X = X,
    beta = true.beta, hazard.fun = h.fn)
names(queuing)
```

The "observed" data is stored in `queuing$data`, with `y` corresponding to the event time and `failed` an indicator of whether the call was answered (`failed = T`) or the customer hung up before the call was answered (`failed = F`). We see that almost $90\%$ of calls were answered.

```{r chunk20}
head(queuing$data)
mean(queuing$data$failed)
```

We now plot Kaplan-Meier survival curves. First, we stratify by `Center`.

```{r ,fig.align='center',fig.width=7}
fit.Center <- survfit(Surv(y, failed) ~ Center,
    data = queuing$data)
plot(fit.Center, xlab = "Seconds",
    ylab = "Probability of Still Being on Hold",
    col = c(2, 4, 5))
legend("topright",
     c("Call Center A", "Call Center B", "Call Center C"),
     col = c(2, 4, 5), lty = 1)
```

Next, we stratify by `Time`.

```{r ,fig.align='center',fig.width=7}
fit.Time <- survfit(Surv(y, failed) ~ Time,
   data = queuing$data)
plot(fit.Time, xlab = "Seconds",
    ylab = "Probability of Still Being on Hold",
    col = c(2, 4, 5))
legend("topright", c("Morning", "Afternoon", "Evening"),
    col = c(5, 2, 4), lty = 1)
```

It seems that calls at Call Center B take longer to be answered than calls at Centers A and C. Similarly, it appears that wait times are longest in the morning and shortest in the evening hours. We can use a log-rank test to determine whether these differences are statistically significant.

```{r }
survdiff(Surv(y, failed) ~ Center, data = queuing$data)
survdiff(Surv(y, failed) ~ Time, data = queuing$data)
```

We find that differences between centers are highly significant, as are differences between times of day.

Finally, we fit Cox's proportional hazards model to the data.

```{r}
fit.queuing <- coxph(Surv(y, failed) ~ .,
    data = queuing$data)
fit.queuing
```

The $p$-values for `Center = B`, `Time = Even.` and `Time = Morn.` are very small. It is also clear that the hazard --- that is, the instantaneous risk that a call will be answered --- increases with the number of operators. Since we generated the data ourselves, we know that the true coefficients for `Operators`, `Center = B`, `Center = C`, `Time = Even.` and `Time = Morn.` are $0.04$, $-0.3$, $0$, $0.2$, and $-0.2$, respectively. The coefficient estimates resulting from the Cox model are fairly accurate.
:::

## Harrel's concordance index (the C-index).

This is a method for assessing a fitted survival model on a test set. For each observation, we calculate the estimated risk score, so with the coefficients we got from the model we calculate the estimated risk score, called eta hat of i $\hat\eta_i = \hat\beta_1x_{i1}+\dots+\hat\beta_px{ip}$ Then for each pairs of observations, we compute the proportion that we got right from the model, this is, if they had a lower risk, they lived longer than their counterpart in the pair.

$$
C = \frac{\sum_{i<j} \delta_{ij} \cdot \mathbb{I}(\hat{T}_i < \hat{T}_j)}{\sum_{i<j} \delta_{ij}}
$$ {#eq-Cindex}

When we are comparing a pair that has one censored observation, we cannot really say if they lived longer or not, so we only count up over those that we can compare.

If we get a c-index of 0.733, roughly speaking that means that given two random observations, the model can predict with 73.3% accuracy which one will survive longer.

::: {.callout-orange appearance="simple" icon="false"}
::: centered-text
**Other considerations of survival analysis**
:::

-   There is not a single type of censoring, there is right censoring, left censoring and interval censoring.

    -   *Right censoring*: This occurs when the event of interest has not happened by the end of the study period or when a subject leaves the study before experiencing the event. Example: If a patient is still alive at the end of the study or drops out, the exact survival time is unknown but is known to be greater than the last observed time.
    -   *Left censoring*: This happens when the event of interest has already occurred before the start of the study or before a subject's entry into the study. Example: If you're studying the time to a certain disease and some individuals already had the disease before the study began, their exact time to event is unknown but is known to be less than the first observed time.
    -   *Interval censoring*: This occurs when the event of interest is known to have occurred within a specific time interval, but the exact time is unknown. Example: If follow-up visits are scheduled annually and a patient was healthy at their last visit but found to have developed a disease at the next visit, the exact time of disease onset is unknown but is known to have occurred within that year.

-   There are considerations to be made about the time scale, for example, use if calendar time or patient's age

-   Time-dependent covariates: we measure certain predictors on the same patient over time
:::

