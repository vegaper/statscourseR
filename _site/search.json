[
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Probability",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(tinytex)"
  },
  {
    "objectID": "probability.html#expectation",
    "href": "probability.html#expectation",
    "title": "Probability",
    "section": "2.7 Expectation",
    "text": "2.7 Expectation\nThe expected value of a random variable (\\(X\\)) is described as a weighted average of the values that (\\(X\\)) can take, with weights given by the probabilities of those values.\nThe formula for the expected value is given as:\n\\[\nE(X) = \\sum_{i=1}^{n} x_i \\cdot P(X = x_i)\n\\]\nAn example is provided for the expected value of a fair six-sided die:\n\\[\nE(X) = \\sum_{i=1}^{6} i \\cdot P(X = i) = \\sum_{i=1}^{6} i \\cdot \\frac{1}{6} = \\frac{1}{6} (1 + 2 + 3 + 4 + 5 + 6) = 3.5\n\\] Note that the die cannot achieve this value, but if you were to roll the die many times and average the values, the result would likely be close to 3.5\nWhile the die cannot achieve this value in a single roll, rolling the die many times and averaging the values would likely result in a value close to 3.5."
  },
  {
    "objectID": "probability.html#indepence-of-the-events",
    "href": "probability.html#indepence-of-the-events",
    "title": "Probability",
    "section": "5.1 Indepence of the events",
    "text": "5.1 Indepence of the events\nIf the two events \\(A\\) and \\(B\\) are independent from each other, then the probability of \\(A\\) given \\(B\\) is just the probability of \\(A\\). In that case the probability of the two together is \\(P(A\\cap B) = P(A)P(B)\\)\nTaking this to the example we just see, we can see that the probability of being a female given that they are a computer science student is not equal to the probability of being a female, which means that these two events are not independent."
  },
  {
    "objectID": "probability.html#partition-of-the-space",
    "href": "probability.html#partition-of-the-space",
    "title": "Probability",
    "section": "7.1 Partition of the Space",
    "text": "7.1 Partition of the Space\nIf the events \\(A_1, A_2, \\ldots, A_m\\) form a partition of the space (exactly one of the \\(A_i\\)’s must occur, i.e., the \\(A_i\\)’s are mutually exclusive and: \\[\n\\sum_{i=1}^{m} P(A_i) = 1\n\\]\nThen, Bayes’ Theorem can be written as: \\[\nP(A_1 \\mid B) = \\frac{P(B \\mid A_1) \\cdot P(A_1)}{\\sum_{i=1}^{m} P(B \\mid A_i) \\cdot P(A_i)}\n\\]\nFor continuous distributions, the sum in Bayes’ Theorem is replaced with an integral. We will explore this in the next lesson.\n\nPractice exercise: Titanic\nRefer to the following table regarding passengers of the famous Titanic, which tragically sank on its maiden voyage in 1912. The table organizes passenger/crew counts according to the class of their ticket and whether they survived.\n\n\nCode\ntitanic_data &lt;- data.frame(\n  Class = c(\"First\", \"Second\", \"Third\", \"Crew\"),\n  Survived = c(203, 118, 178, 212),\n  Not_Survived = c(122, 167, 528, 673)\n)\n\nprint(titanic_data)\n\n\n   Class Survived Not_Survived\n1  First      203          122\n2 Second      118          167\n3  Third      178          528\n4   Crew      212          673\n\n\nIf we randomly select a person’s name from the complete list of passengers and crew, what is the probability that this person traveled in 1st class?\n\n\nCode\npassengers &lt;- sum(titanic_data$Survived)+sum(titanic_data$Not_Survived)\nfirstClass &lt;- sum(titanic_data$Survived[titanic_data$Class=='First' ])+sum(titanic_data$Not_Survived[titanic_data$Class=='First' ])\n\nround(firstClass/passengers,2)\n\n\n[1] 0.15\n\n\nWhat is the probability that a (randomly selected) person survived?\n\n\nCode\nsurvived&lt;- sum(titanic_data$Survived)\nround(survived/passengers,2)\n\n\n[1] 0.32\n\n\nWhat is the probability that a (randomly selected) person survived, given that they were in 1st class? Round your answer to two decimal places.\n\n\nCode\nround(titanic_data$Survived[titanic_data$Class =='First']/firstClass,2)\n\n\n[1] 0.62\n\n\n\n\nPractice exercise: Marvels\nYou have three bags, labeled A, B, and C. - Bag A contains two red marbles and three blue marbles. - Bag B contains five red marbles and one blue marble. - Bag C contains three red marbles only.\n\n\nCode\n# Creating a dataframe for the marbles in the three bags\nmarbles_data &lt;- data.frame(\n  Bag = c(\"A\", \"B\", \"C\"),\n  Red_Marbles = c(2, 5, 3),\n  Blue_Marbles = c(3, 1, 0)\n)\n\n# Display the dataframe\nprint(marbles_data)\n\n\n  Bag Red_Marbles Blue_Marbles\n1   A           2            3\n2   B           5            1\n3   C           3            0\n\n\nIf you select from bag B, what is the probability that you will draw a red marble? Express the exact answer as a simplified fraction.\n\n\nCode\nround(5/6,2)\n\n\n[1] 0.83\n\n\nIf you randomly select one of the three bags with equal probability (so that P(A)=P(B)=P(C)=1/3) and then randomly draw a marble from that bag, what is the probability that the marble will be blue? Round your answer to two decimal places.\nThis is the marginal probability P(blue). You can obtain this using the law of total probability (which appears in the denominator in Bayes’ theorem) \\[\nP(\\text{blue}) = P(\\text{blue} \\cap A) + P(\\text{blue} \\cap B) + P(\\text{blue} \\cap C)\n\\] Using conditional probabilities, we can write this as:\n\\[\n= P(\\text{blue} \\mid A) \\cdot P(A) + P(\\text{blue} \\mid B) \\cdot P(B) + P(\\text{blue} \\mid C) \\cdot P(C)\n\\]\n\n\nCode\npbag = 1/3\nPblueA &lt;- 3/5\nPblueB &lt;- 1/6\npblueC &lt;- 0\n\npblue &lt;- PblueA * pbag + PblueB *pbag +pblueC *pbag\nround(pblue,2)\n\n\n[1] 0.26\n\n\nSuppose a bag is randomly selected (again, with equal probability), but you do not know which it is. You randomly draw a marble and observe that it is blue. What is the probability that the bag you selected this marble from is A? That is, find P(A∣blue) \\[\nP(A|blue) = \\frac{P(Blue|A)\\cdot P(A)}{P(blue)}\n\\]\n\n\nCode\npABlue &lt;- (PblueA *1/3)/pblue\nround(pABlue,2)\n\n\n[1] 0.78\n\n\nSuppose a bag is randomly selected (again, with equal probability), but you do not know which it is. You randomly draw a marble and observe that it is blue. What is the probability that the bag you selected from is C? That is, find P(C∣blue).\n0\nSuppose a bag is randomly selected (again, with equal probability), but you do not know which it is. You randomly draw a marble and observe that it is red. What is the probability that the bag you selected from is C? That is, find P(C∣red).\n\\[\nP(c|red) = \\frac{P(red|c)\\cdot P(c)}{P(red)}\n\\]\n\n\nCode\nPredC = 1\nPred = 1-pblue\n\nPcRed = (1*(1/3))/Pred\n\nround(PcRed,2)\n\n\n[1] 0.45"
  },
  {
    "objectID": "probability.html#expected-value",
    "href": "probability.html#expected-value",
    "title": "Probability",
    "section": "9.1 Expected Value",
    "text": "9.1 Expected Value\nThe Expected Value is the theoretical average of the outcomes. For our binary outcome with possible outcomes 0 and 1 it will be:\n\\[\nE[X] = \\sum_x x \\times P(X=x) \\rightarrow (1)p+(0)p = p\n\\qquad(4)\\]"
  },
  {
    "objectID": "probability.html#variance",
    "href": "probability.html#variance",
    "title": "Probability",
    "section": "9.2 Variance",
    "text": "9.2 Variance\nVariance is the square of the standard deviation. For Bernoulli:\\[\n\\sigma^2 = p \\times (1-p)\n\\qquad(5)\\]"
  },
  {
    "objectID": "probability.html#repeated-experiments-in-binomial-distribution",
    "href": "probability.html#repeated-experiments-in-binomial-distribution",
    "title": "Probability",
    "section": "9.3 Repeated experiments in binomial distribution",
    "text": "9.3 Repeated experiments in binomial distribution\nThe generalization of the Bernoulli when we have \\(N\\) repeated trials follows a binomial distribution with parameters \\(n\\) and \\(p\\). In this case, the probability function is\n\\[\nf(x\\mid p) = \\binom{n}{x} p^x (1-p)^{n-x}\n\\]\n\\[\n\\binom{n}{x}= \\frac{n!}{x!(n-x)!}\n\\]\n\\(binom{4}{3}\\) for example reads “four choose three” and this counts all possible Bernoulli sequences that result in three successes out of four trials\n\n\nCode\nn &lt;- 4\nx &lt;- 3\n\nsol &lt;- factorial(n)/(factorial(x) * factorial(n-x))\nsol\n\n\n[1] 4\n\n\nin R we can simplify this calculus using a handy formula:\n\n\nCode\nn &lt;- 4\nx &lt;- 3\n\nsol &lt;- choose(n, x)\nprint(sol)\n\n\n[1] 4\n\n\nThe expected value is:\\[\nE[X] = n\\times p\n\\]\nAnd the Variance:\\[\n\\sigma^2 = n\\times p(1-p)\n\\]\n\nExpected value\nCalculate the expected value of the following random variable: X takes on values {0,1,2,3} with corresponding probabilities {0.5,0.2,0.2,0.1}.\n\n\nCode\n# Values and corresponding probabilities\nvalues &lt;- c(0, 1, 2, 3)\nprobabilities &lt;- c(0.5, 0.2, 0.2, 0.1)\n\n# Calculate the expected value\nE &lt;- sum(values * probabilities)\nprint(round(E, 1))  \n\n\n[1] 0.9\n\n\n\n\nExercise 2\nImagine you have a coin that lands on heads (success) with a probability of 0.2. You flip the coin 3 times, and you want to find the probability of getting 0 heads (0 successes) in those 3 coin flips.\nGiven a random variable \\(X \\sim \\binom {3}{ 0.2}\\), we want to calculate\\(P(X = 0)\\).\nThe probability mass function for a binomial random variable \\(X\\) with parameters \\(n\\) and \\(p\\) is given by: \\(P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}\\)\n\\(P(X = 0) = \\binom{3}{0} (0.2)^0 (1 - 0.2)^{3 - 0}\\)\n\\(P(X = 0) = \\binom{3}{0} \\cdot 1 \\cdot (0.8)^3\\)\n\\(P(X = 0) = 1 \\cdot (0.8)^3\\)\n\\(P(X = 0) = (0.8)^3\\)\n\\(P(X = 0) = 0.512\\)\n\n\nCode\n# Parameters\nn &lt;- 3\np &lt;- 0.2\nx &lt;- 0\n\n# Calculate the binomial probability\nprob &lt;- dbinom(x, n, p)\nround(prob, 2)\n\n\n[1] 0.51\n\n\nNow calculate the probability that the number of successes is less or equal than 2.\n\\[\nP(X \\le 2)\n\\]This is 1 minus the probability of getting 3 successes.\n\n\nCode\n# Parameters\nn &lt;- 3\np &lt;- 0.2\nx &lt;- 3\n\n# Calculate the binomial probability\nprob &lt;- 1-dbinom(x, n, p)\nround(prob, 2)\n\n\n[1] 0.99\n\n\nor you can also calculate it as P(X=0)+P(X=1)+P(X=2)"
  },
  {
    "objectID": "probability.html#uniform-distribution.",
    "href": "probability.html#uniform-distribution.",
    "title": "Probability",
    "section": "10.2 Uniform distribution.",
    "text": "10.2 Uniform distribution.\nThe uniform distribution is used for random variables whose possible values are equally likely over an interval.\nWe can define a random continue variable \\(X\\) with its Probability Density Function (PDF). If you integrate the PDF over an interval, it will give you the probability that the random variable will be in that interval.\nIf the interval is \\((a,b)\\), then the uniform probability density function (PDF) f(x) is flat for all values in the interval and 0 everywhere else. \\[\nf(X\\mid a,b)= \\frac{1}{b-a} I_{\\{a\\leq x \\leq b\\}}(x)\n\\]\nWe have a uniform distribution that can take any value between 0 and 1 \\(X \\sim U[0,1]\\) or using the indicator function&gt;\\[\nf(x) = \\begin{cases}\n1 & \\text{if } x \\in [0,1] \\\\\n0 & \\text{alternative }\n\\end{cases}\n\\]\n\n\nCode\n# Define the uniform distribution\nx &lt;- seq(0, 1, by = 0.01)\ny &lt;- dunif(x, min = 0, max = 1)\n\n# Create the PDF plot\npdf_plot &lt;- data.frame(x, y)\n\n# Plot using ggplot2\nggplot(pdf_plot, aes(x = x, y = y)) +\n  geom_line(color = \"blue\") +\n  ggtitle(\"PDF of Uniform Distribution (0, 1)\") +\n  xlab(\"x\") +\n  ylab(\"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhat’s the probability that \\(x\\) will be between 0 and 0.5?\nThe probability that \\(x\\) will be between 0 and 0.5 for a uniform distribution \\(U(0,1)\\) is given by:\n\\[\nP(0 \\leq x \\leq 0.5) = \\int_{0}^{0.5} f(x) \\, dx =\\int_{0}^{0.5} 1 \\, dx = [x]_{0}^{0.5} = 0.5 - 0 = 0.5\n\\]\nTherefore, the probability is 0.5. Let’s calculate it using R:\n\n\nCode\n# Define the probability density function for the uniform distribution\npdf_uniform &lt;- function(x) {\n  ifelse(x &gt;= 0 & x &lt;= 1, 1, 0)\n}\n\n# Integrate the PDF from 0 to 0.5\nprob &lt;- integrate(pdf_uniform, lower = 0, upper = 0.5)$value\nprint(prob)\n\n\n[1] 0.5\n\n\n\n\nCode\n# Define the probability density function for the uniform distribution\npdf_uniform &lt;- function(x) {\n  ifelse(x &gt;= 2 & x &lt;= 6, 1, 0)\n}\n\n# Integrate the PDF from 0 to 0.5\nprob &lt;- integrate(pdf_uniform, lower = 2, upper = 3)$value\nprint(prob)\n\n\n[1] 1\n\n\nor in r we can simply:\n\n\nCode\n# Calculate the probability that x is between 0 and 0.5\nprob &lt;- punif(0.5, min = 0, max = 1) - punif(0, min = 0, max = 1)\nprint(prob)\n\n\n[1] 0.5\n\n\nWhat is the probability that \\(x\\) is exactly 0.5, and mathematically, by integrating from 0.5 to 0.5 we will get 0\n\n10.2.1 Expected value\nThe formula for the expected value (mean) of a continuous random variable is given by:\n\\[\nE(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\qquad(6)\\]\nIn this formula: - \\(E(X)\\) represents the expected value of the continuous random variable \\(X\\). - \\(f(x)\\) is the probability density function (PDF) of \\(X\\). - The integral is taken over the entire range of \\(X\\), from \\(-\\infty\\) to \\(\\infty\\).\nFor a continuous random variable \\(X\\) uniformly distributed over the interval \\([a, b]\\), the probability density function (PDF) is given by:\n\\[\nf(x) =\n\\begin{cases}\n\\frac{1}{b-a} & \\text{for } a \\le x \\le b \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThe general formula for the expected value of a continuous random variable is:\n\\[\nE(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n\\]\nGiven \\(f(x)\\) for the uniform distribution, we apply it to the general expected value formula:\n\\[\nE(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx = \\int_{a}^{b} x \\cdot \\frac{1}{b-a} \\, dx\n\\]\nSince \\(f(x) = \\frac{1}{b-a}\\) only over the interval \\([a, b]\\), the integration limits change accordingly. Evaluating this integral:\n\\[\nE(X) = \\frac{1}{b-a} \\int_{a}^{b} x \\, dx\n\\]\nThe integral of \\(x\\) from \\(a\\) to \\(b\\) is:\n\\[\n\\int_{a}^{b} x \\, dx = \\left. \\frac{x^2}{2} \\right|_{a}^{b} = \\frac{b^2}{2} - \\frac{a^2}{2} = \\frac{b^2 - a^2}{2}\n\\]\nSubstituting back, we get:\n\\[\nE(X) = \\frac{1}{b-a} \\cdot \\frac{b^2 - a^2}{2}\n\\]\nThis simplifies to\n\n\\[\nE(X) = \\frac{b + a}{2}\n\\qquad(7)\\]\n\nHence, the formula \\(E(X) = \\frac{a+b}{2}\\) is derived from the integral expression for the expected value of a uniform distribution. Both expressions are equivalent; the integral formulation is the general definition, while the latter is the specific result for the uniform distribution.\n\n\n10.2.2 General Uniform Distribution.\nThe above formulas work for a uniform distribution on the interval [0,1] When we generalize the uniform distribution to any interval\nUniform : \\(X \\sim [\\theta_1, \\theta_2]\\) the PDF for this distribution changes accordingly to ensure that the total probability across this interval is 1: \\[\nf(x\\mid \\theta_1,\\theta_2) = \\frac{1}{\\theta_2-\\theta_1}I_{\\{ \\theta_1 \\leq x \\leq \\theta_2\\}}\n\\qquad(8)\\]\n\\[\nf(x) = \\begin{cases}\n\\frac{1}{\\theta_2-\\theta_1} & \\text{if } x \\in [\\theta_1,\\theta_2] \\\\\n0 & \\text{otherwise }\n\\end{cases}\n\\] this formula generalize the concept of the uniform distribution to any interval.\nExample: Consider a uniform distribution on the interval [2,5]\n\\[\nf(x\\mid 2,5 ) = \\begin{cases}\n\\frac{1}{5-2} = \\frac{1}{3}& \\text{if } x \\in [2,5] \\\\\n0 & \\text{otherwise }\n\\end{cases}\n\\]\n\n\nCode\n# Function to plot uniform distribution\nplot_uniform_distribution &lt;- function(theta1, theta2) {\n  # Create a sequence of x values\n  x &lt;- seq(theta1 - 1, theta2 + 1, by = 0.01)\n  \n  # Calculate the PDF values\n  y &lt;- ifelse(x &gt;= theta1 & x &lt;= theta2, 1 / (theta2 - theta1), 0)\n  \n  # Create a data frame for plotting\n  data &lt;- data.frame(x, y)\n  \n  # Plot the uniform distribution\n  ggplot(data, aes(x = x, y = y)) +\n    geom_line() +\n    geom_area(fill = \"lightblue\", alpha = 0.5) +\n    labs(title = paste(\"Uniform Distribution U[\", theta1, \",\", theta2, \"]\", sep = \"\"),\n         x = \"x\", y = \"Density\") +\n    theme_minimal()\n}\n\n# Plot the uniform distribution U[2, 5]\nplot_uniform_distribution(2, 5)\n\n\n\n\n\nthen to calculate in R the probability of \\(X\\) being between two values:\n\n\nCode\n# Define the limits of the uniform distribution\ntheta1 &lt;- 2\ntheta2 &lt;- 5\n\n# Define the PDF for the uniform distribution U[2,6]\npdf_uniform &lt;- function(x) {\n  ifelse(x &gt;= theta1 & x &lt;= theta2, 1 / (theta2 - theta1), 0)\n}\n\n# Integrate the PDF from 2 to 3\nprob &lt;- integrate(pdf_uniform, lower = 2, upper = 3)$value\nprint(prob)\n\n\n[1] 0.3333333\n\n\n\n\n10.2.3 Variance of uniform Distribution\nVariance \\(\\text{Var}(X)\\) The variance is calculated as:\n\\[\n\\text{Var}(X) = E(X^2) - [E(X)]^2 = \\frac{b^2 + ab + a^2}{3} - \\left(\\frac{a + b}{2}\\right)^2\n\\]\nSimplifying this further, we arrive at:\n\n\\[\n\\sigma^2= \\frac{(b-a)^2}{12}\n\\qquad(9)\\]"
  },
  {
    "objectID": "probability.html#exponential-distribution",
    "href": "probability.html#exponential-distribution",
    "title": "Probability",
    "section": "10.3 Exponential Distribution",
    "text": "10.3 Exponential Distribution\nThe exponential distribution is often used to model the waiting time between random events. Indeed, if the waiting times between successive events are independent from an \\(Exp(\\lambda)\\) distribution, then for any fixed time window of length \\(t\\), the number of events occurring in that window will follow a Poisson distribution with mean \\(t\\lambda\\). Similar to the Poisson distribution, the parameter \\(\\lambda\\) is interpreted as the rate at which the events occur.\nIf \\(X\\) follows an exponential distribution with a rate parameter: \\(X \\sim Exp(\\lambda)\\)\nThe probability density function (PDF) for the exponential distribution can be written as\n\\[\nf(x\\mid \\lambda) = \\lambda e^{-\\lambda x} I_{\\{x \\geq 0\\}}(x)\n\\qquad(10)\\]\n\\[\nf(x \\mid \\lambda )= \\lambda e^{-\\lambda x}\n\\]or:\n\\[ f(x \\mid \\lambda) =\n\\begin{cases}\n\\lambda e^{-\\lambda x} & \\text{for } x \\ge 0 \\\\\n0 & \\text{for } x &lt; 0\n\\end{cases}\n\\]\n\nWe can integrate the density and show that it is indeed a proper density and integrates to one:\n\\[int_{0}^{\\infty} \\lambda e^{-\\lambda x} I\\{x \\ge 0\\}(x) dx = \\int_{0}^{\\infty} \\lambda e^{-\\lambda x} dx = -e^{-\\lambda x} \\bigg|_0^{\\infty} = 0 - (-1) = 1\\]\nThe derivative of the density function would be:\n\\[frac{d}{dx} \\left( \\lambda e^{-\\lambda x} I\\{x \\geq 0\\}(x) \\right) = -\\lambda^2 e^{-\\lambda x} I\\{x \\geq 0\\}(x)\\]\n\nAnd the Expected value is:\\[\nE[X]= \\frac{1}{\\lambda}\n\\]\nThe variance:\\[\n\\sigma^2 = \\frac{1}{\\lambda^2}\n\\]\nand the standard deviation: Standard Deviation**: \\(\\sigma = \\frac{1}{\\lambda}\\)"
  },
  {
    "objectID": "probability.html#normal-distribution",
    "href": "probability.html#normal-distribution",
    "title": "Probability",
    "section": "10.4 Normal distribution",
    "text": "10.4 Normal distribution\nIf variable \\(X\\) follows a normal distribution defined by its mean \\(\\mu\\) and its variance \\(\\sigma^2\\): \\(X \\sim N(\\mu, \\sigma^2)\\)\nthen the probability density function (PDF) comes defined as:\n\\[\nf(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\qquad(11)\\]\nThe expected value will be \\(\\mu\\)\n\n10.4.1 Examples\n\nLifespan of a Light Bulb: If the lifespan of a light bulb follows an exponential distribution with a rate parameter \\(\\lambda = 0.1\\) (i.e., the average lifespan is 10 hours), then the density function is \\(f(x) = 0.1 e^{-0.1x}\\).\nCustomer Arrival Times: If customers arrive at a bank at an average rate of 3 customers per hour (i.e., \\(\\lambda = 3\\)), then the time between arrivals follows an exponential distribution with a density function \\(f(x) = 3 e^{-3x}\\).\n\nHere’s an example of how you can generate and visualize an exponential distribution in R:\n\n\nCode\n# Set the rate parameter\nlambda &lt;- 2\n\n# Generate a sample of 1000 random values from the exponential distribution\nset.seed(123) \nsample &lt;- rexp(1000, rate = lambda)\n\n# Plot the histogram of the sample\nhist(sample, breaks = 30, probability = TRUE,\n     main = \"Histogram of Exponential Distribution\",\n     xlab = \"Value\", ylab = \"Density\")\n\n# Add the theoretical density curve\ncurve(lambda * exp(-lambda * x), col = \"red\", add = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# Mean and variance of the sample\nsample_mean &lt;- mean(sample)\nsample_variance &lt;- var(sample)\n\ncat(\"Sample Mean:\", sample_mean, \"\\n\")\n\n\nSample Mean: 0.5149896 \n\n\nCode\ncat(\"Sample Variance:\", sample_variance, \"\\n\")\n\n\nSample Variance: 0.252207"
  },
  {
    "objectID": "probability.html#probability-of-a-specific-value",
    "href": "probability.html#probability-of-a-specific-value",
    "title": "Probability",
    "section": "2.1 Probability of a Specific Value",
    "text": "2.1 Probability of a Specific Value\nFor a continuous random variable, the probability of a specific value is always 0. This is because the probability is spread over an interval, and the exact point has no width.\n\\[\nP(X = x) = 0\n\\]"
  },
  {
    "objectID": "probability.html#probability-of-an-interval",
    "href": "probability.html#probability-of-an-interval",
    "title": "Probability",
    "section": "2.2 Probability of an Interval",
    "text": "2.2 Probability of an Interval\nThe probability that a continuous random variable falls within an interval is given by the integral of the probability density function over that interval.\n\\[\nP(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx\n\\]"
  },
  {
    "objectID": "probability.html#probability-over-the-entire-range",
    "href": "probability.html#probability-over-the-entire-range",
    "title": "Probability",
    "section": "2.3 Probability over the Entire Range",
    "text": "2.3 Probability over the Entire Range\nThe probability that a continuous random variable falls within its entire range is always 1. This means that the total area under the probability density function is 1.\n\\[\nP(-\\infty &lt; X &lt; \\infty) = 1\n\\]"
  },
  {
    "objectID": "probability.html#complement-rule",
    "href": "probability.html#complement-rule",
    "title": "Probability",
    "section": "2.4 Complement Rule",
    "text": "2.4 Complement Rule\nThe probability of an event not occurring is 1 minus the probability of the event occurring.\n\\[\nP(A^c) = 1 - P(A)\n\\]"
  },
  {
    "objectID": "probability.html#addition-rule",
    "href": "probability.html#addition-rule",
    "title": "Probability",
    "section": "2.5 Addition Rule",
    "text": "2.5 Addition Rule\nFor any two events A and B, the probability of A or B occurring is given by:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]"
  },
  {
    "objectID": "probability.html#multiplication-rule",
    "href": "probability.html#multiplication-rule",
    "title": "Probability",
    "section": "2.6 Multiplication Rule",
    "text": "2.6 Multiplication Rule\nFor any two independent events A and B, the probability of both A and B occurring is given by:\n\\[\nP(A \\cap B) = P(A) \\times P(B)\n\\]\nThese fundamental rules provide a basis for understanding and calculating probabilities in various contexts."
  },
  {
    "objectID": "probability.html#mle-for-uniform-distribution",
    "href": "probability.html#mle-for-uniform-distribution",
    "title": "Probability",
    "section": "11.2 MLE for Uniform Distribution",
    "text": "11.2 MLE for Uniform Distribution\nLet’s consider a uniform distribution defined over the interval \\([a, b]\\). The probability density function (pdf) of the uniform distribution is given by:\n\\[\nf(x \\mid a, b) = \\frac{1}{b - a} \\quad \\text{for} \\quad a \\leq x \\leq b\n\\]\nSuppose we have a sample of observed data points \\(x_1, x_2, \\ldots, x_n\\) from the uniform distribution. The likelihood function $ L(a, b) $ is given by:\n\\[\nL(a, b) = \\prod_{i=1}^{n} f(x_i \\mid a, b) = \\prod_{i=1}^{n} \\frac{1}{b - a} = \\left( \\frac{1}{b - a} \\right)^n\n\\]Every value between a and b are equally likely To find the MLE, we need to maximize the likelihood function. In the case of the uniform distribution, we need to find the values of \\(a\\) and \\(b\\) that maximize the likelihood function. The MLE for \\(a\\) and \\(b\\) are:\n\\[\n\\hat{a} = \\min(x_1, x_2, \\ldots, x_n)\n\\]\\[\n\\hat{b} = \\max(x_1, x_2, \\ldots, x_n)\n\\]\n\n\nCode\n# Sample data\nx &lt;- c(2, 3, 5, 6, 7)\n\n# MLE for the uniform distribution\na_hat &lt;- min(x)\nb_hat &lt;- max(x)\n\n# Print the estimates\ncat(\"MLE for a:\", a_hat, \"\\n\")\n\n\nMLE for a: 2 \n\n\nCode\ncat(\"MLE for b:\", b_hat, \"\\n\")\n\n\nMLE for b: 7"
  },
  {
    "objectID": "probability.html#mle-for-exponential-distribution",
    "href": "probability.html#mle-for-exponential-distribution",
    "title": "Probability",
    "section": "11.3 MLE for Exponential Distribution",
    "text": "11.3 MLE for Exponential Distribution\nThe exponential distribution is defined by its rate parameter $ $. The probability density function (pdf) of the exponential distribution is given by:\n\\[\nf(x \\mid \\lambda) = \\lambda e^{-\\lambda x} \\quad \\text{for} \\quad x \\geq 0\n\\]\nSuppose we have a sample of observed data points $ x_1, x_2, , x_n $ from the exponential distribution. The likelihood function $ L() $ is given by:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} f(x_i \\mid \\lambda) = \\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i} = \\lambda^n e^{-\\lambda \\sum_{i=1}^{n} x_i}\n\\]\nTo find the MLE, we take the natural logarithm of the likelihood function to obtain the log-likelihood function:\n\\[\n\\log L(\\lambda) = n \\log \\lambda - \\lambda \\sum_{i=1}^{n} x_i\n\\]\nNext, we take the derivative of the log-likelihood function with respect to $ $ and set it to zero to find the maximum:\n\\[\n\\frac{d}{d\\lambda} (\\log L(\\lambda)) = \\frac{n}{\\lambda} - \\sum_{i=1}^{n} x_i = 0\n\\]\nSolving for $ $, we get:\n\\[\n\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} x_i}\n\\]\n\n\nCode\n# Sample data\nx &lt;- c(1.2, 0.5, 2.8, 3.3, 1.1)\n\n# MLE for the exponential distribution\nn &lt;- length(x)\nlambda_hat &lt;- n / sum(x)\n\n# Print the estimate\ncat(\"MLE for lambda:\", lambda_hat, \"\\n\")\n\n\nMLE for lambda: 0.5617978 \n\n\n\nPractice: political view\nYou are trying to ascertain your American colleague’s political preferences. To do so, you design a questionnaire with five yes/no questions relating to current issues. The questions are all worded so that a “yes” response indicates a conservative viewpoint.\nLet θ be the unknown political viewpoint of your colleague, which we will assume can only take values θ=conservative or θ=liberal. You have no reason to believe that your colleague leans one way or the other, so you assign the prior P(θ=conservative)=0.5.\nAssume the five questions are independent and let Y count the number of “yes” responses. If your colleague is conservative, then the probability of a “yes” response on any given question is 0.8. If your colleague is liberal, the probability of a “no” response on any given question is 0.7.\nWhat is an appropriate likelihood for this scenario?\nThe likelihood formula:\n\\[\nL(\\theta) = P(X=x_1, X= x_2, \\cdots, X=x_n \\mid \\theta)\n\\] the probability function for a binomial distribution:\n\\[\nf(x\\mid p) = \\binom{n}{x}p^x(1-p)^{n-x}\n\\]\n\\(P(x = yes \\mid \\theta=conservative) = \\binom{n}{x} 0.8^x \\times (1-0.8)^{n-x}\\)\n\\(P(x = no \\mid \\theta=liberal) = \\binom{n}{x} 0.7^x \\times (1-0.7)^{n-x}\\)\n\\(P(x = yes \\mid \\theta=liberal) = \\binom{n}{x} 0.3^x \\times (1-0.3)^{n-x}\\) \\(n=5\\)\n\\[f(x\\mid \\theta) = \\binom{5}{x} 0.8^x \\times 0.2^{5-x} +\\binom{5}{x} 0.3^x \\times (0.7)^{5-x}\\]\nSuppose you ask your colleague the five questions and he answers “no” to all of them. What is the MLE for \\(\\theta\\)\n\n\nCode\nn= 5\nx= 0\npyes_con = 0.8\npno_con = 0.2\n\npyes_lib = 0.3\npno_lib =0.7\n\n# Define the binomial coefficient function\nchoose &lt;- function(n, x) {\n  factorial(n) / (factorial(x) * factorial(n - x))\n}\n\n# Probability functions\nprob_conservative &lt;- function(x, n) {\n  choose(n, x) * pyes_con^x *pno_con^(n-x)\n}\n\nprob_liberal &lt;- function(x, n) {\n  choose(n, x) * pyes_lib^x *pno_lib^(n-x)\n}\n\n# Function to calculate posterior probability for 'fair' coin\nposterior_prob_conv &lt;- function(x, n) {\n  p_cons = prob_conservative(x, n)\n  p_lib = prob_liberal(x, n)\n  p_cons / (p_cons + p_lib)\n}\nprint(paste0(\"conservative =\", round(posterior_prob_conv(x,n),3)))\n\n\n[1] \"conservative =0.002\"\n\n\nCode\nposterior_prob_lib &lt;- function(x, n) {\n  p_cons = prob_conservative(x, n)\n  p_lib = prob_liberal(x, n)\n  p_lib / (p_cons + p_lib)\n}\nprint(paste0(\"liberal =\", round(posterior_prob_lib(x,n),3)))\n\n\n[1] \"liberal =0.998\"\n\n\nhe is most likely liberal according to these questions."
  },
  {
    "objectID": "Inferencial.html#the-sampling-distribution",
    "href": "Inferencial.html#the-sampling-distribution",
    "title": "Inference",
    "section": "2.1 The sampling distribution",
    "text": "2.1 The sampling distribution\nIf we toss a fair coin 100 times, we can get any number of tails from 0 to 100. How likely is each outcome?\nThe number of tails has the binomial distribution with n=100 and p=0.5 where success = tails. So if the statistic of interest is \\(S_n=number\\ of \\ tails\\) , then \\(S_n\\) is a random variable whose probability histogram is given by the binomial distribution. This is called the sampling distribution of the statistic \\(S_n\\). The sampling distribution provides more detailed information about the chance properties of \\(S_n\\) than the summary numbers given by the expected value and the standard error alone.\nThere are three histograms to take into consideration:\n\n2.1.1 Probability histogram\nThe probability histogram for producing the data. Since both tails and heads have the same probability, our histogram would be two columns with the same height.\n\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Empirical histogram\nIf we toss the coins, we will have am empirical histogram\n\n\n\n\n\n\n\n\n\n\n\n2.1.3 Probability histogram of the sample\nAnd finally we can do the probability histogram of the statistic \\(s_{100}\\), which shows the sampling distribution:"
  },
  {
    "objectID": "Inferencial.html#manual-calculation",
    "href": "Inferencial.html#manual-calculation",
    "title": "Inference",
    "section": "5.1 Manual calculation:",
    "text": "5.1 Manual calculation:\nA confidence interval gives a range of plausible values for a population parameter. Usually the confidence interval is centered at an estimate for \\(\\mu\\) which is an average. Since the central limit theorem applies for averages, the confidence interval has a simple form:\n\\[\nCI = estimate \\mp ME\n\\]\nwhere ME is the margin of error and can be decomposed like this:\n\\[\nCI=estimate \\mp z\\ * SE = \\mu \\mp z*SE\n\\tag{7}\\]\nwhere SE is the standard error of the sample. If that is an average or a percentage then it is \\(SE= \\frac{\\sigma}{\\sqrt{n}}\\) as we saw in (Equation 1).\nAnd \\(z\\) is the \\(z\\)-score corresponding to the desired confidence level. For example, if we would like to have a 95% confidence level, our \\(z=1.96\\)\nThis comes from calculating the value of \\(z\\) where 95 of our data is in the middle:\n\n\n\n\n\n\n\n\n\nTo get to that \\(z\\) value we use tables or we can use the quantile function qnorm:\n\n\nCode\n# Calculate the z-score for a 95% confidence interval\nz_score &lt;- qnorm(0.975)\nz_score\n\n\n[1] 1.959964\n\n\nThe 0.975value is used because for a 95% confidence interval, you need to capture the central 95% of the distribution, leaving 2.5% in each tail. Therefore, you look up the 97.5th percentile (0.975) to get the \\(z\\)-score.\nFor 90% confidence interval we are looking for the \\(z\\) value in the normalized distribution where 90% of the data falls in our center range and 10% outside, so 5% in each tail.\n\n\nCode\n# Calculate the z-score for a 95% confidence interval\ndesiredConfidence &lt;- 90\ntails = (100-desiredConfidence )/2\npercentileOfinterest &lt;- desiredConfidence+tails\nz_score &lt;- qnorm(percentileOfinterest/100)\nz_score\n\n\n[1] 1.644854\n\n\nFor a 99 confidence level:\n\n\nCode\n# Calculate the z-score for a 95% confidence interval\ndesiredConfidence &lt;- 99\ntails = (100-desiredConfidence )/2\npercentileOfinterest &lt;- desiredConfidence+tails\nz_score &lt;- qnorm(percentileOfinterest/100)\nz_score\n\n\n[1] 2.575829\n\n\n\nConfidence Intervals for Coin Flips\nExample 1: Flipping a Coin 100 Times\nIn this example of flipping a coin 100 times, we observed 44 heads, resulting in the following 95% confidence interval for \\(p\\):\n\\[\nCI= \\hat{p} \\pm z \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\nwhere: - \\(\\hat{p}\\) is the observed proportion of heads - \\(z\\) is the critical value from the standard normal distribution for a 95% confidence level (approximately 1.96) - \\(n\\) is the number of trials (100 in this case) $ p = $\n\n\nCode\nn1 &lt;- 100\nx1 &lt;- 44\np_hat1 &lt;- x1 / n1\nz &lt;- 1.96\n\n# Confidence interval calculation\n(ci_lower1 &lt;- p_hat1 - z * sqrt(p_hat1 * (1 - p_hat1) / n1))\n\n\n[1] 0.3427082\n\n\nCode\n(ci_upper1 &lt;- p_hat1 + z * sqrt(p_hat1 * (1 - p_hat1) / n1))\n\n\n[1] 0.5372918\n\n\np: (0.343, 0.537)\nFrom this, we concluded that it is plausible that the coin may be fair because \\(p = 0.5\\) is in the interval.\nExample 2: Flipping a Coin 100,000 Times\nSuppose instead that we flipped the coin 100,000 times, observing 44,000 heads (the same percentage of heads as before). Then using the method just presented, the 95% confidence interval for \\(p\\) is:\n\n\nCode\n# Example 2: Flipping a Coin 100,000 Times\nn2 &lt;- 100000\nx2 &lt;- 44000\np_hat2 &lt;- x2 / n2\n\n# Confidence interval calculation\n(ci_lower2 &lt;- p_hat2 - z * sqrt(p_hat2 * (1 - p_hat2) / n2))\n\n\n[1] 0.4369234\n\n\nCode\n(ci_upper2 &lt;- p_hat2 + z * sqrt(p_hat2 * (1 - p_hat2) / n2))\n\n\n[1] 0.4430766\n\n\np: (0.437, 0.443)\nIs it reasonable to conclude that this is a fair coin with 95% confidence?  Based on this narrower confidence interval, it appears less likely that the coin is fair since \\(p = 0.5\\) is not within the interval.\n\n\n\n\n\n\nEstimating the SE with bootstrap principle\nWe still have a problem for calculating the confidence interval following this, and it is that we need to know the standard deviation of the population \\(\\sigma\\) and we usually don’t know it, but the bootstrap principle states that we can calculate sigma by its sample version \\(s\\) and still get an approximately correct confidence interval.\n\nExample: We poll 1000 likely voters and find that 58% approve of the way the president handles his job.\n\\(SE= \\frac{\\sigma}{\\sqrt{n}} * 100\\) where \\(\\sigma = \\sqrt{p(1-p)}\\) where \\(p\\) is the proportion of all voters who approve, but we don’t know p, but the bootstrap principle tells us that we can replace \\(\\sigma\\) by \\(s\\) so we can plug in the values from our survey here: \\(=\\sqrt{0.58(1-0.58)} = 0.49\\)\nSo a 95% confidence interval for \\(p\\) is: \\[\n58\\% \\mp 1.96 \\frac{0.49}{\\sqrt{1000}}*100\n\\]\nwhich is approximately [54.9%,61.1%]\n\nThe width of the confidence interval is determined by \\(z\\) and the standard error SE. To reduce that margin of error we have two options, we can increase the sample size or decrease the confidence level. The sample size is square rooted so this means that to cut the width of the confidence level in half we need four times the sample size, and to reduce it 10 times we would need 100 times the sample size."
  },
  {
    "objectID": "Inferencial.html#getting-the-confidence-intervals-from-a-test-result",
    "href": "Inferencial.html#getting-the-confidence-intervals-from-a-test-result",
    "title": "Inference",
    "section": "5.2 Getting the confidence intervals from a test result",
    "text": "5.2 Getting the confidence intervals from a test result\n\n\nCode\n#simulate our survey results\nsuccess&lt;- rep(1,580)\nfailure&lt;- rep(0,420)\nmydata &lt;- c(success,failure)\n#do a te.test.\ntestResult &lt;- t.test(mydata)\nconfInt95_low &lt;- testResult$conf.int[1]\nconfInt95_upp &lt;- testResult$conf.int[2]\nconfInt95_upp\n\n\n[1] 0.6106429\n\n\nCode\ncat('confidence interval = [',confInt95_low,confInt95_upp,']')\n\n\nconfidence interval = [ 0.5493571 0.6106429 ]\n\n\nCode\nmargin_of_error &lt;- (confInt95_upp - confInt95_low) / 2\ncat('margin of error: ',margin_of_error)\n\n\nmargin of error:  0.03064294\n\n\nFor example in our optical dataset, if we do a t.test over that variable we can get the confidence intervals for a level of confidence 95%:\n\n\nCode\nfile1 &lt;- here::here(\"data\",\"optical_sample.xlsx\")\noptical_sample &lt;- read_excel(file1)\n\ntestResult &lt;- t.test(optical_sample$eye_difference)\n\nconfInt95_low &lt;- testResult$conf.int[1]\nconfInt95_upp &lt;- testResult$conf.int[2]\n\nmargin_of_error &lt;- (confInt95_upp - confInt95_low) / 2 \n\nggplot(optical_sample, aes(x = eye_difference)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(optical_sample$eye_difference),\n             linetype = \"dashed\")+\n  geom_vline(xintercept = confInt95_low,\n           linetype = \"dotted\", color = \"green\")+\n  geom_vline(xintercept = confInt95_upp,\n           linetype = \"dotted\", color = \"green\")\n\n\n\n\n\n\n\n\n\nthe margin of error will be:\n\n\nCode\nmargin_of_error &lt;- (confInt95_upp - confInt95_low) / 2 \nmargin_of_error\n\n\n[1] 0.3655392\n\n\nNow we can calculate the confidence intervals for other 90% and 99% level of confidence and see their differences in a graph:\n\n\nCode\n#level of confidence 90%. \ntestResult &lt;- t.test(optical_sample$eye_difference,\n       conf.level = .90)\nconfInt90_low &lt;- testResult$conf.int[1]\nconfInt90_upp &lt;- testResult$conf.int[2]\n\n#level of confidence 99%. \ntestResult &lt;- t.test(optical_sample$eye_difference,\n                   conf.level = .99)\nconfInt99_low &lt;- testResult$conf.int[1]\nconfInt99_upp &lt;- testResult$conf.int[2]\n\nggplot(optical_sample, aes(x = eye_difference)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(optical_sample$eye_difference),\n             linetype = \"dashed\")+\n  geom_vline(xintercept = confInt90_low,\n             linetype = \"dotted\", color = \"orange\")+\n  geom_vline(xintercept = confInt90_upp,\n             linetype = \"dotted\", color = \"orange\")+\n  geom_vline(xintercept = confInt95_low,\n           linetype = \"dotted\", color = \"green\")+\n  geom_vline(xintercept = confInt95_upp,\n           linetype = \"dotted\", color = \"green\")+\n  geom_vline(xintercept = confInt99_low,\n             linetype = \"dotted\", color = \"yellow\")+\n  geom_vline(xintercept = confInt99_upp,\n             linetype = \"dotted\", color = \"yellow\")\n\n\n\n\n\n\n\n\n\nThree factors feed into the size of the margin of error:\n\nThe confidence level of the interval. That is, the proportion of the time our interval would correctly capture the parameter of interest. Higher confidence requires a larger margin of error.\nThe spread of the observations in the data set. More spread in the data implies more sampling variability and therefore a larger margin of error.\nThe size of the sample.\n\n\nExercises:\nUsing the pm_sample data set,\n\nFind a 95% confidence interval for the mean salary of the project managers in this population. Interpret the results in plain language. Would a 99% confidence interval be wider of more narrow?\nFind a 95% confidence interval for the man non-salary compensation of project managers in this population. Why is the margin of error different than in the first problem?\n\n\n\nCode\nfile &lt;- here::here(\"data\", \"pm_survey.xlsx\") \npm_survey &lt;- read_excel(file) \n\nkable(head(pm_survey))%&gt;%\n  kable_styling(latex_options = \"scale_down\")%&gt;%\n  landscape()\n\n\n\n\n\ntimestamp\nhow_old_are_you\nindustry\njob_title\nadditional_context_on_job_title\nannual_salary\nother_monetary_comp\ncurrency\ncurrency_other\nadditional_context_on_income\ncountry\nstate\ncity\noverall_years_of_professional_experience\nyears_of_experience_in_field\nhighest_level_of_education_completed\ngender\nrace\n\n\n\n\n2021-04-27 11:04:04\n25-34\nNonprofits\nProject Manager\nNA\n66625\n1500\nUSD\nNA\nNA\nUS\nMassachusetts\nBoston\n8 - 10 years\n8 - 10 years\nCollege degree\nWoman\nHispanic, Latino, or Spanish origin, White\n\n\n2021-04-27 11:09:34\n35-44\nProperty or Construction\nProject Manager\nNA\n52000\n6000\nUSD\nNA\nNA\nUnited States\nSouth Carolina\nCharleston\n11 - 20 years\n5-7 years\nHigh School\nWoman\nWhite\n\n\n2021-04-27 11:11:41\n35-44\nEngineering or Manufacturing\nProject Manager\nNA\n65000\n1000\nUSD\nNA\nNA\nUnited States\nPennsylvania\nEast Stroudsburg\n8 - 10 years\n2 - 4 years\nMaster's degree\nMan\nWhite\n\n\n2021-04-27 11:12:58\n45-54\nProperty or Construction\nProject Manager\nNA\n52800\n3600\nUSD\nNA\nNA\nUSA\nArizona\nPhoenix\n21 - 30 years\n11 - 20 years\nHigh School\nWoman\nWhite\n\n\n2021-04-27 11:13:28\n25-34\nNonprofits\nProject Manager\nEnvironmental Health\n65818\n1000\nUSD\nNA\nNA\nUnited States\nIllinois\nChicago\n5-7 years\n5-7 years\nCollege degree\nWoman\nWhite\n\n\n2021-04-27 11:14:04\n25-34\nManufacturing\nProject Manager\nNA\n66250\n6000\nUSD\nNA\nNA\nUnited States\nTennessee\nNashville\n8 - 10 years\n5-7 years\nMaster's degree\nMan\nWhite\n\n\n\n\n\n\n\nCode\nas.data.frame(report(pm_survey$annual_salary))\n\n\nMean     |       SD |   Median |      MAD |      Min |      Max | n_Obs | Skewness | Kurtosis | percentage_Missing\n------------------------------------------------------------------------------------------------------------------\n87653.22 | 32751.30 | 82000.00 | 24907.68 | 28000.00 | 2.80e+05 |   221 |     2.02 |     7.01 |               0.00\n\n\nCode\nt.test(pm_survey$annual_salary) \n\n\n\n    One Sample t-test\n\ndata:  pm_survey$annual_salary\nt = 39.786, df = 220, p-value &lt; 0.00000000000000022\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 83311.35 91995.08\nsample estimates:\nmean of x \n 87653.22 \n\n\nThe confidence interval at the (default) 95% is 83311.3539636, 91995.0804255 which means in plain language that if we were to repeat the sampling and the test multiple times, this confidence interval would capture the true value of the parameter 95% of the time. Although not extrictly true, we can say that there’s a 95% chance that this confidence interval includes the true population mean.\nFor the second exercise\n\n\nCode\npm_survey &lt;- pm_survey |&gt;    \n  mutate(other_monetary_comp = replace_na(other_monetary_comp, 0))  \n\nas.data.frame(report(pm_survey$other_monetary_comp))\n\n\nMean    |      SD |  Median |     MAD |  Min |      Max | n_Obs | Skewness | Kurtosis | percentage_Missing\n----------------------------------------------------------------------------------------------------------\n5353.81 | 9676.19 | 1500.00 | 2223.90 | 0.00 | 70000.00 |   221 |     3.32 |    13.78 |               0.00\n\n\nCode\nt.test(pm_survey$other_monetary_comp)  \n\n\n\n    One Sample t-test\n\ndata:  pm_survey$other_monetary_comp\nt = 8.2253, df = 220, p-value = 0.00000000000001693\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 4071.026 6636.585\nsample estimates:\nmean of x \n 5353.805 \n\n\nour confidence interval is 4071.0255904, 6636.5852693 and so, the margin of error in both examples are:\n\n\nCode\nmonetary_marginError&lt;- (t.test(pm_survey$annual_salary)$conf.int[2] - \n                          t.test(pm_survey$annual_salary)$conf.int[1]) / 2 \nmonetary_marginError\n\n\n[1] 4341.863\n\n\nCode\nnonMonetary_marginError&lt;- (t.test(pm_survey$other_monetary_comp)$conf.int[2] - \n                             t.test(pm_survey$other_monetary_comp)$conf.int[1]) / 2\nnonMonetary_marginError\n\n\n[1] 1282.78\n\n\nThe difference in both is due to the variability of the data, that we can calculate using the standar deviation:\n\n\nCode\nsd(pm_survey$annual_salary) \n\n\n[1] 32751.3\n\n\nCode\nsd(pm_survey$other_monetary_comp)\n\n\n[1] 9676.192\n\n\n\n\n5.2.1 Interpreting confidence intervals: practical example\nWe are going to calculate the confidence interval for 100 samples of 30 observations calculated over the same population and plot it on a graph with a line marking the true mean. We will see how some of them (for a 95% confidence level it should be around 5% of the times) will still miss the population parameter:\n\n\nCode\n#calculate the sample mean and confidence interval from a sample of 100 \nlow = numeric()\nhigh = numeric()\nfor (n in 1:100){\n  sample &lt;- slice_sample(optical, n = 30)\n  test &lt;- t.test(sample$eye_difference)\n  low[n] &lt;- test$conf.int[1]\n  high[n] &lt;- test$conf.int[2]\n}\n\nci_reps &lt;- data.frame(\"replicate\" = 1:100,\n                      low,\n                      high)\nggplot(ci_reps, aes(x = low, \n                    xend = high,\n                    y = replicate, \n                    yend = replicate)) + \n  geom_segment() +\n  geom_vline(xintercept = mean(optical$eye_difference), \n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\nWhen interpreting a confidence interval, remember that not all values in an interval are equal. The center is always more likely than the ends."
  },
  {
    "objectID": "Inferencial.html#confidence-intervals-for-proportions.",
    "href": "Inferencial.html#confidence-intervals-for-proportions.",
    "title": "Inference",
    "section": "7.1 Confidence Intervals for Proportions.",
    "text": "7.1 Confidence Intervals for Proportions.\nConfidence intervals for proportions provide a range of plausible values for population proportions, enabling estimation with a desired level of confidence.\nProportional analysis techniques allow for the assessment and comparison of proportions across different categories, providing valuable insights into categorical data.\nThere are any number of statistical methods for computing confidence intervals for proportions. By far the simplest is:\n\\[\np = \\hat{p} \\pm \\sqrt{\\frac{\\hat{p} \\cdot (1 - \\hat{p})}{n}}\n\\tag{10}\\]\nwhere \\(p\\) is the actual population proportion and \\(\\hat{p}\\) is the observed proportion.\nAs long as the sample size isn’t too small, the difference between methods should be minor. If your sample includes at least 5 of each sort of possible outcomes, you are fine with whatever default method your software uses to calculate the proportions.\nIn R we will use \\(prop.test(n_s , sample size)\\) where \\(n_s\\) is the number of successes.\nPractice:\nWe are going to calculate the proportion of smokers in our population based on our optical_sample data\n\n\nCode\nfile1 &lt;- here::here(\"data\", \"optical_sample.xlsx\")\noptical_sample &lt;- read_excel(file1)\n\nas.data.frame(report(optical_sample$IsSmoker))\n\n\nMean |   SD | Median |  MAD |  Min |  Max | n_Obs | Skewness | Kurtosis | percentage_Missing\n--------------------------------------------------------------------------------------------\n0.48 | 0.50 |   0.00 | 0.00 | 0.00 | 1.00 |   100 |     0.08 |    -2.03 |               0.00\n\n\nCode\nsuccessNumber&lt;- sum(optical_sample$IsSmoker)\nsampleSize &lt;- NROW(optical_sample$IsSmoker)\ntestResult &lt;- prop.test(successNumber, sampleSize)\ntestResult\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  successNumber out of sampleSize, null probability 0.5\nX-squared = 0.09, df = 1, p-value = 0.7642\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.3798722 0.5816817\nsample estimates:\n   p \n0.48 \n\n\nCode\n#proportion of success \nproportion&lt;- testResult$estimate %&gt;% print()\n\n\n   p \n0.48 \n\n\nCode\nconf_int &lt;- testResult$conf.int%&gt;% print()\n\n\n[1] 0.3798722 0.5816817\nattr(,\"conf.level\")\n[1] 0.95\n\n\nWe can use the formula we studied at the beginning of the chapter to manually calculate the confidence intervals as well and see the difference with what the software used:\n\n\nCode\nci_lower&lt;- proportion - sqrt(proportion*(1-proportion)/sampleSize) \nci_upper&lt;- proportion + sqrt(proportion*(1-proportion)/sampleSize) \nci_lower\n\n\n      p \n0.43004 \n\n\nCode\nci_upper\n\n\n      p \n0.52996 \n\n\nAnother way of manually calculating the confidence interval is using a \\(z\\)-score\nIn the example below we use 95%: (qnorm(0.975) is the \\(z-score\\) corresponding for 95% conf. level)\n\n\nCode\nmargin_of_error &lt;- qnorm(0.975) * sqrt(proportion * (1 - proportion) / sampleSize)\n\n# Calculate the confidence interval bounds\nproportion - margin_of_error\n\n\n        p \n0.3820802 \n\n\nCode\nproportion + margin_of_error\n\n\n        p \n0.5779198 \n\n\nFor different confidence levels you will need to find the correct \\(z\\)-score\n\n7.1.1 Non-binary categorical variables\nIf our categorical variable is not binary, we still can use this same test to calculate the proportion of one single category against the rest.\nFor example, to calculate the proportion of women in our pm_survey dataset:\n\n\nCode\nfile2 &lt;- here::here(\"data\", \"pm_survey.xlsx\")\npm_survey &lt;- read_excel(file2)\n\nkable(head(pm_survey))%&gt;%\n  kable_styling(latex_options = \"scale_down\")%&gt;%\n  landscape()\n\n\n\n\n\ntimestamp\nhow_old_are_you\nindustry\njob_title\nadditional_context_on_job_title\nannual_salary\nother_monetary_comp\ncurrency\ncurrency_other\nadditional_context_on_income\ncountry\nstate\ncity\noverall_years_of_professional_experience\nyears_of_experience_in_field\nhighest_level_of_education_completed\ngender\nrace\n\n\n\n\n2021-04-27 11:04:04\n25-34\nNonprofits\nProject Manager\nNA\n66625\n1500\nUSD\nNA\nNA\nUS\nMassachusetts\nBoston\n8 - 10 years\n8 - 10 years\nCollege degree\nWoman\nHispanic, Latino, or Spanish origin, White\n\n\n2021-04-27 11:09:34\n35-44\nProperty or Construction\nProject Manager\nNA\n52000\n6000\nUSD\nNA\nNA\nUnited States\nSouth Carolina\nCharleston\n11 - 20 years\n5-7 years\nHigh School\nWoman\nWhite\n\n\n2021-04-27 11:11:41\n35-44\nEngineering or Manufacturing\nProject Manager\nNA\n65000\n1000\nUSD\nNA\nNA\nUnited States\nPennsylvania\nEast Stroudsburg\n8 - 10 years\n2 - 4 years\nMaster's degree\nMan\nWhite\n\n\n2021-04-27 11:12:58\n45-54\nProperty or Construction\nProject Manager\nNA\n52800\n3600\nUSD\nNA\nNA\nUSA\nArizona\nPhoenix\n21 - 30 years\n11 - 20 years\nHigh School\nWoman\nWhite\n\n\n2021-04-27 11:13:28\n25-34\nNonprofits\nProject Manager\nEnvironmental Health\n65818\n1000\nUSD\nNA\nNA\nUnited States\nIllinois\nChicago\n5-7 years\n5-7 years\nCollege degree\nWoman\nWhite\n\n\n2021-04-27 11:14:04\n25-34\nManufacturing\nProject Manager\nNA\n66250\n6000\nUSD\nNA\nNA\nUnited States\nTennessee\nNashville\n8 - 10 years\n5-7 years\nMaster's degree\nMan\nWhite\n\n\n\n\n\n\n\nCode\nas.data.frame(report(pm_survey$gender))\n\n\nn_Entries | n_Obs | n_Missing | percentage_Missing\n--------------------------------------------------\n5.00      |   221 |         0 |               0.00\n\n\nCode\npm_survey &lt;- pm_survey |&gt; \n  mutate(across(.cols = c(highest_level_of_education_completed, gender), as.factor))\n\nnumberOfWomen&lt;- NROW(pm_survey[pm_survey$gender == 'Woman',])\nsampleSize &lt;- NROW(pm_survey)\nprop.test(numberOfWomen, sampleSize)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  numberOfWomen out of sampleSize, null probability 0.5\nX-squared = 91.24, df = 1, p-value &lt; 0.00000000000000022\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.7653993 0.8701140\nsample estimates:\n        p \n0.8235294 \n\n\n\n\n7.1.2 Few observations for each outcome\nWhile there are statistical methods for dealing with samples with fewer than 5 or each sort of outcome, you should be conservative about making decisions based on them. That said, there are two methods that might be helpful in such situations:\n\nThe wilson score interval works well even for observed proportions near zero or one and has other good theoretical properties as well. Unlike other confidence intervals we’ve seen it isn’t symmetric.\nThe rule of three is a great rought-and-ready way to compute a 95% confidence interval for a proportion when no positive results have been observed. If you have failures only, the interval will go from 0 to 3/n where n is the sample size, and if you have observed successes only, the confidence interval will go from 1 - (3/n) to 0"
  },
  {
    "objectID": "Inferencial.html#significance-testing-for-proportions",
    "href": "Inferencial.html#significance-testing-for-proportions",
    "title": "Inference",
    "section": "7.2 Significance testing for proportions",
    "text": "7.2 Significance testing for proportions\nWe want to know if the proportion of the people taking medication in our sample is the same as the proportion of people in USA taking medication. This data we know is 66% of the USA population so we can plug in that value in our test just like that in the formula using \\(p=\\) where p, in this case is the proportion for our null theory.\n\n\nCode\nfile &lt;- here::here(\"data\", \"optical_sample.xlsx\")\noptical_sample &lt;- read_excel(file)\n\nas.data.frame(report(optical_sample$TakingMedication))\n\n\nn_Entries | n_Obs | n_Missing | percentage_Missing\n--------------------------------------------------\n2.00      |   100 |         0 |               0.00\n\n\nCode\nmedicated&lt;- sum(optical_sample$TakingMedication == \"yes\")\nsampleSize &lt;- nrow(optical_sample)\n\ntestResult &lt;- prop.test(medicated, sampleSize, p = .66)\n\ntestResult$p.value\n\n\n[1] 0.0004955456\n\n\nCode\ntestResult$conf.int\n\n\n[1] 0.3894281 0.5913488\nattr(,\"conf.level\")\n[1] 0.95\n\n\nIn our case we can reject the null hypothesis and say that it is not reasonable to believe that this sample was drawn from a population with sample mean of 66%\n\nExercises\nExercise 1. A media outlet claims that 60% of project managers in the U.S have a college degree as their highest level of education. Is that claim plausible using our pm_survey data set?\n\n\nCode\nfile &lt;- here::here(\"data\", \"pm_survey.xlsx\")\npm_survey &lt;- read_excel(file)\n\npmWithCollegeDegree &lt;- sum(pm_survey$highest_level_of_education_completed == 'College degree')\nsampleSize &lt;- nrow(pm_survey)\n\ntestResult &lt;- prop.test(pmWithCollegeDegree, sampleSize, p= 0.6)\n\ntestResult$p.value\n\n\n[1] 0.09662592\n\n\nCode\ntestResult$conf.int\n\n\n[1] 0.4748865 0.6095674\nattr(,\"conf.level\")\n[1] 0.95\n\n\nIn this case we cannot reject the null hypothesis.\nExercise 2. Use that data set to construct a 95% confidence interval for the proportion of project managers that are under the age of 35.\n\n\nCode\npm_under35 &lt;- nrow (pm_survey[pm_survey$how_old_are_you %in% c(\"18-24\",\"25-34\"),])\n\ntestResult &lt;- prop.test(pm_under35, sampleSize)\ntestResult$conf.int \n\n\n[1] 0.4390950 0.5742398\nattr(,\"conf.level\")\n[1] 0.95\n\n\nCode\ntestResult$estimate\n\n\n        p \n0.5067873 \n\n\nThe confidence interval shows that between 44% and 57% of respondents are under 35"
  },
  {
    "objectID": "Inferencial.html#goodness-of-fit",
    "href": "Inferencial.html#goodness-of-fit",
    "title": "Inference",
    "section": "7.3 Goodness of Fit",
    "text": "7.3 Goodness of Fit\nConsider genetic data where you have two groups of genotypes (A or B) for cases and controls for a given disease. The statistical question is if genotype and disease are associated. As in the examples we have been studying previously, we have two populations (A and B) and then numeric data for each, where disease status can be coded as 0 or 1. So why can’t we perform a t-test? Note that the data is either 0 (control) or 1 (cases). It is pretty clear that this data is not normally distributed so the t-distribution approximation is certainly out of the question. We could use CLT if the sample size is large enough, otherwise, we can use association tests.\nImagine we have 250 individuals, where some of them have a given disease and the rest do not. We observe that 20% of the individuals that are homozygous for the minor allele (group B) have the disease compared to 10% of the rest. Would we see this again if we picked another 250 individuals?\n\n\nCode\ndisease=factor(c(rep(0,180),rep(1,20),rep(0,40),rep(1,10)),\n               labels=c(\"control\",\"cases\"))\ngenotype=factor(c(rep(\"A\",200),rep(\"B\",50)),\n                levels=c(\"A\",\"B\"))\ndat &lt;- data.frame(disease, genotype)\ndat &lt;- dat[sample(nrow(dat)),] #shuffle them up\n\ntab&lt;- table(genotype,disease)\ntab\n\n\n        disease\ngenotype control cases\n       A     180    20\n       B      40    10\n\n\nThe typical statistics we use to summarize these results is the odds ratio (OR). We compute the odds of having the disease if you are an “B”: 10/40, the odds of having the disease if you are an “A”: 20/180, and take the ratio: \\((10/40) / (20/180)\\)\n\n\nCode\n(tab[2,2]/tab[2,1]) / (tab[1,2]/tab[1,1])\n\n\n[1] 2.25\n\n\nTo compute a \\(p\\)-value, we don’t use the OR directly. We instead assume that there is no association between genotype and disease, and then compute what we expect to see in each cell of the table under the null hypothesis: ignoring the groups, the probabilty of having the disease according to our data is:\n\n\nCode\np=mean(disease==\"cases\")\np\n\n\n[1] 0.12\n\n\naccording to this probability, under our null hypothesis we expect to see a table like this:\n\n\nCode\nexpected &lt;- rbind(c(1-p,p)*sum(genotype==\"A\"),\n                  c(1-p,p)*sum(genotype==\"B\"))\ndimnames(expected)&lt;-dimnames(tab)\nexpected\n\n\n        disease\ngenotype control cases\n       A     176    24\n       B      44     6\n\n\nThe Chi-square test uses an asymptotic result (similar to the CLT) related to the sums of independent binary outcomes. Using this approximation, we can compute the probability of seeing a deviation from the expected table as big as the one we saw. The \\(p\\)-value for this table is:\n\n\nCode\nchisq.test(tab)$p.value\n\n\n[1] 0.08857435\n\n\nso we would expect to find this difference in disease ratio by chance approximately 8.8% of the times, which is not enough to reject our null hypothesis.\nLarge Samples, Small \\(p\\)-values\nReporting only \\(p\\)-values is not an appropriate way to report the results of your experiment. Studies with large sample sizes will have impressively small \\(p\\)-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1.\nTo demonstrate, we recalculate the \\(p\\)-value keeping all the proportions identical, but increasing the sample size by 10, which reduces the \\(p\\)-value substantially:\n\n\nCode\ntab&lt;-tab*10\nchisq.test(tab)$p.value\n\n\n[1] 0.000000001219624\n\n\n\n\nFitting Fallacies: Goodness of fit doesn’t guarantee predictive power. A model can fit past data perfectly yet fail miserably on new, unseen data, highlighting the dangers of overfitting.\nThe Sample Size Paradox: Doubling your sample size doesn’t necessarily halve the error. In fact, to do so, you’d need to quadruple the sample, given the square root relationship between sample size and margin of error.\nTwo-Sample Twists: When comparing two samples, it’s possible for each sample’s individual data to appear random, yet their combined data can reveal a distinct pattern or significant difference.\n\n\nA Goodness-of-fit test, also called chi-squared or Pearson goodness-of-fit test, considers whether a categorical variable has a hypothesized distribution\nWarning! A goodness of fit test doesn’t give any information about which specific categories might be out of line with the hypothesized distribution. While you might be able to make an educated guess by looking at the data, this test shouldn’t be used to support that kind of intuition.\nIn 208 the manufacturer of M&Ms published their last color distribution:\n\n\n\nBlue\nOrange\nGreen\nYellow\nRed\nBrow\n\n\n\n\n24%\n20%\n16%\n14%\n13%\n13%\n\n\n\nWe open several packages of M&Ms and count the colors:\n\n\n\nBlue\nOrange\nGreen\nYellow\nRed\nBrow\n\n\n\n\n85\n79\n56\n64\n58\n68\n\n\n\nAre these counts consistent with the last published percentages? is there sufficient evidence to claim that the color distribution is different? This question requires a test of goodness-of-fit for the six categories.\nOur null hypothesis is that the color distribution is the same. The idea is to compare the observed counts to the numbers we would expect if $H_0 $ is true, so for our sample of 410 M&Ms we would expect:\n\n\n\nBlue\nOrange\nGreen\nYellow\nRed\nBrow\n\n\n\n\n98.4\n82\n65.6\n57.4\n53.3\n53.3\n\n\n\nWe look at the difference of the observed and the expected values, we square that difference and we standardize it by dividing by the expected\n\\[\n\\chi^2 =\\sum_{categories} \\frac{(observed-expected)^2}{expected}\n\\]\n\\[\n\\frac{(85 - 98.4)^2}{98.4} + \\frac{(79 - 82)^2}{82} + \\cdots + \\frac{(68 - 53.3)^2}{53.3} = 8.57\n\\]\nLarge values of the chi-square statistic \\(\\chi^2\\) are evidence against the null hypothesis. To calculate the \\(p\\)-value we use the chi-square distribution. The \\(p\\)-value is the right tail of the \\(\\chi^2\\) distribution with degrees of freedom = number of categories -1.\n\n\nCode\ndf &lt;- 5\n# Create the chi-square distribution curve\ncurve(dchisq(x, df = df), from = 0, to = 40, \n      main = paste(\"Chi-Square Distribution (df =\", df, \")\"),\n      ylab = \"Density\", lwd = 2, col = \"steelblue\")\n\n# Draw a vertical line at x = 8.57\nabline(v = 8.57, col = \"red\", lwd = 2, lty = 2)\n\n# Highlight the area to the right of x = 8.57\nx_vector &lt;- seq(8.57, 40, length.out = 1000)\ny_vector &lt;- dchisq(x_vector, df = df)\npolygon(c(8.57, x_vector, 40), c(0, y_vector, 0), \n        col = adjustcolor(\"red\", alpha.f = 0.3), border = NA)\n\n\n\n\n\n\n\n\n\nin our example, this \\(p\\)-value happens to be 12.7%, which does not allow us to reject the null hypothesis. In r we can calculate the \\(p\\)-value like this:\n\n\nCode\n# Set the chi-square value and degrees of freedom\nx &lt;- 8.57\ndf &lt;- 5\n\n# Calculate the p-value\np_value &lt;- pchisq(x, df, lower.tail = FALSE)\n\n# Print the p-value\np_value\n\n\n[1] 0.1274943\n\n\n\nExercise:\nIn the exercise below we want to know if the age population in our project managers survey data matches the distribution of ages for the USA population. We get the distribution of USA from wikipedia and do some adjustments so the ranges matches those in our survey:\n\n\nCode\nfile1 &lt;- here::here(\"data\", \"pm_survey.xlsx\")\npm_survey &lt;- read_excel(file1)\n\nus_ages &lt;- c(.117, .176, .168, .158, .166, .215)\n\ntable(pm_survey$how_old_are_you)\n\n\n\n     18-24      25-34      35-44      45-54      55-64 65 or over \n         4        108         77         27          4          1 \n\n\nCode\nobs_ages &lt;- as.numeric(table(pm_survey$how_old_are_you))\n\ntestResult &lt;- chisq.test(obs_ages, p = us_ages)\ntestResult %&gt;% report()\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Chi-squared test for given probabilities / goodness of fit of obs_ages to a\ndistribution of [: n=25.857, : n=38.896, : n=37.128, : n=34.918, : n=36.686, :\nn=47.515] suggests that the effect is statistically significant, and large\n(chi2 = 260.52, p &lt; .001; Fei = 0.40, 95% CI [0.35, 1.00])\n\n\nCode\ntestResult$p.value\n\n\n[1] 0.000000000000000000000000000000000000000000000000000003035227\n\n\nThe very small \\(p\\)-value indicates that is extremely unlikely that our pm_survey data was extracted at random from a population with the us_ages distribution.\nAs mentioned, the goodness of fit test does not tell us what categories show the discrepancies in the data, if we want to find out what is the difference between our sample range of ages and the USA data we can just subtract the proportions from each and see what categories are sub represented and vice versa\n\n\nCode\nobs_ages / sum(obs_ages)\n\n\n[1] 0.018099548 0.488687783 0.348416290 0.122171946 0.018099548 0.004524887\n\n\nCode\nobs_ages / sum(obs_ages) - us_ages\n\n\n[1] -0.09890045  0.31268778  0.18041629 -0.03582805 -0.14790045 -0.21047511\n\n\nNow we want to see if in our optical sample the customers are evenly distributed between the opticians. If we don’t pass a \\(p\\) attribute to the chi squared test it will assume you are asking for even distribution between all categories:\n\n\nCode\nfile2 &lt;- here::here(\"data\", \"optical_full.xlsx\")\noptical_full &lt;- read_excel(file2)\n\ncounts &lt;- as.numeric(table(optical_full$`Optician Last Name`))\n\ntestResult&lt;- chisq.test(counts)\ntestResult %&gt;% report()\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Chi-squared test for given probabilities / goodness of fit of counts to a\nuniform distribution suggests that the effect is statistically not significant,\nand tiny (chi2 = 46.44, p = 0.164; Fei = 0.01, 95% CI [0.00, 1.00])\n\n\nCode\ntestResult$p.value\n\n\n[1] 0.1636262\n\n\nIn this case we cannot reject the null hypothesis, which in this case is that the count of patients is evenly distributed among all opticians.\n\n\nExercise\nDoes the distribution of populations of towns in the United States follow Benford’s law? Check using the towns data set.\n\n\nCode\ntheme_set(theme_minimal())\n\nfile &lt;- here::here(\"data\", \"towns.xlsx\")\ntowns &lt;- read_excel(file)\n\nbenford &lt;- c(.301, .176, .125, .097, .079, .067, .058, .051, .046)\nsum(benford)\n\n\n[1] 1\n\n\nCode\n#we calculate the frequency of each first digit in the population of towns.\ntable(towns$first_digit)\n\n\n\n   1    2    3    4    5    6    7    8    9 \n2966 1746 1243  939  797  656  609  548  496 \n\n\nCode\ndigits_freq &lt;- as.numeric(table(towns$first_digit))\ndigits_freq\n\n\n[1] 2966 1746 1243  939  797  656  609  548  496\n\n\nCode\ntestResult&lt;- chisq.test(digits_freq, p = benford)\ntestResult %&gt;% report()\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Chi-squared test for given probabilities / goodness of fit of digits_freq\nto a distribution of [: n=3010, : n=1760, : n=1250, : n=970, : n=790, : n=670,\n: n=580, : n=510, : n=460] suggests that the effect is statistically not\nsignificant, and tiny (chi2 = 9.24, p = 0.323; Fei = 6.67e-03, 95% CI [0.00,\n1.00])\n\n\nCode\ntestResult$p.value\n\n\n[1] 0.3226343\n\n\nCode\n# An illustrative plot\nbenford_df &lt;- data.frame(distribution = c(rep(\"Towns\", 9), rep(\"Benford\", 9)),\n                         first_digit = rep(1:9, 2),\n                         frequency = c(digits_freq/sum(digits_freq), benford))\n\nggplot(benford_df, aes(x = first_digit, \n                       y = frequency,\n                       fill = distribution)) + \n  geom_col(position = \"dodge\") +\n  scale_x_continuous(breaks = 1:9) + \n  labs(x = \"First digit\",\n       y = \"Relative frequency\",\n       fill = \"Distribution\") +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\nour \\(p\\)-value indicates that we cannot reject the null hypothesis, meaning in this case that the distribution of the first digit in towns across US is compatible with Benford’s law."
  },
  {
    "objectID": "Inferencial.html#significance-testing-for-two-samples-data",
    "href": "Inferencial.html#significance-testing-for-two-samples-data",
    "title": "Inference",
    "section": "8.1 Significance testing for Two-Samples data",
    "text": "8.1 Significance testing for Two-Samples data\n\n8.1.1 z-test\nThe significance test for two samples uses the null hypothesis that there is no difference in the means of the two population means. The \\(p\\)-value will be used to reject or not that null hypothesys.\nwe can use a \\(z\\)-test for the difference between the two means:\n\\[\nz = \\frac{observed\\ difference- expected\\ difference}{SE\\ of\\ difference} = \\frac{(\\bar{x_2} - \\bar{x_1})-0}{SE\\ of\\ difference}\n\\]\nour expected difference is 0 because that’s our null hypothesis (no difference). An important fact is that if \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) are independent, then:\n\\[\nSE(\\bar{x_2} - \\bar{x_1}) = \\sqrt{ (SE(\\bar{x_1}))^2 +(SE(\\bar{x_2}))^2}\n\\]\n\nExercise: two-sample \\(z\\)-test (proportions.)\nLast month, the president’s approval rating in a sample of 1000 likely voters was 55%. This month, a poll of 1,500 likely voters resulted in a rating of 58%. Is this sufficient evidence to conclude that the rating has changed?\n\\(\\hat{p_1} = 55%\\) and \\(\\hat{p_2} = 58%\\)\n\\[\nz = \\frac{(\\hat{p_2}-\\hat{p_1})-0}{SE_{diff}}=\\frac{(\\hat{p_2}-\\hat{p_1})-0}{\\sqrt{ (SE(\\bar{x_1}))^2 +(SE(\\bar{x_2}))^2}}\n\\] The formula for the standard error for the proportion is\n\\[\nSE= \\sqrt{\\frac{p(1-p)}{n}}\n\\]\nCalculate the standard errors:\nFor \\(\\hat{p_1}\\): \\[\nSE(\\hat{p_1}) = \\sqrt{\\frac{\\hat{p_1}(1 - \\hat{p_1})}{n_1}} = \\sqrt{\\frac{0.55 \\times 0.45}{1000}}= \\sqrt{\\frac{0.2475}{1000}} = \\sqrt{0.0002475} \\approx 0.0157\n\\]\nFor \\(\\hat{p_2}\\):\n\\[\nSE(\\hat{p_2}) = \\sqrt{\\frac{\\hat{p_2}(1 - \\hat{p_2})}{n_2}} = \\sqrt{\\frac{0.58 \\times 0.42}{1500}} = \\sqrt{\\frac{0.2436}{1500}} = \\sqrt{0.0001624} \\approx 0.0127\n\\]\n\\[\nSE_{diff} = \\sqrt{(0.0157)^2 + (0.0127)^2} = \\sqrt{0.00024649 + 0.00016129} = \\sqrt{0.00040778} \\approx 0.0202\n\\]\n\\[\nz = \\frac{(0.58 - 0.55) - 0}{0.0202} = \\frac{0.03}{0.0202} \\approx 1.49\n\\]\nThe calculated \\(z\\)-value is approximately 1.49. To determine if this is statistically significant, you would compare this \\(z\\)-value to the critical value from the standard normal distribution (typically 1.96 for a 95% confidence level). Since 1.49 is less than 1.96, you would fail to reject the null hypothesis at the 95% confidence level, indicating that there is not sufficient evidence to conclude that the president’s approval rating has changed significantly.\nThe \\(p\\)-value can be calculated using standard normal distribution tables or software\n\n\nCode\npValue &lt;- pnorm(1.49)\npValue\n\n\n[1] 0.9318879\n\n\nSince this is a two-tailed test (we are checking if the approval rating has changed, not just increased or decreased), we need to consider both tails of the distribution, so we double our values: The \\(p\\)-value is calculated as \\(2 \\times (1 - \\text{cumulative probability})\\). \\(p = 2 \\times (1 - 0.9318) = 2 \\times 0.0682 = 0.1364\\)\nThe \\(p\\)-value for the \\(z\\)-value of 1.49 is approximately 0.1364. Since this \\(p\\)-value is greater than the typical significance level of 0.05, we fail to reject the null hypothesis. This means there is not sufficient evidence to conclude that the president’s approval rating has changed significantly.\n\nThe exercise above shows how to calculate the \\(p\\) value for proportions, if we are working with average values instead of proportions the calculation is the same, only thing to consider is that in this case is that Standard deviation of each individual sample will be calculated using the formula for the standard deviation for the mean \\(SE(\\bar{x_1})= \\frac{\\sigma_1}{\\sqrt{n_1}}= \\frac{s_1}{\\sqrt{n_1}}\\) . If the sample sizes are not large, then the \\(p\\)-value needs to be computed from the t-distribution instead.\n\n\n8.1.2 The Welch Two Sample t-test\nIf we have two sets, one control and one test, their means being \\(\\mu_t\\) and \\(\\mu_c\\) the two-sample t-statistic is defined as:\n\\[\nT = \\frac{\\hat\\mu_t-\\hat\\mu_c}{s\\sqrt{\\frac{1}{n_t}+\\frac{1}{n_c}}}\n\\tag{11}\\]\nwhere\n\\[\ns=\\sqrt{\\frac{(n_t-1)s^2_t+(n_c-1)s^2_c}{n_t+n_c-2}}\n\\]\nis an estimator of the pooled standard deviation of the two samples. Here, \\(s^2_t\\) and \\(s^2_c\\) are unbiased estimators of the variance of our metric in the treatment and control group, respectively. A large (absolute) value of T provides evidence against \\(H_0\\)\nIn the example below we are going to use the attrition dataset to see if the average age of employees in two different departments is different?\n\n\nCode\nfile &lt;- here::here(\"data\", \"attrition1.xlsx\")\nattrition1 &lt;- read_excel(file)\n\nkable(head(attrition1))%&gt;%\n  kable_styling(latex_options = \"scale_down\")%&gt;%\n  landscape()\n\n\n\n\n\nAge\nAttrition\nBusinessTravel\nDailyRate\nDepartment\nDistanceFromHome\nEducation\nEducationField\nEnvironmentSatisfaction\nGender\nHourlyRate\nJobInvolvement\nJobLevel\nJobRole\nJobSatisfaction\nMaritalStatus\nMonthlyIncome\nMonthlyRate\nNumCompaniesWorked\nOverTime\nPercentSalaryHike\nPerformanceRating\nRelationshipSatisfaction\nStockOptionLevel\nTotalWorkingYears\nTrainingTimesLastYear\nWorkLifeBalance\nYearsAtCompany\nYearsInCurrentRole\nYearsSinceLastPromotion\nYearsWithCurrManager\n\n\n\n\n41\nYes\nTravel_Rarely\n1102\nSales\n1\nCollege\nLife_Sciences\nMedium\nFemale\n94\nHigh\n2\nSales_Executive\nVery_High\nSingle\n5993\n19479\n8\nYes\n11\nExcellent\nLow\n0\n8\n0\nBad\n6\n4\n0\n5\n\n\n49\nNo\nTravel_Frequently\n279\nResearch_Development\n8\nBelow_College\nLife_Sciences\nHigh\nMale\n61\nMedium\n2\nResearch_Scientist\nMedium\nMarried\n5130\n24907\n1\nNo\n23\nOutstanding\nVery_High\n1\n10\n3\nBetter\n10\n7\n1\n7\n\n\n37\nYes\nTravel_Rarely\n1373\nResearch_Development\n2\nCollege\nOther\nVery_High\nMale\n92\nMedium\n1\nLaboratory_Technician\nHigh\nSingle\n2090\n2396\n6\nYes\n15\nExcellent\nMedium\n0\n7\n3\nBetter\n0\n0\n0\n0\n\n\n33\nNo\nTravel_Frequently\n1392\nResearch_Development\n3\nMaster\nLife_Sciences\nVery_High\nFemale\n56\nHigh\n1\nResearch_Scientist\nHigh\nMarried\n2909\n23159\n1\nYes\n11\nExcellent\nHigh\n0\n8\n3\nBetter\n8\n7\n3\n0\n\n\n27\nNo\nTravel_Rarely\n591\nResearch_Development\n2\nBelow_College\nMedical\nLow\nMale\n40\nHigh\n1\nLaboratory_Technician\nMedium\nMarried\n3468\n16632\n9\nNo\n12\nExcellent\nVery_High\n1\n6\n3\nBetter\n2\n2\n2\n2\n\n\n32\nNo\nTravel_Frequently\n1005\nResearch_Development\n2\nCollege\nLife_Sciences\nVery_High\nMale\n79\nHigh\n1\nLaboratory_Technician\nVery_High\nSingle\n3068\n11864\n0\nNo\n13\nExcellent\nHigh\n0\n8\n2\nGood\n7\n7\n3\n6\n\n\n\n\n\n\n\nCode\nattrition1 |&gt; \n  group_by(Department) |&gt; \n  dplyr::summarize(avg_age = mean(Age))\n\n\n# A tibble: 2 × 2\n  Department           avg_age\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 Research_Development    37.0\n2 Sales                   36.5\n\n\nCode\ntestResult &lt;- t.test(Age ~ Department, \n       data = attrition1)\n\ntestResult$p.value\n\n\n[1] 0.3366682\n\n\nCode\nreport(testResult)\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of Age by Department (mean\nin group Research_Development = 37.04, mean in group Sales = 36.54) suggests\nthat the effect is positive, statistically not significant, and very small\n(difference = 0.50, 95% CI [-0.52, 1.52], t(880.05) = 0.96, p = 0.337; Cohen's\nd = 0.06, 95% CI [-0.07, 0.20])"
  },
  {
    "objectID": "Inferencial.html#confidence-intervals-for-two-samples-data.",
    "href": "Inferencial.html#confidence-intervals-for-two-samples-data.",
    "title": "Inference",
    "section": "8.2 Confidence intervals for Two-Samples data.",
    "text": "8.2 Confidence intervals for Two-Samples data.\nWe can use the formula for the standard error of the difference to also do a confidence interval calculation. The confidence interval for \\(p_2-p_1\\) is \\((\\hat{p_2}-\\hat{p_1}) \\mp z \\times SE(\\hat{p_2}-\\hat{p_1})\\)\nwere \\(z\\) is the \\(z\\)-score for the confidence level we are interested in.\n\nin our example about the voters approval of the president:\nthe \\(z\\)-score value for a confidence level of 95% is 1.959964\n\\[\n58-55 \\mp 1.96 \\times 0.0202  \\approx [-0.0705,0.0105]\n\\]\nIf we want to resolve the same exercise using r code it gives similar but not exactly the same results:\n\n\nCode\nsuccesses &lt;- c(0.55 * 1000, 0.58 * 1500)\n\n# Define the sample sizes\nsample_sizes &lt;- c(1000, 1500)\n\n# Perform the two-sample z-test for proportions\ntest_result &lt;- prop.test(successes, sample_sizes)\n\n# Print the result\nprint(test_result)\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  successes out of sample_sizes\nX-squared = 2.0801, df = 1, p-value = 0.1492\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.07051474  0.01051474\nsample estimates:\nprop 1 prop 2 \n  0.55   0.58 \n\n\n\nThere are many statistical techniques for describing the difference between a single variable across two populations. The most universal is the Welch two-sample confidence interval, which is a statistical technique used to compare means between two independent groups, taking into account the unequal variances often encountered in real-world data, so it valid in nearly all circumstances. A few things to bear in mind:\n\nIt is a multi-sample procedure, not a multi-variable one. Use it when you’re asking how much two samples differ in a single variable.\nLike every other statistical tool in this course, it requires that all the observations be independent of one another (not to be used with time-line analysis)\nUnless the data has extreme outliers or is highly asymmetric, it will give good results when both samples are of size 10 or more. If the samples are of size at least 30, it is fine under all but the most extreme circumstances.\n\nThe Welch two-sample confidence interval does not require that the samples are equal in size or that the populations have equal variances, unlike some other procedures.\nWe are going to use the AB_testing data we generated in the previous section and calculate the confidence intervals for the differences observed in one of our samples\n\n\nCode\nset.seed(27)  \nrating &lt;- sample(1:5, 27, replace = TRUE) \nproduct &lt;- c(rep(\"A\", 15), rep(\"B\", 12)) \nAB_testing &lt;- data.frame(product,rating)  \nAB_testing |&gt;    group_by(product)  %&gt;%     \n  dplyr::summarize(avg_rating = mean(rating))  \n\n\n# A tibble: 2 × 2\n  product avg_rating\n  &lt;chr&gt;        &lt;dbl&gt;\n1 A             2.2 \n2 B             2.83\n\n\nCode\nas.data.frame(report(AB_testing))  \n\n\nVariable | n_Obs | n_Missing | n_Entries | percentage_Missing | Mean |   SD | Median |  MAD |  Min |  Max | Skewness | Kurtosis\n-------------------------------------------------------------------------------------------------------------------------------\nproduct  |    27 |         0 |      2.00 |               0.00 |      |      |        |      |      |      |          |         \nrating   |    27 |         0 |           |                    | 2.48 | 1.55 |   2.00 | 1.48 | 1.00 | 5.00 |     0.44 |    -1.38\n\n\nCode\ntestResult&lt;- t.test(rating ~ product, data = AB_testing)  \ntestResult %&gt;% report()  \n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of rating by product (mean\nin group A = 2.20, mean in group B = 2.83) suggests that the effect is\nnegative, statistically not significant, and small (difference = -0.63, 95% CI\n[-1.88, 0.61], t(23.25) = -1.05, p = 0.305; Cohen's d = -0.41, 95% CI [-1.17,\n0.37])\n\n\nCode\ntestResult$conf.int  \n\n\n[1] -1.8804465  0.6137799\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThe confidence interval says that with 95% confidence, the population mean difference is between -1.8804465 and 0.6137799. This is actually saying that the difference in the means could be 0.\nIf you know or can assume that the variance of the two populations is equal, then you can use var.equal = TRUE in the t.test, and in this case instead of using a Welch Two sample t-test, it will use a Two sample t-test\n\n\nCode\nt.test(rating ~ product,         data = AB_testing,        var.equal = TRUE)  \n\n\n\n    Two Sample t-test\n\ndata:  rating by product\nt = -1.055, df = 25, p-value = 0.3015\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.8697428  0.6030761\nsample estimates:\nmean in group A mean in group B \n       2.200000        2.833333 \n\n\nCode\ntestResult&lt;- t.test(rating ~ product, data = AB_testing) \ntestResult %&gt;% report()  \n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of rating by product (mean\nin group A = 2.20, mean in group B = 2.83) suggests that the effect is\nnegative, statistically not significant, and small (difference = -0.63, 95% CI\n[-1.88, 0.61], t(23.25) = -1.05, p = 0.305; Cohen's d = -0.41, 95% CI [-1.17,\n0.37])\n\n\nCode\ntestResult$conf.int  \n\n\n[1] -1.8804465  0.6137799\nattr(,\"conf.level\")\n[1] 0.95\n\n\nIn the example below we want to use our substance_abuse data set and know if there is a difference in the variable DLA_improvement based on the program that the patient was following. We are assuming that the variance is equal in both cases:\nWe can change the confidence level manually if we want:\n\n\nCode\nfile &lt;- here::here(\"data\", \"substance_abuse.xlsx\") \nsubstance_abuse &lt;- read_excel(file) \nsubstance_abuse$DLA_improvement &lt;- substance_abuse$DLA2 - substance_abuse$DLA1 \nt.test(DLA_improvement ~ Program,         data = substance_abuse,        conf.level = .99)\n\n\n\n    Welch Two Sample t-test\n\ndata:  DLA_improvement by Program\nt = 38.23, df = 150.6, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group Intervention and group UsualCare is not equal to 0\n99 percent confidence interval:\n 0.5539340 0.6350745\nsample estimates:\nmean in group Intervention    mean in group UsualCare \n               0.591468531               -0.003035714 \n\n\n\nExercises\nExercise 1: Generate a 95% confidence interval for the difference in average monthly income between the research and development and sales departments of the company\n\n\nCode\ntestResult &lt;- t.test(MonthlyIncome ~Department, \n                     data= attrition1)\ntestResult$conf.int\n\n\n[1] -1166.0385  -189.8011\nattr(,\"conf.level\")\n[1] 0.95\n\n\nCode\nreport(testResult)\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of MonthlyIncome by\nDepartment (mean in group Research_Development = 6281.25, mean in group Sales =\n6959.17) suggests that the effect is negative, statistically significant, and\nvery small (difference = -677.92, 95% CI [-1166.04, -189.80], t(1030.99) =\n-2.73, p = 0.007; Cohen's d = -0.17, 95% CI [-0.29, -0.05])\n\n\nThe confidence interval does not include 0, which means that there is a difference in the means of the populations of both groups\nExercise 2: It is reasonable to claim that the montly rate is the same bewteen these two departments?\n\n\nCode\ntestResult &lt;- t.test(MonthlyRate ~Department, \n                      data= attrition1,\n                      mu =0)\ntestResult$p.value\n\n\n[1] 0.6162771\n\n\nCode\nreport(testResult)\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of MonthlyRate by Department\n(mean in group Research_Development = 14284.87, mean in group Sales = 14489.79)\nsuggests that the effect is negative, statistically not significant, and very\nsmall (difference = -204.93, 95% CI [-1007.25, 597.40], t(858.77) = -0.50, p =\n0.616; Cohen's d = -0.03, 95% CI [-0.17, 0.10])\n\n\nIn light of the results we cannot reject the null hypothesis that they have the same monthly rate.\n\n\n8.2.1 Pooled estimate:\nGoing back to our recent exercise of the voters’s approval rate, we concluded that the two different surveys did not significantly differed.\nSince we could not reject the null hypothesis that the two samples are representing the same approval for the candidate, we can combine the two of them to find a better estimate of our Standard Error\nin the first sample we have 0,55 x 1000 voters who approved, in the second sample we have 0.58x 1500 so in total we have 1420 approvals out of 2500 people surveyed, so our ppoled estimate will be \\(\\frac{1420}{2500}=56.8\\%\\) so now we can calculate the Standard Error using that new value:\n\\[\nSE(\\hat{p_2}-\\hat{p_1}) = \\sqrt{\\frac{0.568(1-0.568)}{1000}+\\frac{0.568(1-0.568)}{1500}}=0.02022\n\\]\n\n\n8.2.2 Pooled standard deviation:\nIf we know (or there is reason to assume) that the standard deviation of the two populations is the same \\(\\sigma_1=\\sigma_2\\) then we can use the pooled estimate:\n\\[\ns^2_{pooled}=\\frac{(n_1-1)s^2_1+(n_2-1)s^2_2}{n_1+n_2-2}\n\\]\n\nTwo-Sample \\(z\\)-Test vs Two sample T-test vs Welch’s Two-Sample T-Test\nTwo sample \\(z\\)-test\nWhen to Use:\n\nKnown Population Variances: The population variances are known.\nLarge Sample Sizes: Typically used when the sample sizes are large (n &gt; 30).\nNormal Distribution: Assumes that the data follows a normal distribution.\n\nTwo sample T-test\n\nUnknown Population Variances: The population variances are unknown and assumed to be equal.\nSmall Sample Sizes: Typically used when the sample sizes are small (n &lt; 30).\nNormal Distribution: Assumes that the data follows a normal distribution.\n\nWelch’s Two-Sample T-Test\nWhen to Use:\n\nUnknown and Unequal Population Variances: The population variances are unknown and not assumed to be equal.\nSmall or Unequal Sample Sizes: Can be used for small or unequal sample sizes.\nNormal Distribution: Assumes that the data follows a normal distribution.\n\nSummary:\n\nPopulation Variances:\n\nZ-Test: Assumes known and equal population variances.\nWelch’s T-Test: Does not assume equal variances and uses sample variances.\n\nSample Size:\n\nZ-Test: Typically used for large sample sizes.\nWelch’s T-Test: Can be used for small or unequal sample sizes."
  },
  {
    "objectID": "Inferencial.html#paired-t-test",
    "href": "Inferencial.html#paired-t-test",
    "title": "Inference",
    "section": "8.3 Paired-t test",
    "text": "8.3 Paired-t test\nWhat do we do when we have two samples, but they are not independent from each other? In this case we cannot use the classical two-sample \\(z\\)-test or two-sample t-test\nWe want to answer the question: Do husbands tend to be older than their wives?\n\n\n\nHusband’s age\nWife’s age\nage difference\n\n\n\n\n43\n41\n2\n\n\n71\n70\n1\n\n\n32\n31\n1\n\n\n68\n66\n2\n\n\n27\n26\n1\n\n\n\nIn a scenario like this, even if the samples were independent, the test would not give us a significant difference because the difference between the two pairs is always small, while the difference in the values in each sample (standard deviations) are large and what the two samples test does is to compare the differences to the fluctuation within each population.\nIn this case we will use the paired t-test. Our \\(H_0\\) is that the population difference is 0. The formula for this test is:\n\\[\nt=\\frac{\\bar{d}-0}{SE_{(d)}}\n\\]\nwhere \\(\\bar{d}\\) is the average of the differences,\n0 is the expected difference that under our null hypothesis is 0 and \\(SE_{\\bar{(d)}}\\) is the standard error for the difference:\n\\[\nSE_{(d)}= \\frac{\\sigma_d}{\\sqrt{n}} = \\frac{s_d}{\\sqrt{n}}\n\\]\nIn the example with our data:\n\\[\nt=\\frac{1.4}{\\frac{0.55}{\\sqrt{5}}}=5.69\n\\]\nand now we have to use a table of a student t-distribution with 4 degrees of freedom to find the area under the curve of the normal distribution to the right of our t value.\nwe can use R code to calculate it like this:\n\n\nCode\n# Given values\nt_value &lt;- 5.69\ndegreesfreedom &lt;- 4\n\n# Calculate the p-value for a two-tailed test\np_value &lt;- 2 * pt(-abs(t_value), degreesfreedom)\np_value\n\n\n[1] 0.004711695\n\n\nIn this case our result means that we can reject the null hypothesis."
  },
  {
    "objectID": "Inferencial.html#the-sign-test",
    "href": "Inferencial.html#the-sign-test",
    "title": "Inference",
    "section": "8.4 The sign test",
    "text": "8.4 The sign test\nImage that we don’t know the age difference, we only know if the husbands are older or not. We can follow here the same approach as with a binomial distribution, like the coin toss. In this specific case our null hypothesis \\(H_0\\) is that half of the husbands are older than their wifes (no difference). We assign labels to the results, for example 1 if the husband is older than the wife and 0 otherwise.\n\\[\nz=\\frac{sum\\ of\\ 1s -\\frac{n}{2}}{SE\\ of\\ sum} =\\frac{sum\\ of\\ 1s -\\frac{n}{2}}{\\sqrt{n}\\times\\sigma_{H_0}}\n\\]\nin our case we have 5 husbands being older than their wifes, so 5 1s and the standard deviation for our null hypothesis will be \\(\\frac{1}{2}\\) because we expect half the husbands to be older then their wives.\n\\[\nz= \\frac{5-\\frac{5}{2}}{\\sqrt{5}\\frac{1}{2}}= 2.24\n\\]\nnow we can find the \\(p\\) value using a table or software:\n\n\nCode\nz_score&lt;- 2.24\n# Calculate the p-value for a two-tailed test\np_value &lt;- 2 * (1 - pnorm(z_score))\np_value\n\n\n[1] 0.02509092\n\n\nwe can see that the result of this test is less significant than the test we did before, this is because we have less data to work with.\nIf we want to resolve the problem using software we can do the calculations:\n\n\nCode\n# Number of successes (husbands older than wives) \nsuccesses &lt;- 5  \n# Total number of pairs \nn &lt;- 5 \n# Z-test (Sign test approximation) \nz_score &lt;- (successes - 0.5 * n) / sqrt(0.25 * n) \nz_p_value &lt;- 2 * (1 - pnorm(z_score)) \nz_p_value\n\n\n[1] 0.02534732\n\n\nbut if we use the corresponding binomial test instead that would apply for this scenario, we get to quite a different result:\n\n\nCode\n# Number of successes (husbands older than wives) \nsuccesses &lt;- 5  \n# Total number of pairs \nn &lt;- 5  \n# Perform the binomial test \ntest_result &lt;- binom.test(successes, n, p = 0.5, alternative = \"two.sided\")  \n\nprint(test_result)\n\n\n\n    Exact binomial test\n\ndata:  successes and n\nnumber of successes = 5, number of trials = 5, p-value = 0.0625\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4781762 1.0000000\nsample estimates:\nprobability of success \n                     1 \n\n\n::: {.callout-orange, appearance=“simple”, icon=“false”}\nBoth the two-sample paired t-test and the sign test are used to compare paired data, but they are applied in different situations based on the assumptions and characteristics of the data.\nTwo-Sample Paired T-Test\nWhen to Use:\n\nNormal Distribution: The differences between the paired observations should be approximately normally distributed.\nInterval or Ratio Data: The data should be measured on an interval or ratio scale.\nParametric Test: This test is parametric, meaning it relies on assumptions about the distribution of the data.\n\nExample:\n\nComparing the blood pressure of patients before and after a treatment.\nMeasuring the weight of individuals before and after a diet program.\n\nSign test\n\nPaired Observations: When you have paired data (e.g., before and after measurements) and you want to test if there is a consistent difference between the pairs. For example, testing if a treatment has an effect by comparing measurements before and after the treatment.\nMedian Differences: When you want to test if the median of differences between pairs is zero. This is useful when the data does not meet the assumptions required for parametric tests like the paired t-test. It does not rely on assumptions about the distribution of the data.\nNon-Normal Data: When the data does not follow a normal distribution, making parametric tests inappropriate. The sign test does not assume any specific distribution for the data.\nOrdinal Data: When the data is ordinal (ranked) rather than interval or ratio. The sign test can handle data that can only be compared as greater than, less than, or equal to.\n\nExample Scenarios\n\nMedical Studies: Comparing the effectiveness of a treatment by measuring patient conditions before and after the treatment.\nQuality Control: Testing if a new manufacturing process consistently produces better results than the old process.\nBehavioral Studies: Comparing responses before and after an intervention. Comparing the number of days patients feel better before and after a new medication.\n\nSummary:\n\nUse a paired t-test when the differences between paired observations are normally distributed and you have interval or ratio data.\nUse a sign test when the data does not meet the normality assumption, is ordinal, or you prefer a non-parametric approach. :::"
  },
  {
    "objectID": "Inferencial.html#wilcoxon-rank-sum-test",
    "href": "Inferencial.html#wilcoxon-rank-sum-test",
    "title": "Inference",
    "section": "8.5 Wilcoxon Rank Sum Test",
    "text": "8.5 Wilcoxon Rank Sum Test\nWe learned how the sample mean and SD are susceptible to outliers. The t-test is based on these measures and is susceptible as well. The Wilcoxon rank test (equivalent to the Mann-Whitney test) provides an alternative. In the code below, we perform a t-test on data for which the null is true. However, we change one sum observation by mistake in each sample and the values incorrectly entered are different. Here we see that the t-test results in a small \\(p\\)-value, while the Wilcoxon test does not:\n\n\nCode\nset.seed(779) ##779 picked for illustration purposes\nN=25\nx&lt;- rnorm(N,0,1)\ny&lt;- rnorm(N,0,1)\n\n\nCreate outliers:\n\n\nCode\nx[1] &lt;- 5\nx[2] &lt;- 7\ncat(\"t-test pval:\",t.test(x,y)$p.value)\n\n\nt-test pval: 0.04439948\n\n\nCode\ncat(\"Wilcox test pval:\",wilcox.test(x,y)$p.value)\n\n\nWilcox test pval: 0.1310212\n\n\nThe basic idea is to 1) combine all the data, 2) turn the values into ranks 3) separate them back into their groups, and 4) compute the sum or average rank and perform a test.\n\n\nCode\nlibrary(rafalib)\nmypar(1,2)\n\nstripchart(list(x,y),vertical=TRUE,ylim=c(-7,7),ylab=\"Observations\",pch=21,bg=1)\nabline(h=0)\n\nxrank&lt;-rank(c(x,y))[seq(along=x)]\nyrank&lt;-rank(c(x,y))[-seq(along=x)]\n\nstripchart(list(xrank,yrank),vertical=TRUE,ylab=\"Ranks\",pch=21,bg=1,cex=1.25)\n\nws &lt;- sapply(x,function(z) rank(c(z,y))[1]-1)\ntext( rep(1.05,length(ws)), xrank, ws, cex=0.8)\n\n\n\n\n\nData from two populations with two outliers. The left plot shows the original data and the right plot shows their ranks. The numbers are the w values\n\n\n\n\nCode\nW &lt;-sum(ws) \n\n\nW is the sum of the ranks for the first group relative to the second group. We can compute an exact \\(p\\)-value for \\(W\\) based on combinatorics. We can also use the CLT since statistical theory tells us that this W is approximated by the normal distribution. We can construct a \\(z\\)-score as follows:\n\n\nCode\nn1&lt;-length(x);n2&lt;-length(y)\nZ &lt;- (mean(ws)-n2/2)/ sqrt(n2*(n1+n2+1)/12/n1)\nprint(Z)\n\n\n[1] 1.523124\n\n\nHere the Z is not large enough to give us a \\(p\\)-value less than 0.05. These are part of the calculations performed by the R function wilcox.test.\nwe are not going to get into mathematical detail about these calculations, but the formula for the \\(z\\) score here is \\[\nz=\\frac{U - \\frac{n_2}{2}}{\\sqrt{\\frac{n_2(n_1+n_2+1)}{12n_1}}}\n\\] and to perform this test in r we use: wilcox.test(x,y)"
  },
  {
    "objectID": "Inferencial.html#two-samples-of-binary-data",
    "href": "Inferencial.html#two-samples-of-binary-data",
    "title": "Inference",
    "section": "8.6 Two-samples of binary data",
    "text": "8.6 Two-samples of binary data\nTwo observed proportions for a single binary variable can be compared directly using the two-proportion \\(z\\)-confidence interval and two-proportion \\(z\\)-test. These apply when there are at least 5 observations of each type in each of the two groups. The formulas are relatively simple. For example a 95% confidence interval for the difference between proportions is\n\\[\n(p_2-p_1) = (\\hat p_2-\\hat p_1) \\pm 1.96 \\sqrt{\\frac{\\hat p_2(1-\\hat p_2)}{n_2}+\\frac{\\hat p_1(1-\\hat p_1)}{n_1}}\n\\]\nwhere \\(p_1\\) and \\(p_2\\) are the population proportions, \\(\\hat p_1\\) and \\(\\hat p_2\\) are the observed sample proportions and \\(n_1\\) and \\(n_2\\) are the sample sizes. R will use a improved version of this formula when computing the proportions.\nIn R we will use \\(prop.test(n_s , sample size)\\) where \\(n_s\\) is the number of successes.\n\nExamples:\nin the attrition dataset. Are the attrition proportions different for the two departments?\n\n\nCode\nkable(head(attrition1))%&gt;%\n  kable_styling(latex_options = \"scale_down\")%&gt;%\n  landscape()\n\n\n\n\n\nAge\nAttrition\nBusinessTravel\nDailyRate\nDepartment\nDistanceFromHome\nEducation\nEducationField\nEnvironmentSatisfaction\nGender\nHourlyRate\nJobInvolvement\nJobLevel\nJobRole\nJobSatisfaction\nMaritalStatus\nMonthlyIncome\nMonthlyRate\nNumCompaniesWorked\nOverTime\nPercentSalaryHike\nPerformanceRating\nRelationshipSatisfaction\nStockOptionLevel\nTotalWorkingYears\nTrainingTimesLastYear\nWorkLifeBalance\nYearsAtCompany\nYearsInCurrentRole\nYearsSinceLastPromotion\nYearsWithCurrManager\n\n\n\n\n41\nYes\nTravel_Rarely\n1102\nSales\n1\nCollege\nLife_Sciences\nMedium\nFemale\n94\nHigh\n2\nSales_Executive\nVery_High\nSingle\n5993\n19479\n8\nYes\n11\nExcellent\nLow\n0\n8\n0\nBad\n6\n4\n0\n5\n\n\n49\nNo\nTravel_Frequently\n279\nResearch_Development\n8\nBelow_College\nLife_Sciences\nHigh\nMale\n61\nMedium\n2\nResearch_Scientist\nMedium\nMarried\n5130\n24907\n1\nNo\n23\nOutstanding\nVery_High\n1\n10\n3\nBetter\n10\n7\n1\n7\n\n\n37\nYes\nTravel_Rarely\n1373\nResearch_Development\n2\nCollege\nOther\nVery_High\nMale\n92\nMedium\n1\nLaboratory_Technician\nHigh\nSingle\n2090\n2396\n6\nYes\n15\nExcellent\nMedium\n0\n7\n3\nBetter\n0\n0\n0\n0\n\n\n33\nNo\nTravel_Frequently\n1392\nResearch_Development\n3\nMaster\nLife_Sciences\nVery_High\nFemale\n56\nHigh\n1\nResearch_Scientist\nHigh\nMarried\n2909\n23159\n1\nYes\n11\nExcellent\nHigh\n0\n8\n3\nBetter\n8\n7\n3\n0\n\n\n27\nNo\nTravel_Rarely\n591\nResearch_Development\n2\nBelow_College\nMedical\nLow\nMale\n40\nHigh\n1\nLaboratory_Technician\nMedium\nMarried\n3468\n16632\n9\nNo\n12\nExcellent\nVery_High\n1\n6\n3\nBetter\n2\n2\n2\n2\n\n\n32\nNo\nTravel_Frequently\n1005\nResearch_Development\n2\nCollege\nLife_Sciences\nVery_High\nMale\n79\nHigh\n1\nLaboratory_Technician\nVery_High\nSingle\n3068\n11864\n0\nNo\n13\nExcellent\nHigh\n0\n8\n2\nGood\n7\n7\n3\n6\n\n\n\n\n\n\n\nCode\ntable(attrition1$Department)\n\n\n\nResearch_Development                Sales \n                 961                  446 \n\n\nCode\nt&lt;- table(attrition1$Department,\n      attrition1$Attrition)\nt\n\n\n                      \n                        No Yes\n  Research_Development 828 133\n  Sales                354  92\n\n\nCode\nyes_counts&lt;- as.numeric(t[,2])\nsampleSize &lt;-as.numeric(table(attrition1$Department))\n\ntestResult &lt;- prop.test(yes_counts, sampleSize)\ntestResult\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  yes_counts out of sampleSize\nX-squared = 9.9491, df = 1, p-value = 0.001609\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.11295996 -0.02280109\nsample estimates:\n   prop 1    prop 2 \n0.1383975 0.2062780 \n\n\nOur \\(p\\)-value 0.0016093 is very low which means we can reject the default null hypothesis (the difference in the two samples is 0). It is also giving us the proportions for the samples in the two different departments: 0.1383975, 0.206278 . And the confidence intervals is giving us the difference in the proportion of the two departments (as in prop1 - prop2). In this case being negative means that the second department has a higher attrition rate. The order of the variables is as entered in the vectors, so if yes_counts had Research_Development first and Sales second, prop1 will be for Research_Development and prop2 for Sales"
  },
  {
    "objectID": "Inferencial.html#two-samples-of-categorical-variables",
    "href": "Inferencial.html#two-samples-of-categorical-variables",
    "title": "Inference",
    "section": "8.7 Two-samples of categorical variables",
    "text": "8.7 Two-samples of categorical variables\nTwo samples of a single categorical variable can be compared with the \\(x^2-test\\) for homogeneity (Chi-squared). Under the hood, this is just a goodness-of-fit test of the hypothesis that the observed proportions in one of the samples are the same as those in the pooled sample.\n\n8.7.1 Testing homogeneity\nThe \\(\\chi^2\\) test of homogeneity test the null hypothesis that the distribution of a categorical variable is the same for several populations. It assumes that the samples are drawn independently within and across populations.\nSee how we can apply this logic to our Titanic survival data:\n\n\n\n\nFirst\nSecond\nThird\nCrew\n\n\n\n\nSurvived\n202\n118\n178\n215\n\n\nDied\n123\n167\n528\n698\n\n\n\nNote that in this case we are not sampling from a population. The data are not a random sample of the people on board, rather the data represent the whole population. Son in this case the chance process resulting in survival or death is not the sampling, but the result of random events occurring when looking for a way out of the ship, like getting into a life boat or into the water, being rescued from the water on time, etc. Then the 325 observations of first class passengers represent 325 independent draws from a probability histogram that gives a certain chance for survival. The 285 observations about second class passengers are drawn from the probability histogram for second class passengers, which may be different. The null hypothesis says that the probability of survival is the same for all four probability histograms. According to this hypothesis we can calculate the probability of survival by pooling all the data = \\(\\frac{713}{2229}=32\\%\\) with this number we can calculate the expected number of surviving passengers for each class:\n\n\n\nSurviving\nFirst\nSecond\nThird\nCrew\n\n\n\n\nObserved\n202\n118\n178\n215\n\n\nExpected\n104.0\n91.2\n225.8\n292.1\n\n\n\n\n\n\nDied\nFirst\nSecond\nThird\nCrew\n\n\n\n\nExpected\n221.0\n193.8\n480.1\n620.8\n\n\nObserved\n123\n167\n528\n698\n\n\n\nNow we can compare our chi statistic as we learned, using all the differences between expected and observed values:\n\\[\n\\chi^2 =\\sum_{categories} \\frac{(observed-expected)^2}{expected}\n\\]\n\\[\n=\\frac{(202-104)^2}{104}+\\frac{(123-221)^2}{221}+\\cdots =192\n\\]\nin this case our degrees of freedom are calculated like this: (4-1)*(2-1) = 3 where 4 is for the number of categories, and 2 is for the two rows of results we are dealing with (surviving and died)\nIn our case our \\(p\\) value will be extremely small, sugesting we should reject the null hypothesis that all ticket classes had the same posibility of survival.\n\n\nCode\nx &lt;- 192.2\ndf &lt;- 3\n\n# Calculate the p-value\np_value &lt;- pchisq(x, df, lower.tail = FALSE)\n\n# Print the p-value\np_value\n\n\n[1] 0.00000000000000000000000000000000000000002043428\n\n\nThe test in r:\n\n\nCode\n# Create the contingency table\ntitanic_data &lt;- matrix(c(202, 118, 178, 215, 123, 167, 528, 698), \n                       nrow = 2, \n                       byrow = TRUE,\n                       dimnames = list(Survival = c(\"Survived\", \"Died\"),\n                                       Class = c(\"First\", \"Second\", \"Third\", \"Crew\")))\n\n# Print the contingency table\nprint(titanic_data)\n\n\n          Class\nSurvival   First Second Third Crew\n  Survived   202    118   178  215\n  Died       123    167   528  698\n\n\nCode\n# Perform the chi-squared test of homogeneity\nchi_squared_test &lt;- chisq.test(titanic_data)\n\n# Print the test results\nprint(chi_squared_test)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  titanic_data\nX-squared = 192.34, df = 3, p-value &lt; 0.00000000000000022\n\n\n\nExercise:\nUse the substance_abuse data set to decide if the distribution of mental health diagnosis is the same for those with and without at least one psychiatric admission.\n\n\nCode\nas.data.frame(report(substance_abuse$MHDx))\n\n\nn_Entries | n_Obs | n_Missing | percentage_Missing\n--------------------------------------------------\n4.00      |   479 |         0 |               0.00\n\n\nCode\nas.data.frame(report(substance_abuse$PsychAdmit))\n\n\nMean |   SD | Median |  MAD |  Min |  Max | n_Obs | Skewness | Kurtosis | percentage_Missing\n--------------------------------------------------------------------------------------------\n0.61 | 0.81 |   0.00 | 0.00 | 0.00 | 5.00 |   479 |     1.41 |     2.39 |               0.00\n\n\nCode\nsubstance_abuse &lt;- substance_abuse %&gt;% mutate(previousAdmit = ifelse(substance_abuse$PsychAdmit == 0, FALSE, TRUE))\n\nt&lt;- table( substance_abuse$previousAdmit,substance_abuse$MHDx)\nt\n\n\n       \n        Anxiety Depression Psychosis Trauma\n  FALSE      84         85        36     59\n  TRUE       46         65        65     39\n\n\nCode\nchisq.test(t)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  t\nX-squared = 21.394, df = 3, p-value = 0.00008719\n\n\nIn this case according to our \\(p\\)-value we cannot say that patients that had at least one psychiatric admission in the past have the same proportion of diagnosis than patients without any admission.\n\nChi squared test for homogeneity is not saying anything specific about any of these diagnosis, it is simply saying that the distribution of these diagnosis between those two groups (admitted vs not admitted) is not the same.\nIs the distribution of SUDx the same for both men and women in our Substance Abuse data?\n\n\nCode\nhead(substance_abuse)\n\n\n# A tibble: 6 × 14\n  `Admission Date`    PPID  Program   Age Gender RaceEthnicity MHDx  SUDx  MedDx\n  &lt;dttm&gt;              &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 2022-01-13 00:00:00 A234… Interv…    34 F      Other         Depr… Alco… 2    \n2 2022-02-18 00:00:00 A232… Interv…    26 M      NonHispWhite  Trau… Opio… 0    \n3 2022-01-28 00:00:00 A259… Interv…    62 M      NativeAm      Depr… Opio… 0    \n4 2022-01-30 00:00:00 A353… Interv…    34 F      NonHispWhite  Depr… Alco… 0    \n5 2022-03-28 00:00:00 A302… UsualC…    46 M      NonHispBlack  Trau… Opio… 0    \n6 2022-02-17 00:00:00 A315… Interv…    51 M      NonHispWhite  Anxi… Opio… 1    \n# ℹ 5 more variables: PsychAdmit &lt;dbl&gt;, DLA1 &lt;dbl&gt;, DLA2 &lt;dbl&gt;,\n#   DLA_improvement &lt;dbl&gt;, previousAdmit &lt;lgl&gt;\n\n\nCode\nas.data.frame(report(substance_abuse$SUDx))\n\n\nn_Entries | n_Obs | n_Missing | percentage_Missing\n--------------------------------------------------\n4.00      |   479 |         0 |               0.00\n\n\n\n\nCode\nt &lt;- table(substance_abuse$Gender, \n           substance_abuse$SUDx)\nhead(t)\n\n\n   \n    Alcohol None Opioid Stimulant\n  F      68   36     58        17\n  M      99   66    104        31\n\n\nOur null hypothesis is that the distribution between gender is the same.\n\n\nCode\nresult &lt;- chisq.test(t)\nreport(result)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Pearson's Chi-squared test of independence between and suggests that the\neffect is statistically not significant, and tiny (chi2 = 1.24, p = 0.744;\nAdjusted Cramer's v = 0.00, 95% CI [0.00, 1.00])\n\n\nIn this case the \\(p\\)-value of the test is over 0.05 so that indicates that there is no significance difference by sex for the SUDx variable, so men and women are abusing the different categories of substances in the same proportions. The df in our results are the degrees of freedom that is the number of categories minus one.\nIn reality this is the same as doing a Goodness of fit test as we saw before, if we remember it was \\(chisq.test(p_s , p)\\) where \\(p_s\\) was the proportions of the different categories in our sample and \\(p\\) the proportion of the population we wanted to test against. Back to our example what we are measuring here is the proportion of women or men against the proportion of the full sample, that will tell us if there is a difference between men and women, so we can also write the test like this getting similar results:\n\n\nCode\nsudxProp &lt;- table(substance_abuse$SUDx)/sum(table(substance_abuse$SUDx))\n\nwomen &lt;- t[1,]\n\nresult&lt;- chisq.test(women, p= sudxProp)\nreport(result)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Chi-squared test for given probabilities / goodness of fit of women to a\ndistribution of [Alcohol: n=62.4070981210856, None: n=38.1169102296451, Opioid:\nn=60.5386221294363, Stimulant: n=17.937369519833] suggests that the effect is\nstatistically not significant, and tiny (chi2 = 0.77, p = 0.856; Fei = 0.02,\n95% CI [0.00, 1.00])\n\n\n\n\n8.7.2 Independence Testing of Categorical Variables\nWe want to answer this question: Is gender (M/F) related to voting preference (liberal/conservative)? Now we have two categorical variables: gender and voting preference. The null hypothesis is that the two variables are independent. The alternative hypothesis is that there is some kind of association.\nThe tool we are going to use is the chi-square test ( \\(x^2\\) ) for independence. This test can be used to test whether two categorical variables are independent or not. That is, whether knowledge about one provides information about the other. The math is exactly the same as for the chi-square test for homogeneity, hence, so is the sintax in most statistical packages, including R, but with a few things to bear in mind:\n\nTechnically, the null hypothesis of a \\(x^2\\) test for independence is that in the larger population, the probability of an observation being in any specific pair of categories is equal to the product of the probabilities of being in each of the individual categories.\nThis is another omnibus test: it says nothing about particular categories.\nLarger cell counts are better, as usual. Do not do this test if any of the counts are less than 5.\n\nWe are going to use our product_rating dataset to see the relationship between age groups and the product the user purchased.\nfirst we can just see the contingency table for those two variables\n\n\nCode\nfile &lt;- here::here(\"data\", \"product_ratings.xlsx\")\nproduct_ratings &lt;- read_excel(file)\nt &lt;- table(product_ratings$age,\n           product_ratings$product)\nt\n\n\n       \n          A   B   C\n  &lt;20   109 136 346\n  20-34 104 155 272\n  35-49 145 149 301\n  50-64  95  86 227\n  65+    75  47 118\n\n\n\n\nCode\nchisq.test(t)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  t\nX-squared = 30.81, df = 8, p-value = 0.0001519\n\n\nThe low \\(p\\)-value from this test is indicating that the theory that these two categorical variables are independent is implausible.\n\nChi-Squared is used for testing the independence of categorical variables. It compares observed frequencies in a contingency table to expected frequencies under independence.\n\n\nExercises\nReferring to the substance_abuse data set:\n\nIs there any evidence of an association between Substance Use Diagnosis and Mental Health Diagnosis among patients receiving an intervention?\n\n\n\nCode\ntreatment &lt;- filter(substance_abuse, \n                    Program == \"Intervention\")\nt&lt;- table (treatment$SUDx, treatment$MHDx)\nchisq.test(t)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  t\nX-squared = 7.6914, df = 9, p-value = 0.5655\n\n\nAccording to this result we cannot conclude that there is a relationship between these two variables.\n\nIs there any evidence of an association between SUDx and DLA_improvement among patients receiving an intervention?\n\n\n\nCode\nggplot(treatment, aes(x= SUDx, y = DLA_improvement))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nCode\ntestResult &lt;- aov( DLA_improvement ~ SUDx, data = treatment)\nsummary(testResult)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nSUDx          3  0.288 0.09606   2.981 0.0336 *\nResiduals   139  4.479 0.03222                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe difference is significant but not by large as the \\(p\\) value shows that in 3% of the cases we could observe these data differences in a sample from the a population where their category means is the same.\n\nIf we run a TukeyHSD test on these we can compare each pair individually and we find out that actually the only pair that shows a statistically significant difference is alcohol with opioids.\n\n\nCode\nTukeyHSD(testResult)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = DLA_improvement ~ SUDx, data = treatment)\n\n$SUDx\n                          diff         lwr          upr     p adj\nNone-Alcohol      -0.068704082 -0.19256733  0.055159168 0.4752566\nOpioid-Alcohol    -0.095893737 -0.18646925 -0.005318221 0.0334714\nStimulant-Alcohol -0.002079082 -0.13648847  0.132330308 0.9999765\nOpioid-None       -0.027189655 -0.14823533  0.093856018 0.9367288\nStimulant-None     0.066625000 -0.08994452  0.223194521 0.6861228\nStimulant-Opioid   0.093814655 -0.03800277  0.225632083 0.2543154\n\n\nIt is easy to confuse the testing for homogeneity and the testing for independence.\n\n\n8.7.3 Comparing the different uses of the chi-square test:\n\n\n\n\n\n\n\n\n\nsample\nresearch question\n\n\n\n\ngoodness of fit\nsingle categorical variable. one sample\nAre the counts of the different categories matching our expected results\n\n\nhomogeneity\nsingle categorical variable measured on several samples\nAre the groups homogeneous (have the same distribution of the categorical variable)\n\n\nindependence\ntwo categorical variables measured on one sample\nAre the two categorical variables independent?\n\n\n\nExamples:\n\nwe want to know if there is more births in different week days (Monday, Tuesday…). We record the week day of 300 births. What test should we use to know if there is a difference per day? –&gt; We would use chi-square test for goodness of fit.\nA food delivery start-up decides to advertise its service by placing ads on web pages. They wonder whether the percentage of viewers who click on the ad changes depending on how often the viewers were shown the ad. They randomly select 100 viewers from among those who were shown the add once, 135 from among those who were shown the add twice, and 150 from among those who were shown the ad three times. –&gt; We would use chi-square test of homogeneity.\nAn airline wants to find out whether there is a connection between the customer’s status in its frequent flyer program and the class of ticket that the customer buys. It samples 1,000 ticket records at random and for each ticket notes the status level (‘none’, ‘silver’, ‘gold’) and the ticket class (‘economy’, ‘business’,‘first’). –&gt; chi-square test of independence\nA county wants to check whether the racial composition of the teachers in the county corresponds to that of the population in the county. It samples 500 teachers at random and wants to compare that sample with the census numbers about the racial groups in that county. –&gt; chi-square test for goodness of fit"
  },
  {
    "objectID": "Inferencial.html#choosing-a-parametric-test-regression-comparison-or-correlation",
    "href": "Inferencial.html#choosing-a-parametric-test-regression-comparison-or-correlation",
    "title": "Inference",
    "section": "11.1 Choosing a parametric test: regression, comparison, or correlation",
    "text": "11.1 Choosing a parametric test: regression, comparison, or correlation\nParametric tests usually have stricter requirements than nonparametric tests, and are able to make stronger inferences from the data. They can only be conducted with data that adheres to the common assumptions of statistical tests.\nThe most common types of parametric test include regression tests, comparison tests, and correlation tests.\n\n11.1.1 Regression tests\nRegression tests look for cause-and-effect relationships. They can be used to estimate the effect of one or more continuous variables on another variable.\n\n\n11.1.2 Comparison tests\nComparison tests look for differences among group means. They can be used to test the effect of a categorical variable on the mean value of some other characteristic.\nT-tests are used when comparing the means of precisely two groups (e.g., the average heights of men and women). ANOVA and MANOVA tests are used when comparing the means of more than two groups (e.g., the average heights of children, teenagers, and adults).\n\n\n\n\n\n\n\n\n\n\nPredictor Variable\nOutcome Variable\nExample\n\n\n\n\nPaired t-test\nCategorical\n1 predictor\nQuantitative\nGroups come from the same population\nWhat is the effect of two different test prep programs on the average exam scores for students from the same class?\n(Predictor: test program)\n\n\nIndependent t-test\nCategorical\n1 predictor\nQuantitative\nGroups come from different populations\nWhat is the difference in average exam scores for students from two different schools?\n(Predictor: school A/B)\n\n\nANOVA\nCategorical\n1 or more predictor\nQuantitative\n1 outcome\nWhat is the difference in average pain levels among post-surgical patients given three different treatments?\n\n\nMANOVA\nCategorical\n1 or more predictor\nQuantitative\n2 or more outcomes\nWhat is the effect of flower species on petal length, petal width and stem length?\n\n\nPearson’s r\n2 continuous variables\n2 continuous variables\nHow are latitude and temperature related?"
  },
  {
    "objectID": "Inferencial.html#flowchart-for-parametric-tests",
    "href": "Inferencial.html#flowchart-for-parametric-tests",
    "title": "Inference",
    "section": "11.2 Flowchart for parametric tests",
    "text": "11.2 Flowchart for parametric tests\n\n\n\n\n\n\n\n\n11.2.1 Parametric Tests\nParametric tests are statistical tests that make certain assumptions about the parameters of the population distribution from which the sample is drawn. These assumptions typically include:\n\nNormality: The data follows a normal distribution.\nHomogeneity of variance: The variances within each group being compared are similar.\nIndependence: The observations are independent of each other.\n\nBecause of these assumptions, parametric tests are generally more powerful and can make stronger inferences about the population. Some common parametric tests include:\n\nT-tests: Used to compare the means of two groups.\nANOVA (Analysis of Variance): Used to compare the means of three or more groups.\nRegression Analysis: Used to examine the relationship between one or more predictor variables and an outcome variable.\n\n\n\n11.2.2 Non-Parametric Tests\nNon-parametric tests, on the other hand, do not make strict assumptions about the population parameters. They are more flexible and can be used when the assumptions of parametric tests are not met, such as when the data does not follow a normal distribution or when sample sizes are small. Non-parametric tests are also useful for ordinal data or data with outliers.\nSome common non-parametric tests include:\n\nMann-Whitney U Test: Used to compare the medians of two independent groups.\nWilcoxon Signed-Rank Test: Used to compare the medians of two related groups.\nKruskal-Wallis Test: Used to compare the medians of three or more independent groups.\nSpearman’s Rank Correlation: Used to assess the relationship between two ordinal variables.\n\n\n\n11.2.3 Key Differences\n\nAssumptions: Parametric tests assume specific population parameters, while non-parametric tests do not.\nData Type: Parametric tests are typically used for interval or ratio data, whereas non-parametric tests can be used for ordinal data or data that does not meet parametric assumptions.\nPower: Parametric tests are generally more powerful when their assumptions are met, meaning they are more likely to detect a true effect. Non-parametric tests are more robust and flexible but may be less powerful.\n\n\n\n11.2.4 Example Scenario\nImagine you want to compare the test scores of students from two different schools. If the test scores are normally distributed and the variances are similar, you would use a parametric test like the independent t-test. However, if the test scores are not normally distributed or have outliers, you might use a non-parametric test like the Mann-Whitney U Test."
  },
  {
    "objectID": "Inferencial.html#choosing-a-non-parametric-test",
    "href": "Inferencial.html#choosing-a-non-parametric-test",
    "title": "Inference",
    "section": "11.3 Choosing a non parametric test",
    "text": "11.3 Choosing a non parametric test\n\n\n\n\n\n\n\n\n\n\n\nPredictor variable\nOutcome variable\nUse in place of…\n\n\n\n\nSpearman’s r\nQuantitative\nQuantitative\nPearson’s r\n\n\nChi square test of independence\nCategorical\nCategorical\nPearson’s r\n\n\nSign test\nCategorical\nQuantitative\nOne-sample t-test\n\n\nKruskal–Wallis H\nCategorical\n3 or more groups\nQuantitative\nANOVA\n\n\nANOSIM\nCategorical\n3 or more groups\nQuantitative\n2 or more outcome variables\nMANOVA\n\n\nWilcoxon Rank-Sum test\nCategorical\n2 groups\nQuantitative\ngroups come from different populations\nIndependent t-test\n\n\nWilcoxon Signed-rank test\nCategorical\n2 groups\nQuantitative\ngroups come from the same population\nPaired t-test"
  },
  {
    "objectID": "MathBackground.html",
    "href": "MathBackground.html",
    "title": "Math background",
    "section": "",
    "text": "This is from the course Bayesian Statistics: From Concept to Data Analysis.\n\n1 1. Products and Exponents\nIn Lesson 1, we introduced the summation notation \\(\\sum_{i=1}^{n} x_i = x_1 + x_2 + \\ldots + x_n\\). Similarly, we can define product notation as:\n\\[\\prod_{i=1}^{n} x_i = x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_n\\] \\(\\prod\\) is the product symbol, which is similar to the summation symbol \\(\\sum\\) but it represents the product of a sequence of numbers rather than their sum.\nExample: We can re-write the factorial function as \\(n! = \\prod_{i=1}^{n} i\\) for \\(n \\geq 1\\).\nExample: Suppose \\(f\\) is a function which returns \\(f(x) = 3x + 1\\). Suppose \\(x\\) can take on only the discrete values \\(x \\in \\{-1, 2, 4\\}\\). Then:\n\\[\\prod_{x} f(x) = (3 \\cdot (-1) + 1) \\cdot (3 \\cdot 2 + 1) \\cdot (3 \\cdot 4 + 1)\\]\n\\[= (-2) \\cdot 7 \\cdot 13 = -182\\]\nExponents are of the form \\(a^x\\) where \\(a\\) (called the base) and \\(x\\) (called the exponent) are any real numbers. Recall that \\(a^0 = 1\\). Exponents have the following useful properties:\n\n\\(a^x \\cdot a^y = a^{x+y}\\)\n\\((a^x)^y = a^{xy}\\)\n\nNote that the first property requires that both terms have the same base \\(a\\). Thus we cannot simplify \\(a^m \\cdot b^n\\) if \\(a \\neq b\\).\nOne common base is the number \\(e\\), which is approximately equal to 2.7183. The function \\(e^x\\) is so common in mathematics that it has its own symbol \\(e^x = \\exp(x)\\). Because \\(e &gt; 0\\), we have \\(e^x &gt; 0\\) for all real numbers \\(x\\), although \\(\\lim_{x \\to -\\infty} e^x = 0\\).\nExample: Using Property 1 above, we have \\[\\prod_{i=1}^{5} e^x = e^x \\cdot e^x \\cdot e^x \\cdot e^x \\cdot e^x = \\exp(\\sum_{i=1}^{5} x) = e^{5x}\\]\n\n\n2 2. Natural Logarithm\nLogarithms can be defined as the inverse of exponential functions. That is, if \\(y = a^x\\) then \\(\\log_a(y) = x\\). The natural logarithm function has base \\(e\\) and is written without the subscript \\(\\log_e(y) = \\log(y)\\). Because \\(e^x &gt; 0\\) for all \\(x\\), \\(\\log(y)\\) is only defined for \\(y &gt; 0\\). We always have \\(\\exp(\\log(y)) = \\log(\\exp(y)) = y\\).\nWe can use the properties of exponents from the previous section to obtain some important properties of logarithms:\n\n\\(\\log(x \\cdot y) = \\log(x) + \\log(y)\\)\n\\(\\log\\left(\\frac{x}{y}\\right) = \\log(x) - \\log(y)\\)\n\\(\\log(x^b) = b \\log(x)\\)\n\\(\\log(1) = 0\\)\n\nBecause the natural logarithm is a monotonically increasing one-to-one function, finding the \\(x\\) which maximizes any (positive-valued function) \\(f(x)\\) is equivalent to maximizing \\(\\log(f(x))\\). This is useful because we often take derivatives to maximize functions. If \\(f(x)\\) has product terms, then \\(\\log(f(x))\\) will have summation terms, which are usually simpler when taking derivatives.\nExample: \\(\\log(10) = 2 \\log(5) - \\log(10) \\approx 0.916\\).\n\n\n3 3. Argmax\nWhen we want to maximize a function \\(f(x)\\), there are two things we may be interested in:\n\nThe value \\(f(x)\\) achieves when it is maximized, which we denote \\(\\max_x f(x)\\).\nThe \\(x\\)-value that results in maximizing \\(f(x)\\), which we denote \\(\\hat{x} = \\arg\\max_x f(x)\\).\n\nThus \\(\\max_x f(x) = f(\\hat{x})\\).\nExample: Suppose \\(f(x) = \\exp(-x^2)\\). Then \\(\\log(f(x)) = -x^2\\) which is maximized at \\(x = 0\\). Hence, \\(\\arg\\max_x f(x) = \\hat{x} = 0\\) and \\(\\max_x f(x) = f(0) = 1\\)."
  },
  {
    "objectID": "classification.html#logisticRegression",
    "href": "classification.html#logisticRegression",
    "title": "Classification Problems",
    "section": "1.1 Logistic regression",
    "text": "1.1 Logistic regression\nThe formula for the logistic regression is: \\[\n{p}(X) = \\frac{e^{(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n)}}{1 + e^{(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n)}}\n\\tag{3}\\]\nThis reads as \\(p\\) of X for the probability that y is 1 given X\n-p(x) is the predicted probability of the dependent variable being 1 (or positive outcome) given the predictors \\(x_1,x_2,\\dots,x_n\\) - \\(\\beta_0\\) is the intercept of the linear model - \\(\\beta_1,\\beta_2\\dots,\\beta_n\\) are the coefficients for the predictors. - \\(e\\) is a mathematical constant (2.71828) (Euler’s number) The result will be always between 0 and .\nThis formula can also be written as:\n\\[\nlog\\left( \\frac{p(X)}{1-p(X)} \\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_nX_n\n\\tag{4}\\]\nand it is called log odds or logit transformacion of p(X)\n\n1.1.1 Maximum Likelihood Estimation (MLE)\nMaximum Likelihood Estimation (MLE) is used to find the parameters (coefficients) in logistic regression. Here’s the lowdown: MLE aims to find the parameter values that make the observed data most probable. It identifies the set of parameters (in our case, the regression coefficients) that maximize the likelihood function.\nIn Logistic Regression: For binary outcomes (0 or 1), logistic regression models the probability of the outcome. The likelihood Function: for logistic regression is: \\[ L(\\beta) = \\prod_{i=1}^n \\hat{p}(x_i)^{y_i} [1 - \\hat{p}(x_i)]^{1 - y_i}\n\\tag{5}\\]\nWhere: \\(\\hat{p}(xi)\\) is the predicted probability for the i-th observation. \\(y_i\\) is the observed outcome (0 or 1) for the i-th observation.\nLog-Likelihood: For computational simplicity, we usually work with the natural logarithm of the likelihood function (log-likelihood): \\[\\ell(\\beta) = \\sum_{i=1}^n y_i \\log(\\hat{p}(x_i)) + (1 - y_i) \\log(1 - \\hat{p}(x_i))\\]\nOptimization: The goal is to find the parameter values (β0,β1,β2,…,βn) that maximize the log-likelihood function.\nThis is usually done using numerical optimization methods since there’s no closed-form solution.\nIntuition: MLE in logistic regression finds the best-fit model that maximizes the probability of observing the given data.\nIt adjusts the coefficients to maximize the match between the predicted probabilities and the actual outcomes.\nIn r we use the function glm()\nThe package ISLR2 has a dataset ‘Default’ and we want to use to see what variables (student, balance(credit balance), and income) are relevant to predict if a person is going to default their credit card payments.\nFirst we are going to see each variable independently:\n\n\nCode\nhead(Default)\n\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n\nCode\nglm.income&lt;- glm(default ~ income, data=Default, family = binomial)\nsummary(glm.income)\n\n\n\nCall:\nglm(formula = default ~ income, family = binomial, data = Default)\n\nCoefficients:\n                Estimate   Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -3.094149153  0.146256899 -21.156 &lt;0.0000000000000002 ***\nincome      -0.000008353  0.000004207  -1.985              0.0471 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 2916.7  on 9998  degrees of freedom\nAIC: 2920.7\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\nglm.student&lt;- glm(default ~ student, data=Default, family = binomial)\nsummary(glm.student)\n\n\n\nCall:\nglm(formula = default ~ student, family = binomial, data = Default)\n\nCoefficients:\n            Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept) -3.50413    0.07071  -49.55 &lt; 0.0000000000000002 ***\nstudentYes   0.40489    0.11502    3.52             0.000431 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 2908.7  on 9998  degrees of freedom\nAIC: 2912.7\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\nglm.balance&lt;- glm(default ~ balance, data=Default, family = binomial)\nsummary(glm.balance)\n\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = Default)\n\nCoefficients:\n               Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -10.6513306   0.3611574  -29.49 &lt;0.0000000000000002 ***\nbalance       0.0054989   0.0002204   24.95 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n1.1.2 Interpreting the results:\nFor the Default data, estimated coefficients of the logistic regression model that predicts the probability of default using balance. A one-unit increase in balance is associated with an increase in the log odds of default by 0.0055 units. Many aspects of the logistic regression output are similar to the linear regression. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic plays the same role as the t-statistic in the linear regression output: the z-statistic associated with \\(\\beta_1\\) is equal to \\(\\hat{\\beta_1}/SE(\\hat{\\beta_1})\\), and so a large (absolute) value of the z-statistic indicates evidence against the null hypothesis \\(H_0:\\beta_1=0\\). This null hypothesis implies that the probability of default does not depend on balance. Since the $p$-value associated with balance is tiny, we can reject the null hypothesis. The intercept is typically not of interest. ### Making predictions Once the coefficients have been estimated, we can compute the probability of default for any given credit balance. we will use the logistic regression formula we already saw (Equation 3)\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta_0} + \\hat{\\beta_1} X_1}}{1 + e^{\\hat{\\beta_0} + \\hat{\\beta_1} X_1 }}\n\\] we just plug in the values from our glm, for example for a balance of 1000: \\[\n\\hat{p}(X) = \\frac{e^{-10.65 + 0.0055 \\times 1000}}{1 + e^{-10.65 + 0.0055 \\times 1000}} =0.00576\n\\] using R we are going to calculate the probability of default for a balance of 1000 and 2000 USD:\n\n\nCode\nnew_data&lt;- data.frame(balance= c(1000,2000))\nglm.probs &lt;- predict(glm.balance,new_data, type = \"response\")\nglm.probs\n\n\n          1           2 \n0.005752145 0.585769370 \n\n\nWe can use qualitative predictors the same way, we will now see the student predictor: Student: Yes \\[\n\\hat{p}(X) = \\frac{e^{-3.50 + 0.4049 \\times 1}}{1 + e^{-3.50 + 0.4049 \\times 1}} =0.0431\n\\] Student:No \\[\n\\hat{p}(X) = \\frac{e^{-3.50 + 0.4049 \\times 0}}{1 + e^{-3.50 + 0.4049 \\times 0}} =0.0292\n\\] In this case this indicates that students tend to have higher default probabilities than non-students.\n\n\nCode\nnew_data= data.frame(student= as.factor(c('Yes','No')))\nglm.probs &lt;- predict(glm.student,new_data, type = \"response\")\nglm.probs\n\n\n         1          2 \n0.04313859 0.02919501 \n\n\n\n\n1.1.3 Multiple Logistic Regression.\nSo far we have just considered one predictor at a time, but like in linear models, we can use them combined:\n\n\nCode\nglm.fit&lt;- glm(default ~ income+balance+student, data=Default, family = binomial)\nsummary(glm.fit)\n\n\n\nCall:\nglm(formula = default ~ income + balance + student, family = binomial, \n    data = Default)\n\nCoefficients:\n                 Estimate    Std. Error z value             Pr(&gt;|z|)    \n(Intercept) -10.869045196   0.492255516 -22.080 &lt; 0.0000000000000002 ***\nincome        0.000003033   0.000008203   0.370              0.71152    \nbalance       0.005736505   0.000231895  24.738 &lt; 0.0000000000000002 ***\nstudentYes   -0.646775807   0.236252529  -2.738              0.00619 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1571.5  on 9996  degrees of freedom\nAIC: 1579.5\n\nNumber of Fisher Scoring iterations: 8\n\n\nIf we pay attention now to the coefficient of student, it is negative, indicating that students are less likely to default than non-students. This is not what we saw when we looked at the model with only the student variable. This is because The variables student and balance are correlated. Students tend to hold higher levels of debt, which is in turn associated with higher probability of default. In other words, students are more likely to have large credit card balances, which, as we know, tend to be associated with high default rates. Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall, students tend to default at a higher rate than non-students.\nThis simple example illustrates the dangers and subtleties associated with performing regressions involving only a single predictor when other predictors may also be relevant. As in the linear regression setting, the results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among the predictors. In general, the phenomenon seen here is known as confounding.\n\n\n\n\n\n\n\n\n\n\nMisclassification rate\nHow many values does our model get right and how many get wrong? for that we can use a confusion matrix :\n\n\nCode\n# Get predicted probabilities\nglm.probs &lt;- predict(glm.fit, type = \"response\")\n\n# Convert probabilities to binary predictions (threshold = 0.5)\nglm.pred &lt;- ifelse(glm.probs &gt; 0.5, \"Yes\", \"No\")\n\n# Print confusion matrix\nprint('Confusion Matrix')\n\n\n[1] \"Confusion Matrix\"\n\n\nCode\ntable(Default$default, glm.pred)\n\n\n     glm.pred\n        No  Yes\n  No  9627   40\n  Yes  228  105\n\n\nCode\ncat('misclassification rate: ', (228+40)/10000)\n\n\nmisclassification rate:  0.0268\n\n\nBut this values are over estimated because we are using the same data that we used to train our model, so this is the training error. As we already know, to get the real fit of our model we should train our model with a subset of data and test it with unseen data.\n\n\nNull rate\nAlthough this misclassification rate might seem very small, we need to take into consideration, that the data has a majority of ‘No’ in the response, so if we would fit every single value to ‘No’ without using any model, we would only get 333/10000 errors, so only 3.33% misclassification. This is called the null rate\nAnother way of looking at it would be seeing what percentage of No’s and Yes’s we have missclassified.\nFor No’s we have missclassified 40 out of 9667 = 0.4% For Yes’s we have missclassified 228 out of 333 = 68.5%\n\n\nVisualizing the relationship between variables\nA way of checking the relationship between different predictors in our model is to visualize a scatter matrix using pairs(). We will use the SAheart dataset for this:\nIn this case we are not trying to predict the probability of getting a heart disease, we are going to assess the risk factors\n\n\nCode\n# Load the SAheart dataset\nlibrary(bestglm)\ndata(SAheart)\n\n# Select columns to include in the pairs plot (excluding 'typea')\nSAheart_selected &lt;- SAheart %&gt;%\n  dplyr::select(-c(typea)) %&gt;%\n  mutate(famhist = ifelse(famhist == \"Present\", 1, 0))\n\n# Define colors based on 'chd'\ncolor_chd &lt;- ifelse(SAheart_selected$chd == 1, \"darkred\", \"darkblue\")\n\n# Exclude the 'chd' column from the pair plot\nSAheart_selected &lt;- SAheart_selected %&gt;%\n  dplyr::select(-chd)\n\n# Create the scatter plot matrix\npairs(SAheart_selected, \n      col = color_chd, \n      pch = 19,           # Use solid points\n      cex = 0.5,          # Reduce point size\n      labels = colnames(SAheart_selected),\n      cex.labels = 0.9,   # Reduce label text size\n      main = \"Scatter-plot Matrix\",\n      oma = c(2, 2, 2, 2)) # Reduce outer margins\n\n\n\n\n\nto fit a model for this data:\n\n\nCode\nheartfit &lt;- glm(chd~.,data=SAheart, family=binomial)\nsummary(heartfit)\n\n\n\nCall:\nglm(formula = chd ~ ., family = binomial, data = SAheart)\n\nCoefficients:\n                 Estimate Std. Error z value   Pr(&gt;|z|)    \n(Intercept)    -6.1507209  1.3082600  -4.701 0.00000258 ***\nsbp             0.0065040  0.0057304   1.135   0.256374    \ntobacco         0.0793764  0.0266028   2.984   0.002847 ** \nldl             0.1739239  0.0596617   2.915   0.003555 ** \nadiposity       0.0185866  0.0292894   0.635   0.525700    \nfamhistPresent  0.9253704  0.2278940   4.061 0.00004896 ***\ntypea           0.0395950  0.0123202   3.214   0.001310 ** \nobesity        -0.0629099  0.0442477  -1.422   0.155095    \nalcohol         0.0001217  0.0044832   0.027   0.978350    \nage             0.0452253  0.0121298   3.728   0.000193 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 596.11  on 461  degrees of freedom\nResidual deviance: 472.14  on 452  degrees of freedom\nAIC: 492.14\n\nNumber of Fisher Scoring iterations: 5\n\n\nInterestingly, obesity and alcohol does not show as significant and this is due to correlation.\n\nExample:\nWe will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, lagone through lagfive. We have also recorded volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and direction (whether the market was Up or Down on this date). Our goal is to predict direction (a qualitative response) using the other features.\n\n\nCode\nhead(Smarket)\n\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\nCode\npairs(Smarket[2:7], col = Smarket$Direction, pch = 20, cex = 0.5)\n\n\n\n\n\n\n\n\n\nThe pairs plot seem to not show a lot of correlation.\n\n\nCode\nglm.fits &lt;- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)\n\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe ’contrast()` function tells us what value of Direction is 0 and which one is 1:\n\n\nCode\ncontrasts(Smarket$Direction)\n\n\n     Up\nDown  0\nUp    1\n\n\nThe negative predictor in lag1, lag2 suggest that if the market had a positive return yesterday, then it is less likely to go up today, however, the \\(p\\)-value is large, so there is no clear association.\nThe null deviance and the residual deviance are very similar, which indicates that the model does not perform much better than just using the average.\nThe predict() function can be used to predict the probability that the market will go up, given the values of the predictors. The type=\"response\" option tells R to output probabilities of the form \\(P(Y=1|X)\\), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\n\n\nCode\nglm.probs &lt;- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\n\nThe first thing that calls our attention is that the values are very close to 50%, so the predictors are not very strong. In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than \\(0.5\\).\n\n\nCode\nglm.pred &lt;- rep(\"Down\", 1250)\nglm.pred[glm.probs &gt; .5] = \"Up\"\n\n\nThe first command creates a vector of 1,250 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds \\(0.5\\). Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\n\nCode\ntable(glm.pred, Smarket$Direction)\n\n\n        \nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n\nCode\n(507 + 145) / 1250\n\n\n[1] 0.5216\n\n\nCode\nmean(glm.pred == Smarket$Direction)\n\n\n[1] 0.5216\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on \\(507\\) days and that it would go down on \\(145\\) days, for a total of \\(507+145 = 652\\) correct predictions. The mean() function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market \\(52.2\\)% of the time.\nAt first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of \\(1,250\\) observations. In other words, \\(100\\%-52.2\\%=47.8\\%\\), is the training error rate. As we have seen previously, the training error rate is often overly optimistic -it tends to underestimate the test error rate-. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model’s performance not on the data that we used to fit the model, but rather on days in the future for which the market’s movements are unknown. To implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005:\n\n\nCode\ntrain &lt;- (Smarket$Year &lt; 2005)\nSmarket.2005 &lt;- Smarket[!train, ]\ndim(Smarket.2005)\n\n\n[1] 252   9\n\n\nCode\nDirection.2005 &lt;- Smarket$Direction[!train]\n\n\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.\n\n\nCode\nglm.fits &lt;- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\nglm.probs &lt;- predict(glm.fits, Smarket.2005,\n    type = \"response\")\n\n\nNotice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\n\nCode\nglm.pred &lt;- rep(\"Down\", 252)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\n\nCode\nmean(glm.pred == Direction.2005)\n\n\n[1] 0.4801587\n\n\nand our test set error rate:\n\n\nCode\nmean(glm.pred != Direction.2005)\n\n\n[1] 0.5198413\n\n\nSuppose that we want to predict the returns associated with particular values. We do this using the predict() function.\n\n\nCode\npredict(glm.fits,\n    newdata = data.frame(Lag1 = c(1.2, 1.5),  Lag2 = c(1.1, -0.8),  Lag3 = c(1.1, -0.8), Lag4 = c(1.1, -0.8), Lag5 = c(1.1, -0.8), Volume= c(10,100), type = \"response\"\n  ))\n\n\n         1          2 \n -1.076397 -11.486641 \n\n\n\n\n\n\n1.1.4 Case-control sampling and logistic regression.\nWe know because medicine tells us, that the risk of heart disease in this type of population of South Africans is actually 5%. But in our dataset we used 160 cases (people with heart disease) and 302 controls. This results in an estimated proportion of \\(\\tilde{\\pi} = 0.35\\), yet the prevalence of the heart disease is \\(\\pi=0.05\\)\nWe can still use these data set to fit our model, and the model will be correct, but the constant term will be incorrect. We can correct the constant using a simple transformation. \\[\n\\hat{\\beta_0^*}=\\hat{\\beta_0}+log\\frac{\\pi}{1-\\pi}-log\\frac{\\tilde{\\pi}}{1-\\tilde{\\pi}}\n\\] This formula is a re-calibration or re-centering of the logistic regression intercept \\(\\hat{\\beta_0}\\) where: - \\(\\hat{\\beta_0}\\) is the original estimated intercept. - \\(\\hat{\\beta_0^*}\\) is the adjusted intercept after re-calibration. - \\(\\pi\\) is the baseline probability in the population. - \\(\\tilde{\\pi}\\) is the target probability or prevalence rate you are adjusting to.\nThis is an approach often followed in epidemiology because the cases that we want to study are rare, so we want to take them all, up to five or six times that number of controls is sufficient. Another example is in advertising, where the click-through ratio for an add is very low, we need usually hundreds or thousands of people that saw the add but did not click on it, for just a few clicks, so following this approach we can reduce our dataset and not include all the control data.\n\n\n1.1.5 Multinomial regression\nSo far we have only considered two classes in the response, but we can extend the model to k number of classes: \\[\nPr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}\n\\tag{6}\\]\nTo do this, we first select a single class to serve as a baseline. The decision to treat one specific class as a baseline is unimportant. The coefficient estimates will differ between two fitted models to different baselines variables due to the differing choice of baseline, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same.\nAlthough we can use multinomial regression for k values, it has its limitations: - When the classes are well separated, the parameter estimates for the logistic regression model are surprisingly unstable. - If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model that we will see next is more stable than the logistic regression model."
  },
  {
    "objectID": "classification.html#DiscriminantAnalysis",
    "href": "classification.html#DiscriminantAnalysis",
    "title": "Classification Problems",
    "section": "1.2 Discriminant Analysis.",
    "text": "1.2 Discriminant Analysis.\n\n1.2.1 Linear Discriminant Analysis(LDA)\nLinear discriminant analysis is popular when we have more than two response classes.\nWe can model de distribution of X in each of the classes separately and then use Bayes theorem to flip things around to get the probability of Y given X. When we use normal distributions for each class, this leads to linear or quadratic discriminant analysis.\n\nBayes theorem for classification.\n\\[\nP(Y=k \\mid X=x) = \\frac{Pr(X=x \\mid Y=k) \\times  Pr(Y=k)}{Pr(X=x)}\n\\tag{7}\\]\n\nPosterior Probability $P(Y=k X=x) $ this is the probability that the target variable Y is in class k given the observed data X=x\nLikelihood: \\(Pr(X=x \\mid Y=k)\\) this is the probability of observing the data X=x given that the variable Y is in class k.\nPrior Probability: \\(Pr(Y=k)\\) This is the initial probability of class k before observing the data, it’s your prior belief about the class distribution.\nMarginal Likelihood: \\(Pr(X=x)\\) this is the overall probability of observing the data X=x across all classes.\n\n\n\n\n1.2.2 k-Nearest Neighbour classification\nIn theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. The k-nearest neighbor classification uses distance to try to classify the predictions: Given a positive integer K and a test observation \\(x_0\\), the KNN classifier first identifies the K points in the training data that are closest to x0, represented by \\(N_0\\). It then estimates the conditional probability for class \\(j\\) as the fraction of points in \\(N_0\\) whose response values equal \\(j\\): \\[\nP(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\epsilon N_0}I(y_i=j)\n\\tag{8}\\]\nThe algorithm relies on a distance metric (commonly Euclidean distance) to measure the similarity between instances. The distance between two points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) in a 2D space is calculated as: \\(d=(x_2-x_1)^2+(y_2-y_1)^2\\)\nChoosing k: The parameter \\(k\\) represents the number of nearest neighbors to consider when making a prediction. For example, if \\(k = 3\\), the algorithm looks at the 3 closest training instances to the query point.\nFor classification, k-NN uses a majority voting mechanism. The class label that appears most frequently among the \\(k\\) nearest neighbors is assigned to the query point.\nk-NN is a non-parametric algorithm, meaning it makes no assumptions about the underlying data distribution. This makes it flexible but also computationally intensive, especially with large datasets.\n\nExample:\nWe will need the library ‘class’ and work with the IRIS dataset. First we divide the dataset into training and test subsets and use the knn function to get the predicted values for the test dataset. We can use a confusion_matrix to see the successes and failures of the model guessing the species.\n\n\nCode\nset.seed(1)\nsample &lt;- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7, 0.3))\ntrain &lt;- iris[sample,1:4 ]\ntest &lt;- iris[!sample,1:4 ]\nspecies&lt;- iris[sample,5]\nspecietest &lt;- iris[!sample,5]\nknn.pred&lt;- class::knn(train,test,species,k=1)\ntable(knn.pred,specietest)\n\n\n            specietest\nknn.pred     setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         16         1\n  virginica       0          0        11\n\n\nCode\nmean(knn.pred ==specietest)\n\n\n[1] 0.9772727\n\n\n\n\nLinear Discriminant Analysis for one variable.\nTo classify Y at the value X=x we only need to see which of the \\(p_x(x)\\) is the largest. For this we use the discriminant score\n\\[\n\\delta_k(x) = x \\frac{\\mu_k}{\\sigma^{2}} - \\frac{\\mu_k^2}{2\\sigma^{2}}+ \\log(\\pi_k)\n\\tag{9}\\]\nWhere \\(\\pi\\) is the prior probability\nNote that \\(\\delta_k(x)\\) is a linear function of x\nIf there are k=2 classes and the prior is the same for both (0.5) then the formula simplifies to: \\[\nx = \\frac{\\mu_1+\\mu_2}{2}\n\\] Below we have a visualization for two classes of Y and its probabilities given x when the prior is the same for both classes. The dashed line shows the decision boundary, anything on the left of the line will be classified as class ‘blue’ and anything on the right as class ‘red’\n\n\n\nWhen the priors are different, we have take them into consideration and compare \\(\\pi_k f_x(x)\\)\nWe usually don’t know the prior, so we have to estimate it looking at the data. The estimated prior in each class will be the total number of elements in that class divided by the total number of cases \\[\n\\hat{\\pi_k}=\\frac{n_k}{n}\n\\]\nThe mean of each class is calculated normally. The estimated variance is a bit more complex because it is not using the variance of each class but the pooled variance estimate\n\\[\n\\hat{\\sigma^2}= \\frac{1}{n-K} \\sum^K_{k=1} \\sum_{y_i=k}(x_1-\\hat{\\mu_k})^2= \\sum^K_{k=1}\\frac{n_k-1}{n-K}\\times\\hat{\\sigma^2_k}\n\\tag{10}\\]\nWhere \\(\\hat{\\sigma^2_k}\\) is the usual formula for the estimated variance in the kth class. \\[\n\\frac{1}{n_k-1}\\sum^N_{i:y_i=k}(x_i-\\hat{\\mu_k})^ 2\n\\] so basically what we are doing in the pooled variance is to calculate the variance for each of the classes and then do a weighted average of them.\n\n\n\n1.2.3 Multivariate linear discrimination.\nThe discriminant score function for multivariate takes this quite complicated form: \\[\n\\delta_k(x) = x^T\\Sigma^{-1} \\mu_k -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+ \\log(\\pi_k)\n\\tag{11}\\]\nOnce we have the estimates for the different parameters \\(\\hat{\\delta}_k(x)\\) we can turn these into estimates for class probabilities: \\[\n\\hat{P_{r}}(Y = k | X = x) = \\frac{e^{\\hat{\\delta_k} (x)}}{\\sum_{l=1}^{K} e^{\\hat{\\delta_l}(x)}}\n\\tag{12}\\]\nWith these estimates we will classify Y=k to the largest probability for the value of x we are interested in.\nWe can run a classification over the Default dataset that we already used in multiple logistic regression. To do so we use the function LDA from library MASS, but first we are going to check the levels of the response variable to know which value the reference (the first one):\n\n\nCode\nlevels(Default$default)\n\n\n[1] \"No\"  \"Yes\"\n\n\nSo our reference level is default= ‘No’\n\n\nCode\nLDA.fit = lda( default ~ income+balance+student, data=Default, )\nLDA.fit\n\n\nCall:\nlda(default ~ income + balance + student, data = Default, )\n\nPrior probabilities of groups:\n    No    Yes \n0.9667 0.0333 \n\nGroup means:\n      income   balance studentYes\nNo  33566.17  803.9438  0.2914037\nYes 32089.15 1747.8217  0.3813814\n\nCoefficients of linear discriminants:\n                      LD1\nincome      0.00000336731\nbalance     0.00224354109\nstudentYes -0.17466314059\n\n\nCode\nLDA.fit.p = predict(LDA.fit, newdata=Default[,2:4])\nLDA.fit.Classes = predict(LDA.fit, newdata=Default[,2:4])$class\n\n\nHow to interpret the results of the LDA\n\nPrior Probabilities of Groups: These are the probabilities assigned to each class (No and Yes) before observing any data, based on the proportion of each class in the training data.\n\nNo: 0.9667 (96.67% of the data is in the “No” category)\nYes: 0.0333 (3.33% of the data is in the “Yes” category)\n\nGroup Means: These are the mean values of each predictor variable for each class.\nCoefficients of Linear Discriminants: These coefficients are used to form the linear discriminant functions (LD1 in this case). The linear discriminant function combines the predictor variables to maximize the separation between the classes. Larger absolute values of coefficients indicate more significant contributions to the discriminant function.\n\nThe negative coefficient of the Student(Yes) indicates that being a Student reduces the possibility of defaulting = ‘yes’.\nIn a two-class LDA model with reference level A, a positive coefficient for a predictor indicates that an increase in that predictor increases the discriminant score. A higher discriminant score makes it more likely for the observation to be classified as B rather than A.\nConversely, a negative coefficient indicates that an increase in the predictor decreases the discriminant score, making it more likely for the observation to be classified as A.\nLD1 is the Linear Disciminative Function LDFs are essentially the axes that best separate the different classes in your data. They are linear combinations of your original variables. (@#eq-discriminantScoreMulti)\nIn this case we only have a LDF (LD1), but with more classess we will get more. The number of LDFs is always one less than the number of classes.\nHistogram of Linear Discriminants: We can also use plots to see the overlapping of our groups: This plot shows a histogram of the discriminant function values for each class. The histograms should ideally show distinct peaks for each class, indicating good separation.\nOverlapping histograms suggest some misclassification, as the discriminant function values are not clearly separated.\n\n\nCode\nplot(LDA.fit)\n\n\n\n\n\n\n\n\n\ncreate a confusion matrix:\n\n\nCode\ntable(Default$default,LDA.fit.Classes)\n\n\n     LDA.fit.Classes\n        No  Yes\n  No  9645   22\n  Yes  254   79\n\n\nAnd as before, we can use the confusion matrix to calculate our misclassification rate:\n\n\nCode\ncat('misclassification rate: ', (254+22)/10000)\n\n\nmisclassification rate:  0.0276\n\n\nFor No’s we have missclassified 22 out of 9667 = 0.2% this is our False positive rate For Yes’s we have missclassified 254 out of 333 = 76.3% this is our False negative rate\nWe can also use the function confusionMatrix() from the package caret to create and analyse the results. It takes two parameters the first one is the predicted values and the second one the real values (reference)\n\n\nCode\n# Create confusion matrix\nconfusion_matrix &lt;- caret::confusionMatrix(data=factor(LDA.fit.Classes), reference=Default$default)\n\nprint(confusion_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  9645  254\n       Yes   22   79\n                                               \n               Accuracy : 0.9724               \n                 95% CI : (0.969, 0.9755)      \n    No Information Rate : 0.9667               \n    P-Value [Acc &gt; NIR] : 0.0006128            \n                                               \n                  Kappa : 0.354                \n                                               \n Mcnemar's Test P-Value : &lt; 0.00000000000000022\n                                               \n            Sensitivity : 0.9977               \n            Specificity : 0.2372               \n         Pos Pred Value : 0.9743               \n         Neg Pred Value : 0.7822               \n             Prevalence : 0.9667               \n         Detection Rate : 0.9645               \n   Detection Prevalence : 0.9899               \n      Balanced Accuracy : 0.6175               \n                                               \n       'Positive' Class : No                   \n                                               \n\n\nInterpreting the results of the confusion matrix \n\nAccuracy: It measures the overall correctness of the model. 97.24% of the predictions are correct.\n95% CI (Confidence Interval): It provides a range within which the true accuracy lies with 95% confidence.\nNo Information Rate (NIR):It represents the accuracy if the model always predicted the majority class (No).\n\\(p\\)-value [Acc &gt; NIR]: This \\(p\\)-value tests if the accuracy is significantly better than the NIR.\nKappa: It measures the agreement between the observed and predicted classifications, adjusted for chance. Values close to 1 indicate better agreement.\nMcnemar’s Test \\(p\\)-value: Tests if there is a significant difference between the number of false positives and false negatives.\n\nClass-Specific Metrics:\n\nSensitivity (Recall):Proportion of actual positives correctly identified.\nSpecificity: Proportion of actual negatives correctly identified.\nPositive Predictive Value (Precision): Proportion of positive results that are true positives.\nNegative Predictive Value: Proportion of negative results that are true negatives.\nPrevalence: Proportion of the actual positive cases in the dataset.\nDetection Rate: Proportion of actual positives correctly identified by the model.\nDetection Prevalence:Proportion of the predicted positives out of the total predictions.\nBalanced Accuracy: It’s the average of sensitivity and specificity, providing a balanced measure for imbalanced datasets.\n\nSummary: The model has a high overall accuracy (97.24%), but its specificity (23.72%) is low, indicating it struggles to correctly identify the negative class (Yes).\nThe high sensitivity (99.77%) suggests it’s good at identifying the majority class (No), but at the cost of false positives.\nAdjusting the model or the decision threshold can help to balance sensitivity and specificity better.\nIn order to reduce the false negative rate we can take change the threshold at which we classify one class to yes or no. By default we have been using equal priors for all variables, but we can change this to put more weight on the minority class (default = yes). This swill increase the false positive rate, so we need to evaluate for our data what type of errors we prefer depending on the risk of making the wrong classifications.\nROC Curve: We can use A ROC plot and AUC (Area under the curve) to summarize the overall performance of our model. A higher AUC is desired.\n\n\nCode\n# Fit the LDA model\nLDA.fit &lt;- lda(default ~ income + balance + student, data = Default)\n\n# Get predicted probabilities\nLDA.fit.prob &lt;- predict(LDA.fit, newdata = Default[, 2:4])$posterior[, 2]\n\n# Generate ROC curve\nroc_curve &lt;- roc(Default$default, LDA.fit.prob)\n\n# Plot ROC curve\nplot(roc_curve, main = \"ROC Curve\", col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\n# Add AUC to the plot\nauc(roc_curve)\n\n\nArea under the curve: 0.9495\n\n\nThe Receiver Operating Characteristic (ROC) curve is a graphical representation used to evaluate the performance of a classification model. It plots the True Positive Rate (Sensitivity) against the False Positive Rate (1-Specificity) at various threshold settings.\n\nTrue Positive Rate (TPR): Also known as sensitivity, it measures the proportion of actual positives correctly identified by the model.\nFalse Positive Rate (FPR): It measures the proportion of actual negatives incorrectly identified as positives by the model.\n\nThe ROC curve helps visualize the trade-off between sensitivity and specificity for different threshold values. It is also used to compare Models: By comparing the ROC curves of different models, you can assess which model performs better.\nThe Area Under the ROC Curve (AUC) is a single scalar value that summarizes the performance of a classification model. - AUC = 1: Perfect model (perfectly separates positive and negative classes). - AUC = 0.5: Random model (no better than random guessing). - AUC &lt; 0.5: Worse than random guessing.\nUse of AUC: - Model Comparison: A higher AUC value indicates a better-performing model. - Robustness: AUC is insensitive to the threshold chosen for classification, providing a more robust evaluation metric.\nIn essence, the ROC curve and AUC provide a comprehensive way to evaluate and compare classification models, ensuring you pick the most accurate and reliable one.\nFollowing what we see in the plot we think that a prior of 0.9 and 0.1 can be better, so we adjust our model accordingly:\n\n\nCode\n# Fit LDA model with adjusted prior probabilities\nLDA.fit &lt;- lda(default ~ income + balance + student, data = Default, prior = c(0.9, 0.1))\n\n# Predict and evaluate\nLDA.fit.Classes &lt;- predict(LDA.fit, newdata = Default[, 2:4])$class\nconfusion_matrix &lt;- caret::confusionMatrix(factor(LDA.fit.Classes), Default$default)\n\n# Print confusion matrix\nprint('Confusion Matrix')\n\n\n[1] \"Confusion Matrix\"\n\n\nCode\nprint(confusion_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  9496  157\n       Yes  171  176\n                                          \n               Accuracy : 0.9672          \n                 95% CI : (0.9635, 0.9706)\n    No Information Rate : 0.9667          \n    P-Value [Acc &gt; NIR] : 0.4041          \n                                          \n                  Kappa : 0.5007          \n                                          \n Mcnemar's Test P-Value : 0.4729          \n                                          \n            Sensitivity : 0.9823          \n            Specificity : 0.5285          \n         Pos Pred Value : 0.9837          \n         Neg Pred Value : 0.5072          \n             Prevalence : 0.9667          \n         Detection Rate : 0.9496          \n   Detection Prevalence : 0.9653          \n      Balanced Accuracy : 0.7554          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nCode\n# Extract the confusion matrix values\ncm &lt;- confusion_matrix$table\n\n# Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\nfalse_positive_rate &lt;- cm[2, 1] / sum(cm[, 1])\nfalse_negative_rate &lt;- cm[1, 2] / sum(cm[, 2])\n\n# Print the rates\ncat('False Positive Rate: ', false_positive_rate, '\\n')\n\n\nFalse Positive Rate:  0.01768905 \n\n\nCode\ncat('False Negative Rate: ', false_negative_rate, '\\n')\n\n\nFalse Negative Rate:  0.4714715 \n\n\n\n\nExample with three classes:\nAs an example we can see Fisher’s Iris dataset. It has 3 classes (Iris Species) and uses 4 different parameters to classify the different species: Sepal Length, Sepal Width, Petal Length and Petal Width. If we plot each variable against each other using pairs() we can see that we can actually see quite a clear differentiation between the species using these variables.\n\n\n\n\n\n\n\n\n\nWe are going to fit a model for our iris data and see how well it works predicting the species.\n\n\nCode\nLDA.fit = lda( Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, iris)\nLDA.fit\n\n\nCall:\nlda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, \n    data = iris)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1         LD2\nSepal.Length  0.8293776 -0.02410215\nSepal.Width   1.5344731 -2.16452123\nPetal.Length -2.2012117  0.93192121\nPetal.Width  -2.8104603 -2.83918785\n\nProportion of trace:\n   LD1    LD2 \n0.9912 0.0088 \n\n\nWe see that now we have to LDFs LD1 and LD2.\nProportion of Trace: The proportion of trace tells you how much of the total between-class variance each linear discriminant function explains.\nIn this case:\n\nLD1: Explains 99.12% of the variance.\nLD2: Explains 0.88% of the variance.\n\nSo, LD1 is the primary driver for separating the species in the iris dataset, while LD2 adds a minor amount of additional discrimination.\nNow we are going to perform the classification:\n\n\nCode\nLDA.fit.C = predict(LDA.fit, newdata=iris[,c(1,2,3,4)])$class\n\nhead(LDA.fit.C)\n\n\n[1] setosa setosa setosa setosa setosa setosa\nLevels: setosa versicolor virginica\n\n\nWe can quickly create a confusion matrix using table:\n\n\nCode\ntable(iris$Species, LDA.fit.C)\n\n\n            LDA.fit.C\n             setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         2\n  virginica       0          1        49\n\n\nAs we have seen before we can also use the confusion matrix function from the package caret:\n\n\nCode\ncaret::confusionMatrix(data=LDA.fit.C, reference=iris$Species )\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n\nOverall Statistics\n                                               \n               Accuracy : 0.98                 \n                 95% CI : (0.9427, 0.9959)     \n    No Information Rate : 0.3333               \n    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022\n                                               \n                  Kappa : 0.97                 \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9600           0.9800\nSpecificity                 1.0000            0.9900           0.9800\nPos Pred Value              1.0000            0.9796           0.9608\nNeg Pred Value              1.0000            0.9802           0.9899\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3200           0.3267\nDetection Prevalence        0.3333            0.3267           0.3400\nBalanced Accuracy           1.0000            0.9750           0.9800\n\n\nLastly, we can create an LDA plot to view the linear discriminants of the model and visualize how well it separated the three different species in our dataset:\n\n\nCode\n# Predict the LDA values\nlda.pred &lt;- predict(LDA.fit, newdata=iris[,c(1,2,3,4)])$x\n\n# Combine the LDA values with the original dataset\nlda.data &lt;- data.frame(lda.pred, Species = iris$Species)\n\n# Define colors for each species\nspecies_colors &lt;- c(setosa = \"red\", versicolor = \"green\", virginica = \"blue\")\n\n# Plot the LDA results using base R plot\nplot(lda.data$LD1, lda.data$LD2, col = species_colors[lda.data$Species],\n     pch = 19, xlab = \"Linear Discriminant 1\", ylab = \"Linear Discriminant 2\",\n     main = \"LDA of Iris Dataset\")\n\n# Add a legend\nlegend(\"topright\", legend = levels(iris$Species), \n       col = species_colors, pch = 19, title = \"Species\")\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.4 Quadratic Discriminant Analysis (QDA).\nSo far we have seen how to use discriminant analysis for a normal distribution, with the same variance for each class. If the distribution is still normal but the variance for each class is different, we use quadratic discriminant analysis The formula for the discriminant score was quite simple for the linear discriminant analysis because the quadratic terms were cancelling each other, but now, because \\(\\Sigma_k\\) are different, we cannot apply that cancellation. \\[\n\\delta_k(x) = -\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k) + \\log{\\pi_k} - \\frac{1}{2}\\log{|\\Sigma_k|}\n\\tag{13}\\]\nWhere - \\(\\Sigma_k\\) represents the covariance matrix for class (k). It describes the spread and orientation of the data points in class (k). - \\(\\pi_k\\): This is the prior probability of class (k). It represents the proportion of data points that belong to class (k) in the training dataset.\nQuadratic discriminant analysis is good when the number of features is not high, because you need to estimate the covariance matrices.\n\nExample:\nWe are going to use the iris dataset to perform quadratic Discriminant Analisys. We will split the dataset into training and testing sets:\n\n\nCode\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\nset.seed(1)\nsample &lt;- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7, 0.3))\ntrain &lt;- iris[sample, ]\ntest &lt;- iris[!sample, ]\n\n\nand now we use the qda function from the MASS package to fit the model over the training data:\n\n\nCode\nmodel &lt;- qda(Species ~ ., data=train)\nprint(model)\n\n\nCall:\nqda(Species ~ ., data = train)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3207547  0.3207547  0.3584906 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa         4.982353    3.411765     1.482353   0.2411765\nversicolor     5.994118    2.794118     4.358824   1.3676471\nvirginica      6.636842    2.973684     5.592105   2.0552632\n\n\nPrior probabilities of group: These represent the proportions of each Species in the training set. For example, 35.8% of all observations in the training set were of species virginica.\nGroup means: These display the mean values for each predictor variable for each species. As ususal, to use the model to get the fitted values we use predict\n\n\nCode\npredictions &lt;- predict(model, test)\nnames(predictions)\n\n\n[1] \"class\"     \"posterior\"\n\n\nPredict returns a list with two variables: - class: the predicted class - Posterior: the posterior probability that an observation belong to each class.\n\n\nCode\nhead(predictions$class)\n\n\n[1] setosa setosa setosa setosa setosa setosa\nLevels: setosa versicolor virginica\n\n\nCode\nhead(predictions$posterior)\n\n\n   setosa                                  versicolor\n4       1 0.00000000000000000007224769747310848916691\n6       1 0.00000000000000000000000006209196139412465\n7       1 0.00000000000000000000124833703395775206884\n15      1 0.00000000000000000000000000000000002319705\n17      1 0.00000000000000000000000000001396840155918\n18      1 0.00000000000000000000000075811654693801403\n                                                    virginica\n4  0.00000000000000000000000000001642236497450937561309522450\n6  0.00000000000000000000000000000000000008550910860907731162\n7  0.00000000000000000000000000000008132699523077754852687371\n15 0.00000000000000000000000000000000000000000000000005094803\n17 0.00000000000000000000000000000000000000000095865040481277\n18 0.00000000000000000000000000000000000086113205635398140910\n\n\nWe can use the following code to see what percentage of observations the QDA model correctly predicted:\n\n\nCode\nmean(predictions$class==test$Species)\n\n\n[1] 1\n\n\nAnd we create a confusion matrix to see the results:\n\n\nCode\ntable(predictions$class, test$Species)\n\n\n            \n             setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         16         0\n  virginica       0          0        12\n\n\n\n\n\n1.2.5 Naive Bayes\nNaive Bayes is best used when the number of features is high because it does not require to calculate the covariance matrices. It assumes that the features are independent in each class, this means your predictor variables are independent, which is almost never true for real data. Despite the strong assumptions that it does, naive Bayes often produces good classification results.\nIt can be used for mixed feature vectors (quantitative and qualitative). ::: {.exercise-box} Example We are going to be working again with the Iris dataset. This time we are going to introduce also a new function ggpairs() from the package GGally, this will help us visualize the correlation between the variables before we start our analysis\n\n\nCode\nGGally::ggpairs(iris[-5], title = \"The correlation between the predictors\")\n\n\n\n\n\n\n\n\n\nWe are going to split the data for the train and the test:\n\n\nCode\nset.seed(1)\nsample &lt;- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7, 0.3))\ntrain &lt;- iris[sample, ]\ntest &lt;- iris[!sample, ]\n\n\nWe will need the library naivebayes for fitting the model:\n\n\nCode\nnaive.fit &lt;- naivebayes::naive_bayes(Species ~., data=train)\nnaive.fit\n\n\n\n================================= Naive Bayes ==================================\n\nCall:\nnaive_bayes.formula(formula = Species ~ ., data = train)\n\n-------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n-------------------------------------------------------------------------------- \n \nA priori probabilities: \n\n    setosa versicolor  virginica \n 0.3207547  0.3207547  0.3584906 \n\n-------------------------------------------------------------------------------- \n \nTables: \n\n-------------------------------------------------------------------------------- \n:: Sepal.Length (Gaussian) \n-------------------------------------------------------------------------------- \n            \nSepal.Length    setosa versicolor virginica\n        mean 4.9823529  5.9941176 6.6368421\n        sd   0.3242433  0.4811232 0.7049712\n\n-------------------------------------------------------------------------------- \n:: Sepal.Width (Gaussian) \n-------------------------------------------------------------------------------- \n           \nSepal.Width    setosa versicolor virginica\n       mean 3.4117647  2.7941176 2.9736842\n       sd   0.4043436  0.3113423 0.3422376\n\n-------------------------------------------------------------------------------- \n:: Petal.Length (Gaussian) \n-------------------------------------------------------------------------------- \n            \nPetal.Length    setosa versicolor virginica\n        mean 1.4823529  4.3588235 5.5921053\n        sd   0.1849989  0.4120295 0.6055516\n\n-------------------------------------------------------------------------------- \n:: Petal.Width (Gaussian) \n-------------------------------------------------------------------------------- \n           \nPetal.Width    setosa versicolor virginica\n       mean 0.2411765  1.3676471 2.0552632\n       sd   0.1183668  0.1804382 0.2698321\n\n--------------------------------------------------------------------------------\n\n\nNow we can use the model to predict values using the test data subset and create a confusion matrix\n\n\nCode\npredictions &lt;- predict(naive.fit, test)\n\n(confusion_matrix&lt;- table(predictions,test$Species))\n\n\n            \npredictions  setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         16         1\n  virginica       0          0        11\n\n\nCode\n(accuracy&lt;- sum(diag(confusion_matrix))/sum(confusion_matrix))\n\n\n[1] 0.9772727\n\n\n:::"
  },
  {
    "objectID": "deepLearningTorch.html",
    "href": "deepLearningTorch.html",
    "title": "Deep Learning Lab with Torch",
    "section": "",
    "text": "Code\nlibrary(ISLR2)\nlibrary(glmnet)\nlibrary(tidyverse)\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(torchvision) # for datasets and image transformation\nlibrary(torchdatasets) # for datasets we are going to use\nlibrary(zeallot)\nIn this section, we show how to fit the examples discussed in the deep Learning section. We use the luz package, which interfaces to the torch package which in turn links to efficient C++ code in the LibTorch library.\nThis version of the lab was produced by Daniel Falbel and Sigrid Keydana, both data scientists at Rstudio where these packages were produced.\nAn advantage over our original keras implementation is that this version does not require a separate python installation."
  },
  {
    "objectID": "deepLearningTorch.html#single-layer-network-on-hitters-data",
    "href": "deepLearningTorch.html#single-layer-network-on-hitters-data",
    "title": "Deep Learning Lab with Torch",
    "section": "1 Single Layer Network on Hitters Data",
    "text": "1 Single Layer Network on Hitters Data\nWe start by fitting the models. We set up the data, and separate out a training and test set.\n\n\nCode\nlibrary(ISLR2)\nGitters &lt;- na.omit(Hitters)\nn &lt;- nrow(Gitters)\nset.seed(13)\nntest &lt;- trunc(n / 3)\ntestid &lt;- sample(1:n, ntest)\n\n\nThe linear model should be familiar, but we present it anyway.\n\n\nCode\nlfit &lt;- lm(Salary ~ ., data = Gitters[-testid, ])\nlpred &lt;- predict(lfit, Gitters[testid, ])\nwith(Gitters[testid, ], mean(abs(lpred - Salary)))\n\n\n[1] 254.6687\n\n\nNotice the use of the with() command: the first argument is a dataframe, and the second an expression that can refer to elements of the dataframe by name. In this instance the dataframe corresponds to the test data and the expression computes the mean absolute prediction error on this data.\nNext we fit the lasso using glmnet. Since this package does not use formulas, we create x and y first.\n\n\nCode\nx &lt;- scale(model.matrix(Salary ~ . - 1, data = Gitters))\ny &lt;- Gitters$Salary\n\n\nThe first line makes a call to model.matrix(), which produces the same matrix that was used by lm() (the -1 omits the intercept). This function automatically converts factors to dummy variables. The scale() function standardizes the matrix so each column has mean zero and variance one.\n\n\nCode\nlibrary(glmnet)\ncvfit &lt;- cv.glmnet(x[-testid, ], y[-testid],\n    type.measure = \"mae\")\ncpred &lt;- predict(cvfit, x[testid, ], s = \"lambda.min\")\nmean(abs(y[testid] - cpred))\n\n\n[1] 252.2994\n\n\nTo fit the neural network, we first set up a model structure that describes the network.\n\n\nCode\ntorch_manual_seed(13)\n\n\n\n\nCode\nmodnn &lt;- nn_module(\n  initialize = function(input_size) {\n    self$hidden &lt;- nn_linear(input_size, 50)\n    self$activation &lt;- nn_relu()\n    self$dropout &lt;- nn_dropout(0.4)\n    self$output &lt;- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x %&gt;% \n      self$hidden() %&gt;% \n      self$activation() %&gt;% \n      self$dropout() %&gt;% \n      self$output()\n  }\n)\n\n\nWe have created a model called modnn by defining the initialize() and forward() functions and passing them to the nn_module() function. The initialize() function is responsible for initializing the submodules that are used by the model. In the forward method we implement what happens when the model is called on input data. In this case we use the layers we defined in initialize() in that specific order.\nself is a list-like special object that is used to share information between the methods of the nn_module(). When you assign an object to self in initialize(), it can then be accessed by forward().\nWe now return to our neural network. The object modnn has a single hidden layer with 50 hidden units, and a ReLU activation function. It then has a dropout layer, in which a random 40% of the 50 activations from the previous layer are set to zero during each iteration of the stochastic gradient descent algorithm. Finally, the output layer has just one unit with no activation function, indicating that the model provides a single quantitative output.\nNext we add details to modnn that control the fitting algorithm. We minimize squared-error loss as in (10.22). The algorithm tracks the mean absolute error on the training data, and on validation data if it is supplied.\n\n\nCode\nmodnn &lt;- modnn %&gt;% \n  setup(\n    loss = nn_mse_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_mae())\n  ) %&gt;% \n  set_hparams(input_size = ncol(x))\n\n\nIn the previous line, the pipe operator passes modnn as the first argument to setup(). The setup() function embeds these specification into a new model object. We also use set_hparam() to specify the arguments that should be passed to the initialize() method of modnn.\nNow we fit the model. We supply the training data and the number of epochs. By default, at each step of SGD, the algorithm randomly selects 32 training observations for the computation of the gradient. Recall that an epoch amounts to the number of SGD steps required to process \\(n\\) observations. Since the training set has \\(n=176\\), an epoch is \\(176/32=5.5\\) SGD steps. The fit() function has an argument valid_data; these data are not used in the fitting, but can be used to track the progress of the model (in this case reporting mean absolute error). Here we actually supply the test data so we can see mean absolute error of both the training data and test data as the epochs proceed. To see more options for fitting, use ?fit.luz_module_generator.\n\n\nCode\nfitted &lt;- modnn %&gt;% \n  fit(\n    data = list(x[-testid, ], matrix(y[-testid], ncol = 1)),\n    valid_data = list(x[testid, ], matrix(y[testid], ncol = 1)),\n    epochs = 20 # 50\n  )\n\n\n(Here and elsewhere we have reduced the number of epochs to make runtimes manageable; users can of course change back)\nWe can plot the fitted model to display the mean absolute error for the training and test data.\n\n\nCode\nplot(fitted)\n\n\n\n\n\nFinally, we predict from the final model, and evaluate its performance on the test data. Due to the use of SGD, the results vary slightly with each fit.\n\n\nCode\nnpred &lt;- predict(fitted, x[testid, ])\nmean(abs(y[testid] - as.matrix(npred)))\n\n\n[1] 270.3537\n\n\nWe had to convert the npred object to a matrix, since the current predict method returns an object of class torch_tensor.\n\n\nCode\nclass(npred)\n\n\n[1] \"torch_tensor\" \"R7\""
  },
  {
    "objectID": "deepLearningTorch.html#multilayer-network-on-the-mnist-digit-data",
    "href": "deepLearningTorch.html#multilayer-network-on-the-mnist-digit-data",
    "title": "Deep Learning Lab with Torch",
    "section": "2 Multilayer Network on the MNIST Digit Data",
    "text": "2 Multilayer Network on the MNIST Digit Data\nThe torchvision package comes with a number of example datasets, including the MNIST digit data. Our first step is to load the MNIST data. The mnist_dataset() function is provided for this purpose.\nThis functions returns a dataset(), a data structure implemented in torch allowing one to represent any dataset without making assumptions on where the data is stored and how the data is organized. Usually, torch datasets also implement the data acquisition process, like downloading and caching some files on disk.\n\n\nCode\ntrain_ds &lt;- mnist_dataset(root = \".\", train = TRUE, download = TRUE)\ntest_ds &lt;- mnist_dataset(root = \".\", train = FALSE, download = TRUE)\n\nstr(train_ds[1])\n\n\nList of 2\n $ x: int [1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...\n $ y: int 6\n\n\nCode\nstr(test_ds[2])\n\n\nList of 2\n $ x: int [1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...\n $ y: int 3\n\n\nCode\nlength(train_ds)\n\n\n[1] 60000\n\n\nCode\nlength(test_ds)\n\n\n[1] 10000\n\n\nThere are 60,000 images in the training data and 10,000 in the test data. The images are \\(28\\times 28\\), and stored as matrix of pixels. We need to transform each one into a vector.\nNeural networks are somewhat sensitive to the scale of the inputs. For example, ridge and lasso regularization are affected by scaling. Here the inputs are eight-bit grayscale values between 0 and 255, so we rescale to the unit interval. (Note: eight bits means \\(2^8\\), which equals 256. Since the convention is to start at \\(0\\), the possible values range from \\(0\\) to \\(255\\).)\nTo apply these transformations we will re-define train_ds and test_ds, now passing a the transform argument that will apply a transformation to each of the image inputs.\n\n\nCode\ntransform &lt;- function(x) {\n  x %&gt;% \n    torch_tensor() %&gt;% \n    torch_flatten() %&gt;% \n    torch_div(255)\n}\ntrain_ds &lt;- mnist_dataset(\n  root = \".\", \n  train = TRUE, \n  download = TRUE, \n  transform = transform\n)\ntest_ds &lt;- mnist_dataset(\n  root = \".\", \n  train = FALSE, \n  download = TRUE,\n  transform = transform\n)\n\n\nNow we are ready to fit our neural network.\n\n\nCode\nmodelnn &lt;- nn_module(\n  initialize = function() {\n    self$linear1 &lt;- nn_linear(in_features = 28*28, out_features = 256)\n    self$linear2 &lt;- nn_linear(in_features = 256, out_features = 128)\n    self$linear3 &lt;- nn_linear(in_features = 128, out_features = 10)\n    \n    self$drop1 &lt;- nn_dropout(p = 0.4)\n    self$drop2 &lt;- nn_dropout(p = 0.3)\n    \n    self$activation &lt;- nn_relu()\n  },\n  forward = function(x) {\n    x %&gt;% \n      \n      self$linear1() %&gt;% \n      self$activation() %&gt;% \n      self$drop1() %&gt;% \n      \n      self$linear2() %&gt;% \n      self$activation() %&gt;% \n      self$drop2() %&gt;% \n      \n      self$linear3()\n  }\n)\n\n\nWe define the intialize() and forward() methods of the nn_module().\nIn initialize we specify all layers that are used in the model. For example, nn_linear(784, 256) defines a dense layer that goes from \\(28\\times28=784\\) input units to a hidden layer of \\(256\\) units. The model will have 3 of them, each one decreasing the number of output units. The last will have 10 output units, because each unit will be associated to a different class, and we have a 10-class classification problem. We also defined dropout layers using nn_dropout(). These will be used to perform dropout regularization. Finally we define the activation layer using nn_relu().\nIn forward() we define the order in which these layers are called. We call them in blocks like (linear, activation, dropout), except for the last layer that does not use an activation function or dropout.\nFinally, we use print to summarize the model, and to make sure we got it all right.\n\n\nCode\nprint(modelnn())\n\n\nAn `nn_module` containing 235,146 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• linear1: &lt;nn_linear&gt; #200,960 parameters\n• linear2: &lt;nn_linear&gt; #32,896 parameters\n• linear3: &lt;nn_linear&gt; #1,290 parameters\n• drop1: &lt;nn_dropout&gt; #0 parameters\n• drop2: &lt;nn_dropout&gt; #0 parameters\n• activation: &lt;nn_relu&gt; #0 parameters\n\n\nThe parameters for each layer include a bias term, which results in a parameter count of 235,146. For example, the first hidden layer involves \\((784+1)\\times 256=200{,}960\\) parameters.\nNext, we add details to the model to specify the fitting algorithm. We fit the model by minimizing the cross-entropy function given by (10.13).\nNotice that in torch the cross entropy function is defined in terms of the logits, for numerical stability and memory efficiency reasons. It does not require the target to be one-hot encoded.\n\n\nCode\nmodelnn &lt;- modelnn %&gt;% \n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop, \n    metrics = list(luz_metric_accuracy())\n  )\n\n\nNow we are ready to go. The final step is to supply training data, and fit the model.\n\n\nCode\nsystem.time(\n   fitted &lt;- modelnn %&gt;%\n      fit(\n        data = train_ds, \n        epochs = 10, #15, \n        valid_data = 0.2,\n        dataloader_options = list(batch_size = 256),\n        verbose = TRUE\n      )\n )\n\n\n   user  system elapsed \n 602.60 1710.04  191.14 \n\n\nCode\nplot(fitted)\n\n\n\n\n\nWe have suppressed the output here. The output is a progress report on the fitting of the model, grouped by epoch. This is very useful, since on large datasets fitting can take time. Fitting this model took 215 seconds on a 2.7GHz MacBook Pro with 4 cores and 16 GB of RAM. Here we specified a validation split of 20%, so training is actually performed on 80% of the 60,000 observations in the training set. This is an alternative to actually supplying validation data, like we did in Section 10.9.1. See ?fit.luz_module_generator for all the optional fitting arguments. SGD uses batches of 256 observations in computing the gradient, and doing the arithmetic, we see that an epoch corresponds to 188 gradient steps. The last plot() command produces a figure similar to Figure 10.18.\nTo obtain the test error in Table 10.1, we first write a simple function accuracy() that compares predicted and true class labels, and then use it to evaluate our predictions.\n\n\nCode\naccuracy &lt;- function(pred, truth) {\n   mean(pred == truth) }\n\n# gets the true classes from all observations in test_ds.\ntruth &lt;- sapply(seq_along(test_ds), function(x) test_ds[x][[2]])\n\nfitted %&gt;% \n  predict(test_ds) %&gt;% \n  torch_argmax(dim = 2) %&gt;%  # the predicted class is the one with higher 'logit'.\n  as_array() %&gt;% # we convert to an R object\n  accuracy(truth)\n\n\n[1] 0.9592\n\n\nThe table also reports LDA (Chapter 4) and multiclass logistic regression. Although packages such as glmnet can handle multiclass logistic regression, they are quite slow on this large dataset. It is much faster and quite easy to fit such a model using the luz software. We just have an input layer and output layer, and omit the hidden layers!\n\n\nCode\nmodellr &lt;- nn_module(\n  initialize = function() {\n    self$linear &lt;- nn_linear(784, 10)\n  },\n  forward = function(x) {\n    self$linear(x)\n  }\n)\nprint(modellr())\n\n\nAn `nn_module` containing 7,850 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• linear: &lt;nn_linear&gt; #7,850 parameters\n\n\nWe fit the model just as before.\n\n\nCode\nfit_modellr &lt;- modellr %&gt;% \n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_accuracy())\n  ) %&gt;% \n  fit(\n    data = train_ds, \n    epochs = 5,\n    valid_data = 0.2,\n    dataloader_options = list(batch_size = 128)\n  )\n\nfit_modellr %&gt;% \n  predict(test_ds) %&gt;% \n  torch_argmax(dim = 2) %&gt;%  # the predicted class is the one with higher 'logit'.\n  as_array() %&gt;% # we convert to an R object\n  accuracy(truth)\n\n\n[1] 0.9172\n\n\nCode\n# alternatively one can use the `evaluate` function to get the results\n# on the test_ds\nevaluate(fit_modellr, test_ds)\n\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 0.3055\nacc: 0.9172\n\n\n\n2.1 Convolutional Neural Networks\nIn this section we fit a CNN to the CIFAR data, which is available in the torchvision package. It is arranged in a similar fashion as the MNIST data.\n\n\nCode\ntransform &lt;- function(x) {\n  transform_to_tensor(x)\n}\n\ntrain_ds &lt;- cifar100_dataset(\n  root = \"./\", \n  train = TRUE, \n  download = TRUE, \n  transform = transform\n)\n\ntest_ds &lt;- cifar100_dataset(\n  root = \"./\", \n  train = FALSE, \n  transform = transform\n)\n\nstr(train_ds[1])\n\n\nList of 2\n $ x:Float [1:3, 1:32, 1:32]\n $ y: int 20\n\n\nCode\nlength(train_ds)\n\n\n[1] 50000\n\n\nThe CIFAR dataset consists of 50,000 training images, each represented by a 3d tensor: each three-color image is represented as a set of three channels, each of which consists of \\(32\\times 32\\) eight-bit pixels. We standardize as we did for the digits, but keep the array structure. This is accomplished with the transform argument.\nBefore we start, we look at some of the training images; similar code produced Figure 10.5 on page 411.\n\n\nCode\npar(mar = c(0, 0, 0, 0), mfrow = c(5, 5))\nindex &lt;- sample(seq(50000), 25)\nfor (i in index) plot(as.raster(as.array(train_ds[i][[1]]$permute(c(2,3,1)))))\n\n\n\n\n\nThe as.raster() function converts the feature map so that it can be plotted as a color image.\nHere we specify a moderately-sized CNN for demonstration purposes, similar in structure to Figure 10.8.\n\n\nCode\nconv_block &lt;- nn_module(\n  initialize = function(in_channels, out_channels) {\n    self$conv &lt;- nn_conv2d(\n      in_channels = in_channels, \n      out_channels = out_channels, \n      kernel_size = c(3,3), \n      padding = \"same\"\n    )\n    self$relu &lt;- nn_relu()\n    self$pool &lt;- nn_max_pool2d(kernel_size = c(2,2))\n  },\n  forward = function(x) {\n    x %&gt;% \n      self$conv() %&gt;% \n      self$relu() %&gt;% \n      self$pool()\n  }\n)\n\nmodel &lt;- nn_module(\n  initialize = function() {\n    self$conv &lt;- nn_sequential(\n      conv_block(3, 32),\n      conv_block(32, 64),\n      conv_block(64, 128),\n      conv_block(128, 256)\n    )\n    self$output &lt;- nn_sequential(\n      nn_dropout(0.5),\n      nn_linear(2*2*256, 512),\n      nn_relu(),\n      nn_linear(512, 100)\n    )\n  },\n  forward = function(x) {\n    x %&gt;% \n      self$conv() %&gt;% \n      torch_flatten(start_dim = 2) %&gt;% \n      self$output()\n  }\n)\nmodel()\n\n\nAn `nn_module` containing 964,516 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• conv: &lt;nn_sequential&gt; #388,416 parameters\n• output: &lt;nn_sequential&gt; #576,100 parameters\n\n\nNotice that we used the padding = \"same\" argument to nn_conv2d(), which ensures that the output channels have the same dimension as the input channels. There are 32 channels in the first hidden layer, in contrast to the three channels in the input layer. We use a \\(3\\times 3\\) convolution filter for each channel in all the layers. Each convolution is followed by a max-pooling layer over \\(2\\times2\\) blocks. By studying the summary, we can see that the channels halve in both dimensions after each of these max-pooling operations. After the last of these we have a layer with 256 channels of dimension \\(2\\times 2\\). These are then flattened to a dense layer of size 1,024: in other words, each of the \\(2\\times 2\\) matrices is turned into a \\(4\\)-vector, and put side-by-side in one layer. This is followed by a dropout regularization layer, then another dense layer of size 512, and finally, the output layer.\nFinally, we specify the fitting algorithm, and fit the model.\n\n\nCode\nfitted &lt;- model %&gt;% \n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_rmsprop, \n    metrics = list(luz_metric_accuracy())\n  ) %&gt;% \n  set_opt_hparams(lr = 0.001) %&gt;% \n  fit(\n    train_ds,\n    epochs = 10, #30,\n    valid_data = 0.2,\n    dataloader_options = list(batch_size = 128)\n  )\n\nprint(fitted)\n\n\nA `luz_module_fitted`\n── Time ────────────────────────────────────────────────────────────────────────\n• Total time: 33m 1.4s\n• Avg time per training epoch: 3m 10.3s\n\n── Results ─────────────────────────────────────────────────────────────────────\nMetrics observed in the last epoch.\n\nℹ Training:\nloss: 2.3467\nacc: 0.3819\n\n── Model ───────────────────────────────────────────────────────────────────────\nAn `nn_module` containing 964,516 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• conv: &lt;nn_sequential&gt; #388,416 parameters\n• output: &lt;nn_sequential&gt; #576,100 parameters\n\n\nCode\nevaluate(fitted, test_ds)\n\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 2.476\nacc: 0.3641\n\n\nThis model takes 10 minutes to run and achieves 36% accuracy on the test data. Although this is not terrible for 100-class data (a random classifier gets 1% accuracy), searching the web we see results around 75%. Typically it takes a lot of architecture carpentry, fiddling with regularization, and time to achieve such results."
  },
  {
    "objectID": "deepLearningTorch.html#using-pretrained-cnn-models",
    "href": "deepLearningTorch.html#using-pretrained-cnn-models",
    "title": "Deep Learning Lab with Torch",
    "section": "3 Using Pretrained CNN Models",
    "text": "3 Using Pretrained CNN Models\nWe now show how to use a CNN pretrained on the imagenet database to classify natural images. We copied six jpeg images from a digital photo album into the directory book_images. (These images are available from the data section of www.statlearning.com, the ISLR book website. Download book_images.zip; when clicked it creates the book_images directory.) We first read in the images, and convert them into the array format expected by the torch software to match the specifications in imagenet. Make sure that your working directory in R is set to the folder in which the images are stored.\n\n\nCode\nimg_dir &lt;- \"book_images\"\nimage_names &lt;- list.files(img_dir)\nnum_images &lt;- length(image_names)\nx &lt;- torch_empty(num_images, 3, 224, 224)\nfor (i in 1:num_images) {\n   img_path &lt;- file.path(img_dir, image_names[i])\n   img &lt;- img_path %&gt;% \n     base_loader() %&gt;% \n     transform_to_tensor() %&gt;% \n     transform_resize(c(224, 224)) %&gt;% \n     # normalize with imagenet mean and stds.\n     transform_normalize(\n       mean = c(0.485, 0.456, 0.406),\n       std = c(0.229, 0.224, 0.225)\n     )\n   x[i,,, ] &lt;- img\n}\n\n\nWe then load the trained network. The model has 18 layers, with a fair bit of complexity.\n\n\nCode\nmodel &lt;- torchvision::model_resnet18(pretrained = TRUE)\nmodel$eval() # put the model in evaluation mode\n\n\nFinally, we classify our six images, and return the top three class choices in terms of predicted probability for each.\n\n\nCode\npreds &lt;- model(x)\n\nmapping &lt;- jsonlite::read_json(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\") %&gt;% \n  sapply(function(x) x[[2]])\n\ntop3 &lt;- torch_topk(preds, dim = 2, k = 3)\n\ntop3_prob &lt;- top3[[1]] %&gt;% \n  nnf_softmax(dim = 2) %&gt;% \n  torch_unbind() %&gt;% \n  lapply(as.numeric)\n\ntop3_class &lt;- top3[[2]] %&gt;% \n  torch_unbind() %&gt;% \n  lapply(function(x) mapping[as.integer(x)])\n\nresult &lt;- purrr::map2(top3_prob, top3_class, function(pr, cl) {\n  names(pr) &lt;- cl\n  pr\n})\nnames(result) &lt;- image_names\nprint(result)\n\n\n$flamingo.jpg\n   flamingo   spoonbill white_stork \n0.978211880 0.017045777 0.004742406 \n\n$hawk.jpg\n        eel       agama common_newt \n  0.5391127   0.2527187   0.2081686 \n\n$hawk_cropped.jpeg\n     kite       jay    magpie \n0.6157802 0.2311868 0.1530330 \n\n$huey.jpg\n          Lhasa Tibetan_terrier        Shih-Tzu \n     0.79760361      0.12013040      0.08226597 \n\n$kitty.jpg\n       Saint_Bernard           guinea_pig Bernese_mountain_dog \n           0.3946661            0.3427000            0.2626339 \n\n$weaver.jpg\nhummingbird    lorikeet   bee_eater \n  0.3633284   0.3577289   0.2789428"
  },
  {
    "objectID": "deepLearningTorch.html#imdb-document-classification",
    "href": "deepLearningTorch.html#imdb-document-classification",
    "title": "Deep Learning Lab with Torch",
    "section": "4 IMDb Document Classification",
    "text": "4 IMDb Document Classification\nNow we perform document classification on the IMDB dataset, which is available as part of the torchdatasets package. We limit the dictionary size to the 10,000 most frequently-used words and tokens.\n\n\nCode\nset.seed(1)\nmax_features &lt;- 10000\nimdb_train &lt;- imdb_dataset(\n  root = \".\", \n  download = TRUE,\n  split=\"train\",\n  num_words = max_features\n)\nimdb_test &lt;- imdb_dataset(\n  root = \".\", \n  download = TRUE,\n  split=\"test\",\n  num_words = max_features\n)\n\n\nEach element of imdb_train is a vector of numbers between 1 and 10000 (the document), referring to the words found in the dictionary. For example, the first training document is the positive review on page 419. The indices of the first 12 words are given below.\n\n\nCode\nimdb_train[1]$x[1:12]\n\n\n [1]    2   25  124   25   26   11 1113  149    6  211   54    4\n\n\nTo see the words, we create a function, decode_review(), that provides a simple interface to the dictionary.\n\n\nCode\nword_index &lt;- imdb_train$vocabulary\ndecode_review &lt;- function(text, word_index) {\n   word &lt;- names(word_index)\n   idx &lt;- unlist(word_index, use.names = FALSE)\n   word &lt;- c(\"&lt;PAD&gt;\", \"&lt;START&gt;\", \"&lt;UNK&gt;\", word)\n   words &lt;- word[text]\n   paste(words, collapse = \" \")\n}\ndecode_review(imdb_train[1]$x[1:12], word_index)\n\n\n[1] \"&lt;START&gt; you know you are in trouble watching a comedy when the\"\n\n\nNext we write a function to one-hot encode each document in a list of documents, and return a binary matrix in sparse-matrix format.\n\n\nCode\nlibrary(Matrix)\none_hot &lt;- function(sequences, dimension) {\n   seqlen &lt;- sapply(sequences, length)\n   n &lt;- length(seqlen)\n   rowind &lt;- rep(1:n, seqlen)\n   colind &lt;- unlist(sequences)\n   sparseMatrix(i = rowind, j = colind,\n      dims = c(n, dimension))\n}\n\n\nTo construct the sparse matrix, one supplies just the entries that are nonzero. In the last line we call the function sparseMatrix() and supply the row indices corresponding to each document and the column indices corresponding to the words in each document, since we omit the values they are taken to be all ones. Words that appear more than once in any given document still get recorded as a one.\n\n\nCode\n# collect all values into a list\ntrain &lt;- seq_along(imdb_train) %&gt;% \n  lapply(function(i) imdb_train[i]) %&gt;% \n  purrr::transpose()\ntest &lt;- seq_along(imdb_test) %&gt;% \n  lapply(function(i) imdb_test[i]) %&gt;% \n  purrr::transpose()\n\n# num_words + padding + start + oov token = 10000 + 3\nx_train_1h &lt;- one_hot(train$x, 10000 + 3)\nx_test_1h &lt;- one_hot(test$x, 10000 + 3)\ndim(x_train_1h)\n\n\n[1] 25000 10003\n\n\nCode\nnnzero(x_train_1h) / (25000 * (10000 + 3))\n\n\n[1] 0.0131682\n\n\nOnly 1.3% of the entries are nonzero, so this amounts to considerable savings in memory. We create a validation set of size 2,000, leaving 23,000 for training.\n\n\nCode\nset.seed(3)\nival &lt;- sample(seq(along = train$y), 2000)\nitrain &lt;- seq_along(train$y)[-ival]\n\n\nFirst we fit a lasso logistic regression model using glmnet() on the training data, and evaluate its performance on the validation data. Finally, we plot the accuracy, acclmv, as a function of the shrinkage parameter, \\(\\lambda\\). Similar expressions compute the performance on the test data, and were used to produce the left plot in Figure 10.11.\nThe code takes advantage of the sparse-matrix format of x_train_1h, and runs in about 5 seconds; in the usual dense format it would take about 5 minutes.\n\n\nCode\nlibrary(glmnet)\ny_train &lt;- unlist(train$y)\n\nfitlm &lt;- glmnet(x_train_1h[itrain, ], unlist(y_train[itrain]),\n    family = \"binomial\", standardize = FALSE)\nclasslmv &lt;- predict(fitlm, x_train_1h[ival, ]) &gt; 0\nacclmv &lt;- apply(classlmv, 2, accuracy,  unlist(y_train[ival]) &gt; 0)\n\n\nWe applied the accuracy() function that we wrote in Lab 10.9.2 to every column of the prediction matrix classlmv, and since this is a logical matrix of TRUE/FALSE values, we supply the second argument truth as a logical vector as well.\nBefore making a plot, we adjust the plotting window.\n\n\nCode\npar(mar = c(4, 4, 4, 4), mfrow = c(1, 1))\nplot(-log(fitlm$lambda), acclmv)\n\n\n\n\n\n\n\n\n\nNext we fit a fully-connected neural network with two hidden layers, each with 16 units and ReLU activation.\n\n\nCode\nmodel &lt;- nn_module(\n  initialize = function(input_size = 10000 + 3) {\n    self$dense1 &lt;- nn_linear(input_size, 16)\n    self$relu &lt;- nn_relu()\n    self$dense2 &lt;- nn_linear(16, 16)\n    self$output &lt;- nn_linear(16, 1)\n  },\n  forward = function(x) {\n    x %&gt;% \n      self$dense1() %&gt;% \n      self$relu() %&gt;% \n      self$dense2() %&gt;% \n      self$relu() %&gt;% \n      self$output() %&gt;% \n      torch_flatten(start_dim = 1)\n  }\n)\nmodel &lt;- model %&gt;% \n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_binary_accuracy_with_logits())\n  ) %&gt;% \n  set_opt_hparams(lr = 0.001)\n\nfitted &lt;- model %&gt;% \n  fit(\n    # we transform the training and validation data into torch tensors\n    list(\n      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), \n      torch_tensor(unlist(train$y[itrain]))\n    ),\n    valid_data = list(\n      torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()), \n      torch_tensor(unlist(train$y[ival]))\n    ),\n    dataloader_options = list(batch_size = 512),\n    epochs = 10\n  )\n\nplot(fitted)  \n\n\n\n\n\n\n\n\n\nThe fitted object has a get_metrics method that gets both the training and validation accuracy at each epoch. Figure 10.11 includes test accuracy at each epoch as well. To compute the test accuracy, we rerun the entire sequence above, replacing the last line with\n\n\nCode\nfitted &lt;- model %&gt;% \n  fit(\n    list(\n      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), \n      torch_tensor(unlist(train$y[itrain]))\n    ),\n    valid_data = list(\n      torch_tensor(as.matrix(x_test_1h), dtype = torch_float()), \n      torch_tensor(unlist(test$y))\n    ),\n    dataloader_options = list(batch_size = 512),\n    epochs = 10\n  )"
  },
  {
    "objectID": "deepLearningTorch.html#recurrent-neural-networks",
    "href": "deepLearningTorch.html#recurrent-neural-networks",
    "title": "Deep Learning Lab with Torch",
    "section": "5 Recurrent Neural Networks",
    "text": "5 Recurrent Neural Networks\nIn this lab we fit the models illustrated in Section 10.5.\n\n5.1 Sequential Models for Document Classification\nHere we fit a simple LSTM RNN for sentiment analysis with the IMDB movie-review data, as discussed in Section 10.5.1. We showed how to input the data in 10.9.5, so we will not repeat that here.\nWe first calculate the lengths of the documents.\n\n\nCode\nwc &lt;- sapply(seq_along(imdb_train), function(i) length(imdb_train[i]$x))\nmedian(wc)\n\n\n[1] 178\n\n\nCode\nsum(wc &lt;= 500) / length(wc)\n\n\n[1] 0.916\n\n\nWe see that over 91% of the documents have fewer than 500 words. Our RNN requires all the document sequences to have the same length. We hence restrict the document lengths to the last \\(L=500\\) words, and pad the beginning of the shorter ones with blanks. We will use torchdatasets functionality for this.\n\n\nCode\nmaxlen &lt;- 500\nnum_words &lt;- 10000\nimdb_train &lt;- imdb_dataset(root = \".\", split = \"train\", num_words = num_words,\n                           maxlen = maxlen)\nimdb_test &lt;- imdb_dataset(root = \".\", split = \"test\", num_words = num_words,\n                           maxlen = maxlen)\n\nvocab &lt;- c(rep(NA, imdb_train$index_from - 1), imdb_train$get_vocabulary())\ntail(names(vocab)[imdb_train[1]$x])\n\n\n[1] \"compensate\" \"you\"        \"the\"        \"rental\"     \"\"          \n[6] \"d\"         \n\n\nThe last expression shows the last few words in the first document. At this stage, each of the 500 words in the document is represented using an integer corresponding to the location of that word in the 10,000-word dictionary. The first layer of the RNN is an embedding layer of size 32, which will be learned during training. This layer one-hot encodes each document as a matrix of dimension \\(500 \\times 10,000\\), and then maps these \\(10,000\\) dimensions down to \\(32\\).\n\n\nCode\nmodel &lt;- nn_module(\n  initialize = function() {\n    self$embedding &lt;- nn_embedding(10000 + 3, 32)\n    self$lstm &lt;- nn_lstm(input_size = 32, hidden_size = 32, batch_first = TRUE)\n    self$dense &lt;- nn_linear(32, 1)\n  },\n  forward = function(x) {\n    c(output, c(hn, cn)) %&lt;-% (x %&gt;% \n      self$embedding() %&gt;% \n      self$lstm())\n    output[,-1,] %&gt;%  # get the last output\n      self$dense() %&gt;% \n      torch_flatten(start_dim = 1)\n  }\n)\n\n\nThe second layer is an LSTM with 32 units, and the output layer is a single logit for the binary classification task. The rest is now similar to other networks we have fit. We track the test performance as the network is fit, and see that it attains 87% accuracy.\n\n\nCode\nmodel &lt;- model %&gt;% \n  setup(\n    loss = nn_bce_with_logits_loss(),\n    optimizer = optim_rmsprop,\n    metrics = list(luz_metric_binary_accuracy_with_logits())\n  ) %&gt;% \n  set_opt_hparams(lr = 0.001)\n\nfitted &lt;- model %&gt;% fit(\n  imdb_train, \n  epochs = 10,\n  dataloader_options = list(batch_size = 128),\n  valid_data = imdb_test\n)\nplot(fitted)\n\n\n\n\n\n\n\n\n\nCode\npredy &lt;- torch_sigmoid(predict(fitted, imdb_test)) &gt; 0.5\nevaluate(fitted, imdb_test, dataloader_options = list(batch_size = 512))\n\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 0.3737\nacc: 0.8382\n\n\n\n\n5.2 Time Series Prediction\nWe now show how to fit the models in Section 10.5.2 for time series prediction. We first set up the data, and standardize each of the variables.\n\n\nCode\nlibrary(ISLR2)\nxdata &lt;- data.matrix(\n NYSE[, c(\"DJ_return\", \"log_volume\",\"log_volatility\")]\n )\nistrain &lt;- NYSE[, \"train\"]\nxdata &lt;- scale(xdata)\n\n\nThe variable istrain contains a TRUE for each year that is in the training set, and a FALSE for each year in the test set.\nWe first write functions to create lagged versions of the three time series. We start with a function that takes as input a data matrix and a lag \\(L\\), and returns a lagged version of the matrix. It simply inserts \\(L\\) rows of NA at the top, and truncates the bottom.\n\n\nCode\nlagm &lt;- function(x, k = 1) {\n   n &lt;- nrow(x)\n   pad &lt;- matrix(NA, k, ncol(x))\n   rbind(pad, x[1:(n - k), ])\n}\n\n\nWe now use this function to create a data frame with all the required lags, as well as the response variable.\n\n\nCode\narframe &lt;- data.frame(log_volume = xdata[, \"log_volume\"],\n   L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),\n   L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),\n   L5 = lagm(xdata, 5)\n )\n\n\nIf we look at the first five rows of this frame, we will see some missing values in the lagged variables (due to the construction above). We remove these rows, and adjust istrain accordingly.\n\n\nCode\narframe &lt;- arframe[-(1:5), ]\nistrain &lt;- istrain[-(1:5)]\n\n\nWe now fit the linear AR model to the training data using lm(), and predict on the test data.\n\n\nCode\narfit &lt;- lm(log_volume ~ ., data = arframe[istrain, ])\narpred &lt;- predict(arfit, arframe[!istrain, ])\nV0 &lt;- var(arframe[!istrain, \"log_volume\"])\n1 - mean((arpred - arframe[!istrain, \"log_volume\"])^2) / V0\n\n\n[1] 0.413223\n\n\nThe last two lines compute the \\(R^2\\) on the test data, as defined in (3.17).\nWe refit this model, including the factor variable day_of_week.\n\n\nCode\narframed &lt;-\n    data.frame(day = NYSE[-(1:5), \"day_of_week\"], arframe)\narfitd &lt;- lm(log_volume ~ ., data = arframed[istrain, ])\narpredd &lt;- predict(arfitd, arframed[!istrain, ])\n1 - mean((arpredd - arframe[!istrain, \"log_volume\"])^2) / V0\n\n\n[1] 0.4598616\n\n\nTo fit the RNN, we need to reshape these data, since it expects a sequence of \\(L=5\\) feature vectors \\(X=\\{X_\\ell\\}_1^L\\) for each observation, as in (10.20) on page 428. These are lagged versions of the time series going back \\(L\\) time points.\n\n\nCode\nn &lt;- nrow(arframe)\nxrnn &lt;- data.matrix(arframe[, -1])\nxrnn &lt;- array(xrnn, c(n, 3, 5))\nxrnn &lt;- xrnn[,, 5:1]\nxrnn &lt;- aperm(xrnn, c(1, 3, 2))\ndim(xrnn)\n\n\n[1] 6046    5    3\n\n\nWe have done this in four steps. The first simply extracts the \\(n\\times 15\\) matrix of lagged versions of the three predictor variables from arframe. The second converts this matrix to a \\(n\\times 3\\times 5\\) array. We can do this by simply changing the dimension attribute, since the new array is filled column wise. The third step reverses the order of lagged variables, so that index \\(1\\) is furthest back in time, and index \\(5\\) closest. The final step rearranges the coordinates of the array (like a partial transpose) into the format that the RNN module in torch expects.\nNow we are ready to proceed with the RNN, which uses 12 hidden units.\n\n\nCode\nmodel &lt;- nn_module(\n  initialize = function() {\n    self$rnn &lt;- nn_rnn(3, 12, batch_first = TRUE)\n    self$dense &lt;- nn_linear(12, 1)\n    self$dropout &lt;- nn_dropout(0.2)\n  },\n  forward = function(x) {\n    c(output, ...) %&lt;-% (x %&gt;% \n      self$rnn())\n    output[,-1,] %&gt;% \n      self$dropout() %&gt;% \n      self$dense() %&gt;% \n      torch_flatten(start_dim = 1)\n  }\n)\n\nmodel &lt;- model %&gt;% \n  setup(\n    optimizer = optim_rmsprop,\n    loss = nn_mse_loss()\n  ) %&gt;% \n  set_opt_hparams(lr = 0.001)\n\n\nThe output layer has a single unit for the response.\nWe fit the model in a similar fashion to previous networks. We supply the fit function with test data as validation data, so that when we monitor its progress and plot the history function we can see the progress on the test data. Of course we should not use this as a basis for early stopping, since then the test performance would be biased.\n\n\nCode\nfitted &lt;- model %&gt;% fit(\n    list(xrnn[istrain,, ], arframe[istrain, \"log_volume\"]),\n    epochs = 30, # = 200,\n    dataloader_options = list(batch_size = 64),\n    valid_data =\n      list(xrnn[!istrain,, ], arframe[!istrain, \"log_volume\"])\n  )\nkpred &lt;- as.numeric(predict(fitted, xrnn[!istrain,, ]))\n1 - mean((kpred - arframe[!istrain, \"log_volume\"])^2) / V0\n\n\n[1] 0.4028609\n\n\nThis model takes about one minute to train.\nWe could replace the nn_module() command above with the following command:\n\n\nCode\nmodel &lt;- nn_module(\n  initialize = function() {\n    self$dense &lt;- nn_linear(15, 1)\n  },\n  forward = function(x) {\n    x %&gt;% \n      torch_flatten(start_dim = 2) %&gt;% \n      self$dense()\n  }\n)\n\n\nHere, torch_flatten() simply takes the input sequence and turns it into a long vector of predictors. This results in a linear AR model. To fit a nonlinear AR model, we could add in a hidden layer.\nHowever, since we already have the matrix of lagged variables from the AR model that we fit earlier using the lm() command, we can actually fit a nonlinear AR model without needing to perform flattening. We extract the model matrix x from arframed, which includes the day_of_week variable.\n\n\nCode\nx &lt;- model.matrix(log_volume ~ . - 1, data = arframed)\ncolnames(x)\n\n\n [1] \"dayfri\"            \"daymon\"            \"daythur\"          \n [4] \"daytues\"           \"daywed\"            \"L1.DJ_return\"     \n [7] \"L1.log_volume\"     \"L1.log_volatility\" \"L2.DJ_return\"     \n[10] \"L2.log_volume\"     \"L2.log_volatility\" \"L3.DJ_return\"     \n[13] \"L3.log_volume\"     \"L3.log_volatility\" \"L4.DJ_return\"     \n[16] \"L4.log_volume\"     \"L4.log_volatility\" \"L5.DJ_return\"     \n[19] \"L5.log_volume\"     \"L5.log_volatility\"\n\n\nThe -1 in the formula avoids the creation of a column of ones for the intercept. The variable day\\_of\\_week is a five-level factor (there are five trading days), and the -1 results in five rather than four dummy variables.\nThe rest of the steps to fit a nonlinear AR model should by now be familiar.\n\n\nCode\narnnd &lt;- nn_module(\n  initialize = function() {\n    self$dense &lt;- nn_linear(20, 32)\n    self$dropout &lt;- nn_dropout(0.5)\n    self$activation &lt;- nn_relu()\n    self$output &lt;- nn_linear(32, 1)\n    \n  },\n  forward = function(x) {\n    x %&gt;% \n      torch_flatten(start_dim = 2) %&gt;% \n      self$dense() %&gt;% \n      self$activation() %&gt;% \n      self$dropout() %&gt;% \n      self$output() %&gt;% \n      torch_flatten(start_dim = 1)\n  }\n)\narnnd &lt;- arnnd %&gt;% \n  setup(\n    optimizer = optim_rmsprop,\n    loss = nn_mse_loss()\n  ) %&gt;% \n  set_opt_hparams(lr = 0.001)\n\nfitted &lt;- arnnd %&gt;% fit(\n    list(x[istrain,], arframe[istrain, \"log_volume\"]),\n    epochs = 30, \n    dataloader_options = list(batch_size = 64),\n    valid_data =\n      list(x[!istrain,], arframe[!istrain, \"log_volume\"])\n  )\nplot(fitted)\n\n\n\n\n\n\n\n\n\nCode\nnpred &lt;- as.numeric(predict(fitted, x[!istrain, ]))\n1 - mean((arframe[!istrain, \"log_volume\"] - npred)^2) / V0\n\n\n[1] 0.4669683"
  },
  {
    "objectID": "exploratoryAnalysis.html#the-empirical-rule",
    "href": "exploratoryAnalysis.html#the-empirical-rule",
    "title": "Exploratory Analysis (EDA)",
    "section": "3.1 The empirical rule:",
    "text": "3.1 The empirical rule:\nIf the data follows the normal curve then\n\nabout 2/3 (68%) of the data fall within one standard deviation of the mean.\nabout 95% fall within 2 standard deviations of the mean\n99.7% fall within 3 standard deviations of the mean\n\n\n\n\n\n\n\n\n\n\nBiases, systematic errors and unexpected variability are common in data. Failure to discover these problems often leads to flawed analyses and false discoveries. As an example, consider that experiments sometimes fail and not all data processing pipelines, such as the `t.test` function in R, are designed to detect these. Yet, these pipelines still give you an answer. Furthermore, it may be hard or impossible to notice an error was made just from the reported results.\nGraphing data is a powerful approach to detecting these problems. We refer to this as exploratory data analysis (EDA). Many important methodological contributions to existing techniques in data analysis were initiated by discoveries made via EDA."
  },
  {
    "objectID": "exploratoryAnalysis.html#the-p-value",
    "href": "exploratoryAnalysis.html#the-p-value",
    "title": "Exploratory Analysis (EDA)",
    "section": "12.1 The p-value",
    "text": "12.1 The p-value\nKnowing this distribution is incredibly useful in science. If we know the distribution of our data when the null hypothesis is true, referred to as the null distribution, we can compute the probability of observing a value as large as we did in our experiment, referred to as a \\(p\\)-value.\n\nExample.\nWe have data from 24 mice fed two different diets. We want to know if the mice fed with high fat diet increased their body weight more than the mice fed with the control diet.\n\n\nCode\nfemaleMiceWeights &lt;- read.csv(\"data/femaleMiceWeights.csv\")\n\nhead(femaleMiceWeights)\n\n\n  Diet Bodyweight\n1 chow      21.51\n2 chow      28.14\n3 chow      24.04\n4 chow      23.45\n5 chow      23.68\n6 chow      19.79\n\n\nCode\ncontrol&lt;- femaleMiceWeights %&gt;% filter (Diet=='chow')\ntreatment &lt;-  femaleMiceWeights %&gt;% filter (Diet=='hf')\ntm&lt;- mean(treatment$Bodyweight)\ncm&lt;- mean(control$Bodyweight)\nobsdiff &lt;- tm - cm\nprint(paste(\"treatment mean:\", tm))\n\n\n[1] \"treatment mean: 26.8341666666667\"\n\n\nCode\nprint(paste(\"control mean:\", cm))\n\n\n[1] \"control mean: 23.8133333333333\"\n\n\nCode\nprint(paste(\"difference =\", obsdiff))\n\n\n[1] \"difference = 3.02083333333334\"\n\n\nWe can see that there is a difference between the two means but if we choose random samples of 12 individuals from the mice population, each of the samples would have different means just by chance. So how can we be sure if that difference is due to the change in diet or to the random sampling?\nWe are going to recreate that by choosing 24 mice and assigning them randomly to either treatment or control. We will repeat this 10000 times:\n\n\nCode\nn &lt;- 10000\nnull &lt;- vector(\"numeric\",n)\nfor (i in 1:n) {\n  control &lt;- sample(femaleMiceWeights$Bodyweight,12)\n  treatment &lt;- sample(femaleMiceWeights$Bodyweight,12)\n  null[i] &lt;- mean(treatment) - mean(control)\n}\nhead(null)\n\n\n[1] -1.98666667 -0.04333333 -0.76666667  0.95500000 -0.03583333 -0.30250000\n\n\nThe values in null form what we call the null distribution.\nWhat percentage of our 10000 experiments have a difference weight between control and treatment higher or equal to the one we observed for the different diets?\n\n\nCode\nmean(null &gt;= obsdiff)\n\n\n[1] 0.0029\n\n\nWe see a difference as big as the one we observed only a small percentage of the time. This is what is known as a \\(p\\)-value. We can plot the null histogram and mark the line with our observed weight difference for the high fat diet.\n\n\n\n\n\nNull distribution with observed difference marked with vertical red line.\n\n\n\n\n\nAn important point to keep in mind here is that while we defined \\(\\mbox{Pr}(a)\\) by counting cases, we will learn that, in some circumstances, mathematics gives us formulas for \\(\\mbox{Pr}(a)\\) that save us the trouble of computing them as we did here. One example of this powerful approach uses the normal distribution approximation.\nWe will see later on how to calculate the \\(p\\)-values"
  },
  {
    "objectID": "exploratoryAnalysis.html#standardizing-data",
    "href": "exploratoryAnalysis.html#standardizing-data",
    "title": "Exploratory Analysis (EDA)",
    "section": "13.1 Standardizing data",
    "text": "13.1 Standardizing data\nA normal curve is determined by the mean \\(\\bar{x}\\) and the standard deviation \\(s\\) . If the data follow the normal curve, then knowing those two values mean we know the whole histogram. To compute areas under the normal curve, we first standardize the data by subtracting \\(\\bar{x}\\) and dividing by \\(s\\). The resulting value is called the standardized value or \\(z\\)-score \\(z\\)\n\\[\nz = \\frac{x_1 - \\bar{x}}{s}\n\\qquad(10)\\]\nAfter standardizing all points in our data we will get a distribution with mean = 0 and standard deviation =1\n\n\n\n\n\n\n\n\n\nSo now, the value of \\(z\\) for a specific value \\(x_1\\) is indicating how many standard deviations our value is away from the mean, and just by simple approximation using the empirical rule, if for example you standardize your height and you get a value of 1, you already know without having to look into more detail that you are approximately higher than 84% of the population."
  },
  {
    "objectID": "exploratoryAnalysis.html#rnorm",
    "href": "exploratoryAnalysis.html#rnorm",
    "title": "Exploratory Analysis (EDA)",
    "section": "14.1 rnorm()",
    "text": "14.1 rnorm()\nThe rnorm() function generates random numbers from a normal distribution. rnorm(n, mean = 0, sd = 1)\n\nn: Number of observations.\nmean: Mean of the distribution (default is 0).\nsd: Standard deviation of the distribution (default is 1).\n\n\n\nCode\n# Generate 5 random numbers from a normal distribution with mean 10 and standard deviation 2\nrnorm(5, mean = 10, sd = 2)\n\n\n[1] 14.202684 11.409225  8.332027 13.823871 10.672816"
  },
  {
    "objectID": "exploratoryAnalysis.html#dnorm",
    "href": "exploratoryAnalysis.html#dnorm",
    "title": "Exploratory Analysis (EDA)",
    "section": "14.2 dnorm()",
    "text": "14.2 dnorm()\nThe dnorm() function computes the density (height of the probability density function) of the normal distribution. dnorm(x, mean = 0, sd = 1, log = FALSE)\n\nx: A vector of quantiles.\nmean: Mean of the distribution (default is 0).\nsd: Standard deviation of the distribution (default is 1).\nlog: If TRUE, returns the logarithm of the density (default is FALSE).\n\n\n\nCode\n# Compute the density of the normal distribution at x = 2\ndnorm(2, mean = 0, sd = 1)\n\n\n[1] 0.05399097"
  },
  {
    "objectID": "exploratoryAnalysis.html#pnorm",
    "href": "exploratoryAnalysis.html#pnorm",
    "title": "Exploratory Analysis (EDA)",
    "section": "14.3 pnorm()",
    "text": "14.3 pnorm()\nThe pnorm() function computes the cumulative distribution function (CDF) of the normal distribution. pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) - q: A vector of quantiles.\n\nmean: Mean of the distribution (default is 0).\nsd: Standard deviation of the distribution (default is 1).\nlower.tail: If TRUE, returns the cumulative probability up to q (default is TRUE).\nlog.p: If TRUE, returns the logarithm of the cumulative probability (default is FALSE).\n\n\n\nCode\n# Compute the cumulative probability for x = 1.5\npnorm(1.5, mean = 0, sd = 1)\n\n\n[1] 0.9331928"
  },
  {
    "objectID": "exploratoryAnalysis.html#qnorm",
    "href": "exploratoryAnalysis.html#qnorm",
    "title": "Exploratory Analysis (EDA)",
    "section": "14.4 qnorm()",
    "text": "14.4 qnorm()\nThe qnorm() function computes the quantile (inverse of the CDF) of the normal distribution. qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) - p: A vector of probabilities.\n\nmean: Mean of the distribution (default is 0).\nsd: Standard deviation of the distribution (default is 1).\nlower.tail: If TRUE, returns the quantile corresponding to the cumulative probability p (default is TRUE).\nlog.p: If TRUE, p is given as log(p) (default is FALSE).\n\n\n\nCode\n# Compute the quantile for cumulative probability 0.975\nqnorm(0.975, mean = 0, sd = 1)\n\n\n[1] 1.959964\n\n\n\nExamples\nBinomial: Suppose \\(X \\sim \\text{Binomial}(5, 0.6)\\). Then we can evaluate \\(F(1) = P(X \\leq 1) \\approx 0.087\\) in R with pbinom(q = 1, size = 5, prob = 0.6). Note also that qbinom(p = 0.087, size = 5, prob = 0.6) will return 1 as expected.\n\n\nCode\n# Binomial CDF in R\npbinom(q = 1, size = 5, prob = 0.6)\n\n\n[1] 0.08704\n\n\nCode\nqbinom(p = 0.087, size = 5, prob = 0.6)\n\n\n[1] 1\n\n\nExponential: Suppose \\(Y \\sim \\text{Exp}(1)\\). The middle 80% of probability mass is located between the 0.1 and 0.9 quantiles. To find these quantiles of the Exp(1) distribution, save them as a vector in R: a = c(0.1, 0.9) followed by qexp(p = a, rate = 1) which returns the vector (0.105, 2.303). Therefore, we have \\(P(0.105 &lt; Y \\leq 2.303) = 0.8\\).\n\n\nCode\n# Exponential quantiles in R\na &lt;- c(0.1, 0.9)\nqexp(p = a, rate = 1)\n\n\n[1] 0.1053605 2.3025851\n\n\nQuick examples with different distributions\n\nLet \\(X \\sim \\text{Pois}(3)\\). Find \\(P(X = 1)\\). (0.149)\n\n\n\nCode\ndpois(x = 1, lambda = 3)\n\n\n[1] 0.1493612\n\n\n\nLet \\(X \\sim \\text{Pois}(3)\\). Find \\(P(X \\leq 1)\\). (0.199)\n\n\n\nCode\nppois(q = 1, lambda = 3)\n\n\n[1] 0.1991483\n\n\n\nLet \\(X \\sim \\text{Pois}(3)\\). Find \\(P(X &gt; 1)\\). (0.801)\n\n\n\nCode\n1 - ppois(q = 1, lambda = 3)\n\n\n[1] 0.8008517\n\n\n\nLet \\(Y \\sim \\text{Gamma}(2, 1/3)\\). Find \\(P(0.5 &lt; Y &lt; 1.5)\\). (0.078)\n\n\n\nCode\npgamma(q = 1.5, shape = 2, rate = 1/3) - pgamma(q = 0.5, shape = 2, rate = 1/3)\n\n\n[1] 0.07776602\n\n\n\nLet \\(Z \\sim \\text{N}(0,1)\\). Find \\(z\\) such that \\(P(Z &lt; z) = 0.975\\). (1.96)\n\n\n\nCode\nqnorm(p = 0.975)\n\n\n[1] 1.959964\n\n\n\nLet \\(Z \\sim \\text{N}(0,1)\\). Find \\(P(-1.96 &lt; Z &lt; 1.96)\\). (0.95)\n\n\n\nCode\npnorm(q = 1.96) - pnorm(q = -1.96)\n\n\n[1] 0.9500042\n\n\n\nLet \\(Z \\sim \\text{N}(0,1)\\). Find \\(z\\) such that \\(P(-∞ &lt; Z &lt; z) = 0.90\\). (1.64)\n\n\n\nCode\nqnorm(p = 0.90)\n\n\n[1] 1.281552"
  },
  {
    "objectID": "exploratoryAnalysis.html#qq-plot",
    "href": "exploratoryAnalysis.html#qq-plot",
    "title": "Exploratory Analysis (EDA)",
    "section": "15.1 QQ plot",
    "text": "15.1 QQ plot\nTo corroborate that the normal distribution is in fact a good approximation we can use quantile-quantile plots (QQ-plots). Quantiles are best understood by considering the special case of percentiles. The \\(p-th\\) percentile of a list of a distribution is defined as the number \\(q\\) that is bigger than \\(p\\%\\) of numbers. For example, the median \\(50-th\\) percentile is the median. We can compute the percentiles for our list and for the normal distribution.\n\n\nCode\n#generate the percentiles\nps &lt;- seq(0.01,0.99,0.01)\n# The quantile function returns the values below which a given percentage of data falls. \nqs &lt;- quantile(heights,ps)\n#calculates the theoretical quantiles from a normal distribution with the same mean and standard deviation as the heights data. The qnorm function returns the quantiles of the normal distribution for the given probabilities (ps), mean, and standard deviation\nnormalqs &lt;- qnorm(ps,mean(heights),rafalib::popsd(heights))\nplot(normalqs,qs,xlab=\"Normal percentiles\",ylab=\"Height percentiles\")\nabline(0,1) ##identity line\n\n\n\n\n\n\n\n\n\nThis line adds an identity line (a line with slope 1 and intercept 0) to the plot. The abline(0, 1) function draws a 45-degree line through the origin. This line helps to visually assess how closely the data follows a normal distribution. If the points lie close to this line, it suggests that the data is approximately normally distributed. This code is creating a Normal Quantile-Quantile (QQ) Plot. A QQ plot is used to compare the distribution of a dataset to a theoretical distribution—in this case, the normal distribution. If the heights data is normally distributed, the points in the plot will lie approximately along the identity line.\nWe can generate the same plot with a simplified code:\n\n\nCode\nqqnorm(heights)\nqqline(heights) \n\n\n\n\n\n\n\n\n\nData is not always normally distributed. Income is widely cited example. In these cases the average and standard deviation are not necessarily informative since one can’t infer the distribution from just these two numbers. The properties described above are specific to the normal. For example, the normal distribution does not seem to be a good approximation for the direct compensation for 199 United States CEOs in the year 2000\n\n\n\n\n\n\n\n\n\nthe QQ plot would look like this:"
  },
  {
    "objectID": "exploratoryAnalysis.html#pvalues",
    "href": "exploratoryAnalysis.html#pvalues",
    "title": "Exploratory Analysis (EDA)",
    "section": "16.1 Calculating the p-values",
    "text": "16.1 Calculating the p-values\nOnce we have calculated the \\(z\\)-score we can calculate the area on the left of that number under the normal distribution curve, and that will be the probability we are looking for.\n\n\n\n\n\n\n\n\n\nWe can now can find manually the probability using \\(z\\)-score tables. In R, we may use the pnorm() function to find the \\(p\\)-value associated with a \\(z\\)-score, which has the following syntax.\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE)\nWhere\n- q = z-Score value.\n- lower.tail: If TRUE, the probability in the normal distribution to the left of \\(z\\)-score is returned. The probability to the right is returned if FALSE. TRUE is the default value.\nTo find the \\(p\\)-value for the two-tailed test:\n2*pnorm(q=0.71, lower.tail=FALSE)\n\n\nCode\n# Calculate the area to the left of the z-score\n(area_left &lt;- pnorm(zScore))\n\n\n[1] 0.7602499\n\n\nCode\n# Calculate the area to the right of the z-score\n(area_right &lt;- 1 - area_left)\n\n\n[1] 0.2397501\n\n\nCode\n#or\n(area_rigt&lt;- pnorm(zScore,lower.tail=FALSE))\n\n\n[1] 0.2397501\n\n\nCode\n# Calculate the cumulative probability using the z-score, in our case we use the left area:\nprobability &lt;- pnorm(zScore)\nprint(probability)\n\n\n[1] 0.7602499\n\n\nInstead of plugging in the values and calculate the \\(z\\)-score first, we can simplify our operations in r like this:\n\n\nCode\n# Parameters\nn &lt;- 50\nk &lt;- 12\np &lt;- 0.20\n# Calculate the cumulative probability using an expression\nprobability &lt;- pnorm((k - n * p) / sqrt(n * p * (1 - p)))\nprint(probability)\n\n\n[1] 0.7602499"
  },
  {
    "objectID": "linearModels.html#prediction",
    "href": "linearModels.html#prediction",
    "title": "Statistical Relationship and Analysis",
    "section": "1.1 Prediction",
    "text": "1.1 Prediction\nIn many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y: \\[\n\\hat{Y}= \\hat{f}(X)\n\\] where \\(\\hat{f}\\) represents our estimate for \\(f\\) and \\(\\hat{Y}\\) represents the resulting prediction for Y. In this setting \\(\\hat{f}\\) is often treated as a black box and we are not tipically concerned with the extact function, provided that it yiedls accurate predictions of Y."
  },
  {
    "objectID": "linearModels.html#inference",
    "href": "linearModels.html#inference",
    "title": "Statistical Relationship and Analysis",
    "section": "1.2 Inference",
    "text": "1.2 Inference\nWe are often interested in understanding the association between Y and X1, . . . ,Xp. In this situation we wish to estimate f, but our goal is not necessarily to make predictions for Y . Now ˆ f cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions: • Which predictors are associated with the response? It is often the case that only a small fraction of the available predictors are substantially associated with Y . Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application. • What is the relationship between the response and each predictor? Some predictors may have a positive relationship with Y , in the sense that larger values of the predictor are associated with larger values of Y . Other predictors may have the opposite relationship. Depending on the complexity of f, the relationship between the response and a given predictor may also depend on the values of the other predictors. • Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Historically, most methods for estimating f have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.\nRelationships between variables are usually better visualized using scatterplots:\n\n\nCode\ndata(father.son,package=\"UsingR\") \nx &lt;- father.son$fheight\ny &lt;- father.son$sheight\nplot(x,y,xlab=\"Father's height in inches\",ylab=\"Son's height in inches\",main=paste(\"correlation =\",signif(cor(x,y),2)))\n\n\n\n\n\n\n\n\n\nThe scatter plot shows a general trend: the taller the father the taller to son. A summary of this trend is the correlation coefficient which in this case is 0.5. We motivate this statistic by trying to predict son’s height using the father’s.\nQuick facts:\n\n\nCorrelation’s curveball: Correlation doesn’t imply causation. Two variables can move together without one causing the other, often due to lurking third variables.\nResidual Revelations: Residuals plots, which showcase deviations from a model, can often tell more about data relationships than the fit itself, hinting at non-linearity, heteroscedasticity, or other intricacies.\nMulticollinearity Maze: In multiple regression, if predictors are too related, it can muddy interpretations."
  },
  {
    "objectID": "linearModels.html#correlation-coefficient",
    "href": "linearModels.html#correlation-coefficient",
    "title": "Statistical Relationship and Analysis",
    "section": "2.1 Correlation coefficient",
    "text": "2.1 Correlation coefficient\nIf we standardize the values of our data, the line that we use to predict one value from the other follows a slope. That slope is the correlation.\nIn our substance_abuse dataset we have two variables for each observation DLA1 and DLA2 that we can plot in a scatter plot to see their relationship:\n\n\nCode\nfile &lt;- here::here(\"data\", \"substance_abuse.xlsx\") \nsubstance_abuse &lt;- read_excel(file) \nsubstance_abuse$DLA_improvement &lt;- substance_abuse$DLA2 - substance_abuse$DLA1 \ntreatment &lt;- dplyr::filter(substance_abuse,\n                    Program == \"Intervention\")\n\nggplot(treatment, aes(x = DLA1, y = DLA2)) + \n  geom_point() # Linear\n\n\n\n\n\n\n\n\n\nIt shows a linear relationship between the two variables: one one increase the other tend to increase as well.\nTo show other possible relationships between data we will generate a dataset that shows a non-linear relationship\n\n\nCode\nset.seed(7)\nx &lt;- abs(rchisq(100, 10) - 2.7)\ny &lt;- 1/x + rnorm(100, 1, .1) - .9\ndata &lt;- data.frame(x, y)\nggplot(data, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThe correlation coefficient formula:\n\\[\nr = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\bar{x}}{s_x} \\right) \\times \\left( \\frac{y_i - \\bar{y}}{s_y} \\right)\n\\]\nThis formula standardizes the values of ( x ) and ( y ) by subtracting their means \\(\\bar{x}\\) and \\(\\bar{y}\\) and dividing by their standard deviations \\(s_x\\) and \\(s_y\\), then calculates the average product of these standardized values.\nSample correlation measures the strength of an observed linear relationship between two variables in a data set. Fundamentally, it measures the spread (or variability) of sample data along a line of best fit\nAs a convention, the variable on the horizontal axis is called explanatory variable or predictor and the one in the vertical axis is called response variable.\n\n\nCode\nggplot(treatment, aes(x = DLA1, y = DLA2)) + \n  geom_point() +\n  geom_smooth(method = \"lm\",\n                           se = FALSE,\n                           color = \"grey\")\n\n\n\n\n\n\n\n\n\nTo calculate the correlation we just have to use the command ‘cor’ and it’s value will go from -1 to 1. The sign of r gives the direction of the association and its absolute value gives the strength.\nSince both x and y were standardized when computing r, r has no units ans it is not affected by changing the center or the scale of either variable.\n\n\nCode\ncor(treatment$DLA1,\n    treatment$DLA2)\n\n\n[1] 0.930508\n\n\nBe careful to refer to correlation only when a pair of variables has a linear relationship. For instance, the following variables have a clear association, but their coefficient of correlation is close to 0:\n\n\nCode\nset.seed(2)\nx &lt;- rnorm(100, 0, .5)\ny &lt;-  (x^2-1) + rnorm(1.5, -2.5, .8)\ndata &lt;- data.frame(x, y)\nggplot(data, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\ncor(x,y)\n\n\n[1] 0.05255463\n\n\nWe need to be aware that when we are working with samples, we may find correlation between two variables just by random chance. For example, if we plot the variable DLA_improvement against Age we can see that there is no correlation between the two:\n\n\nCode\nggplot(treatment, aes(Age, y = DLA_improvement)) + \n  geom_point() # No association\n\n\n\n\n\n\n\n\n\nCode\ncor(substance_abuse$Age, substance_abuse$DLA_improvement)\n\n\n[1] -0.08746504\n\n\nbut if we get a random sample of those variables, we may find some correlation:\n\n\nCode\nset.seed(0)\n\nsample &lt;- slice_sample(treatment, n = 25)\ncor(sample$Age, sample$DLA_improvement)\n\n\n[1] 0.3802099\n\n\nCode\nggplot(sample, aes(x = Age, y = DLA_improvement)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"grey\")\n\n\n\n\n\n\n\n\n\nIn the next chapters we will learn how to find out if the correlation found in our sample shows a true relationship in the population or if it is due to random chance.\nRemember that we already saw that the correlation coefficient is very sensitive to outliers. (See spearman correlation)"
  },
  {
    "objectID": "linearModels.html#covariance",
    "href": "linearModels.html#covariance",
    "title": "Statistical Relationship and Analysis",
    "section": "2.2 Covariance",
    "text": "2.2 Covariance\nCovariance measures the extent to which two variables change together. If the variables tend to increase together, the covariance is positive. If one tends to increase while the other decreases, the covariance is negative. For variables X and Y: \\[ \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\tag{1}\\]\nCovariance and correlation are related, but they’re not the same thing:\nCovariance:\n\nMeasures: The degree to which two variables change together.\nScale: It’s dependent on the units of the variables, making it difficult to interpret the strength of the relationship.\n\nCorrelation:\n\nMeasures: The strength and direction of the linear relationship between two variables.\nScale: Standardizes the measure of covariance, ranging between -1 and 1.\n\nSummary:\nCovariance tells you if two variables tend to increase or decrease together, but not how strong that relationship is.\nCorrelation standardizes this measure, making it easier to interpret the strength and direction of the relationship on a consistent scale.\nSo, while covariance gives you a raw measure of how two variables vary together, correlation refines it to a standardized measure of that relationship.\nCalculating the covariance matrix in r is easy using the cov() function:\n\n\nCode\n# Sample data frame\ndata &lt;- data.frame(X = c(4, 6, 8, 10, 12), Y = c(2, 4, 6, 8, 10), Z = c(1, 2, 3, 4, 5))\n\n# Calculate covariance matrix\ncov_matrix &lt;- cov(data)\nprint(cov_matrix)\n\n\n   X  Y   Z\nX 10 10 5.0\nY 10 10 5.0\nZ  5  5 2.5\n\n\nHow to Interpret: Diagonal Elements: These are the variances of the individual variables.\n\nVar(X): 10.00\nVar(Y): 10.00\nVar(Z): 2.50\n\nOff-Diagonal Elements: These are the covariances between pairs of variables.\n\nCov(X, Y): 10.00\nCov(X, Z): 5.00\nCov(Y, Z): 5.00\n\nExample Interpretation: Variance: The variance of X is 10, meaning X values are spread out with a variance of 10 around their mean.\nCovariance (X and Y): The positive covariance of 10 between X and Y suggests that when X increases, Y tends to increase as well.\nCovariance (X and Z): The positive covariance of 5 between X and Z indicates a positive relationship, though not as strong as between X and Y.\nCovariance (Y and Z): The positive covariance of 5 between Y and Z also indicates a positive relationship.\nIn essence, the covariance matrix provides insights into how pairs of variables vary together, helping you understand relationships and dependencies in your data."
  },
  {
    "objectID": "linearModels.html#Residualsumofsquares",
    "href": "linearModels.html#Residualsumofsquares",
    "title": "Statistical Relationship and Analysis",
    "section": "2.3 Regression Line and the Method of Least Squares (RSS)",
    "text": "2.3 Regression Line and the Method of Least Squares (RSS)\nIf the scatterplot shows a linear association, then this relationship between our data points can be summarized by a line. The equation for a line is \\(\\hat{y}*i=a+bx_*i\\). The idea is to choose the line that minimizes the sum of the squared distances between the observed \\(y_i\\) and the predicted \\(\\hat{y_i}\\)\nThe method of least squares is the method we use to minimize this Residual sum of squares (RSS)\n\\[\nRSS=\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\tag{2}\\]\n\\[\nRSS=\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\left( y_i - (\\hat{\\beta_o} + \\hat{\\beta_1} x_i) \\right)^2\n\\]\n\\[\nRSS = (y_1 - \\beta_0 - \\beta_1 x_{1})^2 + (y_2 - \\beta_0 - \\beta_1 x_{2})^2 + \\cdots + (y_n - \\beta_0 - \\beta_1 x_{n})^2\n\\]\nThe intercept \\(\\beta_0\\) is the expected value of Y when X=0, in other words, the value at which the regression line crosses the Y axis.\n\\[\n\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\] \\[\n\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means\nIt turns out that \\(\\beta_1=r\\frac{s_y}{s_x}\\) where \\(r\\) is the correlation coefficient and \\(s_y\\) and \\(s_x\\) are the two standard deviations and \\(\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_i}\\bar{x}\\).\nThe main use of regression is to predict y from x. If we are given x, then we need \\(r, \\bar{x},\\bar{y},s_x, s_y\\) to calculate the regression line. In code, we can compute that line using ‘lm’ in r language\n\nExercise: midterm vs final exam student’s grades (cont)\nWe know that the average score for the midterm exams was 49.5 and the average score for the final exam is 69.1, and the standard deviation for the midterm exams was 10.2 while the standard deviation for the final exams was 11.8. We also know that the correlation coefficient is 0.67.\nPredict the final exam score of a student who scored 41 on the midterm.\nSol: 41 is 8,5 below the midterm average. We want to standardize this value. We already know how to calculate this: \\[z = \\frac{x_1-\\bar{x}}{sd} = \\frac{41-49.5}{10.2}=-0.83\\] so now looking at the formula for the regression line we expect \\(y\\) to be r times 0.83 times the standard deviation of the final exam below the average final score. \\[\\bar{y}\\pm r*sd_{final}*0.83 =69.1 -0.67*11.8*0.83=62.5\\]\n\n\n2.3.1 Predicting x from y:\nWhen predicting x from y it is a mistake to use the regression line that we calculated for regressing y on x and solve for x. This is because regressing x on y will result in a different regression line."
  },
  {
    "objectID": "linearModels.html#normal-approximation-in-regression",
    "href": "linearModels.html#normal-approximation-in-regression",
    "title": "Statistical Relationship and Analysis",
    "section": "2.4 Normal approximation in regression",
    "text": "2.4 Normal approximation in regression\nLinear regression requires that the data in the scatter plot is more or less following an elypse shape. Once we have the scatter plot with the regression line, we know that given a value of \\(x\\), the value of \\(y\\) will be close to the \\(y\\) point of the regression line for the \\(x\\) value. In the image below, given \\(x\\), we trace a vertical line to the regression line and then we trace a perpendicular line to the \\(y\\) axis (green line) and that is approximately the expected value of \\(y\\).\n\n\n\n\n\n\n\n\n\nbut we also know that \\(y\\) will not be necessarily exactly at that point, but at certain distance of it, and that approximation follows the normal curve, so we can then use normal approximation to calculate the value of \\(y\\): subtract off the predicted value \\(\\hat{y}\\), then divide by \\(\\sqrt{1-r^2} * s_y\\).\n\\[\nz = \\frac{y_1-\\bar{y}}{\\sqrt{1-r^2}\\cdot s_y}\n\\]\n\nExercise:\nAmong the students who scored around 41 on the midterm, what percentage scored above 60 on the final?\nWe already predicted that the expected value for the score on the final exam for a student that scored 41 in midterm is 62.5. That means that the normal curve is centered at 62.5 so the percentage of students that will score above 60 is the area bellow that normal curve to the right of 60 (red line).\n\n\n\n\n\n\n\n\n\nFor that we standardize 60 like this: \\(z = \\frac{y_1 - \\bar{y}}{\\sqrt{1 - r^2} \\cdot s_y} = \\frac{60 - 62.5}{\\sqrt{1 - 0.67^2} \\cdot 11.8} = -0.29\\) so the standardized curve would look like this:\n\n\n\n\n\n\n\n\n\nnow we just have to use a table or software to calculate that value\n\n\nCode\nzscore &lt;- -0.29\n\n# Calculate the area to the left of the z-score\narea_left &lt;- pnorm(zscore)\n\n# Calculate the area to the right of the z-score\narea_right &lt;- 1 - area_left\n\n# Print the result\narea_right\n\n\n[1] 0.6140919\n\n\nin our case we are interested to the area to the right, so our answer is 61.41%\n\n\nExercise: midterm vs final exam student’s grades\nIn a biology class, both the midterm scores and the final exam scores have an average of 50 and a standard deviation of 10. The scatterplot looks football-shaped and the correlation coefficient is 0.6.\nEmily got exactly the mean score of 50 on the midterm. What is your prediction for Emily’s score on the final?\nsol:\nSince the distance in standard deviations of Emily’s midterm score from the average midterm score is 0, the corresponding distance of Emily’s predicted final score from the average final score is r*0 = 0, so our prediction is the average =50\nWhat is the “give or take” number for your prediction?\nsol:\n\\[\n10\\sqrt{1-0.6^2} = 8\n\\]"
  },
  {
    "objectID": "linearModels.html#residualplots",
    "href": "linearModels.html#residualplots",
    "title": "Statistical Relationship and Analysis",
    "section": "2.5 Residuals",
    "text": "2.5 Residuals\nAs mentioned before, the observed values of \\(Y\\) will not fall directly over the regression line. At each value of \\(X=x\\) there is typically a distribution of possible \\(Y\\) values.\nFor each observation we will have the predicted value \\(\\hat{y}\\) and the observed value \\(y\\) . That difference is called the residual. The residual plot is a scatter plot of the residuals against the \\(x\\) values. It should show an unstructured horizontal band and it is used to check if the regression you are using is appropriate.\nFor example if we have data following a linear correlation:\n\n\nCode\n#|echo = FALSE\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\n\n# Create the scatter plot\nplot(x, y, main = \"Scatter Plot with Linear Regression Line\", \n     xlab = \"X-axis\", ylab = \"Y-axis\", pch = 19, col = \"blue\")\n\n# Add the linear regression line\nabline(lm(y ~ x), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nthe residual scatter plot will look like this:\n\n\nCode\nmodel &lt;- lm(y ~ x)\n\n# Get the residuals and fitted values\nresiduals &lt;- resid(model)\nfitted_values &lt;- fitted(model)\n\n# Create the residual scatter plot\nplot(fitted_values, residuals, \n     main = \"Residuals vs Fitted Values\", \n     xlab = \"Fitted Values\", \n     ylab = \"Residuals\", \n     pch = 19, \n     col = \"blue\")\n\n# Add a horizontal line at 0\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nnow, if we have data that does not follow a linear correlation:\n\n\nCode\n#|echo = FALSE\nset.seed(7)\nx &lt;- abs(rchisq(100, 10) - 2.7)\ny &lt;- 1/x + rnorm(100, 1, .1) - .9\ndata &lt;- data.frame(x, y)\nggplot(data, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nthe residual will look different:\n\n\nCode\n#|echo = FALSE\nmodel &lt;- lm(y ~ x)\n\n# Get the residuals and fitted values\nresiduals &lt;- resid(model)\nfitted_values &lt;- fitted(model)\n\n# Create the residual scatter plot\nplot(fitted_values, residuals, \n     main = \"Residuals vs Fitted Values\", \n     xlab = \"Fitted Values\", \n     ylab = \"Residuals\", \n     pch = 19, \n     col = \"blue\")\n\n# Add a horizontal line at 0\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nthat is an indication that the relation is not linear, and we should not use regression for this data.\n\n2.5.1 Heteroscedascidity\nAnother variation is when the scatter shows in a fan way, this means that the variability increases with the value of \\(X\\), this is called heteroscedastic.\n\n\nCode\n#|echo = FALSE\n\n# Generate some example data with heteroscedasticity\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100, sd = 0.9 * x)\n\nplot(x, y, \n     main = \"(Heteroscedasticity)\", \n     pch = 19, \n     xlim = c(0, max(x)),\n     col = \"blue\")\n\n\n\n\n\n\n\n\n\nSometimes we can fix a non linear relationship by modifying y values, x values or both, for example applying log transformation.\n\n\n2.5.2 Outliers, leverage and influential points\nPoints with very large residuals are called outliers and they should be examined to see if they represent an interesting phenomena or an error in the data. {#highLeverage} When the value that deviates is the \\(x\\) value and not the \\(y\\) we say that it is a high leverage point, and it has the potential to cause a big change in the regression line.\n\n\nCode\n#|echo = FALSE\n# Generate some example data\nset.seed(123)\nx &lt;- rnorm(20)\ny &lt;- 2 * x + rnorm(20)\n\n# Add a high leverage point\nx &lt;- c(x, 10)\ny &lt;- c(y,4)\n\n# Fit linear models\nmodel_with_point &lt;- lm(y ~ x)\nmodel_without_point &lt;- lm(y ~ x, subset = -21)\n\n# Create the scatter plot\nplot(x, y, \n     main = \"High Leverage Point Influence\", \n     xlab = \"X\", \n     ylab = \"Y\", \n     pch = 19, \n     col = ifelse(1:21 == 21, \"red\", \"blue\"))\n\n# Add regression lines\nabline(model_with_point, col = \"red\", lwd = 2)\nabline(model_without_point, col = \"green\", lwd = 2, lty = 2)\n\n# Add a legend\nlegend(\"bottomright\", legend = c(\"With High Leverage Point\", \"Without High Leverage Point\"), \n       col = c(\"red\", \"green\"), lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\n\nIn order to quantify and observation’s leverage we can compute the leverage statistic. A large value of this statistic indicates an observation with high leverage. The formula for simple linear regression is: \\[\nh_i=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum^n_{i'=1}(x_{i'}-\\bar{x})^2}\n\\]{#leverageStatistic} There is another formula for multiple predictors, but we won’t see the formula here. The leverage statistic is always between \\(1/n\\) and 1 and the average leverage for all the observations is always equal to \\((p+1)/n\\). So if a given observation has a leverage statistic that greatly exceeds \\((p+1)/n\\) then we may suspect that the corresponding point has leverage.\n\nBeware of data that are summaries (e.g. averages of data). Those are less variable than individual observations and correlations between averages tend to overstate the strength of the relationship"
  },
  {
    "objectID": "linearModels.html#assessing-the-accuracy-of-the-coefficient-estimates-and-confidence-intervals",
    "href": "linearModels.html#assessing-the-accuracy-of-the-coefficient-estimates-and-confidence-intervals",
    "title": "Statistical Relationship and Analysis",
    "section": "2.6 Assessing the Accuracy of the Coefficient Estimates and Confidence Intervals",
    "text": "2.6 Assessing the Accuracy of the Coefficient Estimates and Confidence Intervals\nThe standard error of an estimator reflects how it varies under repeated sampling.\n\\[\nSE(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^ n(x_i-\\bar{x})^2}\n\\tag{3}\\] What the denominator in this formula is telling us is that the more spread out our values are along the x axis, the better we will be able to predict the correct slope.\n\\[\nSE(\\hat{\\beta_0})^2 = \\sigma^2 \\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^ n(x_i-\\bar{x})^2}\\right]\n\\tag{4}\\] where \\(\\sigma^2\\) is the variance of the error \\(\\sigma^2=Var(\\epsilon)\\)\nThese standard errors can be used to compute confidence intervals For example for 95 confidence:\n\\[\n\\hat{\\beta_1} \\mp 2 * SE(\\hat{\\beta_1}) = \\left[\\hat{\\beta_1}-2 * SE(\\hat{\\beta_1}),\\hat{\\beta_1}+2 * SE(\\hat{\\beta_1})\\right]\n\\] ## Hypothesis testing and significance of correlation\nThe most important thing to remember about correlation testing is that it only applies to quantitative variables that have a generally linear relationship.\nA correlation test checks the null hypothesis that the population correlation \\(\\rho\\) is equal to 0. This is, there is no relationship between X and Y. \\(H_0:\\beta_1=0\\)\nTo test the null hypothesis, we compute a t-statistic \\[\nt= \\frac{\\hat{\\beta_1}-0}{SE(\\hat{\\beta_1})}\n\\] This will have a t-distribution with n-2 degrees of freedom. Using statistical software, it is easy to compute the probability of observing any value equal to t or larger. We call this probability the \\(p\\)-value. The confidence intervals can also tell us if we should reject the null hypothesis. If 0 falls in between the range of the confidence interval, that means that we cannot exclude the possibility that the slope is 0, meaning that there is no relation between the parameters. The confidence interval is also going to tell you how big the effect is, so it is always a good practice to compute confidence intervals as well as doing a hypothesis testing.\nThe test assumes that the data isn’t too skewed in any other direction and that there aren’t extreme outliers. As usual, a larger sample provides a certain degree of protection.\nIf the sample is very small, the test will be unlikely to rule out the possibility that \\(\\rho = 0\\) even if that is the case. That is, the test will be underpowered.\nSimilarly, if the sample is very large, the test will be likely to conclude that \\(\\rho \\neq 0\\) **. This gets to an important idea that we’ve touched on before which is that there is a difference between a sample statistic being statistically significant and it actually being important or meaningful. For instance in a very large sample you may get a sample correlation of 0.001 but it may come back as statistically significant. In the real world, you should always take into consideration not just statistical significance but also effect size when you make real world decisions.\n\n\nCode\ntestResult&lt;- cor.test(treatment$DLA2,\n         treatment$DLA1)\nreport(testResult)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Pearson's product-moment correlation between treatment$DLA2 and\ntreatment$DLA1 is positive, statistically significant, and very large (r =\n0.93, 95% CI [0.90, 0.95], t(141) = 30.17, p &lt; .001)\n\n\nIn this type of test our null hypothesis is that the true correlation between those two variables is 0. In our example, the test is giving us a sample correlation of 0.930508 and a \\(p\\)-value 0 which in simple terms mean that the probability of getting this correlation results in our sample if the true correlation in the population was actually 0 would be less than 0.001, so highly unlikely. The test also gives us a confidence interval, in our case 0.9045168, 0.9496115 and its interpretation is exactly the same as it always is for our confidence interval: if we were to go out and get many many samples from the same population and compute correlations of them, 95% of the time that confidence interval would capture the true parameter, in our case the true population correlation.\nWith this new knowledge we are going to test the sample we created before showing a relationship between age and DLA_improvement and compare it with the correlation for the population (all records in our file)\n\n\nCode\n#population correlation between age and dla improvement\n\nggplot(treatment, aes(x = Age, y = DLA_improvement)) + \n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"grey\")\n\n\n\n\n\n\n\n\n\nCode\nreport(cor.test(treatment$Age,\n         treatment$DLA_improvement))\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Pearson's product-moment correlation between treatment$Age and\ntreatment$DLA_improvement is positive, statistically not significant, and very\nsmall (r = 0.06, 95% CI [-0.10, 0.22], t(141) = 0.74, p = 0.461)\n\n\nCode\n#small sample correlation:\nset.seed(0)\nsample &lt;- slice_sample(treatment, n = 25)\nreport(cor.test(sample$Age,\n         sample$DLA_improvement))\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Pearson's product-moment correlation between sample$Age and\nsample$DLA_improvement is positive, statistically not significant, and large (r\n= 0.38, 95% CI [-0.02, 0.67], t(23) = 1.97, p = 0.061)\n\n\n\nA significant correlation means that the likelihood of observing such a correlation (or stronger) by random chance is low, given the null hypothesis of no correlation."
  },
  {
    "objectID": "linearModels.html#interpretation-of-linear-regression-results",
    "href": "linearModels.html#interpretation-of-linear-regression-results",
    "title": "Statistical Relationship and Analysis",
    "section": "2.7 Interpretation of Linear Regression results",
    "text": "2.7 Interpretation of Linear Regression results\nAny statistical package will have a function to calculate the line that fits the linear relationship in our data. In r it is lm command. We put the response variable first, and then the explanatory variable, finally the dataset.\n\n\nCode\nlm(DLA2 ~ DLA1, data = treatment)\n\n\n\nCall:\nlm(formula = DLA2 ~ DLA1, data = treatment)\n\nCoefficients:\n(Intercept)         DLA1  \n     0.7243       0.9660  \n\n\nIt is read DLA2 is explained by DLA1 and we get two coefficients called the slope and the intercept. The interpretation of these numbers is very important. When you have a straight line, the slope shows the increase in \\(y\\) when \\(x\\) increases by 1. The intercept means where is the \\(y\\) value when \\(x=0\\)\nThe most important use of a regression line is that it allows you to make predictions.\nIn our example, if we get a new patient with DLA1 of 3.6 we can use these values to calculate what it is the expected DLA2 value:\n\n\nCode\nDLA1 &lt;- 3.6\nDLA2 &lt;- .07243 + .9660 *DLA1\nDLA2\n\n\n[1] 3.55003\n\n\nA very important thing to bear in mind is that a regression line should only be used to make predictions on individuals whose explanatory variable falls in the range of the values used to calculate the linear regression model, for example in our case our model was trained with DLA1 between 2.5 and 5 so we should not use this model to make predictions on individuals whose DLA1 is 6, for example.\nWe can get more information about our model if we ask for their summary:\n\n\nCode\nsummary(lm(DLA2 ~ DLA1, data = treatment))\n\n\n\nCall:\nlm(formula = DLA2 ~ DLA1, data = treatment)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37181 -0.12929  0.01212  0.12537  0.37095 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  0.72426    0.12610   5.743         0.0000000548 ***\nDLA1         0.96603    0.03202  30.167 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1831 on 141 degrees of freedom\nMultiple R-squared:  0.8658,    Adjusted R-squared:  0.8649 \nF-statistic:   910 on 1 and 141 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe \\(p\\)-value that corresponds with the intercept is a the \\(p\\)-value of a test done against the null hypothesis that the intercept is actually 0.\nThe \\(p\\)-value that corresponds with the dependent variable is the result of a test done against the null hypothesis that the slope is actually 0.\nWe can retrieve those \\(p\\)-values using the coefficients table from our lm result:\n\n\nCode\ntestResult&lt;- summary(lm(DLA2 ~ DLA1, data = treatment))\ntestResult$coefficients[,4]\n\n\n                                                           (Intercept) \n0.00000005484994337451973682624750683345382640254683792591094970703125 \n                                                                  DLA1 \n0.00000000000000000000000000000000000000000000000000000000000000225742 \n\n\nWhen looking at coefficients one must be aware that the units we used in our data will affect them. The z-statistic is not affected by units.\nWe can use the names() function in order to find out what other pieces of information are stored. Although we can extract these quantities by name—e.g. testResults$coefficients—it is safer to use the extractor functions like coef() to access them.\n\n\nCode\nlm.fit&lt;- lm(DLA2 ~ DLA1, data = treatment)\nnames(lm.fit)\n\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nCode\ncoef(lm.fit)\n\n\n(Intercept)        DLA1 \n  0.7242570   0.9660278 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\n\n\nCode\nconfint(lm.fit)\n\n\n                2.5 %    97.5 %\n(Intercept) 0.4749591 0.9735549\nDLA1        0.9027204 1.0293353\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of Y for a given value of X.\n\n\nCode\n# Choose a random value in x\nrandom_x &lt;- sample(x, 1)\n\npredict(lm.fit, newdata = data.frame(x = random_x),\n    interval = \"confidence\")\n\n\n       fit      lwr      upr\n1 4.201957 4.165918 4.237996\n\n\nCode\npredict(lm.fit, newdata = data.frame(x = random_x),\n    interval = \"prediction\")\n\n\n       fit      lwr      upr\n1 4.201957 3.838095 4.565819\n\n\nR is also retrieving a p value for the overall model that’s coming from an Anova F-Test. that corresponds with the coefficient for the slope.\nour diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\n\nCode\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\n\n\n\n\n\n\nExercises:\nUsing the mpg dataset:\n\nIs there a linear relationship between city mileage and highway mileage in this set?\n\n\nCode\nfile2 &lt;- here::here(\"data\", \"mpg_2008.xlsx\")\nmpg_2008 &lt;- read_excel(file2)\n\nggplot(mpg_2008, aes(x = cty, y = hwy))+\n  geom_point()+\n  geom_smooth(method ='lm',\n              se = FALSE,\n              color = 'purple' )\n\n\n\n\n\n\n\n\n\nWhat is the sample correlation?\n\n\nCode\ncor(mpg_2008$cty, mpg_2008$hwy)\n\n\n[1] 0.964125\n\n\nDoes the sample correlation provide evidence that the population correlation is different from zero?\n\n\nCode\ntestResult&lt;- cor.test(mpg_2008$cty, mpg_2008$hwy)\ntestResult\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mpg_2008$cty and mpg_2008$hwy\nt = 38.949, df = 115, p-value &lt; 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9486199 0.9750111\nsample estimates:\n     cor \n0.964125 \n\n\nCode\nreport(testResult)\n\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Pearson's product-moment correlation between mpg_2008$cty and mpg_2008$hwy\nis positive, statistically significant, and very large (r = 0.96, 95% CI [0.95,\n0.98], t(115) = 38.95, p &lt; .001)\n\n\nYes.\nFind the equation of the regression line\n\n\n\nCode\nlm (cty ~hwy, data= mpg_2008)\n\n\n\nCall:\nlm(formula = cty ~ hwy, data = mpg_2008)\n\nCoefficients:\n(Intercept)          hwy  \n     1.0171       0.6687  \n\n\nCode\nlm (hwy ~ cty, data= mpg_2008)\n\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg_2008)\n\nCoefficients:\n(Intercept)          cty  \n     0.2388       1.3900  \n\n\nthe equation will be \\(cty = 0.6687 * hwy + 1.017\\) or \\(hwy = 1.39 *cty + .2388\\)\nEstimate the highway mileage of an unknown car in this population with a city mileage of 24 miles per gallon.\n\n\nCode\n1.29*24+.2388\n\n\n[1] 31.1988"
  },
  {
    "objectID": "linearModels.html#simulating-random-numbers-from-a-linear-model-in-r",
    "href": "linearModels.html#simulating-random-numbers-from-a-linear-model-in-r",
    "title": "Statistical Relationship and Analysis",
    "section": "2.8 Simulating Random Numbers from a linear model in r",
    "text": "2.8 Simulating Random Numbers from a linear model in r\nSuppose we want to simulate data from the linear model:\n\\[\ny = \\beta_0+\\beta_1 x+ \\epsilon\n\\]\nwhere epsilon has a normal distribution with sd =2, the intercept \\(\\beta_0\\) = 0.5 and the slope \\(\\beta_1\\) = 2\n\n\nCode\nset.seed(20)\nx&lt;- rnorm(100)\ne&lt;- rnorm(100,0,2)\ny= 0.5 +2*x+e\nsummary(y)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-6.4084 -1.5402  0.6789  0.6893  2.9303  6.5052 \n\n\nCode\nplot(x,y)\n\n\n\n\n\nif \\(x\\) is binomial:\n\n\nCode\nset.seed(10)\nset.seed(20)\nx&lt;- rbinom(100,1,0.5)\ne&lt;- rnorm(100,0,2)\ny= 0.5 +2*x+e\nsummary(y)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-3.4361 -0.2675  1.7804  1.5723  2.8810  6.9169 \n\n\nCode\nplot(x,y)\n\n\n\n\n\n\nLinear regression models the relationship between a dependent variable and one or more independent variables. Remember a well-fitting model doesn’t always imply causality."
  },
  {
    "objectID": "linearModels.html#linearmodelscategorical",
    "href": "linearModels.html#linearmodelscategorical",
    "title": "Statistical Relationship and Analysis",
    "section": "3.1 Categorical predictors in regression models.",
    "text": "3.1 Categorical predictors in regression models.\nSometimes we may want to include in the model a variable that is not numeric, for example gender. To do that we create what we call a dummy variable, for example \\(x_i = 1\\) if the person is female and \\(x_i = 0\\) if it is a male. The resulting model will be: \\[\ny_i=\\beta_o+\\beta_1x_i+\\epsilon_i\n\\] which will result in this if the person is female: \\[\ny_i=\\beta_o+\\beta_1+\\epsilon_i\n\\] and if the person is male: \\[\ny_i=\\beta_o+\\epsilon_i\n\\] so what this is telling us is that \\(\\beta_1\\) is the effect of being female vs the baseline (in this case male)\nIf we have more than two levels what we do is create more dummy variables. For example we look at three different etnicities, Asian, Caucasian and African American, we create:\n\\(x_{i1} = 0\\) if person is Asian and \\(x_{i1} = 1\\) if the person is not Asian \\(x_{i2} = 0\\) if person is Caucasian and \\(x_{i2} = 1\\) if the person is not. We don’t need a level for African American because that will be deducted when \\(x_{i1} = 0\\) and \\(x_{i2} = 0\\). So for categorical variables we create \\(k-1\\) dummy variables. The level with no dummy variable is known as baseline. The choice of a baseline will not affect the fit of the model, the residual sum of squares would be the same, but the coefficient and the \\(p\\)-values will change because each other category will be compared with the baseline.\nThe equation will now look like this: \\[\ny_1 = \\beta_o+\\beta_1x_{i1}+\\beta_2x_{i2}+\\epsilon_i\n\\begin{cases}\n\\beta_0 + \\beta_1 + e_1 & \\text{if condition 1} \\\\\n\\beta_0 + \\beta_2 + e_1 & \\text{if condition 2} \\\\\n\\beta_0 + e_1 & \\text{if condition 3}\n\\end{cases}\n\\]\n\nExample\nThe Carseats data from the library ISLR2 includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically.\n\n\nCode\nlm.fit &lt;- lm(Sales ~ ShelveLoc, \n    data = Carseats)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = Sales ~ ShelveLoc, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3066 -1.6282 -0.0416  1.5666  6.1471 \n\nCoefficients:\n                Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       5.5229     0.2388  23.131 &lt; 0.0000000000000002 ***\nShelveLocGood     4.6911     0.3484  13.464 &lt; 0.0000000000000002 ***\nShelveLocMedium   1.7837     0.2864   6.229         0.0000000012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.339 on 397 degrees of freedom\nMultiple R-squared:  0.3172,    Adjusted R-squared:  0.3138 \nF-statistic: 92.23 on 2 and 397 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\ncontrasts(Carseats$ShelveLoc)\n\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location."
  },
  {
    "objectID": "linearModels.html#interactions",
    "href": "linearModels.html#interactions",
    "title": "Statistical Relationship and Analysis",
    "section": "3.2 Interactions",
    "text": "3.2 Interactions\nIn our previous examples, we have assume independence from one parameter to the others, but that is not always the case, sometimes the change in one predictor affects the results in another.\n\nExample Imagine we run a campaign to sell a product, and we advertise in three mediums, newspapers, radio and tv. It could be that the effect of radio increases the effectiveness of the adds in tv, this in marketing is called synergy effect, and in statistics we refer to it as interaction effect. If we detect this interaction, spending part of our budget in radio and part on tv could be more effective than spending all in only the media with the most effect.\n\n\nCode\nAdvertising &lt;- readr::read_csv(\"data/Advertising.csv\")\nlm.fit&lt;- lm(sales ~ TV + radio, data = Advertising)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = sales ~ TV + radio, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7977 -0.8752  0.2422  1.1708  2.8328 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  2.92110    0.29449   9.919 &lt;0.0000000000000002 ***\nTV           0.04575    0.00139  32.909 &lt;0.0000000000000002 ***\nradio        0.18799    0.00804  23.382 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.681 on 197 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8962 \nF-statistic: 859.6 on 2 and 197 DF,  p-value: &lt; 0.00000000000000022\n\n\nto include interactions in our model we create a new variable with the product of the two predictors. If we ignore newspaper our equation would be:\n\\[\nsales = \\beta_0+\\beta_1\\times TV+\\beta_2\\times Radio + \\beta_3 \\times(radio\\times TV)+ \\epsilon\n\\]\n\\[\nsales = \\beta_0+(\\beta_1 + \\beta_3\\times Radio) \\times TV+\\beta_2\\times Radio + \\epsilon\n\\] and if we get a summary of the linear model we will see if the interaction between tv and radio is indeed significant or not:\n\n\nCode\nlm.fit_interaction &lt;- lm(sales ~ TV * radio , data = Advertising)\nsummary(lm.fit_interaction)\n\n\n\nCall:\nlm(formula = sales ~ TV * radio, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3366 -0.4028  0.1831  0.5948  1.5246 \n\nCoefficients:\n              Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 6.75022020 0.24787137  27.233 &lt;0.0000000000000002 ***\nTV          0.01910107 0.00150415  12.699 &lt;0.0000000000000002 ***\nradio       0.02886034 0.00890527   3.241              0.0014 ** \nTV:radio    0.00108649 0.00005242  20.727 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9435 on 196 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 \nF-statistic:  1963 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\n# or\nlm.fit_interaction &lt;- lm(sales ~ TV + radio + TV:radio , data = Advertising)\nsummary(lm.fit_interaction)\n\n\n\nCall:\nlm(formula = sales ~ TV + radio + TV:radio, data = Advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3366 -0.4028  0.1831  0.5948  1.5246 \n\nCoefficients:\n              Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 6.75022020 0.24787137  27.233 &lt;0.0000000000000002 ***\nTV          0.01910107 0.00150415  12.699 &lt;0.0000000000000002 ***\nradio       0.02886034 0.00890527   3.241              0.0014 ** \nTV:radio    0.00108649 0.00005242  20.727 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9435 on 196 degrees of freedom\nMultiple R-squared:  0.9678,    Adjusted R-squared:  0.9673 \nF-statistic:  1963 on 3 and 196 DF,  p-value: &lt; 0.00000000000000022\n\n\nthe \\(p\\)-value of the interaction indicates that in our example this interaction is significant, we can also see how our R-squared is higher now (0.9678) than when we did not include the interaction in our model (0.8972). This means that (96.8-89.7)/(100-89.7)=69% of the variability in sales that remains after fitting the initial model has been explained by the interaction term.\nThe coefficients estimates in the table suggest that an increase in radio advertising of $1000 is associated with increased sales of \\((\\hat{\\beta_2}+\\hat{\\beta_3}\\times TV)\\times 1000 = 29+1.1\\times TV units\\) An increase in TV advertising of $1000 is associated with an increase of sales of \\((\\hat{\\beta_1}+\\hat{\\beta_3}\\times radio)\\times 1000 = 19+1.1\\times radio units\\)\n\nSometimes it is the case that an interaction term has a very small \\(p\\)-value, but the associated main effects do not. The hierarchy principle states that if we include an interaction in a model, we should also include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant."
  },
  {
    "objectID": "linearModels.html#non-linear-transformations-of-the-predictors.",
    "href": "linearModels.html#non-linear-transformations-of-the-predictors.",
    "title": "Statistical Relationship and Analysis",
    "section": "3.3 Non-linear transformations of the predictors.",
    "text": "3.3 Non-linear transformations of the predictors.\nIf a linear model does not quite fit our data:\n\n\nCode\nlm.fit&lt;- lm(mpg~ hp, data= mtcars)\nplot(mtcars$hp, mtcars$mpg)\nabline(lm.fit)\n\n\n\n\n\n\n\n\n\nCode\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421 &lt; 0.0000000000000002 ***\nhp          -0.06823    0.01012  -6.742          0.000000179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 0.0000001788\n\n\nwe can transform this into a polynomial regression by making extra variables to accommodate polynomials, for example we add another variable horsepower squared:\n\n\nCode\nlm.fit &lt;- lm(mpg ~ hp + I(hp^2), data = mtcars)\n\nplot(mtcars$hp, mtcars$mpg, xlab = \"Horsepower\", ylab = \"Miles per Gallon\")\n\npoints(mtcars$hp, fitted(lm.fit), col = \"pink\", lwd = 2)\n\n#or if we want to show a line:\n# Create a sequence of hp values for prediction\nhp_seq &lt;- seq(min(mtcars$hp), max(mtcars$hp), length.out = 100)\n\n# Predict mpg for each hp value in the sequence\npredicted_mpg &lt;- predict(lm.fit, newdata = data.frame(hp = hp_seq))\n\nlines(hp_seq, predicted_mpg, col = \"blue\", lwd = 2)\n\n\n\n\n\nCode\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = mpg ~ hp + I(hp^2), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5512 -1.6027 -0.6977  1.5509  8.7213 \n\nCoefficients:\n               Estimate  Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 40.40911720  2.74075944  14.744 0.00000000000000523 ***\nhp          -0.21330826  0.03488387  -6.115 0.00000116297223609 ***\nI(hp^2)      0.00042082  0.00009844   4.275            0.000189 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.077 on 29 degrees of freedom\nMultiple R-squared:  0.7561,    Adjusted R-squared:  0.7393 \nF-statistic: 44.95 on 2 and 29 DF,  p-value: 0.000000001301\n\n\nbut there is a better way of fitting polynomials with r using the poly() function:\n\n\nCode\nlm.fit &lt;- lm(mpg ~ poly(hp,4), data = mtcars) \nplot(mtcars$mpg ~ mtcars$hp)\npoints(mtcars$hp, fitted(lm.fit), col = \"pink\", lwd = 2)"
  },
  {
    "objectID": "linearModels.html#potential-problems-in-linear-models",
    "href": "linearModels.html#potential-problems-in-linear-models",
    "title": "Statistical Relationship and Analysis",
    "section": "3.4 Potential problems in linear models",
    "text": "3.4 Potential problems in linear models\nWe already talked about most of them, but let’s do a summary:\n\n\nNon linearity: The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. We saw how we can make use of the residual plot to see if the relation is liner or not when we talked about residuals\nCorrelation of error terms: An important assumption of the linear regression model is that the error terms are uncorrelated.If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors and this will affect the \\(p\\)-values, confidence intervals etc. Why might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. time series. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no discernible pattern. On the other hand, if the error terms are positively correlated, then we may see tracking in the residuals.Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals’ heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family, eat the same diet,or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.\nNon-constant variance of error terms (heteroscedasticity) : error terms have a constant variance, Var(ϵi) = σ2. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. Unfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response\nOutliers: An outlier is a point for which \\(y\\) is far from the value predicted by the model. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance it affects the RSE. Since the RSE is used to compute all confidence intervals and \\(p\\)-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Residual plots can be used to identify outliers.\nHigh-leverage points: Observations with high leverage have an unusual value for \\(x_i\\). Removing a high leverage point has much more impact than removing an outlier. In a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors.\nCollinearity: we already dedicated a section to these problem."
  },
  {
    "objectID": "linearModels.html#advanced-anova-techniques",
    "href": "linearModels.html#advanced-anova-techniques",
    "title": "Statistical Relationship and Analysis",
    "section": "4.1 Advanced ANOVA Techniques",
    "text": "4.1 Advanced ANOVA Techniques\nNow we are going to use the mpg_2008 data set again to see if there is a difference in highway millage with respect to the ‘Drive’ categorical variable (front wheel, rear wheel and 4 wheel drive). Let’s do the same we did above and calculate the average per group first\n\n\nCode\nmpg_2008 |&gt; \n  group_by(drv) |&gt; \n  dplyr::summarize(mean(hwy))\n\n\n# A tibble: 3 × 2\n  drv   `mean(hwy)`\n  &lt;chr&gt;       &lt;dbl&gt;\n1 4            19.5\n2 f            28.4\n3 r            21.3\n\n\nand we plot the data:\n\n\n\n\n\n\n\n\n\nThe boxplot allows us to see that the spread of the data in these groups is similar (the width of these boxes is about the same) so we can run an anova test on these data.\nif we calculate the anova for this we find that the evidence support the hypothesis that there is in fact a difference in hwy between these groups:\n\n\nCode\nmodel &lt;- aov(hwy ~ drv, data = mpg_2008)\nsummary(model)\n\n\n             Df Sum Sq Mean Sq F value              Pr(&gt;F)    \ndrv           2   2140    1070    66.8 &lt;0.0000000000000002 ***\nResiduals   114   1826      16                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs we mentioned, the ANOVA test does not tell us anything about each group in particular, for this we can run another test, one recommended is the Tukey Honest Significant Difference test and this will give us a \\(p\\)-value and a confidence interval for each pair of categories in our data.\n\n\nCode\nTukeyHSD(model)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = hwy ~ drv, data = mpg_2008)\n\n$drv\n         diff        lwr       upr     p adj\nf-4  8.967498   7.092124 10.842873 0.0000000\nr-4  1.804233  -1.046509  4.654975 0.2933625\nr-f -7.163265 -10.043796 -4.282734 0.0000001\n\n\nThe Tukey Honest Significant Difference test is better than running a Welch Two sample test by selecting just two variables in our dataset and testing one against the other. If we just run three different t-test comparing the three groups, we’re potentially going to have an increased probability of a false positive, so the Tukey HSD test is specifically controlling for that multiple comparison problem.\nLet’s run a Welch Two sample test over one of the pairs in this dataset and see how it varies from the results from the TukeyHSD test:\n\n\nCode\nmpg_r4 &lt;- filter(mpg_2008, drv != \"f\")\nt.test(mpg_r4$hwy ~ mpg_r4$drv)\n\n\n\n    Welch Two Sample t-test\n\ndata:  mpg_r4$hwy by mpg_r4$drv\nt = -1.6257, df = 24.322, p-value = 0.1169\nalternative hypothesis: true difference in means between group 4 and group r is not equal to 0\n95 percent confidence interval:\n -4.0931471  0.4846815\nsample estimates:\nmean in group 4 mean in group r \n       19.48148        21.28571 \n\n\nwe can see that the \\(p\\)-value here is smaller than the one calculated for that same pair in the Tukey Test.\n\nANOVA tests if there are statistically significant differences between the means of three or more groups. It essentially extends the t-test to multiple groups.\n\nRecap:\n\n\nSample correlation measures the strength and direction of the linear relationship between two variables, providing insights into their association.\nHypothesis testing and significance of correlation allow you to determine whether the observed correlation is statistically significant, indicating a relationship beyond random chance.\nLinear regression enables you to model relationships between variables, predicting outcomes and understanding the impact of independent variables on the dependent variable.\nANOVA is a statistical technique used to compare means between multiple groups, assessing whether there are significant differences among the groups.\nIndependence testing of categorical variables examines whether there is a relationship between two categorical variables, determining if they are independent or associated.\nKnown the assumptions: whether it’s correlation, regression or ANOVA, always ensure that assumptions (like normality or homoscedasticity) are met before drawing any conclusions."
  },
  {
    "objectID": "linearModels.html#poisson-distribution",
    "href": "linearModels.html#poisson-distribution",
    "title": "Statistical Relationship and Analysis",
    "section": "5.1 Poisson Distribution",
    "text": "5.1 Poisson Distribution\nThe Poisson distribution is a probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space, provided that these events occur with a known constant mean rate and independently of the time since the last event.\n\n5.1.1 Characteristics\n\nDiscrete distribution: The Poisson distribution applies to events that can be counted in whole numbers.\nParameter: The Poisson distribution is characterized by a single parameter, () (lambda), which represents the average rate (mean) of occurrences within a given interval.\nProbability Mass Function (PMF): The probability of observing (k) events (where (k) is a non-negative integer) is given by the formula:\n\n\\[\n  P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\] where (e) is the base of the natural logarithm (approximately equal to 2.71828), and (k!) is the factorial of (k).\n\n\n5.1.2 Usage\n\nModeling Rare Events: The Poisson distribution is often used to model rare events, such as the number of phone calls received by a call center per hour or the number of accidents occurring at a busy intersection per day.\nIndependent Events: It assumes that the number of events occurring in disjoint intervals are independent.\n\n\n\n5.1.3 Example\nSuppose a bookstore observes that, on average, 3 customers enter the store per hour. To calculate the probability that exactly 5 customers will enter the store in the next hour, we can use the Poisson distribution with (= 3) and (k = 5):\n\n\nCode\n# Calculate the probability of exactly 5 customers entering the bookstore\nlambda &lt;- 3\nk &lt;- 5\nprobability &lt;- (lambda^k * exp(-lambda)) / factorial(k)\nprobability\n\n\n[1] 0.1008188\n\n\n\n\nCode\n# Compute the probability using dpois\ndpois(5, lambda = 3)\n\n\n[1] 0.1008188"
  },
  {
    "objectID": "linearModels.html#poison-regression-model",
    "href": "linearModels.html#poison-regression-model",
    "title": "Statistical Relationship and Analysis",
    "section": "5.2 Poison regression model",
    "text": "5.2 Poison regression model\nWe will use the dataset bikershare from ISLR2 package. The response is bikers that are the number of hourly users in bikeshare program in Washington DC.\nFirst we are just going to plot the data in the dataset, without any fitting for the number of riders per hour of the day:\n\n\n\n\n\n\n\n\n\nNow if we use boxplots we can see how as the number of bikers increase, so does the standard deviation:\n\n\n\n\n\n\n\n\n\nIf we use linear model to make predictions and we plot the results as we did before in a scatter plot:\nTo perform this analysis, first we fit a linear regression model. We are going to show two different ways of doing it:\n\n\nCode\nlm.fit &lt;- lm(\nbikers ~ mnth + hr + workingday + temp + weathersit ,\ndata = Bikeshare)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                          Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)                -68.632      5.307 -12.932 &lt; 0.0000000000000002 ***\nmnthFeb                      6.845      4.287   1.597             0.110398    \nmnthMarch                   16.551      4.301   3.848             0.000120 ***\nmnthApril                   41.425      4.972   8.331 &lt; 0.0000000000000002 ***\nmnthMay                     72.557      5.641  12.862 &lt; 0.0000000000000002 ***\nmnthJune                    67.819      6.544  10.364 &lt; 0.0000000000000002 ***\nmnthJuly                    45.324      7.081   6.401  0.00000000016282293 ***\nmnthAug                     53.243      6.640   8.019  0.00000000000000121 ***\nmnthSept                    66.678      5.925  11.254 &lt; 0.0000000000000002 ***\nmnthOct                     75.834      4.950  15.319 &lt; 0.0000000000000002 ***\nmnthNov                     60.310      4.610  13.083 &lt; 0.0000000000000002 ***\nmnthDec                     46.458      4.271  10.878 &lt; 0.0000000000000002 ***\nhr1                        -14.579      5.699  -2.558             0.010536 *  \nhr2                        -21.579      5.733  -3.764             0.000168 ***\nhr3                        -31.141      5.778  -5.389  0.00000007260801066 ***\nhr4                        -36.908      5.802  -6.361  0.00000000021092958 ***\nhr5                        -24.135      5.737  -4.207  0.00002611246755722 ***\nhr6                         20.600      5.704   3.612             0.000306 ***\nhr7                        120.093      5.693  21.095 &lt; 0.0000000000000002 ***\nhr8                        223.662      5.690  39.310 &lt; 0.0000000000000002 ***\nhr9                        120.582      5.693  21.182 &lt; 0.0000000000000002 ***\nhr10                        83.801      5.705  14.689 &lt; 0.0000000000000002 ***\nhr11                       105.423      5.722  18.424 &lt; 0.0000000000000002 ***\nhr12                       137.284      5.740  23.916 &lt; 0.0000000000000002 ***\nhr13                       136.036      5.760  23.617 &lt; 0.0000000000000002 ***\nhr14                       126.636      5.776  21.923 &lt; 0.0000000000000002 ***\nhr15                       132.087      5.780  22.852 &lt; 0.0000000000000002 ***\nhr16                       178.521      5.772  30.927 &lt; 0.0000000000000002 ***\nhr17                       296.267      5.749  51.537 &lt; 0.0000000000000002 ***\nhr18                       269.441      5.736  46.976 &lt; 0.0000000000000002 ***\nhr19                       186.256      5.714  32.596 &lt; 0.0000000000000002 ***\nhr20                       125.549      5.704  22.012 &lt; 0.0000000000000002 ***\nhr21                        87.554      5.693  15.378 &lt; 0.0000000000000002 ***\nhr22                        59.123      5.689  10.392 &lt; 0.0000000000000002 ***\nhr23                        26.838      5.688   4.719  0.00000241267941358 ***\nworkingday                   1.270      1.784   0.711             0.476810    \ntemp                       157.209     10.261  15.321 &lt; 0.0000000000000002 ***\nweathersitcloudy/misty     -12.890      1.964  -6.562  0.00000000005600358 ***\nweathersitlight rain/snow  -66.494      2.965 -22.425 &lt; 0.0000000000000002 ***\nweathersitheavy rain/snow -109.745     76.667  -1.431             0.152341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 0.00000000000000022\n\n\nIn this model the first level of hr (0) and mnth (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines.\nWe created another second model with numbered values for mnth and hr.\n\n\nCode\ncontrasts (Bikeshare$hr) = contr.sum (24)\ncontrasts (Bikeshare$mnth) = contr.sum (12)\nlm.fit &lt;- lm(\n  bikers ~  mnth + hr + workingday + temp + weathersit ,\ndata = Bikeshare)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                           Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)                 73.5974     5.1322  14.340 &lt; 0.0000000000000002 ***\nmnth1                      -46.0871     4.0855 -11.281 &lt; 0.0000000000000002 ***\nmnth2                      -39.2419     3.5391 -11.088 &lt; 0.0000000000000002 ***\nmnth3                      -29.5357     3.1552  -9.361 &lt; 0.0000000000000002 ***\nmnth4                       -4.6622     2.7406  -1.701              0.08895 .  \nmnth5                       26.4700     2.8508   9.285 &lt; 0.0000000000000002 ***\nmnth6                       21.7317     3.4651   6.272   0.0000000003747098 ***\nmnth7                       -0.7626     3.9084  -0.195              0.84530    \nmnth8                        7.1560     3.5347   2.024              0.04295 *  \nmnth9                       20.5912     3.0456   6.761   0.0000000000146005 ***\nmnth10                      29.7472     2.6995  11.019 &lt; 0.0000000000000002 ***\nmnth11                      14.2229     2.8604   4.972   0.0000006740476467 ***\nhr1                        -96.1420     3.9554 -24.307 &lt; 0.0000000000000002 ***\nhr2                       -110.7213     3.9662 -27.916 &lt; 0.0000000000000002 ***\nhr3                       -117.7212     4.0165 -29.310 &lt; 0.0000000000000002 ***\nhr4                       -127.2828     4.0808 -31.191 &lt; 0.0000000000000002 ***\nhr5                       -133.0495     4.1168 -32.319 &lt; 0.0000000000000002 ***\nhr6                       -120.2775     4.0370 -29.794 &lt; 0.0000000000000002 ***\nhr7                        -75.5424     3.9916 -18.925 &lt; 0.0000000000000002 ***\nhr8                         23.9511     3.9686   6.035   0.0000000016537185 ***\nhr9                        127.5199     3.9500  32.284 &lt; 0.0000000000000002 ***\nhr10                        24.4399     3.9360   6.209   0.0000000005566528 ***\nhr11                       -12.3407     3.9361  -3.135              0.00172 ** \nhr12                         9.2814     3.9447   2.353              0.01865 *  \nhr13                        41.1417     3.9571  10.397 &lt; 0.0000000000000002 ***\nhr14                        39.8939     3.9750  10.036 &lt; 0.0000000000000002 ***\nhr15                        30.4940     3.9910   7.641   0.0000000000000239 ***\nhr16                        35.9445     3.9949   8.998 &lt; 0.0000000000000002 ***\nhr17                        82.3786     3.9883  20.655 &lt; 0.0000000000000002 ***\nhr18                       200.1249     3.9638  50.488 &lt; 0.0000000000000002 ***\nhr19                       173.2989     3.9561  43.806 &lt; 0.0000000000000002 ***\nhr20                        90.1138     3.9400  22.872 &lt; 0.0000000000000002 ***\nhr21                        29.4071     3.9362   7.471   0.0000000000000874 ***\nhr22                        -8.5883     3.9332  -2.184              0.02902 *  \nhr23                       -37.0194     3.9344  -9.409 &lt; 0.0000000000000002 ***\nworkingday                   1.2696     1.7845   0.711              0.47681    \ntemp                       157.2094    10.2612  15.321 &lt; 0.0000000000000002 ***\nweathersitcloudy/misty     -12.8903     1.9643  -6.562   0.0000000000560036 ***\nweathersitlight rain/snow  -66.4944     2.9652 -22.425 &lt; 0.0000000000000002 ***\nweathersitheavy rain/snow -109.7446    76.6674  -1.431              0.15234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 0.00000000000000022\n\n\nNotice that we used contrast for month and hour. Bikeshare$hr and Bikeshare$mnth are categorical variables representing hour and month, respectively. Sum-to-Zero contrasts ensure that the coefficients of the categories sum to zero. This makes the interpretation of the regression coefficients easier.\nFor instance, if you have 24 hours in a day and set up sum-to-zero contrasts (contr.sum(24)), the sum of all hour coefficients will be zero.\nThis helps in comparing each category to the overall mean effect, rather than to a reference category.\nBy default, categorical variables are typically converted to dummy variables (0/1), which can lead to difficulties in interpreting the results, especially with a large number of categories.\nIn this second model the coefficient estimate for the last level of mnth is not zero: instead, it equals the negative of the sum of the coefficient estimates for all of the other levels. Similarly, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all of the other levels. This indicates that the difference between the mean level and the coefficients of hr and mnth in the second model will always total to zero.\nWe use the linear model to predict the values:\n\n\n\n\n\n\n\n\n\nOne of first things that we notice is that there are negative values (red points) predicted, and this is because the linear model does not have any type of constraint for that, so linear models are not the best choice for counts, we have a better alternative and this is the Poisson Regression Model.\nThe Probability mass function (PMF) is this for a single variance: \\[\nP(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}\n\\tag{12}\\] where:\n\n\\(P(Y = k)\\): Probability of observing ( k ) events.\n\\(\\lambda\\): Average rate (mean number of events) in a given interval.\n\\(e\\): Euler’s number (approximately 2.71828).\n\\(k!\\): Factorial of \\(k\\) (number of events).\n\nNote that for the Poisson distribution, the mean and the variance are directly related, meaning that when the mean is higher, the variance is also higher. In fact we assume that the variance equals the mean.\nThis formula calculates the probability of exactly \\(k\\) events occurring in a fixed interval when events happen at a constant mean rate and independently of the time since the last event. The Poisson distribution is useful for modeling the number of events in a specific time.\nWhen we have multiple parameters, the model changes to include the covariates:\n\\[\n\\lambda(X_1,\\dots,X_p)=e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}\n\\tag{13}\\]\nNow we are going to use glm() function with family Poisson in r to fit this model:\n\n\nCode\nglm.fit &lt;- glm(bikers ~  mnth + hr + workingday + temp + weathersit ,\ndata = Bikeshare, family=poisson)\nsummary(glm.fit)\n\n\n\nCall:\nglm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    family = poisson, data = Bikeshare)\n\nCoefficients:\n                           Estimate Std. Error  z value             Pr(&gt;|z|)\n(Intercept)                4.118245   0.006021  683.964 &lt; 0.0000000000000002\nmnth1                     -0.670170   0.005907 -113.445 &lt; 0.0000000000000002\nmnth2                     -0.444124   0.004860  -91.379 &lt; 0.0000000000000002\nmnth3                     -0.293733   0.004144  -70.886 &lt; 0.0000000000000002\nmnth4                      0.021523   0.003125    6.888   0.0000000000056631\nmnth5                      0.240471   0.002916   82.462 &lt; 0.0000000000000002\nmnth6                      0.223235   0.003554   62.818 &lt; 0.0000000000000002\nmnth7                      0.103617   0.004125   25.121 &lt; 0.0000000000000002\nmnth8                      0.151171   0.003662   41.281 &lt; 0.0000000000000002\nmnth9                      0.233493   0.003102   75.281 &lt; 0.0000000000000002\nmnth10                     0.267573   0.002785   96.091 &lt; 0.0000000000000002\nmnth11                     0.150264   0.003180   47.248 &lt; 0.0000000000000002\nhr1                       -0.754386   0.007879  -95.744 &lt; 0.0000000000000002\nhr2                       -1.225979   0.009953 -123.173 &lt; 0.0000000000000002\nhr3                       -1.563147   0.011869 -131.702 &lt; 0.0000000000000002\nhr4                       -2.198304   0.016424 -133.846 &lt; 0.0000000000000002\nhr5                       -2.830484   0.022538 -125.586 &lt; 0.0000000000000002\nhr6                       -1.814657   0.013464 -134.775 &lt; 0.0000000000000002\nhr7                       -0.429888   0.006896  -62.341 &lt; 0.0000000000000002\nhr8                        0.575181   0.004406  130.544 &lt; 0.0000000000000002\nhr9                        1.076927   0.003563  302.220 &lt; 0.0000000000000002\nhr10                       0.581769   0.004286  135.727 &lt; 0.0000000000000002\nhr11                       0.336852   0.004720   71.372 &lt; 0.0000000000000002\nhr12                       0.494121   0.004392  112.494 &lt; 0.0000000000000002\nhr13                       0.679642   0.004069  167.040 &lt; 0.0000000000000002\nhr14                       0.673565   0.004089  164.722 &lt; 0.0000000000000002\nhr15                       0.624910   0.004178  149.570 &lt; 0.0000000000000002\nhr16                       0.653763   0.004132  158.205 &lt; 0.0000000000000002\nhr17                       0.874301   0.003784  231.040 &lt; 0.0000000000000002\nhr18                       1.294635   0.003254  397.848 &lt; 0.0000000000000002\nhr19                       1.212281   0.003321  365.084 &lt; 0.0000000000000002\nhr20                       0.914022   0.003700  247.065 &lt; 0.0000000000000002\nhr21                       0.616201   0.004191  147.045 &lt; 0.0000000000000002\nhr22                       0.364181   0.004659   78.173 &lt; 0.0000000000000002\nhr23                       0.117493   0.005225   22.488 &lt; 0.0000000000000002\nworkingday                 0.014665   0.001955    7.502   0.0000000000000627\ntemp                       0.785292   0.011475   68.434 &lt; 0.0000000000000002\nweathersitcloudy/misty    -0.075231   0.002179  -34.528 &lt; 0.0000000000000002\nweathersitlight rain/snow -0.575800   0.004058 -141.905 &lt; 0.0000000000000002\nweathersitheavy rain/snow -0.926287   0.166782   -5.554   0.0000000279379459\n                             \n(Intercept)               ***\nmnth1                     ***\nmnth2                     ***\nmnth3                     ***\nmnth4                     ***\nmnth5                     ***\nmnth6                     ***\nmnth7                     ***\nmnth8                     ***\nmnth9                     ***\nmnth10                    ***\nmnth11                    ***\nhr1                       ***\nhr2                       ***\nhr3                       ***\nhr4                       ***\nhr5                       ***\nhr6                       ***\nhr7                       ***\nhr8                       ***\nhr9                       ***\nhr10                      ***\nhr11                      ***\nhr12                      ***\nhr13                      ***\nhr14                      ***\nhr15                      ***\nhr16                      ***\nhr17                      ***\nhr18                      ***\nhr19                      ***\nhr20                      ***\nhr21                      ***\nhr22                      ***\nhr23                      ***\nworkingday                ***\ntemp                      ***\nweathersitcloudy/misty    ***\nweathersitlight rain/snow ***\nweathersitheavy rain/snow ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance:  228041  on 8605  degrees of freedom\nAIC: 281159\n\nNumber of Fisher Scoring iterations: 5\n\n\nand the predicted values plot show that we don’t have negative counts anymore. We must use the argument type= “response” which tells R to output probabilities of the form \\(P(Y=1|X)\\) as opposed to other information such as the logit.\n\n\n\n\n\n\n\n\n\nIn order to visualize the outputs of the models, we plot the response estimates against the coefficients for both the linear model and the Poisson model. But first, it is important to obtain the coefficient estimates associated with the last month and hour. The coefficients for January through November can be obtained directly from the lm.fit object. The coefficient for December must be explicitly computed as the negative sum of all the other months. The linear model results:\n\n\n\n\n\n\n\n\n\nand the glm results:\n\n\n\n\n\n\n\n\n\nAnd for the hours using the linear model:\n\n\n\n\n\n\n\n\n\nAnd the glm poisson model:\n\n\n\n\n\n\n\n\n\nThe predictions from the Poisson regression model are correlated with those from the linear model; however, our Poisson model produces non-negative outputs. For Poisson regression the responses at each level of X become more variable with increasing means, where variance=mean.In addition, the mean values of Y at each level of X fall on a curve, not a line. AS a result, at either very low or very high levels of Y, the Poisson regression predictions tend to be larger than those from the linear model."
  },
  {
    "objectID": "linearModels.html#simulating-data-from-a-generalized-linear-model",
    "href": "linearModels.html#simulating-data-from-a-generalized-linear-model",
    "title": "Statistical Relationship and Analysis",
    "section": "5.3 Simulating data from a Generalized Linear Model",
    "text": "5.3 Simulating data from a Generalized Linear Model\nSuppose we want to simulate from a Poisson model where Y~poisson(\\(\\mu\\)) $ log = _0_1x$ and \\(\\beta_0 = 0.5\\) and \\(\\beta_1 = 0.3\\)\n\n\nCode\nset.seed(1)\nx&lt;- rnorm(100)\nlog.mu&lt;- 0.5+0.3 *x\ny &lt;- rpois(100, exp(log.mu))\nsummary(y)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    1.00    1.55    2.00    6.00 \n\n\nCode\nplot(x,y)"
  },
  {
    "objectID": "matrix.html",
    "href": "matrix.html",
    "title": "Matrix Algebra",
    "section": "",
    "text": "This document is a summary of different stats courses:"
  },
  {
    "objectID": "matrix.html#matrix-multiplication-by-scalar",
    "href": "matrix.html#matrix-multiplication-by-scalar",
    "title": "Matrix Algebra",
    "section": "5.1 Matrix multiplication by scalar",
    "text": "5.1 Matrix multiplication by scalar\nWhen you have a matrix and you multiply it by a scalar, you multiply each element of the matrix by that scalar: Given a scalar (k) and a matrix (A):\n\\[\nk = 3, \\quad A = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}\n\\]\nThe result of multiplying the matrix (A) by the scalar (k) is:\n\\[\nkA = 3 \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 \\cdot 1 & 3 \\cdot 2 \\\\\n3 \\cdot 3 & 3 \\cdot 4\n\\end{pmatrix} = \\begin{pmatrix}\n3 & 6 \\\\\n9 & 12\n\\end{pmatrix}\n\\]\nin r is also very simple:\n\n\nCode\nX&lt;- matrix(1:12,4,3)\nprint(X)\n\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nCode\na&lt;-2\nprint(X*a)\n\n\n     [,1] [,2] [,3]\n[1,]    2   10   18\n[2,]    4   12   20\n[3,]    6   14   22\n[4,]    8   16   24"
  },
  {
    "objectID": "matrix.html#matrices-multiplication",
    "href": "matrix.html#matrices-multiplication",
    "title": "Matrix Algebra",
    "section": "5.2 Matrices multiplication",
    "text": "5.2 Matrices multiplication\nMatrix multiplication is performed by taking the dot product of rows from the first matrix (𝐴) with columns of the second matrix (𝐵). The key steps are:\n\nCheck compatibility: Ensure the number of columns in 𝐴 matches the number of rows in 𝐵.\nDot Product Computation: Each element in the resulting matrix is calculated by multiplying corresponding entries from a row of 𝐴 and a column of𝐵, summing the results.\nThe resulting matrix has dimensions \\(m \\times p\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\).\n\nTo multiply a \\(3 \\times 4\\) matrix \\(A\\) with a \\(4 \\times 2\\) matrix \\(B\\), we follow the rule that each row of \\(A\\) interacts with each column of \\(B\\) using the dot product.\nGiven Matrices:\n\\[\nA = \\begin{pmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4}\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\nb_{1,1} & b_{1,2} \\\\\nb_{2,1} & b_{2,2} \\\\\nb_{3,1} & b_{3,2} \\\\\nb_{4,1} & b_{4,2}\n\\end{pmatrix}\n\\] These matrices are compatible for multiplication because \\(A\\) has 4 columns, matching \\(B\\)’s 4 rows.\nThe resulting \\(3 \\times 2\\) matrix \\(C\\) is computed as follows:\n\\[\nC = A B =\n\\begin{pmatrix}\na_{1,1} \\cdot b_{1,1} + a_{1,2} \\cdot b_{2,1} + a_{1,3} \\cdot b_{3,1} + a_{1,4} \\cdot b_{4,1} &\na_{1,1} \\cdot b_{1,2} + a_{1,2} \\cdot b_{2,2} + a_{1,3} \\cdot b_{3,2} + a_{1,4} \\cdot b_{4,2} \\\\\na_{2,1} \\cdot b_{1,1} + a_{2,2} \\cdot b_{2,1} + a_{2,3} \\cdot b_{3,1} + a_{2,4} \\cdot b_{4,1} &\na_{2,1} \\cdot b_{1,2} + a_{2,2} \\cdot b_{2,2} + a_{2,3} \\cdot b_{3,2} + a_{2,4} \\cdot b_{4,2} \\\\\na_{3,1} \\cdot b_{1,1} + a_{3,2} \\cdot b_{2,1} + a_{3,3} \\cdot b_{3,1} + a_{3,4} \\cdot b_{4,1} &\na_{3,1} \\cdot b_{1,2} + a_{3,2} \\cdot b_{2,2} + a_{3,3} \\cdot b_{3,2} + a_{3,4} \\cdot b_{4,2}\n\\end{pmatrix}\n\\]\nEach element in \\(C\\) is derived from the dot product of a row in \\(A\\) and a column in \\(B\\).\nFor example, the top-left element of \\(C\\) (i.e., \\(c_{1,1}\\)) is calculated as:\n\\[\nc_{1,1} = a_{1,1} \\cdot b_{1,1} + a_{1,2} \\cdot b_{2,1} + a_{1,3} \\cdot b_{3,1} + a_{1,4} \\cdot b_{4,1}\n\\]\nLikewise, every position in \\(C\\) follows the same logic.\n\nGiven two matrices (A) and (B):\n\\[\nA = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n\\]\nThe result of multiplying matrix (A) by matrix (B) is:\n\\[\nAB = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 2 \\cdot 0 + 3 \\cdot (-1) \\\\\n4 \\cdot 1 + 5 \\cdot 0 + 6 \\cdot (-1) \\\\\n7 \\cdot 1 + 8 \\cdot 0 + 9 \\cdot (-1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n-2 \\\\\n-2 \\\\\n-2\n\\end{pmatrix}\n\\]\nand in r we use %*%\n\n\nCode\nX&lt;- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)\nX\n\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3   -2    1\n[3,]    2    1   -1\n\n\nCode\nbeta&lt;- c(3,2,1)\nX%*%beta\n\n\n     [,1]\n[1,]    6\n[2,]    6\n[3,]    7\n\n\n\n\nGiven two matrices (A) and (B):\n\\[\nA = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{pmatrix}\n\\]\nThe result of multiplying matrix (A) by matrix (B) is:\n\\[\nAB = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 7 + 2 \\cdot 10 & 1 \\cdot 8 + 2 \\cdot 11 & 1 \\cdot 9 + 2 \\cdot 12 \\\\\n3 \\cdot 7 + 4 \\cdot 10 & 3 \\cdot 8 + 4 \\cdot 11 & 3 \\cdot 9 + 4 \\cdot 12 \\\\\n5 \\cdot 7 + 6 \\cdot 10 & 5 \\cdot 8 + 6 \\cdot 11 & 5 \\cdot 9 + 6 \\cdot 12\n\\end{pmatrix}\n= \\begin{pmatrix}\n27 & 30 & 33 \\\\\n61 & 68 & 75 \\\\\n95 & 106 & 117\n\\end{pmatrix}\n\\]\n\n\n5.2.1 Understanding Matrix Multiplication\nMatrix multiplication is an operation where the product of two matrices is obtained by computing the dot product of rows from the first matrix with columns from the second matrix. However, unlike regular arithmetic multiplication, matrix multiplication does not follow the commutative property: \\[\nA \\times B \\neq B \\times A\n\\] in general. This means swapping the order of multiplication can lead to different results or may even be undefined.\n\n\n5.2.2 Conditions for Matrix Multiplication\nFor matrices A and B to be multipliable, their dimensions must satisfy: - \\(A\\) is an \\(m \\times n\\) matrix. - \\(B\\) is an \\(n \\times p\\) matrix. - The resulting matrix \\(C\\) has dimensions \\(m \\times p\\).\nExample: Demonstrating Non-Commutativity\nLet’s consider two matrices:\n\\[\nA = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}\n\\]\nComputing \\(A \\times B\\):\n\\[\nA B =\n\\begin{pmatrix}\n(1 \\cdot 0 + 2 \\cdot 1) & (1 \\cdot 1 + 2 \\cdot 0) \\\\\n(3 \\cdot 0 + 4 \\cdot 1) & (3 \\cdot 1 + 4 \\cdot 0)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 & 1 \\\\\n4 & 3\n\\end{pmatrix}\n\\]\nNow computing \\(B \\times A\\):\n\\[\nB A =\n\\begin{pmatrix}\n(0 \\cdot 1 + 1 \\cdot 3) & (0 \\cdot 2 + 1 \\cdot 4) \\\\\n(1 \\cdot 1 + 0 \\cdot 3) & (1 \\cdot 2 + 0 \\cdot 4)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 & 4 \\\\\n1 & 2\n\\end{pmatrix}\n\\]\nClearly, \\(A \\times B \\neq B \\times A\\), demonstrating non-commutativity.\n\n\n5.2.3 Additional Matrix Multiplication Properties\n\nAssociative Property: \\[ (A \\times B) \\times C = A \\times (B \\times C) \\]\nDistributive Property: \\[ A \\times (B + C) = A \\times B + A \\times C \\]\nIdentity Matrix Property: If \\(I\\) is the identity matrix: \\[ A \\times I = I \\times A = A \\]\nZero Matrix Property: \\[ A \\times 0 = 0 \\]\n\nMatrix multiplication plays a fundamental role in linear algebra, forming the basis for transformations, systems of equations, and numerous applications in data science, physics, and engineering."
  },
  {
    "objectID": "matrix.html#identity-matrix",
    "href": "matrix.html#identity-matrix",
    "title": "Matrix Algebra",
    "section": "5.3 Identity matrix",
    "text": "5.3 Identity matrix\nAn identity matrix (also known as a unit matrix) is a square matrix in which all the elements of the principal diagonal are ones, and all other elements are zeros. It is denoted by (I). The identity matrix plays a crucial role in matrix multiplication, as multiplying any matrix by the identity matrix leaves the original matrix unchanged. he identity matrix (I) of order 3 is:\n\\[\nI_3 = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\] in r we use the function diag() with the number of dimensions we want:\n\n\nCode\ndiag(5)\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1"
  },
  {
    "objectID": "matrix.html#inversion",
    "href": "matrix.html#inversion",
    "title": "Matrix Algebra",
    "section": "1.4 Inversion",
    "text": "1.4 Inversion\nThe inverse of a square matrix \\(X\\) is denoted as \\(X^{-1}\\) it has the property that if you multiply a matrix by its inverse, it gives you the identity matrix. \\(X^{-1}X=I\\)\nNote that not all matrices have an inverse.\nIn r we use the function solve to get the inverse, and we use it to solve our original equation it gives us the values for a, b and c to resolve the system of equations:\n\\[\n\\begin{align*}\na + b + c &= 6 \\\\\n3a - 2b + c &= 2 \\\\\n2a + b - c &= 1\n\\end{align*}\n\\\\\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n3 & -2 & 1 \\\\\n2 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\na \\\\\nb \\\\\nc\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\\n2 \\\\\n1\n\\end{pmatrix}\n\\]\n\n\nCode\nX &lt;- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)\ny &lt;- matrix(c(6,2,1),3,1)\nsolve(X)%*%y\n\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n\n\n\nExample\nA small factory produces two products: Chairs and Tables. Each product requires a certain amount of wood and labor hours:\n\nA Chair requires 2 units of wood and 3 hours of labor.\nA Table requires 5 units of wood and 2 hours of labor.\n\nThe factory has available resources of:\n\n40 units of wood\n30 hours of labor\n\nWe want to determine how many Chairs (x) and Tables (y) the factory can produce using all available resources.\nStep 1: Represent the System as Equations\nWe can write the constraints as:\n\\[\n\\begin{aligned}\n2x + 5y &= 40 \\quad \\text{(wood constraint)} \\\\\n3x + 2y &= 30 \\quad \\text{(labor constraint)}\n\\end{aligned}\n\\]\nStep 2: Matrix Form\nThis system can be written in matrix form:\n\\[\nAX = B\n\\]\nWhere:\n\\[\nA = \\begin{bmatrix} 2 & 5 \\\\ 3 & 2 \\end{bmatrix}, \\quad\nX = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 40 \\\\ 30 \\end{bmatrix}\n\\]\nStep 3: Solve in R\n\n\nCode\n# Coefficient matrix A\nA &lt;- matrix(c(2, 3, 5, 2), nrow = 2, byrow = TRUE)\n\n# Resource vector B\nB &lt;- matrix(c(40, 30), nrow = 2)\n\n# Solve for X (number of chairs and tables)\nX &lt;- solve(A, B)\n\n# Display the result\nX\n\n\n           [,1]\n[1,]  0.9090909\n[2,] 12.7272727"
  },
  {
    "objectID": "matrix.html#transpose",
    "href": "matrix.html#transpose",
    "title": "Matrix Algebra",
    "section": "5.4 Transpose",
    "text": "5.4 Transpose\nTranspose simply turns the rows into columns and vice versa, in r we use t\n\n\nCode\nX&lt;- matrix(1:15,5,3)\nX\n\n\n     [,1] [,2] [,3]\n[1,]    1    6   11\n[2,]    2    7   12\n[3,]    3    8   13\n[4,]    4    9   14\n[5,]    5   10   15\n\n\nCode\nt(X)\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15"
  },
  {
    "objectID": "matrix.html#calculate-an-average-using-matrices",
    "href": "matrix.html#calculate-an-average-using-matrices",
    "title": "Matrix Algebra",
    "section": "1.6 Calculate an average using matrices",
    "text": "1.6 Calculate an average using matrices\n\n\nCode\ny&lt;- father.son$fheight\nmean(y)\n\n\n[1] 67.6871\n\n\nCode\n#using matrices:\nN&lt;- length(y)\nY&lt;- matrix(y,N,1)\nA&lt;- matrix(1,N,1)\nbarY&lt;- t(A)%*%Y/N\n##equivalent to\nbarY&lt;- crossprod(A,Y)/N\nprint(barY)\n\n\n        [,1]\n[1,] 67.6871"
  },
  {
    "objectID": "matrix.html#sample-variance",
    "href": "matrix.html#sample-variance",
    "title": "Matrix Algebra",
    "section": "1.7 Sample variance",
    "text": "1.7 Sample variance\nFirst, remember that the residuals are given by \\(e=Y-\\hat{Y}\\) where \\(e\\) is the nX1 vector of residuals, Y is the nx1 vector of observed values and \\(\\hat{Y}\\) is the nx1 vector of predicted values from our model. The formula for the sample variance \\(s^2\\) of the residuals is \\[\ns^2 = \\frac{\\mathbf{e}^T \\mathbf{e}}{n - p}\n\\] This gives you the average squared deviation of the residuals from their mean, which is an estimate of the variance of the errors in your model. in r:\n\n\nCode\ne&lt;- y -barY\ncrossprod(e)/N\n\n\n         [,1]\n[1,] 7.527313\n\n\nExample:\n\n\nCode\n# Sample data\nY &lt;- matrix(c(2, 3, 5, 7, 9), ncol = 1)\nY\n\n\n     [,1]\n[1,]    2\n[2,]    3\n[3,]    5\n[4,]    7\n[5,]    9\n\n\nCode\nX &lt;- matrix(c(1, 1, 1, 1, 1, 1, 2, 3, 4, 5), ncol = 2)\nX\n\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    2\n[3,]    1    3\n[4,]    1    4\n[5,]    1    5\n\n\nCode\n# Calculate the coefficients (beta_hat)\nbeta_hat &lt;- solve(crossprod(X)) %*% crossprod(X, Y)\nbeta_hat\n\n\n     [,1]\n[1,] -0.2\n[2,]  1.8\n\n\nCode\n# Calculate the predicted values (Y_hat)\nY_hat &lt;- X %*% beta_hat\n\n# Calculate the residuals\nresiduals &lt;- Y - Y_hat\n\n# Number of observations and parameters\nn &lt;- nrow(Y)\np &lt;- ncol(X)\n\n# Calculate the sum of squared residuals using crossprod\nss_res &lt;- crossprod(residuals)\n\n# Calculate the sample variance\ns_squared &lt;- ss_res / (n - p)\n\ns_squared\n\n\n          [,1]\n[1,] 0.1333333"
  },
  {
    "objectID": "matrix.html#motivating-examples",
    "href": "matrix.html#motivating-examples",
    "title": "Matrix Algebra",
    "section": "9.3 Motivating Examples",
    "text": "9.3 Motivating Examples\n\nFalling objects\nImagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error:\n\n\nCode\nset.seed(1)\ng &lt;- 9.8 ##meters per second\nn &lt;- 25\ntt &lt;- seq(0,3.4,len=n) ##time in secs, note: we use tt because t is a base function\nd &lt;- 56.67  - 0.5*g*tt^2 + rnorm(n,sd=1) ##meters\n\n\nThe assistants hand the data to Galileo and this is what he sees:\n\n\nCode\nmypar()\n\nplot(tt,d,ylab=\"Distance in meters\",xlab=\"Time in seconds\")\n\n\n\n\n\nSimulated data for distance travelled versus time of falling object measured with error.\n\n\n\n\nHe does not know the exact equation, but by looking at the plot above he deduces that the position should follow a parabola. So he models the data with:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i, i=1,\\dots,n\n\\]\nWith \\(Y_i\\) representing location, \\(x_i\\) representing the time, and \\(\\varepsilon_i\\) accounting for measurement error. This is a linear model because it is a linear combination of known quantities (the \\(x\\)’s) referred to as predictors or covariates and unknown parameters (the \\(\\beta\\)’s).\nso we are have our measures d and we want to calculate the unknown parameters or betas: h is the hight of the Tower of Pisa and should result in a value similar to 56.67 g is the acceleration due to gravity, but we will actually get \\(\\frac{1}{2}g\\) due to physics. and we will have some errors due to measurament errors that we introduced in the formula above using rnorm(n,sd=1)\nwe want to find the values of beta that minimize the sum square of errors (RSS) Our first step is to create a matrix with tt and \\(tt^2\\) and we add a column of 1s:\n\n\nCode\nX&lt;- cbind(1,tt,tt^2)\nX\n\n\n               tt            \n [1,] 1 0.0000000  0.00000000\n [2,] 1 0.1416667  0.02006944\n [3,] 1 0.2833333  0.08027778\n [4,] 1 0.4250000  0.18062500\n [5,] 1 0.5666667  0.32111111\n [6,] 1 0.7083333  0.50173611\n [7,] 1 0.8500000  0.72250000\n [8,] 1 0.9916667  0.98340278\n [9,] 1 1.1333333  1.28444444\n[10,] 1 1.2750000  1.62562500\n[11,] 1 1.4166667  2.00694444\n[12,] 1 1.5583333  2.42840278\n[13,] 1 1.7000000  2.89000000\n[14,] 1 1.8416667  3.39173611\n[15,] 1 1.9833333  3.93361111\n[16,] 1 2.1250000  4.51562500\n[17,] 1 2.2666667  5.13777778\n[18,] 1 2.4083333  5.80006944\n[19,] 1 2.5500000  6.50250000\n[20,] 1 2.6916667  7.24506944\n[21,] 1 2.8333333  8.02777778\n[22,] 1 2.9750000  8.85062500\n[23,] 1 3.1166667  9.71361111\n[24,] 1 3.2583333 10.61673611\n[25,] 1 3.4000000 11.56000000\n\n\nNow we choose a random matrix for beta of 3 rows (so we can multiply by X) Note that the values chosen for the matrix are arbitrary:\n\n\nCode\nBeta &lt;- matrix(c(55,0,5),3,1)\nBeta\n\n\n     [,1]\n[1,]   55\n[2,]    0\n[3,]    5\n\n\nthe residuals will be y - X times Beta.\n\n\nCode\nr&lt;- d - X%*%Beta\nr\n\n\n               [,1]\n [1,]    1.04354619\n [2,]    1.65495582\n [3,]    0.03962139\n [4,]    1.47709330\n [5,]   -1.17949223\n [6,]   -4.11765588\n [7,]   -4.99532095\n [8,]   -7.32736279\n [9,]  -10.47021865\n[10,]  -14.72907589\n[11,]  -16.68696883\n[12,]  -21.98134426\n[13,]  -27.56224058\n[14,]  -34.12288739\n[15,]  -36.14781908\n[16,]  -43.07962111\n[17,]  -49.21019026\n[18,]  -54.80685129\n[19,]  -61.88352880\n[20,]  -69.46228618\n[21,]  -76.88602263\n[22,]  -85.16905120\n[23,]  -94.42018502\n[24,] -105.42503920\n[25,] -112.15417425\n\n\nand the Residual Sum of Squares (RSS) will be:\n\n\nCode\nRSS&lt;- crossprod(r)\nRSS\n\n\n         [,1]\n[1,] 66131.18\n\n\nnow to get the values for our unknown parameters we solve the least squares estimate (LSE) for those\n\n\nCode\nbetahat &lt;- solve(crossprod(X))%*% crossprod(X,d)\nbetahat\n\n\n         [,1]\n   56.5317368\ntt  0.5013565\n   -5.0386455\n\n\nwhich gives us: 57.0212322 is the hight of the tower of Pisa -0.4223921 is the starting velocity (should be 0) -4.8175119 is half of the gravity acceleration.\nwhich will result in our formula: d &lt;- 57.0212322 - 0.4223921 tt - 4.8175119 tt^2\n\n\nCode\nfun &lt;- function(x){\n  57.0212322 - (0.4223921*x) - (4.8175119*x^2)}\ny_1 &lt;- fun(tt)\n\n\nNow we can plot the measured values along with the calculated values using the equation (note that I have slightly displaced the calculated values to avoid overlapping)\n\n\nCode\n# Plot the measured values\nplot(tt, d, xlab = \"Time in secs\", ylab = \"Distance in m.\", col = \"blue\", pch = 19)\n\n# Add the fitted values to the plot\npoints(tt+0.1, y_1, col = \"red\", pch = 17)\n\n# Add a legend to differentiate between the two lines\nlegend(\"bottomleft\", legend = c(\"Measured values\", \"Fitted values\"), col = c(\"blue\", \"red\"), pch = c(19, 17))\n\n\n\n\n\n\n\n\n\nWe could have solved this without matrices using the linear model formula in r:\n\n\nCode\ntt2&lt;- tt^2\nfit &lt;- lm(d~tt+tt2)\nsummary(fit)\n\n\n\nCall:\nlm(formula = d ~ tt + tt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5295 -0.4882  0.2537  0.6560  1.5455 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  56.5317     0.5451 103.701 &lt;0.0000000000000002 ***\ntt            0.5014     0.7426   0.675               0.507    \ntt2          -5.0386     0.2110 -23.884 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9822 on 22 degrees of freedom\nMultiple R-squared:  0.9973,    Adjusted R-squared:  0.997 \nF-statistic:  4025 on 2 and 22 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "matrix.html#standard-error-in-the-context-of-linear-models",
    "href": "matrix.html#standard-error-in-the-context-of-linear-models",
    "title": "Matrix Algebra",
    "section": "9.4 Standard Error in the context of linear models",
    "text": "9.4 Standard Error in the context of linear models\nWe have shown how to find the least squares estimates with matrix algebra. These estimates are random variables as they are linear combinations of the data. For these estimates to be useful we also need to compute the standard errors.\nIt is useful to think about where randomness comes from. In our falling object example, randomness was introduced through measurement errors. Every time we rerun the experiment a new set of measurement errors will be made which implies our data will be random. This implies that our estimate of the gravitational constant will change. The constant is fixed, but our estimates are not. To see this we can run a Monte Carlo simulation. Specifically we will generate the data repeatedly and compute the estimate for the quadratic term each time.\n\n\nCode\ng = 9.8 ## meters per second\nh0 = 56.67\nv0 = 0\nn = 25\ntt = seq(0,3.4,len=n) ##time in secs, t is a base function\ny = h0 + v0 *tt  - 0.5* g*tt^2 + rnorm(n,sd=1)\n\n\nnow we act as if we didn’t know h0, v0 and -0.5g and use regression to estimate these. We can rewrite the models as y=b0+b1 t+ b2 t^2 +e and obtain LSE. Note that g will be g=-2*b2\nTo obtain the LSE in r\n\n\nCode\nX = cbind(1,tt,tt^2)\nA = solve(crossprod(X))%*%t(X)%*%y\n\n\nso g will be measured after this experiment as:\n\n\nCode\n-2*A[3]\n\n\n[1] 9.788724\n\n\nnow we are going to repeat the experiment 100,000 times and calculate the standard deviation for the estimate g.\n\n\nCode\ng = 9.8 ## meters per second\nh0 = 56.67\nv0 = 0\nn = 25\ntt = seq(0,3.4,len=n) ##time in secs, t is a base function\nset.seed(1)\nmyfunc &lt;- function(){\ny = h0 + v0 *tt  - 0.5* g*tt^2 + rnorm(n,sd=1)\n\nX = cbind(1,tt,tt^2)\nA = solve(crossprod(X))%*%t(X)%*%y\nA\ng&lt;- -2*A[3]\nreturn (g)\n}\n\ngs&lt;- replicate(100000,myfunc())\nsd(gs)\n\n\n[1] 0.429747\n\n\nNow we are going to use matrix algebra to compute standard errors of regression coefficients. We will start by defining the variance covariance matrix.\n\n9.4.1 Variance-covariance matrix (Advanced)\nAs a first step we need to define the variance-covariance matrix, \\(\\boldsymbol{\\Sigma}\\). For a vector of random variables, \\(\\mathbf{Y}\\), we define \\(\\boldsymbol{\\Sigma}\\) as the matrix with the \\(i,j\\) entry:\n\\[ \\Sigma_{i,j} \\equiv \\mbox{Cov}(Y_i, Y_j) \\] The covariance is equal to the variance if \\(i = j\\) and equal to 0 if the variables are independent. In the kinds of vectors considered up to now, for example, a vector \\(\\mathbf{Y}\\) of individual observations \\(Y_i\\) sampled from a population, we have assumed independence of each observation and assumed the \\(Y_i\\) all have the same variance \\(\\sigma^2\\), so the variance-covariance matrix has had only two kinds of elements:\n\\[ \\mbox{Cov}(Y_i, Y_i) = \\mbox{var}(Y_i) = \\sigma^2\\]\n\\[ \\mbox{Cov}(Y_i, Y_j) = 0, \\mbox{ for } i \\neq j\\]\nwhich implies that \\(\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}\\) with \\(\\mathbf{I}\\), the identity matrix.\nLater, we will see a case, specifically the estimate coefficients of a linear model, \\(\\hat{\\boldsymbol{\\beta}}\\), that has non-zero entries in the off diagonal elements of \\(\\boldsymbol{\\Sigma}\\). Furthermore, the diagonal elements will not be equal to a single value \\(\\sigma^2\\).\n\n\n9.4.2 Variance of a linear combination\nA useful result provided by linear algebra is that the variance covariance-matrix of a linear combination \\(\\mathbf{AY}\\) of \\(\\mathbf{Y}\\) can be computed as follows:\n\\[\n\\mbox{var}(\\mathbf{AY}) = \\mathbf{A}\\mbox{var}(\\mathbf{Y}) \\mathbf{A}^\\top\n\\]\nFor example, if \\(Y_1\\) and \\(Y_2\\) are independent both with variance \\(\\sigma^2\\) then:\n\\[\\mbox{var}\\{Y_1+Y_2\\} =\n\\mbox{var}\\left\\{ \\begin{pmatrix}1&1\\end{pmatrix}\\begin{pmatrix} Y_1\\\\Y_2\\\\ \\end{pmatrix}\\right\\}\\]\n\\[ =\\begin{pmatrix}1&1\\end{pmatrix} \\sigma^2 \\mathbf{I}\\begin{pmatrix} 1\\\\1\\\\ \\end{pmatrix}=2\\sigma^2\\]\nas we expect. We use this result to obtain the standard errors of the LSE (least squares estimate)."
  },
  {
    "objectID": "matrix.html#least-squares-estimates-lse-standard-errors-advanced",
    "href": "matrix.html#least-squares-estimates-lse-standard-errors-advanced",
    "title": "Matrix Algebra",
    "section": "9.5 Least Squares Estimates (LSE) standard errors (Advanced)",
    "text": "9.5 Least Squares Estimates (LSE) standard errors (Advanced)\nNote that the LSE \\(\\boldsymbol{\\hat{\\beta}}\\) is a linear combination of \\(\\mathbf{Y}\\): \\(\\mathbf{AY}\\) with \\(\\mathbf{A}=\\mathbf{(X^\\top X)^{-1}X}^\\top\\), so we can use the equation above to derive the variance of our estimates:\n\\[\\mbox{var}(\\boldsymbol{\\hat{\\beta}}) = \\mbox{var}(\\mathbf{(X^\\top X)^{-1}X^\\top Y}) =  \\]\n\\[\\mathbf{(X^\\top X)^{-1} X^\\top} \\mbox{var}(Y) (\\mathbf{(X^\\top X)^{-1} X^\\top})^\\top = \\]\n\\[\\mathbf{(X^\\top X)^{-1} X^\\top} \\sigma^2 \\mathbf{I} (\\mathbf{(X^\\top X)^{-1} X^\\top})^\\top = \\]\n\\[\\sigma^2 \\mathbf{(X^\\top X)^{-1} X^\\top}\\mathbf{X} \\mathbf{(X^\\top X)^{-1}} = \\]\n\\[\\sigma^2\\mathbf{(X^\\top X)^{-1}}\\]\nThe diagonal of the square root of this matrix contains the standard error of our estimates.\n\nEstimating \\(\\sigma^2\\)\nTo obtain an actual estimate in practice from the formulas above, we need to estimate \\(\\sigma^2\\). Previously we estimated the standard errors from the sample. However, the sample standard deviation of \\(Y\\) is not \\(\\sigma\\) because \\(Y\\) also includes variability introduced by the deterministic part of the model: \\(\\mathbf{X}\\boldsymbol{\\beta}\\). The approach we take is to use the residuals.\nWe form the residuals like this:\n\\[\n\\mathbf{r}\\equiv\\boldsymbol{\\hat{\\varepsilon}} = \\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\hat{\\beta}}\n\\]\nBoth \\(\\mathbf{r}\\) and \\(\\boldsymbol{\\hat{\\varepsilon}}\\) notations are used to denote residuals.\nThen we use these to estimate, in a similar way, to what we do in the univariate case:\n\\[ s^2 \\equiv \\hat{\\sigma}^2 = \\frac{1}{N-p}\\mathbf{r}^\\top\\mathbf{r} = \\frac{1}{N-p}\\sum_{i=1}^N r_i^2\\]\nHere \\(N\\) is the sample size and \\(p\\) is the number of columns in \\(\\mathbf{X}\\) or number of parameters (including the intercept term \\(\\beta_0\\)). The reason we divide by \\(N-p\\) is because mathematical theory tells us that this will give us a better (unbiased) estimate.\nLet’s try this in R and see if we obtain the same values as we did with the Monte Carlo simulation above:\n\n\nCode\nn &lt;- nrow(father.son)\nN &lt;- 50\nindex &lt;- sample(n,N)\nsampledat &lt;- father.son[index,]\nx &lt;- sampledat$fheight\ny &lt;- sampledat$sheight\nX &lt;- model.matrix(~x)\n\nN &lt;- nrow(X)\np &lt;- ncol(X)\n\nXtXinv &lt;- solve(crossprod(X))\n\nresid &lt;- y - X %*% XtXinv %*% crossprod(X,y)\n\ns &lt;- sqrt(sum(resid^2)/(N-p))\nses &lt;- sqrt(diag(XtXinv))*s \n\n\nLet’s compare to what lm provides:\n\n\nCode\nsummary(lm(y~x))$coef[,2]\n\n\n(Intercept)           x \n  8.7487832   0.1284109 \n\n\nCode\nses\n\n\n(Intercept)           x \n  8.7487832   0.1284109 \n\n\nThey are identical because they are doing the same thing. Also, note that we approximate the Monte Carlo results:\n\n\nCode\napply(betahat,2,sd)\n\n\n[1] 34.06124"
  },
  {
    "objectID": "matrix.html#linear-combination-of-estimates",
    "href": "matrix.html#linear-combination-of-estimates",
    "title": "Matrix Algebra",
    "section": "9.6 Linear combination of estimates",
    "text": "9.6 Linear combination of estimates\nImagine that you estimated the effects of several treatments and now you are interested in the difference in the effects of two of those treatments. You already have the \\(\\hat{\\beta}\\) and want to calculate \\(\\hat{\\beta}_2-\\hat{\\beta}_1\\)\nIf we want to compute the standard deviation of a linear combination of estimates such as \\(\\hat{\\beta}_2 - \\hat{\\beta}_1\\), this is a linear combination of \\(\\hat{\\boldsymbol{\\beta}}\\):\n\\[\\hat{\\beta}_2 - \\hat{\\beta}_1 =\n\\begin{pmatrix}0&-1&1&0&\\dots&0\\end{pmatrix} \\begin{pmatrix}\n\\hat{\\beta}_0\\\\\n\\hat{\\beta}_1 \\\\\n\\hat{\\beta}_2 \\\\\n\\vdots\\\\\n\\hat{\\beta}_p\n\\end{pmatrix}\\]\nUsing the above, we know how to compute the variance covariance matrix of \\(\\hat{\\boldsymbol{\\beta}}\\)."
  },
  {
    "objectID": "matrix.html#clt-and-t-distribution",
    "href": "matrix.html#clt-and-t-distribution",
    "title": "Matrix Algebra",
    "section": "9.7 CLT and t-distribution",
    "text": "9.7 CLT and t-distribution\nWe have shown how we can obtain standard errors for our estimates. However, as we learned in the first chapter, to perform inference we need to know the distribution of these random variables. The reason we went through the effort to compute the standard errors is because the CLT applies in linear models. If \\(N\\) is large enough, then the LSE will be normally distributed with mean \\(\\boldsymbol{\\beta}\\) and standard errors as described. For small samples, if the \\(\\varepsilon\\) are normally distributed, then the \\(\\hat{\\beta}-\\beta\\) follow a t-distribution. We do not derive this result here, but the results are extremely useful since it is how we construct p-values and confidence intervals in the context of linear models.\n\nCode versus math\nThe standard approach to writing linear models either assume the values in \\(\\mathbf{X}\\) are fixed or that we are conditioning on them. Thus \\(\\mathbf{X} \\boldsymbol{\\beta}\\) has no variance as the \\(\\mathbf{X}\\) is considered fixed. This is why we write \\(\\mbox{var}(Y_i) = \\mbox{var}(\\varepsilon_i)=\\sigma^2\\). This can cause confusion in practice because if you, for example, compute the following:\n\n\nCode\nx =  father.son$fheight\nbeta =  c(34,0.5)\nvar(beta[1]+beta[2]*x)\n\n\n[1] 1.883576\n\n\nit is nowhere near 0. This is an example in which we have to be careful in distinguishing code from math. The function var is simply computing the variance of the list we feed it, while the mathematical definition of variance is considering only quantities that are random variables. In the R code above, x is not fixed at all: we are letting it vary, but when we write \\(\\mbox{var}(Y_i) = \\sigma^2\\) we are imposing, mathematically, x to be fixed. Similarly, if we use R to compute the variance of \\(Y\\) in our object dropping example, we obtain something very different than \\(\\sigma^2=1\\) (the known variance):\n\n\nCode\nn &lt;- length(tt)\ny &lt;- h0 + v0*tt  - 0.5*g*tt^2 + rnorm(n,sd=1)\nvar(y)\n\n\n[1] 334.0487\n\n\nAgain, this is because we are not fixing tt.\n\n\nExercise\nWe are going to calculate the standard error of \\(\\hat{\\beta}\\) for the heights of father and son. \\[\nSE(\\hat{\\beta})= sqrt{Var(\\hat{\\beta})}\n\\]\n\\[\nVar(\\hat{\\beta})= \\sigma^2(X^TX)^{-1}\n\\]\nFirst, we want to estimate \\(\\sigma^2\\), the variance of Y. As we have seen in the previous unit, the random part of Y is only coming from \\(\\epsilon\\), because we assume \\(X\\beta\\) is fixed. So we can try to estimate the variance of the epsilons from the residuals: the Y minus the fitted values from the linear model.\n\n\nCode\nx = father.son$fheight\ny = father.son$sheight\nn = length(y)\nN = 50\nset.seed(1)\nindex = sample(n,N)\nsampledat = father.son[index,]\nx = sampledat$fheight\ny = sampledat$sheight\nbetahat = lm(y~x)$coef\n\n\nfirst we calculate the \\(\\hat{Y}\\) values from a linear model:\n\n\nCode\nfit = lm(y ~ x)\nYhat&lt;- fit$fitted.values\n\n\nand now we can calculate the sum of the squares residuals SSR that will be the diffeence between \\(y_i\\) and \\(\\hat{y_i}\\)\n\n\nCode\nres2 &lt;- (y- Yhat)^2\nSSR &lt;- sum(res2)\nSSR\n\n\n[1] 331.2952\n\n\nOur estimate of \\(\\sigma^2\\) will be the sum of squared residuals divided by (N-p), the sample size minus the number of terms in the model. Since we have a sample of 50 and 2 terms in the model (the intercept and a slope), our estimate will be:\n\n\nCode\nsigma2 &lt;- SSR/(N-2)\nsigma2\n\n\n[1] 6.901984\n\n\nNow we form the matrix X, this can be done by combining a column of 1s with a column with the father’s heights.\n\n\nCode\nX = cbind(rep(1,N),x)\n\n\nAnd now we calculate \\((X^TX)^{-1}\\)\n\n\nCode\nI&lt;-solve(t(X)%*%X)\nI\n\n\n                        x\n  14.5639039 -0.214704345\nx -0.2147043  0.003169572\n\n\nNow we are only one step away from getting the standard error of \\(\\hat{\\beta}\\) Take the diagonals of \\((X^TX)^{-1}\\) and multiply by our estimate of \\(\\sigma^2\\). This is the estimated variance of \\(\\hat{\\beta}\\)\n\n\nCode\nvarianceBeta &lt;- diag(I)*sigma2\nvarianceBeta\n\n\n                        x \n100.51983131   0.02187634 \n\n\nand finally the SE will be the square root of this\n\n\nCode\nSE&lt;- sqrt(varianceBeta)\nSE\n\n\n                    x \n10.0259579  0.1479065 \n\n\nthis gives us two numbers, the standard error of the intercept and the standard error for the slope. (Note that the standard error estimate is also printed int eh second column of summary(fit))\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8912 -1.3527  0.3593  1.5703  4.4794 \n\nCoefficients:\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  44.3377    10.0260   4.422 0.0000558 ***\nx             0.3530     0.1479   2.387     0.021 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.627 on 48 degrees of freedom\nMultiple R-squared:  0.1061,    Adjusted R-squared:  0.08747 \nF-statistic: 5.697 on 1 and 48 DF,  p-value: 0.02098"
  },
  {
    "objectID": "matrix.html#crossed-designs",
    "href": "matrix.html#crossed-designs",
    "title": "Matrix Algebra",
    "section": "11.1 Crossed designs",
    "text": "11.1 Crossed designs\nimagine we are running an experiment and we have two different treatments, group A is the control group and receives no treatment (a), group B receives treatment 1 (b), group C receives treatment 2 (c) and a fourth group receives treatment 1 and 2 (d). In this case we consider the effects of receiving both treatments additive. If we write down a model it will look like this: \\[\n\\begin{pmatrix}\n\\color{red}1 & \\color{red}0 & \\color{red}0 \\\\\n\\color{red}1 & \\color{red}0 & \\color{red}0 \\\\\n\\color{blue}1 & \\color{blue}1 & \\color{blue}0 \\\\\n\\color{blue}1 & \\color{blue}1 & \\color{blue}0 \\\\\n\\color{green}1 & \\color{green}0 & \\color{green}1 \\\\\n\\color{green}1 & \\color{green}0 & \\color{green}1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\color{red}{\\beta_0 }\\\\\n\\color{blue}{\\beta_1 }\\\\\n\\color{green}{\\beta_2}\n\\end{pmatrix}\n\\]\nwe can see that the first two rows are no treatment, the rows 3rd and 4th receive treatment 1 but no treatment 2 , the 5th and the 6th rows receive treatment 2 but no treatment 1 and the last two rows receive both treatments.\nIf the effects of treatment 1 + treatment 2 are not additive, then we need to plug in a fourth element that will gives us a different mean for each group and we call that fourth term interaction\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0\\\\\n1 & 0 & 1 & 0\\\\\n1 & 1 & 0 & 0\\\\\n1 & 0 & 0 & 0\\\\\n1 & 1 & 1 & 0\\\\\n1 & 0 & 1 & 1\\\\\n1 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\\\\\n\\beta_{1:2}\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "matrix.html#linear-model-with-two-variables",
    "href": "matrix.html#linear-model-with-two-variables",
    "title": "Matrix Algebra",
    "section": "12.1 linear model with two variables",
    "text": "12.1 linear model with two variables\nNow we are going to use the 4 pairs of legs to show a more complex linear model.\n\n\nCode\nX &lt;- model.matrix(~ type + leg, data=spider)\ncolnames(X)\n\n\n[1] \"(Intercept)\" \"typepush\"    \"legL2\"       \"legL3\"       \"legL4\"      \n\n\nCode\nhead(X)\n\n\n  (Intercept) typepush legL2 legL3 legL4\n1           1        0     0     0     0\n2           1        0     0     0     0\n3           1        0     0     0     0\n4           1        0     0     0     0\n5           1        0     0     0     0\n6           1        0     0     0     0\n\n\nCode\nimagemat(X, main=\"Model matrix for linear model with two factors\")\n\n\n\n\n\nImage of the model matrix for a formula with type + leg\n\n\n\n\nWe have a row for each data point. The first column is the intercept, and so it has 1’s (black) for all samples. The second column expresses if the data is for a pull or push: it has 1’s for the push samples, and we can see that there are four groups of them (one for each pair of legs). Finally, the third, fourth and fifth columns expresses what leg pair we are referencing and have 1’s for the L2, L3 and L4 samples. The L1 samples do not have a column, because L1 is the reference level for leg. Similarly, there is no pull column, because pull is the reference level for the type variable.\nTo estimate coefficients for this model, we use lm with the formula ~ type + leg. We’ll save the linear model to fitTL standing for a fit with Type and Leg.\n\n\nCode\nfitTL &lt;- lm(friction ~ type + leg, data=spider)\nsummary(fitTL)\n\n\n\nCall:\nlm(formula = friction ~ type + leg, data = spider)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46392 -0.13441 -0.00525  0.10547  0.69509 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.05392    0.02816  37.426 &lt; 0.0000000000000002 ***\ntypepush    -0.77901    0.02482 -31.380 &lt; 0.0000000000000002 ***\nlegL2        0.17192    0.04569   3.763             0.000205 ***\nlegL3        0.16049    0.03251   4.937   0.0000013710214077 ***\nlegL4        0.28134    0.03438   8.183   0.0000000000000101 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2084 on 277 degrees of freedom\nMultiple R-squared:  0.7916,    Adjusted R-squared:  0.7886 \nF-statistic:   263 on 4 and 277 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\n(coefs &lt;- coef(fitTL))\n\n\n(Intercept)    typepush       legL2       legL3       legL4 \n  1.0539153  -0.7790071   0.1719216   0.1604921   0.2813382 \n\n\nR uses the name coefficient to denote the component containing the least squares estimates. It is important to remember that the coefficients are parameters that we do not observe, but only estimate.\nWe can make the same plot as before, with arrows for each of the estimated coefficients in the model.\n\n\n\n\n\nDiagram of the estimated coefficients in the linear model. As before, the teal-green arrow represents the Intercept, which fits the mean of the reference group (here, the pull samples for leg L1). The purple, pink, and yellow-green arrows represent differences between the three other leg groups and L1. The orange arrow represents the difference between the push and pull samples for all groups.\n\n\n\n\nThe intercept is the coefficient for L1 Pull. (dark green) The typepush is the difference between pull and push. (orange) The LegL2 is the difference between L1 Pull and L2 Pull The LegL3 is the difference between L1 Pull and L3 Pull The LegL4 is the difference between L1 Pull and L4 Pull\nThe red diamonds represent the mean of each group.\nNotice that the intercept is no longer exactly equal to the mean of the L1 Pull values:\n\n\nCode\ns &lt;- split(spider$friction, spider$group)\ncat('mean:' , mean(s[[\"L1pull\"]]))\n\n\nmean: 0.9214706\n\n\nand same for the other coefficients, the linear model with 8 groups does not manage to accurately fit the LSE in a way that the coefficients match the exact mean for each group.\nHere we can demonstrate that the push vs. pull estimated coefficient, coefs[2], is a weighted average of the difference of the means for each group. Furthermore, the weighting is determined by the sample size of each group. The math works out simply here because the sample size is equal for the push and pull subgroups within each leg pair. If the sample sizes were not equal for push and pull within each leg pair, the weighting is more complicated.\n\n\nCode\n(means &lt;- sapply(s, mean))\n\n\n   L1pull    L1push    L2pull    L2push    L3pull    L3push    L4pull    L4push \n0.9214706 0.4073529 1.1453333 0.5273333 1.2738462 0.3759615 1.4007500 0.4907500 \n\n\nCode\n##the sample size of push or pull groups for each leg pair\nns &lt;- sapply(s, length)[c(1,3,5,7)]\n(w &lt;- ns/sum(ns))\n\n\n   L1pull    L2pull    L3pull    L4pull \n0.2411348 0.1063830 0.3687943 0.2836879 \n\n\nCode\nsum(w * (means[c(2,4,6,8)] - means[c(1,3,5,7)]))\n\n\n[1] -0.7790071\n\n\nCode\ncoefs[2]\n\n\n  typepush \n-0.7790071"
  },
  {
    "objectID": "matrix.html#contrasting-coefficients",
    "href": "matrix.html#contrasting-coefficients",
    "title": "Matrix Algebra",
    "section": "12.2 Contrasting coefficients",
    "text": "12.2 Contrasting coefficients\nSo all these coefficients are comparing each leg to Leg 1. What do we do if we want to compare one group to another group that is not the reference level, for example we want to compare L2 vs L3? We can use the library contrast\nIf we want to compare leg pairs L3 and L2, this is equivalent to contrasting two coefficients from the linear model because, in this contrast, the comparison to the reference level L1 cancels out:\n\\[ (\\mbox{L3} - \\mbox{L1}) - (\\mbox{L2} - \\mbox{L1}) = \\mbox{L3} - \\mbox{L2 }\\]\nAn easy way to make these contrasts of two groups is to use the contrast function from the contrast package. We just need to specify which groups we want to compare. We have to pick one of pull or push types, although the answer will not differ, as we will see below.\n\n\nCode\nL3vsL2 &lt;- contrast::contrast(fitTL,list(leg=\"L3\",type=\"pull\"),list(leg=\"L2\",type=\"pull\"))\nL3vsL2\n\n\nlm model parameter contrast\n\n    Contrast       S.E.      Lower      Upper     t  df Pr(&gt;|t|)\n -0.01142949 0.04319685 -0.0964653 0.07360632 -0.26 277   0.7915\n\n\nThe first column Contrast gives the L3 vs. L2 estimate from the model we fit above.\n\n\nCode\nL3vsL2 &lt;- contrast::contrast(fitTL,list(leg=\"L3\",type=\"push\"),list(leg=\"L2\",type=\"push\"))\nL3vsL2\n\n\nlm model parameter contrast\n\n    Contrast       S.E.      Lower      Upper     t  df Pr(&gt;|t|)\n -0.01142949 0.04319685 -0.0964653 0.07360632 -0.26 277   0.7915\n\n\nwe can check the matrix of the contrast like this:\n\n\nCode\nL3vsL2$X\n\n\n  (Intercept) typepush legL2 legL3 legL4\n1           0        0    -1     1     0\nattr(,\"assign\")\n[1] 0 1 2 2 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$type\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$leg\n[1] \"contr.treatment\"\n\n\nand we see that it gives us 0,0,-1,1,0 which means that to find the contrast we are interested in we need to go take the third coefficient from the linear model times -1 and sum the 4th coefficient. Let’s check if that applies:\n\n\nCode\ncoefs&lt;- coef(fitTL)\n-coefs[3]+coefs[4]\n\n\n      legL2 \n-0.01142949"
  },
  {
    "objectID": "matrix.html#linear-model-with-interactions",
    "href": "matrix.html#linear-model-with-interactions",
    "title": "Matrix Algebra",
    "section": "12.3 Linear model with interactions",
    "text": "12.3 Linear model with interactions\nIn the previous linear model, we assumed that the push vs. pull effect was the same for all of the leg pairs (the same orange arrow). You can easily see that this does not capture the trends in the data that well. That is, the tips of the arrows did not line up perfectly with the group averages. For the L1 leg pair, the push vs. pull estimated coefficient was too large, and for the L3 leg pair, the push vs. pull coefficient was somewhat too small.\nInteraction terms will help us overcome this problem by introducing additional coefficients to compensate for differences in the push vs. pull effect across the 4 groups. As we already have a push vs. pull term in the model, we only need to add three more terms to have the freedom to find leg-pair-specific push vs. pull differences. As we will see, interaction terms are added to the design matrix by multiplying the columns of the design matrix representing existing terms.\nWe can rebuild our linear model with an interaction between type and leg, by including an extra term in the formula type:leg. The : symbol adds an interaction between the two variables surrounding it. An equivalent way to specify this model is ~ type*leg, which will expand to the formula ~ type + leg + type:leg, with main effects for type, leg and an interaction term type:leg.\n\n\nCode\nX &lt;- model.matrix(~ type + leg + type:leg, data=spider)\ncolnames(X)\n\n\n[1] \"(Intercept)\"    \"typepush\"       \"legL2\"          \"legL3\"         \n[5] \"legL4\"          \"typepush:legL2\" \"typepush:legL3\" \"typepush:legL4\"\n\n\nCode\nhead(X)\n\n\n  (Intercept) typepush legL2 legL3 legL4 typepush:legL2 typepush:legL3\n1           1        0     0     0     0              0              0\n2           1        0     0     0     0              0              0\n3           1        0     0     0     0              0              0\n4           1        0     0     0     0              0              0\n5           1        0     0     0     0              0              0\n6           1        0     0     0     0              0              0\n  typepush:legL4\n1              0\n2              0\n3              0\n4              0\n5              0\n6              0\n\n\nCode\nimagemat(X, main=\"Model matrix for linear model with interactions\")\n\n\n\n\n\nImage of model matrix with interactions.\n\n\n\n\nColumns 6-8 (typepush:legL2, typepush:legL3, and typepush:legL4) are the product of the 2nd column (typepush) and columns 3-5 (the three leg columns). Looking at the last column, for example, the typepush:legL4 column is adding an extra coefficient \\(\\beta_{\\textrm{push,L4}}\\) to those samples which are both push samples and leg pair L4 samples. This accounts for a possible difference when the mean of samples in the L4-push group are not at the location which would be predicted by adding the estimated intercept, the estimated push coefficient typepush, and the estimated L4 coefficient legL4.\nWe can run the linear model using the same code as before:\n\n\nCode\nfitX &lt;- lm(friction ~ type + leg + type:leg, data=spider)\nsummary(fitX)\n\n\n\nCall:\nlm(formula = friction ~ type + leg + type:leg, data = spider)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.46385 -0.10735 -0.01111  0.07848  0.76853 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)     0.92147    0.03266  28.215 &lt; 0.0000000000000002 ***\ntypepush       -0.51412    0.04619 -11.131 &lt; 0.0000000000000002 ***\nlegL2           0.22386    0.05903   3.792             0.000184 ***\nlegL3           0.35238    0.04200   8.390  0.00000000000000262 ***\nlegL4           0.47928    0.04442  10.789 &lt; 0.0000000000000002 ***\ntypepush:legL2 -0.10388    0.08348  -1.244             0.214409    \ntypepush:legL3 -0.38377    0.05940  -6.461  0.00000000047335813 ***\ntypepush:legL4 -0.39588    0.06282  -6.302  0.00000000117063701 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1904 on 274 degrees of freedom\nMultiple R-squared:  0.8279,    Adjusted R-squared:  0.8235 \nF-statistic: 188.3 on 7 and 274 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\ncoefs &lt;- coef(fitX)\n\n\nHere is where the plot with arrows really helps us interpret the coefficients. The estimated interaction coefficients (the yellow, brown and silver arrows) allow leg-pair-specific differences in the push vs. pull difference. The orange arrow now represents the estimated push vs. pull difference only for the reference leg pair, which is L1. If an estimated interaction coefficient is large, this means that the push vs. pull difference for that leg pair is very different than the push vs. pull difference in the reference leg pair.\nNow, as we have eight terms in the model and eight parameters, you can check that the tips of the arrowheads are exactly equal to the group means (code not shown).\n\n\n\n\n\nDiagram of the estimated coefficients in the linear model. In the design with interaction terms, the orange arrow now indicates the push vs. pull difference only for the reference group (L1), while three new arrows (yellow, brown and grey) indicate the additional push vs. pull differences in the non-reference groups (L2, L3 and L4) with respect to the reference group.\n\n\n\n\nNow we want to compare push vs pull in L2:\n\n\nCode\nL2push.vs.pull &lt;- contrast::contrast(fitX,\n  list(leg=\"L2\", type = \"push\"),\n  list(leg=\"L2\", type = \"pull\")\n)\nL2push.vs.pull\n\n\nlm model parameter contrast\n\n Contrast      S.E.      Lower      Upper     t  df Pr(&gt;|t|)\n   -0.618 0.0695372 -0.7548951 -0.4811049 -8.89 274        0\n\n\nwe can look at the contrast vector that will be :\n\n\nCode\n(C&lt;- L2push.vs.pull$X)\n\n\n  (Intercept) typepush legL2 legL3 legL4 typepush:legL2 typepush:legL3\n1           0        1     0     0     0              1              0\n  typepush:legL4\n1              0\nattr(,\"assign\")\n[1] 0 1 2 2 2 3 3 3\nattr(,\"contrasts\")\nattr(,\"contrasts\")$type\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$leg\n[1] \"contr.treatment\"\n\n\nwich is: 0,1,0,0,0,1,0 and means we need to add the 2nd coefficient to the 6th coefficient\n\n\nCode\ncoefs[2]+coefs[6]\n\n\ntypepush \n  -0.618 \n\n\nNow we are interested in comparing if the difference between push and pull from one leg with the difference between push and pull from another leg, let’s say L3 and L4. This is a differences of differences and we cannot use the same contrast package. We will use the library multcomp. Remember we had 8 coefficients from the linear model:\n\n\nCode\ncoefs\n\n\n   (Intercept)       typepush          legL2          legL3          legL4 \n     0.9214706     -0.5141176      0.2238627      0.3523756      0.4792794 \ntypepush:legL2 typepush:legL3 typepush:legL4 \n    -0.1038824     -0.3837670     -0.3958824 \n\n\nIf we look in the plot we are interested in the difference between the brown line and the yellow line and those are represented by the coefficients 6 and 7. So we have to construct a matrix with 1 on the coefficients we are interested in and 0 in the rest:\n\n\nCode\nC&lt;- matrix(c(0,0,0,0,0,-1,1,0),1)\nL3vsL2interaction &lt;- multcomp::glht(fitX, linfct=C)\nsummary(L3vsL2interaction)\n\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: lm(formula = friction ~ type + leg + type:leg, data = spider)\n\nLinear Hypotheses:\n       Estimate Std. Error t value Pr(&gt;|t|)    \n1 == 0 -0.27988    0.07893  -3.546  0.00046 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nand we see that it is the same as subtracting the coefficients, but the function above gives us also a t-statistic and a p-value.\n\n\nCode\ncoefs[7]-coefs[6]\n\n\ntypepush:legL3 \n    -0.2798846 \n\n\nFinally we can ask if the pull vs push difference is different for each pair of legs, and this can be answered using anova.\n\n\nCode\nanova(fitX)\n\n\nAnalysis of Variance Table\n\nResponse: friction\n           Df Sum Sq Mean Sq  F value                Pr(&gt;F)    \ntype        1 42.783  42.783 1179.713 &lt; 0.00000000000000022 ***\nleg         3  2.921   0.974   26.847  0.000000000000002972 ***\ntype:leg    3  2.098   0.699   19.282  0.000000000022555601 ***\nResiduals 274  9.937   0.036                                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nthe Sum Sq column in the anova results is the variance of the aggregated coefficients and it is telling us what variables are responsible for the variance, so for example in our results the Sum Sq for the type (push vs pull) is 42.783 it’s the highest of them all, which means that this is most responsible for the variance in the coefficients. Then we have a Sum Sq for the leg (in our graph are the purple, pink and green arrows) with a value of of 2.921 so they also explain part of the variance. Finally we also have a 2.098 value in Sum Sq for the interaction type:leg (push vs pull by Leg pair) (in our graph the yellow, brown and grey arrows) which means that there is also a difference attributed to that. The f-value is like the t-value in a t-test. The p-value works the same as in a t-test, in our case it is smaller than 0.05 so it means that the difference we are seeing in those values are more than what we would expect by chance."
  },
  {
    "objectID": "matrix.html#collinearity-and-least-squares",
    "href": "matrix.html#collinearity-and-least-squares",
    "title": "Matrix Algebra",
    "section": "13.1 Collinearity and Least Squares",
    "text": "13.1 Collinearity and Least Squares\nConsider a design matrix \\(\\mathbf{X}\\) with two collinear columns. Here we create an extreme example in which one column is the opposite of another:\n\\[\n\\mathbf{X} = \\begin{pmatrix}\n\\mathbf{1}&\\mathbf{X}_1&\\mathbf{X}_2&\\mathbf{X}_3\\\\\n\\end{pmatrix}\n\\mbox{ with, say, }\n\\mathbf{X}_3 = - \\mathbf{X}_2\n\\]\nThis means that we can rewrite the residuals like this:\n\\[\n\\mathbf{Y}- \\left\\{ \\mathbf{1}\\beta_0 + \\mathbf{X}_1\\beta_1 + \\mathbf{X}_2\\beta_2 + \\mathbf{X}_3\\beta_3\\right\\}\\\\\n= \\mathbf{Y}- \\left\\{ \\mathbf{1}\\beta_0 + \\mathbf{X}_1\\beta_1 + \\mathbf{X}_2\\beta_2 - \\mathbf{X}_2\\beta_3\\right\\}\\\\\n= \\mathbf{Y}- \\left\\{\\mathbf{1}\\beta_0 + \\mathbf{X}_1 \\beta_1 + \\mathbf{X}_2(\\beta_2  - \\beta_3)\\right\\}\n\\] so if we have a solution, adding one to both beta2 and beta3 will also be a solution, so there is not a single value that minimizes the error.\nand if \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\), \\(\\hat{\\beta}_3\\) is a least squares solution, then, for example, \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2+1\\), \\(\\hat{\\beta}_3+1\\) is also a solution.\n\nConfounding as an example\nNow we will demonstrate how collinearity helps us determine problems with our design using one of the most common errors made in current experimental design: confounding. To illustrate, let’s use an imagined experiment in which we are interested in the effect of four treatments A, B, C and D. We assign two mice to each treatment. After starting the experiment by giving A and B to female mice, we realize there might be a sex effect. We decide to give C and D to males with hopes of estimating this effect. But can we estimate the sex effect? The described design implies the following design matrix:\n\\[\n\\,\n\\begin{pmatrix}\nSex & A & B & C & D\\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 1\\\\\n\\end{pmatrix}\n\\]\nHere we can see that sex and treatment are confounded. Specifically, the sex column can be written as a linear combination of the C and D matrices.\n\\[\n\\,\n\\begin{pmatrix}\nSex \\\\\n0\\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1\\\\\n1\\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nC \\\\\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n1\\\\\n1\\\\\n0\\\\\n0\\\\\n\\end{pmatrix}\n+\n\\begin{pmatrix}\nD \\\\\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n1\\\\\n1\\\\\n\\end{pmatrix}\n\\]\nThis implies that a unique least squares estimate is not achievable.\nIt can be difficult to perceive that just looking at the matrix. In r we have a function that will help us with this:\n\n\n13.1.1 Rank\nThe rank of a matrix columns is the number of columns that are independent of all the others. If the rank is smaller than the number of columns, then the LSE are not unique. In R, we can obtain the rank of matrix with the function qr, which we will describe in more detail in a following section.\n\n\nCode\nSex &lt;- c(0,0,0,0,1,1,1,1)\nA &lt;-   c(1,1,0,0,0,0,0,0)\nB &lt;-   c(0,0,1,1,0,0,0,0)\nC &lt;-   c(0,0,0,0,1,1,0,0)\nD &lt;-   c(0,0,0,0,0,0,1,1)\nX &lt;- model.matrix(~Sex+A+B+C+D-1)\ncat(\"ncol=\",ncol(X),\"rank=\", qr(X)$rank,\"\\n\")\n\n\nncol= 5 rank= 4 \n\n\nThis particular experiment could have been designed better. Using the same number of male and female mice, we can easily design an experiment that allows us to compute the sex effect as well as all the treatment effects. Specifically, when we balance sex and treatments, the confounding is removed as demonstrated by the fact that the rank is now the same as the number of columns:\n\n\nCode\nSex &lt;- c(0,1,0,1,0,1,0,1)\nA &lt;-   c(1,1,0,0,0,0,0,0)\nB &lt;-   c(0,0,1,1,0,0,0,0)\nC &lt;-   c(0,0,0,0,1,1,0,0)\nD &lt;-   c(0,0,0,0,0,0,1,1)\nX &lt;- model.matrix(~Sex+A+B+C+D-1)\ncat(\"ncol=\",ncol(X),\"rank=\", qr(X)$rank,\"\\n\")\n\n\nncol= 5 rank= 5 \n\n\nHere we will not be able to estimate the effect of sex."
  },
  {
    "objectID": "modelAccuracy.html",
    "href": "modelAccuracy.html",
    "title": "Assessing Model Accuracy",
    "section": "",
    "text": "In this section, we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specific data set."
  },
  {
    "objectID": "modelAccuracy.html#best-subset-selection",
    "href": "modelAccuracy.html#best-subset-selection",
    "title": "Assessing Model Accuracy",
    "section": "10.1 Best Subset selection",
    "text": "10.1 Best Subset selection\nThe most direct approach to answer this question is called all subsets or best subsets regression. We compute the least squares fit for all possible subsets and then choose between them based on some criterion that balances training error with model size. This gets really hard when the number of variables is high, the number of possible models is \\(2^p\\) so for example for p=40 there are over a billion models. Instead, we need an automated approach that searches through a subset of them. We discuss two commonly use approaches next.\nAlthough we are presenting this here in the context of linear models, the same ideas apply to other types of models, such as logistic regression. One difference will be that other models will work with a similar metric to the residual sum of squares that is called deviance.\nA limitation of the subset selection method is the number of p. Most statistical packages have a limit of 30 to 40 features. Another problem of this method with high number of \\(p\\) is overfitting, so this method is only valid when \\(p\\) is small (&lt;10).\nWe start with the null model that is a model that contains the intercept but no predictors. The intercept is the mean of \\(y\\) Now you start adding variables one at a time: for each variable you fit \\(p\\) simple linear models each with one of the variables and the intercept, and you look at each of them. After that you choose the variable that results in the lowest RSS (that is the largest \\(R^2\\)). Now, having picked that, you fix that variable in the model and now you add one by one all the variables to this model again, and choose the one that best improve the residual sum of squares.\nYou can continue like this until some stopping rule is satisfied, for example, when all the remaining variables have a \\(p\\) value above some threshold.\nOnce we have a correct combination of predictors, to select if the best model is the model with 2,3,or n predictors, we cannot use RSS anymore because naturally RSS will decrease as we include more predictors, but that does not necessarily means it’s a best model. We need to select the model that will have the smallest test error, and not the one with the smallest training error. Remember we talked about these concepts already. Frequently for this we use cross-validation\n\nExample\nHere we apply the best subset selection approach to the Hitters data from ISLR2 library. We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year. Note that the Salary variable is missing in some rows, so we are going to omit them\n\n\nCode\n#nubmer of nas. \nsum(is.na(Hitters$Salary))\n\n\n[1] 59\n\n\nCode\n#remove nas\nHitters&lt;- na.omit(Hitters)\nsum(is.na(Hitters$Salary))\n\n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for ‘lm()’. The ‘summary()’ command outputs the best set of variables for each model size.\n\n\nCode\nregfit.full &lt;- leaps::regsubsets(Salary ~ ., Hitters)\n(reg.summary&lt;- summary(regfit.full))\n\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nFor each subset size (number of features 1…n) it returns the best subset models. An asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI.\nBy default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired.\n\n\nCode\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\n(reg.summary &lt;- summary(regfit.full))\n\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: exhaustive\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\nWe can see the R squared using reg.summary$rsq. Here we can see that the R2 statistic increases from 32%, when only one variable is included in the model, to almost 55%, when all variables are included. As expected, the R2 statistic increases monotonically as more variables are included, so this is not a good statistic for choosing the right size for the model because it will always recommend the model with more variables.\n\n\nCode\nreg.summary$rsq\n\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nSo now that we have the best combination of variables for each model size, we can use cross validation or other methods to select the model with the best number of predictors.\nWe will see study Mallow’s Cp, AIC or BIC soon, and regsubset gives us the statistics for the different models for those methods:\n\n\nCode\npar(mfrow = c(2, 2))\nplot(reg.summary$bic, xlab = \"Number of Variables\",\n    ylab = \"BIC\", type = \"l\")\nplot(reg.summary$cp, xlab = \"Number of Variables\",\n    ylab = \"Cp\", type = \"l\")\nplot(reg.summary$rsq, xlab = \"Number of Variables\",\n    ylab = \"RSS\", type = \"l\")\nplot(reg.summary$rss, xlab = \"Number of Variables\",\n    ylab = \"rss\", type = \"l\")\n\n\n\n\n\n\n\n\n\nwith this information we will be able to select the model with the correct number of variables."
  },
  {
    "objectID": "modelAccuracy.html#stepwise-selection",
    "href": "modelAccuracy.html#stepwise-selection",
    "title": "Assessing Model Accuracy",
    "section": "10.2 Stepwise Selection",
    "text": "10.2 Stepwise Selection\n\n10.2.1 Forward Stepwise selection\nThe stepwise selection is very similar to the best subset as we start with a model with only the intercept and start aggregating predictors and selecting those with less RSS. The difference with best subset selection is that these models are nested and brings the number of models that we are considering from \\(2^p\\) to \\(p^2\\) which is more manageable. Because we are not considering every possible combination of all the predictors, this method is not guaranteed to find the model with less RSS.\nAgain, once we have the best version of the model with each number of predictors, we will choose among them using cross-validation or a similar technique.\n\n\n10.2.2 Backward stepwise selection\nIt runs a similar process but starting with a model with all the variables, then you run the model removing each variable and see which one is the least statistically significant one. The new (p-1) variable model is fit, and the variable with the largest \\(p\\)-value is removed. Continue until a stopping rule is reached, for instance, we may stop when all remaining variables have a significant \\(p\\)-value defined by some significance threshold.\nOne limitation of the backward stepwise selection is that we can only do it with n&gt;p (more samples than predictors). This limitation does not apply to forward stepwise selection.\n\nExample\nWe can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method = \"forward\" or method=\"backward\". We are going to format a bit the output of the summary so it is more friendly to the reader:\n\n\nCode\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"forward\")\n\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19, method = \"backward\")\n\ncreateSummaryTable &lt;- function(regfitObject){\n\nsummary &lt;- summary(regfitObject)\nmatrix_of_models &lt;- as.data.frame(summary$which)\nmatrix_of_models &lt;- matrix_of_models %&gt;%\n  mutate_all(~ifelse(., \"**\", \"\"))\n\nkable(matrix_of_models, escape = FALSE) %&gt;%\n  kable_styling(full_width = FALSE) %&gt;%\n  row_spec(0, bold = TRUE) \n}\n\ncreateSummaryTable(regfit.fwd)\n\n\n\n\n\n(Intercept)\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeagueN\nDivisionW\nPutOuts\nAssists\nErrors\nNewLeagueN\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n**\n\n**\n\n\n\n\n\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n**\n\n**\n\n\n\n\n\n\n\n\n\n**\n\n\n\n**\n\n\n\n\n\n**\n\n**\n\n\n\n\n\n\n\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n\n\n\n\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n\n**\n**\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n**\n**\n**\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n**\n\n\n**\n**\n**\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n**\n\n\n**\n**\n**\n\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n**\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n**\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n\n**\n\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n**\n**\n\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n**\n**\n\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n\n\n\n\nCode\ncreateSummaryTable(regfit.bwd)\n\n\n\n\n\n(Intercept)\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeagueN\nDivisionW\nPutOuts\nAssists\nErrors\nNewLeagueN\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n\n**\n\n**\n\n\n\n\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n\n**\n\n**\n\n\n\n\n\n\n\n\n**\n\n\n\n\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n\n\n\n\n\n**\n\n\n\n\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n**\n\n\n\n\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n**\n\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n**\n\n**\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n**\n**\n**\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n**\n\n\n**\n**\n**\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n**\n\n\n**\n**\n**\n\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n**\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n**\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n\n**\n\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n\n**\n\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n**\n**\n\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n**\n**\n\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n\n\n\n\nFor instance, we see that using forwards stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best model for our number of variabless identified by forward and backward stepwise selection and best subset selection are different.\n\n\nCode\ncoef(regfit.full, 7)\n\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\nCode\ncoef(regfit.fwd, 7)\n\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\n\nCode\ncoef(regfit.bwd, 7)\n\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\nAs we saw before, the summary() function also returns \\(R^2\\), RSS, adjusted \\(R^2\\), Cp, and BIC. We can examine these to try to select the best overall model.\n\n\nCode\nreg.summary &lt;- summary(regfit.fwd)\nnames(reg.summary)\n\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nPlotting RSS, adjusted R Squared, Cp and BIC for all the models at once will help us decide which model to select. Note the type =\"1\" option tells R to connect the plotted points with lines. Here we are plotting RSS and Adjusted R Squared\n\n\nCode\npar(mfrow = c(1, 2))\nplot(reg.summary$rss, xlab = \"Number of Variables\",\n    ylab = \"RSS\", type = \"l\")\nplot(reg.summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\n\n\n\n\n\n\n\n\n\nThe points() command works like the plot() command, except that it puts points on a plot that has already been created, instead of creating a new plot. The which.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted R2 statistic, which would be the preferred one.\n\n\nCode\nmax&lt;- which.max(reg.summary$adjr2)\nplot(reg.summary$adjr2, xlab = \"Number of Variables\",\n    ylab = \"Adjusted RSq\", type = \"l\")\npoints(max, reg.summary$adjr2[max], col = \"red\", cex = 2, \n    pch = 20)\n\n\n\n\n\n\n\n\n\nSimilarly we can plot the Cp and the BIC statistics and indicate the models with the smallest statistic:\n\n\nCode\nmin&lt;- which.min(reg.summary$cp)\nplot(reg.summary$cp, xlab = \"Number of Variables\",\n    ylab = \"Cp\", type = \"l\")\npoints(min,reg.summary$cp[min], col = \"red\", cex = 2, \n    pch = 20)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmin&lt;- which.min(reg.summary$bic)\nplot(reg.summary$bic, xlab = \"Number of Variables\",\n    ylab = \"BIC\", type = \"l\")\npoints(min, reg.summary$bic[min], col = \"red\", cex = 2,\n    pch = 20)\n\n\n\n\n\n\n\n\n\nThe ‘regsubsets()’ function has a built-in ‘plot()’ command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, Cp, adjusted R2, or AIC. To find out more about this function, type ?plot.regsubsets. We will see the BIC here:\n\n\nCode\nplot(regfit.fwd, scale = \"bic\")\n\n\n\n\n\n\n\n\n\nThe top row of the plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to −150. However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the coef() function to see the coefficient estimates associated with this model.\n\n\nCode\ncoef(regfit.fwd, 6)\n\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\nAs we did before we can plot the results for Cp and and see the difference between our bakwards and forward step wise selections:\n\n\nCode\nplot(regfit.fwd, scale=\"Cp\")\n\n\n\n\n\nCode\nplot(regfit.bwd, scale=\"Cp\")"
  },
  {
    "objectID": "modelAccuracy.html#mallows-cp",
    "href": "modelAccuracy.html#mallows-cp",
    "title": "Assessing Model Accuracy",
    "section": "11.1 Mallow’s Cp",
    "text": "11.1 Mallow’s Cp\nThis technique adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables. \\[\nC_p = \\frac{1}{n}(RSS+2d\\hat{\\sigma}^2)\n\\tag{7}\\]\nWhere d is the total number of parameters used and - \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error term \\(\\epsilon\\) associated with each response measurement, calculated as: \\[\n\\hat{\\sigma}^2 = \\frac{SSE}{n-p}\n\\tag{8}\\] SSE is the Sum of Squared Errors or Residual Sum of Squares\n\\[\nSSE = \\sum^n_{i=1}(y_i-\\hat{y_i})^2\n\\tag{9}\\]\nThis technique is restricted to those models where p&lt;n. Example using the mtcars dataset\n\n\nCode\n# Fit the model\nmodel &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n# Extract the residuals\nresiduals &lt;- model$residuals\n\n# Calculate SSE\nSSE &lt;- sum(residuals^2)\n\n# Number of observations\nn &lt;- length(mtcars$mpg)\n\n# Number of predictors (including intercept)\np &lt;- length(coefficients(model))\n\n# Calculate \\(\\hat{\\sigma}^2\\)\nsigma_hat_sq &lt;- SSE / (n - p)\n\n# Print the result\nprint(sigma_hat_sq)\n\n\n[1] 6.725785\n\n\nNow we calculate Mallow’s Cp for the same dataset for a model with with two parameters:\n\n\nCode\n# Fit the subset model\nsubset_model &lt;- lm(mpg ~ hp + wt, data = mtcars)\n\n# Calculate residuals and SSE for the subset model\nresiduals_subset &lt;- subset_model$residuals\nSSE_subset &lt;- sum(residuals_subset^2)\n\n# Number of predictors in the subset model (including intercept)\np_subset &lt;- length(coefficients(subset_model))\nprint(SSE_subset)\n\n\n[1] 195.0478\n\n\nCode\n# Calculate Mallows' Cp \nCp &lt;- (SSE_subset / sigma_hat_sq) + 2 * p_subset - n \nprint(Cp)\n\n\n[1] 3\n\n\nWhen comparing two values for different models we want to choose the smallest Cp."
  },
  {
    "objectID": "modelAccuracy.html#aic",
    "href": "modelAccuracy.html#aic",
    "title": "Assessing Model Accuracy",
    "section": "11.2 AIC",
    "text": "11.2 AIC\nThe criterion is defined for a large class of models fit by maximum likelihood. We are not going to look at the formula right now, but it is similar to the previous and it is actually proportional to Cp, so again, we will choose the model with the smallest value."
  },
  {
    "objectID": "modelAccuracy.html#bayesian-information-criterion-bic",
    "href": "modelAccuracy.html#bayesian-information-criterion-bic",
    "title": "Assessing Model Accuracy",
    "section": "11.3 Bayesian Information Criterion (BIC)",
    "text": "11.3 Bayesian Information Criterion (BIC)\n\\[\nBIC = \\frac{1}{n}(RSS+log(n)d\\hat{\\sigma}^2)\n\\tag{10}\\]\nWe also look for the smallest value of the BIC for our models. The BIC statistic generally places a heavier penalty on models with many variables than the two methods seen before.\nBecause all these three methods rely on calculating hat sigma squared, all of them require that n&gt;p due to the formula to calculate it (Equation 8)"
  },
  {
    "objectID": "modelAccuracy.html#adjusted-rsquared",
    "href": "modelAccuracy.html#adjusted-rsquared",
    "title": "Assessing Model Accuracy",
    "section": "11.4 Adjusted RSquared",
    "text": "11.4 Adjusted RSquared\nFor a least squared model with \\(d\\) variables, the adjusted \\(R^2\\) statistic is calculated as:\n\\[\n\\text{Adjusted } R^2= 1- \\frac{RSS/(n-d-1)}{TSS/(n-1)}\n\\tag{11}\\]\nwhere TSS is the total sum of squares.\nUnlike Cp, AIC and BIC, for which a small value indicates a model with low test error, a large value of adjusted R squared indicates a model with small test error. Maximizing the adjusted \\(R^2\\) is equivalent to minimizing RSS/(n-d-1). While RSS always decreases as the number of variables in the model increases, the presence of d in the denominator corrects for that, so the inclusion of unnecessary variables in the model pays a price.\nThe drawback of Adjusted Rsquared is that it cannot be used for other models, like logistic, for example.\n\nExample: Validation Set Approach:\nWe saw how to choose our best models for the Hitters dataset using Best subset selection, Stepwise Selection and then using methods like BIC, AIC to select the best size for our model.\nNow we will consider how to do this using the validation set and cross-validation approaches. In order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of the model-fitting, including variable selection. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error. In order to use the validation approach, we begin by splitting the observations into a training set and a test set. We choose the size of the train set as 180 (approx 2/3 of the data)\n\n\nCode\nobjects_to_remove &lt;- ls(pattern = \"^regfit\") \nrm(list = objects_to_remove)\nn&lt;-nrow(Hitters)\nset.seed(1)\ntrain &lt;- sample(seq(1:n), 180, replace= FALSE)\n\n\nnow we apply regsubsets() to the training set in order to perform best subset selection.\n\n\nCode\nregfit.fwd&lt;- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax= 19, method=\"forward\")\ncreateSummaryTable(regfit.fwd)\n\n\n\n\n\n(Intercept)\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeagueN\nDivisionW\nPutOuts\nAssists\nErrors\nNewLeagueN\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n**\n\n\n\n\n\n**\n\n\n\n\n\n**\n\n\n\n\n\n\n\n\n\n**\n\n\n\n\n\n**\n\n\n\n\n\n**\n\n\n**\n\n\n\n\n\n\n**\n\n\n\n\n\n**\n\n\n\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n\n**\n\n\n\n**\n\n\n\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n\n\n\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n**\n\n\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n**\n\n**\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n**\n**\n**\n\n\n**\n\n\n**\n**\n\n\n\n\n\n**\n**\n**\n\n\n\n**\n**\n**\n**\n\n\n**\n\n\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n\n\n**\n**\n**\n**\n**\n\n**\n\n\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n\n**\n\n\n**\n**\n**\n\n\n\n\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n\n**\n\n\n**\n**\n**\n**\n\n\n\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n\n\n\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n**\n\n\n\n\n\n\n\nWe now make a model matrix from the test data:\n\n\nCode\ntest.mat &lt;- model.matrix(Salary ~ ., data = Hitters[-train, ])\nhead(test.mat)\n\n\n                  (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat\n-Andre Dawson               1   496  141    20   65  78    37    11   5628\n-Andres Galarraga           1   321   87    10   39  42    30     2    396\n-Al Newman                  1   185   37     1   23   8    21     2    214\n-Argenis Salazar            1   298   73     0   24  24     7     3    509\n-Andres Thomas              1   323   81     6   26  32     8     2    341\n-Alex Trevino               1   202   53     4   31  26    27     9   1876\n                  CHits CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts\n-Andre Dawson      1575    225   828  838    354       1         0     200\n-Andres Galarraga   101     12    48   46     33       1         0     805\n-Al Newman           42      1    30    9     24       1         0      76\n-Argenis Salazar    108      0    41   37     12       0         1     121\n-Andres Thomas       86      6    32   34      8       1         1     143\n-Alex Trevino       467     15   192  186    161       1         1     304\n                  Assists Errors NewLeagueN\n-Andre Dawson          11      3          1\n-Andres Galarraga      40      4          1\n-Al Newman            127      7          0\n-Argenis Salazar      283      9          0\n-Andres Thomas        290     19          1\n-Alex Trevino          45     11          1\n\n\nThe model.matrix() function is used in many regression packages for building an X matrix from data. Now we run a loop, and for each size i, we extract the coefficients from regfit.fwd for the best model that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test Mean Square for Error (MSE).\nFirst we will do a sample iteration showing some of the the values so we can see what the loop is doing we choose the model with three variables + the intercept:\n\n\nCode\n(coefi &lt;- coef(regfit.fwd, id = 3))\n\n\n (Intercept)        Walks         CRBI    DivisionW \n 116.3421387    6.7810155    0.7516748 -177.8700862 \n\n\nCode\n print(names(coefi))\n\n\n[1] \"(Intercept)\" \"Walks\"       \"CRBI\"        \"DivisionW\"  \n\n\nCode\n head(test.mat[, names(coefi)])\n\n\n                  (Intercept) Walks CRBI DivisionW\n-Andre Dawson               1    37  838         0\n-Andres Galarraga           1    30   46         0\n-Al Newman                  1    21    9         0\n-Argenis Salazar            1     7   37         1\n-Andres Thomas              1     8   34         1\n-Alex Trevino               1    27  186         1\n\n\nCode\n pred &lt;- test.mat[, names(coefi)] %*% coefi\n head(pred)\n\n\n                       [,1]\n-Andre Dawson     997.14318\n-Andres Galarraga 354.34964\n-Al Newman        265.50854\n-Argenis Salazar   13.75113\n-Andres Thomas     18.27712\n-Alex Trevino     261.37098\n\n\nCode\n salaryTestData&lt;- Hitters$Salary[-train]\n meanSquareError&lt;- mean((salaryTestData - pred)^2)\n\n\nLet’s run the loop now:\n\n\nCode\nval.errors &lt;- rep(NA, 19)\nfor (i in 1:19){\n  coefi &lt;- coef(regfit.fwd, id = i)\n  pred &lt;- test.mat[, names(coefi)] %*% coefi\n  val.errors[i] &lt;- mean((Hitters$Salary[-train] - pred)^2)\n}\n\n\nwe can now plot the square root of MSE to see the results. Taking the square root of MSE gives us the Root Mean Squared Error (RMSE), which is on the same scale as the original data. This makes it easier to interpret and compare. The model summary does not give us MSE directly, but we can calculate it: \\(MSE=RSS/n\\). In our code: regfit.fwd$rss[-1]/length(Hitters$Salary[test]) the minus one is to remove the intercept.\n\n\nCode\nplot(sqrt(val.errors), ylab= \"Sqrt MSE\", pch=1,type=\"b\",ylim= c(250,450))\npoints(which.min(val.errors), sqrt(val.errors[which.min(val.errors)]), col=\"red\", cex = 2,\n    pch = 20, )\n\npoints(sqrt(regfit.fwd$rss[-1]/length(Hitters$Salary[train])),col=\"blue\", pch =1, type=\"b\")\nlegend(\"topright\", legend=c(\"Training\",\"Validation\"), col= c(\"blue\",\"black\"), pch=1)\n\n\n\n\n\n\n\n\n\nthe best model is the one with the min error:\n\n\nCode\nval.errors\n\n\n [1] 168475.6 155752.3 155117.6 143942.4 133666.6 123143.9 123647.3 129815.6\n [9] 134533.1 134622.5 136185.3 141050.9 139742.9 139190.8 140488.9 135309.0\n[17] 132339.1 134276.7 135389.4\n\n\nCode\nmin&lt;-which.min(val.errors)\ncoef(regfit.fwd, min)\n\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n 100.1066984   -1.2667362    4.9577691    4.5588605    0.7525976 -160.7461336 \n     PutOuts \n   0.2695514 \n\n\nThis was a little tedious because there is no predict method for regsubsets(). Since we will be using this function again, we can write our own method:\n\n\nCode\npredict.regsubsets &lt;- function(object, newdata, id, ...) {\n  form &lt;- as.formula(object$call[[2]]) #extract call formula\n  mat &lt;- model.matrix(form, newdata) \n  coefi &lt;- coef(object, id = id)\n  xvars &lt;- names(coefi)\n  mat[, xvars] %*% coefi\n }\n\n\nOur function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to regsubsets(). We demonstrate how we use this function below, when we do cross-validation.\nFinally, we perform best subset selection on the full data set, and select the best model for our number of variables. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates. Note that we perform best subset selection on the full data set and select the best model for our number of variables, rather than simply using the variables that were obtained from the training set, because the best model for our number of variables on the full data set may differ from the corresponding model on the training set.\nIn fact, we see that the best model for our number of variables on the full data set has a different set of variables than the best model for our number of variables on the training set.\n\n\nCode\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(regfit.best, min)\n\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076"
  },
  {
    "objectID": "modelAccuracy.html#monte-carlo-method",
    "href": "modelAccuracy.html#monte-carlo-method",
    "title": "Assessing Model Accuracy",
    "section": "12.1 Monte Carlo Method",
    "text": "12.1 Monte Carlo Method\nThe Monte Carlo method is a powerful statistical technique used to understand the impact of risk and uncertainty in prediction and forecasting models.\nIf for example we want to know the average height of all people living in USA, we estimate the parameter we are interested in (average height of the population) using a statistic estimator that is the average height of a sample of the population drawn at random. \\(\\theta\\) is the parameter we want to calculate and we use \\(\\hat{\\theta}\\) that is the average of our sample. By the law of large numbers, the approximation error can be made arbitrarily small by using a large enough sample size.\nWhat if we are interested in an estimator \\(\\hat{\\theta}\\) for some parameter \\(\\theta\\) of the population and the normal approximation is not valid for that estimator? In such situations, simulations can often be used to estimate the parameters.\nThe Monte Carlo method relies on repeated random sampling to obtain numerical results. The core idea is to use randomness to solve problems that might be deterministic in nature. This method is named after the Monte Carlo Casino in Monaco, reflecting the element of chance and randomness.\nIn our example to estimate the height of the population of USA adults we get many (let’s say 1000) samples of n=100 observations. We then compute the estimator \\(\\hat{\\theta}\\) for each sample (in our case the average height), resulting in 1000 estimates. Now we compute the standard deviation of these 1000 estimates and it results to be close to the standard error of my estimator. :\n\\[\nSE(\\hat{\\theta})=\\sqrt{E(\\hat{\\theta}-E(\\hat{\\theta}))^ 2}\n\\]\n\\[\ns(\\hat{\\theta}_1,...,\\hat{\\theta}_{1000})= \\sqrt{\\frac{1}{999}\\sum^{1000}_{i=1}(\\hat{\\theta}-mean(\\hat{\\theta_i}))^ 2} \\approx SE(\\hat{\\theta})\n\\]\nThe caveat of this method is that it will only work where we can extract as many samples as we wish.\nUsually we cannot have as many samples as we wish, when we want to use the Monte Carlo Method in practice, it is typical to assume a parametric distribution and generate a population from this, which is called a parametric simulation. This means that we take parameters estimated from the real data and using the mean and standard deviation, we plug these into a model of normal distribution and generate new samples.\nFor example for the case of mice weights, we know that mice typically weight 24 gr with a SD of about 3.5 gr. and that the distribution is approximately normal, to generate population data:\n\n\nCode\ncontrols&lt;- rnorm(5000, mean=24, sd=3.5)"
  },
  {
    "objectID": "modelAccuracy.html#crossvalidation",
    "href": "modelAccuracy.html#crossvalidation",
    "title": "Assessing Model Accuracy",
    "section": "12.2 Cross-validation",
    "text": "12.2 Cross-validation\nAs the models become more complex (the more features we add to our model), the training error becomes lower. On the other hand, the test error does not reduces at the same rate as the model gets more complex, it starts going down but then it starts increasing again as the model starts overfitting to the training dataset.\nThe prediction error comes from bias and variance. The bias is how var off on the average the model is from the truth. The variance is how much the estimate varies around its average. The less parameters we fit into our model, the more bias we have (we will find it more difficult to find the true mean) and as we increase the parameters, the bias reduces, but the variance goes up, because we have more and more parameters to estimate from the same amount of data. The solution for this would be a large designated test set, but this is not often available.\nThere are some methods to try to adjust the training error so it does not over fit and produces an increase in the testing error rate. We randomly divide the available set of samples into two parts: a training set and a validation or hold out set. The model is fit on the training set and the fitted model is used to predict responses for the observations in the validation set. The resulting validation-set error provides an estimate of the test error. This is typically assessed using Mean Squared Error (MSE) in the case of quantitative response and misclassification rate in the case of qualitative (discrete) responses.\n\n\nCode\n# Set up the plotting area for two plots side by side\npar(mfrow = c(1, 2))\n\n# Plot 1: MSE with All Data\n# Initialize a vector to store MSE errors for each polynomial degree\nmse_error_all &lt;- rep(0, 10)\n\n# Loop to calculate MSE errors for polynomial degrees 1 to 10\nfor (i in 1:10) {\n  glm_fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  predictions &lt;- predict(glm_fit)\n  actuals &lt;- Auto$mpg\n  mse_error_all[i] &lt;- mean((predictions - actuals)^2)\n}\n\n# Plot the MSE errors for all data\nplot(1:10, mse_error_all, \n     type = \"b\", \n     col = 1, \n     ylim= c(0,(max(mse_error_all) + 1)),\n     xlab = \"Degree of Polynomial\", \n     ylab = \"MSE\", \n     main = \"MSE with All Data\")\n\n# Plot 2: MSE with Different Samples\n# Initialize a list to store MSE errors for each sample\nmse_error_samples &lt;- list()\n\n# Loop to create different samples and calculate MSE errors\nfor (j in 1:10) {\n  # Create a different sample\n  train &lt;- sample(c(TRUE, FALSE), nrow(Auto), replace = TRUE, prob = c(0.6, 0.4))\n  \n  # Initialize a vector to store MSE errors for the current sample\n  mse_error_samples[[j]] &lt;- rep(0, 10)\n  \n  # Loop to calculate MSE errors for polynomial degrees 1 to 10\n  for (i in 1:10) {\n    glm_fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto[train, ])\n    predictions &lt;- predict(glm_fit, newdata = Auto[!train, ])\n    actuals &lt;- Auto$mpg[!train]\n    mse_error_samples[[j]][i] &lt;- mean((predictions - actuals)^2)\n  }\n}\n\ny_max_samples &lt;- max(unlist(mse_error_samples)) + 1\n\n# Plot the MSE errors for different samples\nplot(1:10, mse_error_samples[[1]], type = \"b\", col = 1, xlab = \"Degree of Polynomial\", ylab = \"MSE\", main = \"MSE with Different Samples\",\n  ylim = c(0, y_max_samples))\nfor (j in 2:10) {\n  lines(1:10, mse_error_samples[[j]], type = \"b\", col = j)\n}\nlegend(\"bottomleft\", legend = paste(\"Sample\", 1:10), col = 1:10, lty = 1, cex = 0.8)\n\n\n\n\n\ntwo-fold cross validation\n\n\n\n\nIn the first graph we can see that a polynomial of level 2 reduces drastically the MSE from a linear model, but more polynomial terms do not have the same effect. The second graph shows us how the MSE vary with different samples, so the choice of the sample will have an effect on the fit of the model. Another drawback of this method is that, by fitting the model to a smaller number of data points, the estimated error will be higher than fitting the model to the whole available data points.\nNow we will see some techniques that try to solve some of those drawbacks.\n\n12.2.1 K-fold Cross-validation\nK-fold cross validation is a widely used approach for estimating test error. The idea is to randomly divide the data into \\(K\\) equal-sized parts. We leave out part \\(k\\), fit the model to the other \\(K-1\\) parts (combined) and then obtain predictions for the left-out \\(kth\\) part.This is done in turn for each part k, and then the results are combined. \\(K\\) is usually between 5 and 10. Let’s the \\(K\\) parts be \\(C_1,C_2,\\dots,C_k\\). where \\(C_k\\) denotes the indices of the observations in part k. There are \\(n_k\\) observations in part k. \\[\nCV_{(K)}= \\sum^K_{k=1}\\frac{n_k}{n} MSE_k\n\\tag{12}\\]\nWhere: \\(CV_{(K)}\\): This represents the cross-validation error estimate for K-fold cross-validation. \\(\\sum^K_{k=1}\\): This indicates that the sum is taken over all \\(K\\) folds. \\(\\frac{n_k}{n}\\): This is the proportion of the total data used in the k-th fold, where \\(n_k\\) is the number of observations in the k-th fold and \\(n\\) is the total number of observations. \\(MSE_k\\): This is the Mean Squared Error for the k-th fold. \\(MSE_k=\\sum_{i\\epsilon C_k}(y_i-\\hat{y_i})^2/n_k\\) and \\(\\hat{y_i}\\) is the fit for the observation i, obtained from the data with the part \\(K\\) removed.\nOne special use of the k-fold cross validation is setting K=n, this is called leave-one-out cross validation (LOOCV): each observation is left out and used as a validation set, while the whole set of other data points are used for training the model. Leave one out cross validation for linear or polynomial model uses this formula: \\[\nCV_{(n)}=\\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{y_i-\\hat{y_i}}{1-h_i} \\right)^2\n\\tag{13}\\]\nwhere \\(\\hat{y_i}\\) is the ith fitted value for the original least squares fit and \\(h_i\\) is the leverage (diagonal of the “hat” matrix). This is like the ordinary MSE except the ith residual is divided by \\(1-h_i\\) The \\(h_i\\) tells you how much influence a single point has on the its own fit, it will be a number between 0 and 1.\nCross-validation takes the average of the errors over the n-folds so in LOOCV each sample looks much more like the other ones, because it only leaves one point out. As a result, each error estimate are highly correlated, when you average these highly correlated errors, the resulting average has a high variance -it is more sensitive to slight changes or outliers- because there’s little variation in the training data across folds. It’s not about the individual errors being different; it’s about them moving together because the training sets are too similar. Remember the trade off we talked about between bias and variance in the Bias-variance trade off section\nFor this reason it is often preferred to use k=5 or 10. Let’s compute the cross-validation in r using the formula we saw above. First we are going to fit the model to a linear model. We can use the function glm and if we don’t pass any family type it will fit to a linear model.\n\n\nCode\n#linear model:\nfit.lm&lt;- glm(mpg ~ horsepower, data = Auto)\nloocv&lt;- function(fit){\n  h &lt;-lm.influence(fit)$h\n  mean((residuals(fit)/(1-h))^2)\n}\n\nloocv(fit.lm)\n\n\n[1] 24.23151\n\n\nBut we have a handy function in r that will do this for us: We will use the library(boot) to calculate the cross validation errors for the Auto dataset using the function cv.glm. If we don’t pass any value of k, it will default to leave-one-out cross-validation.\n\n\nCode\nglm.fit &lt;- glm(mpg ~ horsepower, data = Auto)\ncv.err &lt;- boot::cv.glm(Auto, glm.fit)\ncv.err$delta\n\n\n[1] 24.23151 24.23114\n\n\nAnd we see that we get the same result that the one we calculated manually above. The first Delta is the cross-validation prediction error, and the second one is a biased corrected version of it, taking into consideration that the dataset is smaller for the dataset in cross validation. This has little effect in leave one out, but it will have a bigger effect in k=n cross-validation.\nNow we are going to fit polynomials of different degrees to our data because it looks non-linear, so we expect some of these to fit better than a linear model.\n\n\nCode\n# Leave one out\ncv.error &lt;- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] &lt;- boot::cv.glm(Auto, glm.fit)$delta[1]\n}\n# Plot the MSE errors for all data\nplot(1:10, cv.error, \n     type = \"b\", \n     col = 1, \n     ylim= c(2,(max(cv.error) + 1)),\n     xlab = \"Degree of Polynomial\", \n     ylab = \"MSE\", \n     main = \"Leave One Out\")\n\n\n\n\n\nLOOCV for different polynomials\n\n\n\n\nNow let’s try 10-k fold cross-validation.\n\n\nCode\n## $k$-Fold Cross-Validation\n# Initialize a list to store CV errors for each sample\ncv.error.10 &lt;- list()\n\n# Loop to create different samples and calculate MSE errors\nfor (j in 1:10) {\n  # Initialize a vector to store MSE errors for the current sample\n  cv.error.10[[j]] &lt;- rep(0, 10)\n  \n  # Loop to calculate MSE errors for polynomial degrees 1 to 10\n  for (i in 1:10) {\n    glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv.error.10[[j]][i] &lt;- boot::cv.glm(Auto, glm.fit, K = 10)$delta[1]\n  }\n}\n\ny_max_samples &lt;- max(unlist(cv.error.10)) + 1\n\n# Plot the MSE errors for different samples\nplot(1:10, cv.error.10[[1]], type = \"b\", col = 1, xlab = \"Degree of Polynomial\", ylab = \"MSE\", main = \"k=10 fold CV\",\n  ylim = c(2, y_max_samples))\nfor (j in 2:10) {\n  lines(1:10, cv.error.10[[j]], type = \"b\", col = j)\n}\nlegend(\"bottomleft\", legend = paste(\"k\", 1:10), col = 1:10, lty = 1, cex = 0.8)\n\n\n\n\n\n10k-fold cross validation\n\n\n\n\nWe can see how the variability in 10-k fold is much less than what we obtained before using half of the data (2-k)\nFor classification problems the k-fold cross-validation works exactly the same, the only difference is that instead of MSE to measure the cross validation error we use the [misclassification error rate] (#misclassificationErrorRate).\n\n\n12.2.2 Potential problems with cross-validations.\nConsider a simple classifier applied to some two-class data. We have 5000 predictors and 50 samples. With this, we find the 100 predictors having the largest correlation with the class labels. We then apply a classifier such as logistic regression using only those 100 predictors. If we now try to estimate the test set performance of this classifier using cross-validation, we will get a much lower error estimate than real. This is because when we choose the 100 best predictors, this is a form of training for the model, and must be included in the validation process. If for example we simulate data with the class labels independent of the outcome (so our test error is 50%) and then we choose the best predictors and create a model with them, when we calculate the cross-validation, our error estimate will be smaller than it should be.\nTo avoid this we will have to create our folds and remove our test fold before the process of fitting and choosing the predictors, and then apply the cross validation over the fold we set apart.\nYou can find more information in the lectureVideo\n\nExample: Model selection with Cross Validation:\nWe now try to choose among the models of different sizes using cross-validation. We must perform best subset selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy.\nFirst, we create a vector that allocates each observation to one of \\(k=10\\) folds, and we create a matrix in which we will store the results.\n\n\nCode\nk &lt;- 10\nn &lt;- nrow(Hitters)\n\nfolds &lt;- sample(rep(1:k, length = n))\ncv.errors &lt;- matrix(NA, k, 19,\n    dimnames = list(NULL, paste(1:19)))\n\n\nNow we write a for loop that performs cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors. Note that in the following code R will automatically use our predict.regsubsets() function when we call predict() because the best.fit object has class regsubsets.\n\n\nCode\nfor (j in 1:k) {\n  best.fit &lt;- regsubsets(Salary ~ .,\n       data = Hitters[folds != j, ],\n       nvmax = 19)\n  for (i in 1:19) {\n    pred &lt;- predict(best.fit, Hitters[folds == j, ], id = i)\n    cv.errors[j, i] &lt;-\n         mean((Hitters$Salary[folds == j] - pred)^2)\n   }\n}\n\n\nThis has given us a \\(10 \\times 19\\) matrix, of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\n\nCode\n(mean.cv.errors &lt;- apply(cv.errors, 2, mean))\n\n\n       1        2        3        4        5        6        7        8 \n148937.9 125976.0 133478.0 143989.3 137791.8 124703.5 130471.8 121228.7 \n       9       10       11       12       13       14       15       16 \n123781.0 119869.0 120956.0 123365.1 123806.9 122523.4 122435.2 122442.8 \n      17       18       19 \n122400.9 122566.9 122602.5 \n\n\nCode\npar(mfrow = c(1, 1))\n\nplot(sqrt(mean.cv.errors), type = \"b\", ylab=\"sqrt MSE\")\n\n\n\n\n\n\n\n\n\nWe see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.\n\n\nCode\nreg.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(reg.best, 10)\n\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 \n        CRBI       CWalks    DivisionW      PutOuts      Assists \n   0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680"
  },
  {
    "objectID": "modelAccuracy.html#bootstrap",
    "href": "modelAccuracy.html#bootstrap",
    "title": "Assessing Model Accuracy",
    "section": "12.3 The bootstrap principle",
    "text": "12.3 The bootstrap principle\nThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical method. For example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient.\nWe have an estimate \\(\\hat{\\theta}\\) for a parameter \\(\\theta\\) and want to know how accurate our estimate is. For that we need to know the \\(SE\\) of the estimate or give a confidence interval for the parameter \\(\\theta\\). The bootstrap can do this in quite general settings:\nExample \\(\\theta\\)= average height of all people in the US. It is unknown but estimated with the average height of 100 randomly selected people. We can’t compute the population mean because we can’t access the whole population. So we ’ plug in’ the sample in place of the population and compute the mean of the sample instead. The rationale for the plug-in principle is that the sample mean will be close to the population mean because the sample histogram is close to the population histogram.\nThe bootstrap uses the plug-in principle and the Monte Carlo Method to approximate quantities such as \\(SE(\\hat{ \\theta})\\) when we cannot get many samples.\nThe bootstrap pretends that the sample histogram is the population histogram and uses the Monte Carlo to simulate the quantity of interest.\nWhat we do is we draw n samples with replacement from the original sample, each sample is the same size \\(n\\) as the original dataset, and with those samples we use the Monte Carlo principle to calculate the quantity of interest. Example of 4 samples chosen at random from a dataset with three points:\n\n\nCode\nx &lt;- rnorm(3,mean=5,sd=2)\nY &lt;- 0.2+0.33*x+ rnorm(1, -1.1, .8)\n\npop&lt;- data.frame(obs=c(\"A\",\"B\",\"C\"),x,Y)\npop\n\n\n  obs        x         Y\n1   A 3.076568 0.3463163\n2   B 4.554017 0.8338746\n3   C 4.059499 0.6706834\n\n\nCode\n#bootstrap samples:\nset.seed(1)\ns&lt;- sample(c(1,2,3),3,replace=TRUE)\npop[s,]\n\n\n    obs        x         Y\n1     A 3.076568 0.3463163\n3     C 4.059499 0.6706834\n1.1   A 3.076568 0.3463163\n\n\nCode\nset.seed(2)\ns&lt;- sample(c(1,2,3),3,replace=TRUE)\npop[s,]\n\n\n  obs        x         Y\n1   A 3.076568 0.3463163\n3   C 4.059499 0.6706834\n2   B 4.554017 0.8338746\n\n\nCode\nset.seed(3)\ns&lt;- sample(c(1,2,3),3,replace=TRUE)\npop[s,]\n\n\n  obs        x         Y\n1   A 3.076568 0.3463163\n2   B 4.554017 0.8338746\n3   C 4.059499 0.6706834\n\n\nCode\nset.seed(4)\ns&lt;- sample(c(1,2,3),3,replace=TRUE)\npop[s,]\n\n\n    obs        x         Y\n3     C 4.059499 0.6706834\n3.1   C 4.059499 0.6706834\n3.2   C 4.059499 0.6706834\n\n\nDrawing a bootstrap sample by sampling with replacement from the data is called nonparametric bootstrap.\nIf we know that the data follow a normal distribution, but we don’t know the mean on the standard deviation. The obvious thing to do there is to simply estimate the unknown mean and standard deviation, and then simply sample from the normal distribution, with that mean and standard deviation. That’s called parametric bootstrap.\nThis type of sampling works if the data are independent, that is, \\(X_1\\) to \\(X_n\\) are really generated independently. But oftentimes, there’s dependence in the data. For example, the data are observed over time.\nIf the sampling distribution of \\(\\hat{\\theta}\\) is approximately normal, then the confidence interval formula is :\n\\[\n\\left[ \\hat{\\theta} - z_{\\alpha/2} SE(\\hat{\\theta}), \\hat{\\theta} + z_{\\alpha/2} SE(\\hat{\\theta}) \\right]\n\\tag{14}\\]\nThis formula represents a confidence interval for an estimator \\(\\hat{\\theta}\\), where:\n\n\\(\\hat{\\theta}\\) is the point estimate.\n\\(z_{\\alpha/2}\\) is the critical value from the standard normal distribution for a given confidence level. For example for a confidence level of 95% \\(\\alpha =(1-0.95)=0.05\\) ; \\(\\frac{\\alpha}{2}=0.025\\), \\(z_{\\frac{\\alpha}{2}}=1.96\\)\n\\(SE(\\hat{\\theta})\\) is the standard error of the estimator.\n\nIf the distribution is not normal, we can still use the bootstrap by estimating that the distribution of the estimate can be approximated by the distribution of the bootstrap-copies extracted from it, so we can draw a distribution curve and calculate the 95% confidence interval using that curve:\n\\[\n\\left[ \\hat{\\theta}^*_{\\alpha/2}, \\hat{\\theta}^*_{1-\\alpha/2} \\right]\n\\tag{15}\\]\nwhere \\(\\hat{\\theta}^*_{\\alpha/2}\\) is the \\(\\alpha/2\\) percentile of the \\(\\hat{\\theta}^*_{1},....\\hat{\\theta}^*_{B}\\)\n\nEstimating the accuracy of a linear regression Model\nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\) (intercept and slope) for a linear regression model that uses horsepower to predict mpg (miles per gallon) in the Auto data set.\nWe first create a simple function boot.fn() which takes in the Auto data set as well as the indices for the observations, and returns the intercept and slope estimates for the linear regression model.\n\n\nCode\nboot.fn&lt;- function (data, index){\n  coef(lm(mpg ~ horsepower, data= data, subset = index))\n}\n\n\nWe then apply this function to the full set of 392 observations in order to compute get the slope and intersect for the full data set:\n\n\nCode\nboot.fn(Auto,1:392)\n\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nif we use bootstrap principle we just need to create a sample with replacement of the same size of the full data set. This is one sample example:\n\n\nCode\nboot.fn(Auto, sample(392,392, replace=TRUE))\n\n\n(Intercept)  horsepower \n 39.7916909  -0.1562981 \n\n\nbut what we do in r is to use the boot() function from the boot library to compute the standard errors of 1,000 bootstrap estimates for the intercept and the slope:\n\n\nCode\nset.seed(1)\nboot(Auto, boot.fn,1000)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0553942585 0.843931305\nt2* -0.1578447 -0.0006285291 0.007367396\n\n\nThis indicates that the bootstrap estimate for \\(SE(\\beta_0)\\) is 0.84 and for \\(SE(\\beta_1)\\) is 0.0073\nStandard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.\n\n\nCode\nsummary(lm(mpg ~ horsepower, data= Auto))$coef\n\n\n              Estimate  Std. Error   t value\n(Intercept) 39.9358610 0.717498656  55.65984\nhorsepower  -0.1578447 0.006445501 -24.48914\n                                                                                                                                                                                                       Pr(&gt;|t|)\n(Intercept) 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001220362\nhorsepower  0.0000000000000000000000000000000000000000000000000000000000000000000000000000000070319890294050630351850583732442601103684864938259124755859375000000000000000000000000000000000000000000000000000\n\n\nthese standard errors are calculated using the formulas we saw in Assessing the Accuracy of the Coefficient Estimates and Confidence Intervals\nThe formula for the slope and for the intercept\nThese formulas rely on certain assumptions (out of the scope for this document, check the ISLR book for more info). The bootstrap approach does not rely on any of these assumptions, and so, it is likely giving more accurate estimate of the standard errors than the summary function\n\nIf the data is a time series, we can’t simply sample the observations with replacement because the data is not independent. There’s correlation in time series. For example we want to predict the value of a stock price using the previous days stock price. We expect the stock price for a given day to be correlated with the stock price from the previous day. This creates a problem for the bootstrap because bootstrap assumes the data are independent.\nWhat people uses for time series is the block boostrap, it divides the data up into blocks and we assume that in between blocks, the data is independent.\nAnother instance where bootstrap is not a good approach is for estimating the prediction error in cross-validation problems. In cross-validation, each of the \\(K\\) validation folds is distinct from the other K-1 folds used for training, there is no overlap, this is crucial for its success. But each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. This will cause the bootstrap to seriously underestimate the true prediction error.\n\nEstimating the Accuracy of a Statistic of Interest\nWe will be using the dataset Portfolio from LSLR package for this practice. Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of X and Y, respectively, where X and Y are random quantities. We will invest a fraction α of our money in X, and will invest the remaining 1 − α in Y . Since there is variability associated with the returns on these two assets, we wish to choose α to minimize the total risk, or variance, of our investment.In other words, we want to minimize Var(αX +(1−α)Y ). The value that minimizes the risk is given by the formula:\n\\[\n\\alpha= \\frac{\\sigma^2_Y-\\sigma_{XY}}{\\sigma^2_X+\\sigma^2_Y-2\\sigma_{XY}}\n\\]\nwhere \\(\\sigma^2_X\\)= VAR(X), \\(\\sigma^2_Y\\)= VAR(Y) and \\(\\sigma_{XY}\\) = COV(X,Y). These quantities are unknown, so we will estimate them using our data. We can write that function in r:\n\n\nCode\nalpha.fn &lt;- function(data, index) {\n  X &lt;- data$X[index]\n  Y &lt;- data$Y[index]\n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\n\nalpha.fn(Portfolio, 1:nrow(Portfolio))\n\n\n[1] 0.5758321\n\n\nbut what is the sampling variability of α? we use boostrap for this:\n\n\nCode\nset.seed(1)\nboot.out&lt;- boot::boot(Portfolio, alpha.fn, R = 1000)\nboot.out\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot::boot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 -0.001596422  0.09376093\n\n\nCode\nplot(boot.out)\n\n\n\n\n\n\n\n\n\nthe statistic came as 0.5758 as before and we now know our standard error that came as 0.093\nThe plots show you the distribution of the samples and the QQ plot."
  },
  {
    "objectID": "modelAccuracy.html#use-of-monte-carlo-simulation-and-bootstrap",
    "href": "modelAccuracy.html#use-of-monte-carlo-simulation-and-bootstrap",
    "title": "Assessing Model Accuracy",
    "section": "12.4 Use of Monte Carlo Simulation and Bootstrap",
    "text": "12.4 Use of Monte Carlo Simulation and Bootstrap\nSimulations can also be used to check theoretical or analytical results. Also, many of the theoretical results we use in statistics are based on asymptotics: they hold when the sample size goes to infinity. In practice, we never have an infinite number of samples so we may want to know how well the theory works with our actual sample size. Sometimes we can answer this question analytically, but not always. Simulations are extremely useful in these cases.\nAs an example we are going to use simulation to compare the Central Limit Theorem to the t-distribution for different sample sizes. We will use our mice data and extract control and ‘fake’ treatment samples. The fake treatment samples are just samples labeled as treatment but extracted from the control population.\nWe will build a function that automatically generates a t-statistic under the null hypothesis for a sample size of n.\n\n\nCode\ndat &lt;- read.csv(\"data/mice_pheno.csv\")\ncontrolPopulation &lt;- filter(dat,Sex == \"F\" & Diet == \"chow\") %&gt;%  \n  dplyr::select(Bodyweight) %&gt;% unlist\n\nttestgenerator &lt;- function(n) {\n  cases &lt;- sample(controlPopulation,n)\n  controls &lt;- sample(controlPopulation,n)\n  tstat &lt;- (mean(cases)-mean(controls)) / \n      sqrt( var(cases)/n + var(controls)/n ) \n  return(tstat)\n  }\nttests &lt;- replicate(1000, ttestgenerator(10))\n\n\n\n\nCode\nhist(ttests)\n\n\n\n\n\nHistogram of 1000 Monte Carlo simulated t-statistics.\n\n\n\n\nwe can use quantile-quantile plots to see how well this distribution approximates to the normal:\n\n\nCode\nqqnorm(ttests)\nabline(0,1)\n\n\n\n\n\nQuantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics to theoretical normal distribution.\n\n\n\n\nThis looks like a very good approximation. For this particular population, a sample size of 10 was large enough to use the CLT approximation. How about sample size 3?\n\n\nCode\nttests &lt;- replicate(1000, ttestgenerator(3))\nqqnorm(ttests)\nabline(0,1)\n\n\n\n\n\nQuantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical normal distribution.\n\n\n\n\nNow we see that the large quantiles, referred to by statisticians as the tails, are larger than expected (below the line on the left side of the plot and above the line on the right side of the plot). In the previous module, we explained that when the sample size is not large enough and the population values follow a normal distribution, then the t-distribution is a better approximation. Our simulation results seem to confirm this:\nIn the code below first we generate the percentiles: ps &lt;- (seq(0,999)+0.5)/1000 seq(0,999) generates a sequence of numbers from 0 to 999. Adding 0.5 to each element centers the sequence values. Dividing by 1000 scales the values to be between 0 and 1. This results in 1000 equally spaced percentiles between 0.0005 and 0.9995. qt(ps, df=2*3-2) computes the quantiles of the t-distribution for the given percentiles ps with degrees of freedom df=2*3-2 qqplot creates a QQ plot comparing the quantiles of our t-test results to the theoretical quantiles of the t-distribution.\n\n\nCode\nps &lt;- (seq(0,999)+0.5)/1000\nqqplot(qt(ps,df=2*3-2),ttests,xlim=c(-6,6),ylim=c(-6,6))\nabline(0,1)\n\n\n\n\n\nQuantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical t-distribution.\n\n\n\n\nThe QQ plot helps you visually assess how well your t-test results follow the theoretical t-distribution. The t-distribution is a much better approximation in this case, but it is still not perfect. This is due to the fact that the original data is not that well approximated by the normal distribution. We can see this plotting the qqplot for the full population data:\n\n\nCode\nqqnorm(controlPopulation)\nqqline(controlPopulation)\n\n\n\n\n\nQuantile-quantile of original data compared to theoretical quantile distribution."
  },
  {
    "objectID": "modelAccuracy.html#family-wise-error-rate-fwer",
    "href": "modelAccuracy.html#family-wise-error-rate-fwer",
    "title": "Assessing Model Accuracy",
    "section": "15.1 Family-Wise Error Rate (FWER)",
    "text": "15.1 Family-Wise Error Rate (FWER)\nThe family-wise rate is the probability of making at least one Type I error when conducting hypothesis tests.\nWhen performing the test, we know how many hypotheses we have tested \\(m\\), we also know how many \\(H_0\\) we have rejected, but we don’t know if we rejected them correctly or not, but we can approximate that. \\[\nFWER = 1 - (1 - \\alpha)^m\n\\]\nIf the null hypothesis is true for each of \\(m\\) independent hypothesis tests, then the FWER is equal to \\(1-(1-\\alpha)^m\\). We can use this expression to compute the FWER for \\(m=1,\\ldots, 500\\) and \\(\\alpha=0.05\\), \\(0.01\\), and \\(0.001\\).\n\n\nCode\nm &lt;- 1:500\nfwe1 &lt;- 1 - (1 - 0.05)^m\nfwe2 &lt;- 1 - (1 - 0.01)^m\nfwe3 &lt;- 1 - (1 - 0.001)^m\n\n\nWe plot these three vectors. The red, blue, and green lines correspond to \\(\\alpha=0.05\\), \\(0.01\\), and \\(0.001\\), respectively.\n\n\nCode\npar(mfrow = c(1, 1))\nplot(m, fwe1, type = \"l\", log = \"x\", ylim = c(0, 1), col = 2,\n    ylab = \"Family - Wise Error Rate\",\n    xlab = \"Number of Hypotheses\")\nlines(m, fwe2, col = 4)\nlines(m, fwe3, col = 3)\nabline(h = 0.05, lty = 2)\n\n\n\n\n\n\n\n\n\nEven for moderate values of \\(m\\) such as \\(50\\), the FWER exceeds \\(0.05\\) unless \\(\\alpha\\) is set to a very low value, such as \\(0.001\\). Of course, the problem with setting \\(\\alpha\\) to such a low value is that we are likely to make a number of Type II errors: in other words, our power is very low."
  },
  {
    "objectID": "modelAccuracy.html#bonferroni-method-for-controlling-fwer",
    "href": "modelAccuracy.html#bonferroni-method-for-controlling-fwer",
    "title": "Assessing Model Accuracy",
    "section": "15.2 Bonferroni Method for Controlling FWER",
    "text": "15.2 Bonferroni Method for Controlling FWER\nThe Bonferroni method adjusts the significance level (𝛼) for each individual hypothesis test to ensure that the overall probability of making at least one Type I error (false positive) remains controlled. Given a number of hypotheses being tested \\(m\\), we adjust the significance level for each test \\(\\alpha_{adj}\\) by dividing the desired significance level \\(\\alpha\\) byt the number of hypotheses \\(m\\)\n\\[\n\\alpha_{adj} = \\frac{\\alpha}{m}\n\\] Now we compare the p-values of each test against the adjusted significance level and reject those null hypothesis for any test with a \\(p\\)-value less than or equal to \\(\\alpha_{adj}\\)\nWe now consider the Fund dataset. It contains data for 50 months for a list of managers and their excess return, this is, the percentage points difference between the funds managed by these people compared with the stock market in general for the same period. In the table below we show the stats for the first 5 managers in our list.\nIf we control the Type I error at level α = 0.05 for each fund manager separately, then we will conclude that the first and third managers have significantly non-zero excess returns; in other words, we will reject H01 : μ1 = 0 and H03 : μ3 = 0. However, as discussed in previous sections, this procedure does not account for the fact that we have tested multiple hypotheses, and therefore it will lead to a FWER greater than 0.05. If we instead wish to control the FWER at level 0.05, then, using a Bonferroni correction, we must control the Type I error for each individual manager at level α/m = 0.05/5 = 0.01. Consequently, we will reject the null hypothesis only for the first manager, since the p-values for all other managers exceed 0.01.\nWe are going to do the calculation manually here, and later in the exercises we will use an R function that calculate this for us.\n\n\nCode\nfund &lt;- ISLR2::Fund\nmanagers &lt;- numeric(0)\npvals &lt;- numeric(0)\ntstat &lt;- numeric(0)\nmeans &lt;- numeric(0)\nsds &lt;- numeric(0)\nfor (i in 1:5){\n  testResult &lt;- t.test(fund[,i], mu = 0) \nmanagers &lt;- c(managers, i)\npvals&lt;- c(pvals, round(testResult$p.value,3))\ntstat&lt;- c(tstat, round(testResult$statistic,2))  \nmeans&lt;- c(means, round(mean(fund[,i]),1))\nsds &lt;- c(sds, round(sd(fund[,i]),1))\n}\ndf&lt;- data.frame(manager = managers, mean = means, StandDev = sds,  tStatistic = tstat,pValues = pvals)\ndf\n\n\n  manager mean StandDev tStatistic pValues\n1       1  3.0      7.4       2.86   0.006\n2       2 -0.1      6.9      -0.10   0.918\n3       3  2.8      7.5       2.62   0.012\n4       4  0.5      6.7       0.53   0.601\n5       5  0.3      6.8       0.31   0.756\n\n\nThe Bonferroni correction is by far the best-known and most commonly used multiplicity correction in all of statistics. Its ubiquity is due in large part to the fact that it is very easy to understand and simple to implement, and also from the fact that it successfully controls Type I error regardless of whether the m hypothesis tests are independent. However, as we will see, it is typically neither the most powerful nor the best approach for multiple testing correction. In particular, the Bonferroni correction can be quite conservative, in the sense that the true FWER is often quite a bit lower than the nominal (or target) FWER."
  },
  {
    "objectID": "modelAccuracy.html#holms-method-for-controlling-fwer",
    "href": "modelAccuracy.html#holms-method-for-controlling-fwer",
    "title": "Assessing Model Accuracy",
    "section": "15.3 Holm’s Method for controlling FWER",
    "text": "15.3 Holm’s Method for controlling FWER\nFor this method we start like with the Bonferroni, we just do our test and calculate our \\(p\\)-values as per normal, and then we sort those \\(p\\)-values from smallest to greatest, and one by one we calculate L that validates this formula\n\\[\nL = \\min \\left\\{ j : p_{j} &gt; \\frac{\\alpha}{m+1-j} \\right\\}\n\\] where \\(j\\) is the index of the hypotheses tested, so \\(j= 1,2,\\dots,m\\)\nOnce we have L, we reject all null hypothesis that are below L.\n\n\nCode\ndf &lt;- df %&gt;% arrange(df$pValues)\nalpha &lt;- 0.05\nm = 5\nj=1\n\nfor (i in df$pValues){\n  df$reject = df$pValues &lt; (alpha/(m+1-j))\n  j=j+1\n  \n}\ndf\n\n\n  manager mean StandDev tStatistic pValues reject\n1       1  3.0      7.4       2.86   0.006   TRUE\n2       3  2.8      7.5       2.62   0.012   TRUE\n3       4  0.5      6.7       0.53   0.601  FALSE\n4       5  0.3      6.8       0.31   0.756  FALSE\n5       2 -0.1      6.9      -0.10   0.918  FALSE\n\n\nWe see that in this setting, we reject the null hypothesis for managers 1 and 3 using Holm’s method.\nHolm’s method will reject at least as many hypothesis than the Bonferroni method, but usually rejects more.\n\nPractice: Type I errors, FWER, Bonferrony, Holm’s\nWe are going to simulate some random data to test 100 hypotheses to check if their mean is equal to 0. Each dataset contains 10 observations.\nWe create the data and then modify the first 50 columns so its mean is not 0 but 0.5, so we would expect to reject the first 50 \\(H_0\\) hypotheses and do not reject the last 50 \\(H_0\\) hypotheses.\n\n\nCode\nset.seed(6)\nX&lt;- matrix(rnorm(10*100), 10,100)\nX[,1:50]&lt;- X[,1:50] +0.5\n\np.values = rep(0,100)\nfor (i in 1:100)\n  p.values[i] &lt;- t.test(X[,i],mu=0)$p.value\ndecision &lt;- rep(\"Do not reject H0\", 100)\ndecision[p.values &lt;= .05]&lt;- 'reject H0'\ntable(decision,\n  c(rep(\"H0 is False\", 50), rep(\"H0 is true\",50)))\n\n\n                  \ndecision           H0 is False H0 is true\n  Do not reject H0          40         47\n  reject H0                 10          3\n\n\nThe situation in which we rejected the null hypothesis when it is in fact true are our Type I errors, in our case 3.\nWith only 10 observations, we could only reject 10 out of the 50 false null hypotheses, specially because our mean was only 0.5 difference. We can see how if we change the mean of the first 50 hypotheses to 1 instead of 0.5, we have a better success on the number of Type II errors:\n\n\nCode\nX&lt;- matrix(rnorm(10*100), 10,100)\nX[,1:50]&lt;- X[,1:50] +1\n\np.values = rep(0,100)\nfor (i in 1:100)\n  p.values[i] &lt;- t.test(X[,i],mu=0)$p.value\ndecision &lt;- rep(\"Do not reject H0\", 100)\ndecision[p.values &lt;= .05]&lt;- 'reject H0'\ntable(decision,\n  c(rep(\"H0 is False\", 50), rep(\"H0 is true\",50)))\n\n\n                  \ndecision           H0 is False H0 is true\n  Do not reject H0           9         49\n  reject H0                 41          1\n\n\nAs we have done in the lecture above, we now conduct a one-sample \\(t\\)-test for each of the first five managers in the Fund dataset, in order to test the null hypothesis that the \\(j\\)th fund manager’s mean return equals zero, \\(H_{0j}: \\mu_j=0\\).\n\n\nCode\nfund.mini &lt;- Fund[, 1:5]\nt.test(fund.mini[, 1], mu = 0)\n\n\n\n    One Sample t-test\n\ndata:  fund.mini[, 1]\nt = 2.8604, df = 49, p-value = 0.006202\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.8923397 5.1076603\nsample estimates:\nmean of x \n        3 \n\n\nCode\nfund.pvalue &lt;- rep(0, 5)\nfor (i in 1:5)\n  fund.pvalue[i] &lt;- t.test(fund.mini[, i], mu = 0)$p.value\nfund.pvalue\n\n\n[1] 0.006202355 0.918271152 0.011600983 0.600539601 0.755781508\n\n\nThe \\(p\\)-values are low for Managers One and Three, and high for the other three managers. However, we cannot simply reject \\(H_{01}\\) and \\(H_{03}\\), since this would fail to account for the multiple testing that we have performed. Instead, we will conduct Bonferroni’s method and Holm’s method to control the FWER.\nBonferroni\nTo do this, we use the p.adjust() function. Given the \\(p\\)-values, the function outputs adjusted \\(p\\)-values, which can be thought of as a new set of \\(p\\)-values that have been corrected for multiple testing. If the adjusted \\(p\\)-value for a given hypothesis is less than or equal to \\(\\alpha\\), then that hypothesis can be rejected while maintaining a FWER of no more than \\(\\alpha\\). In other words, the adjusted \\(p\\)-values resulting from the p.adjust() function can simply be compared to the desired FWER in order to determine whether or not to reject each hypothesis.\nFor example, in the case of Bonferroni’s method, the raw \\(p\\)-values are multiplied by the total number of hypotheses, \\(m\\), in order to obtain the adjusted \\(p\\)-values. (However, adjusted \\(p\\)-values are not allowed to exceed \\(1\\).)\n\n\nCode\np.adjust(fund.pvalue, method = \"bonferroni\")\n\n\n[1] 0.03101178 1.00000000 0.05800491 1.00000000 1.00000000\n\n\nCode\npmin(fund.pvalue * 5, 1)\n\n\n[1] 0.03101178 1.00000000 0.05800491 1.00000000 1.00000000\n\n\nTherefore, using Bonferroni’s method, we are able to reject the null hypothesis only for Manager One while controlling the FWER at \\(0.05\\).\nHolm’s method\nBy contrast, using Holm’s method, the adjusted \\(p\\)-values indicate that we can reject the null hypotheses for Managers One and Three at a FWER of \\(0.05\\).\n\n\nCode\np.adjust(fund.pvalue, method = \"holm\")\n\n\n[1] 0.03101178 1.00000000 0.04640393 1.00000000 1.00000000\n\n\nAs discussed previously, Manager One seems to perform particularly well, whereas Manager Two has poor performance.\n\n\nCode\napply(fund.mini, 2, mean)\n\n\nManager1 Manager2 Manager3 Manager4 Manager5 \n     3.0     -0.1      2.8      0.5      0.3 \n\n\nTukey’s Honest Significant Difference (HSD) test\nIs there evidence of a meaningful difference in performance between these two managers? Performing a paired \\(t\\)-test using the t.test() function results in a \\(p\\)-value of \\(0.038\\), suggesting a statistically significant difference.\n\n\nCode\nt.test(fund.mini[, 1], fund.mini[, 2], paired = T)\n\n\n\n    Paired t-test\n\ndata:  fund.mini[, 1] and fund.mini[, 2]\nt = 2.128, df = 49, p-value = 0.03839\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.1725378 6.0274622\nsample estimates:\nmean difference \n            3.1 \n\n\nHowever, we decided to perform this test only after examining the data and noting that Managers One and Two had the highest and lowest mean performances.\nIn a sense, this means that we have implicitly performed \\({5 \\choose 2} = 5(5-1)/2=10\\) hypothesis tests, rather than just one. Hence, we use the TukeyHSD() function to apply Tukey’s method in order to adjust for multiple testing. This function takes as input the output of an ANOVA regression model, which is essentially just a linear regression in which all of the predictors are qualitative. In this case, the response consists of the monthly excess returns achieved by each manager, and the predictor indicates the manager to which each return corresponds.\n\n\nCode\nreturns &lt;- as.vector(as.matrix(fund.mini))\nmanager &lt;- rep(c(\"1\", \"2\", \"3\", \"4\", \"5\"), rep(50, 5))\na1 &lt;- aov(returns ~ manager)\nTukeyHSD(x = a1)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = returns ~ manager)\n\n$manager\n    diff        lwr       upr     p adj\n2-1 -3.1 -6.9865435 0.7865435 0.1861585\n3-1 -0.2 -4.0865435 3.6865435 0.9999095\n4-1 -2.5 -6.3865435 1.3865435 0.3948292\n5-1 -2.7 -6.5865435 1.1865435 0.3151702\n3-2  2.9 -0.9865435 6.7865435 0.2452611\n4-2  0.6 -3.2865435 4.4865435 0.9932010\n5-2  0.4 -3.4865435 4.2865435 0.9985924\n4-3 -2.3 -6.1865435 1.5865435 0.4819994\n5-3 -2.5 -6.3865435 1.3865435 0.3948292\n5-4 -0.2 -4.0865435 3.6865435 0.9999095\n\n\nThe TukeyHSD() function provides confidence intervals for the difference between each pair of managers (lwr and upr), as well as a \\(p\\)-value. All of these quantities have been adjusted for multiple testing. Notice that the \\(p\\)-value for the difference between Managers One and Two has increased from \\(0.038\\) to \\(0.186\\), so there is no longer clear evidence of a difference between the managers’ performances. We can plot the confidence intervals for the pairwise comparisons using the plot() function.\n\n\nCode\nplot(TukeyHSD(x = a1))"
  },
  {
    "objectID": "modelAccuracy.html#benjamini-hochberg-procedure-to-control",
    "href": "modelAccuracy.html#benjamini-hochberg-procedure-to-control",
    "title": "Assessing Model Accuracy",
    "section": "15.4 Benjamini-Hochberg Procedure to control",
    "text": "15.4 Benjamini-Hochberg Procedure to control\nThe False Discovery Rate When \\(m\\) is large, then trying to prevent any false positives (as in FWER control) is simply too stringent. Instead, we might try to make sure that the ratio of false positives to total positives (is sufficiently low, so that most of the rejected null hypotheses are not false positives. The ratio is known as the false discovery proportion (FDP).\nIt might be tempting to ask the data analyst to control the FDP: to make sure that no more than, say, 20% of the rejected null hypotheses are false positives. However, in practice, controlling the FDP is an impossible task for the data analyst, since she has no way to be certain, on any particular dataset, which hypotheses are true and which are false, Therefore, we instead control the false discovery rate (FDR) defined as FDR = E(FDP) = E(False Rejections/Total Rejections) When we control the FDR at (say) level q = 20%, we are rejecting as many null hypotheses as possible while guaranteeing that no more than 20% of those rejected null hypotheses are false positives, on average. The expectation is taken over the population from which the data are generated. For instance, suppose we control the FDR for \\(m\\) null hypotheses at \\(q = 0.2\\). This means that if we repeat this experiment a huge number of times, and each time control the FDR at \\(q = 0.2\\), then we should expect that, on average, 20% of the rejected null hypotheses will be false positives. On a given dataset, the fraction of false positives among the rejected hypotheses may be greater than or less than 20%.\nIt is worth noting that unlike \\(p\\)-values, for which a threshold of 0.05 is typically viewed as the minimum standard of evidence for a “positive” result, and a threshold of 0.01 or even 0.001 is viewed as much more compelling, there is no standard accepted threshold for FDR control. Instead, the choice of FDR threshold is typically context-dependent, or even dataset dependent.\nThis method is useful when we have a large number of hypothesis to test and we are ok with rejecting a percentage of true null Hypotheses.\nWe need some way to connect the \\(p\\)-values, \\(p1,\\dots,pm\\), from the \\(m\\) null hypotheses to the desired FDR value, \\(q\\). It turns out that a very simple procedure can be used to control the FDR\n\nFirst we specify \\(q\\) that is the level at which we wish to control FDR\nCompute the \\(p\\)-values for each of our hypotheses - Order the \\(p\\)-values from smallest to greatest - Define L where \\[\nL= max \\{ j:p_{(j)} &lt; qj/m\\}\n\\]\nReject all null hypotheses \\(H_{0j}\\) for which \\(p_j \\leq p_{(L)}\\)\n\nWe are going to use now the full dataset of Fund with 2000 managers and apply the Bonferroni method and the Benjamini-Hochberg method. First, we are going to do the calculation manually and later we will use a function.\n\n\nCode\nfund &lt;- ISLR2::Fund\nmanagers &lt;- numeric(0)\npvals &lt;- numeric(0)\nbonfRej &lt;- numeric(0)\nalpha = 0.05\nm= length(fund)\n\n#bonferroni rejections\nfor (i in 1:m){\n  testResult &lt;- t.test(fund[,i], mu = 0) \nmanagers &lt;- c(managers, i)\npvals&lt;- c(pvals, round(testResult$p.value,3))\nbonf &lt;- testResult$p.value &lt;= (alpha/m)\nbonfRej &lt;- c(bonfRej, bonf)\n}\ndf&lt;- data.frame(manager = managers, pValues = pvals, bonfRej = bonfRej)\n\n\n#Benjamini-Hochberg rejections\n\nps &lt;- sort(df$pValues)\nq &lt;- 0.1\nwh.ps &lt;- which(ps &lt; q * (1:m) / m)\nif (length(wh.ps) &gt;0) {\n  wh &lt;- 1:max(wh.ps)\n } else {\n  wh &lt;- numeric(0)\n }\n\n\nplot(ps, log = \"xy\", ylim = c(4e-6, 1), ylab = \"P-Value\",\n    xlab = \"Index\", main = \"\")\npoints(wh, ps[wh], col = 4)\nabline(a = 0, b = (q / m), col = 2, untf = TRUE)\nabline(h = 0.1 / 2000, col = 3)\n\n\n\n\n\n\n\n\n\nCode\nsum(df$bonfRej)\n\n\n[1] 0\n\n\nCode\nmax(wh)\n\n\n[1] 147\n\n\nThe graph shows in blue the rejected hypotheses using BH\nDue to the large number of \\(m\\), the Bonferroni method does not reject any hypothesis, while Benjamin-Hochberg rejects 147, but we have to remember that we expect a percentage of them will be false rejections at our level of \\(q\\).\nNow we will calculate the Benjamini-Hochberg using p.adjust() function in R. We will show the first 10\n\n\nCode\nq.values.BH &lt;- p.adjust(df$pValues, method = \"BH\")\nq.values.BH[1:10]\n\n\n [1] 0.08633094 0.99189627 0.12244898 0.92319508 0.95635674 0.06521739\n [7] 0.07575758 0.06521739 0.06521739 0.06521739\n\n\nThe q-values output by the Benjamini-Hochberg procedure can be interpreted as the smallest FDR threshold at which we would reject a particular null hypothesis. For instance, a \\(q\\)-value of \\(0.1\\) indicates that we can reject the corresponding null hypothesis at an FDR of \\(10\\%\\) or greater, but that we cannot reject the null hypothesis at an FDR below \\(10\\%\\).\nIf we control the FDR at \\(10\\%\\), then for how many of the fund managers can we reject \\(H_{0j}: \\mu_j=0\\)?\n\n\nCode\nsum(q.values.BH &lt;= .1)\n\n\n[1] 147\n\n\nWe find that 147 of the 2,000 fund managers have a \\(q\\)-value below 0.1; therefore, we are able to conclude that 147 of the fund managers beat the market at an FDR of \\(10\\%\\). Only about 15 (\\(10\\%\\) of 147) of these fund managers are likely to be false discoveries."
  },
  {
    "objectID": "modelAccuracy.html#resampling-approaches",
    "href": "modelAccuracy.html#resampling-approaches",
    "title": "Assessing Model Accuracy",
    "section": "15.5 Resampling approaches",
    "text": "15.5 Resampling approaches\nSo far, we have assumed that we want to test some null hypothesis with some statistic \\(T\\) and that we know, or can assume, the distribution of \\(T\\) under \\(H_0\\), this allows us to compute the \\(p\\)-value, but what if this theoretical null distribution is unknown?\nWe are only going to see this approach for a two-sample \\(t\\)-Test, and this method cannot be automatically used for other types of tests.\nWe have two samples \\(X\\) and \\(Y\\) and the \\(H_0\\) is that the means of thos two samples is the same. As we have seen in the central limit theorem if \\(N\\) is big enough, we know that \\(T\\) approximately follows a normal distribution under \\(H_0\\), but if \\(N\\) is small, then we don’t know the theoretical null distribution of \\(T\\), so we can can use re sampling and let the computer find out the right distribution.\n\nCompute the two-sample \\(t\\)-statistic \\(T\\) on the original data \\(X\\) and \\(Y\\).\nchoose a large number (like 1000 or higher) \\(B\\)\nRandomly shuffle the \\(X\\) and \\(Y\\) observations and select two samples \\(x^*\\) and \\(y^*\\)\nCompute a two sample \\(t\\)-statistic on the shuffled data \\(T^*\\)\nrepeat the two last steps \\(B\\) times.\nthe \\(p\\)-value is given by the fraction of the times that the \\(t\\)-statistic on the shuffled data exceeded the absolute value, the \\(t\\)-statistic on the real data.\n\n\\[\np=\\frac{ \\sum^B_{b=1}1_{(|T^{*b}| \\geq |T|)}}{B}\n\\] - now we can compare this calculated \\(p\\)-value with that one calculated at the beginning on the original two sample test.\nThe reason why we work with absolute values for the \\(T\\) is because in this particular case it is a two-sided \\(t\\)-test so we are interested in whether \\(X\\) is larger than \\(Y\\) or vice-versa but if it’s a one side \\(t\\)-test then we can get rid of the absolute values.\n\nPractice\nHere, we implement the re-sampling approach to hypothesis testing using the Khan dataset. We merge together train and test datasets because we are not using that here and we do the same for the response \\(y\\) that gives us the type of cancer.\nFirst, we merge the training and testing data, which results in observations (rows) on \\(83\\) patients for \\(2{,}308\\) columns. Each column represent a gene. So we are going to perform \\(2{,}308\\) hypothesis test here.\n\n\nCode\nattach(Khan)\nx &lt;- rbind(xtrain, xtest)\ny &lt;- c(as.numeric(ytrain), as.numeric(ytest))\ndim(x)\n\n\n[1]   83 2308\n\n\nCode\ntable(y)\n\n\ny\n 1  2  3  4 \n11 29 18 25 \n\n\nThere are four classes of cancer.To simplify this example we are only going to compare two of the 4 classes only. For each gene, we compare the mean expression in the second class (rhabdomyosarcoma) to the mean expression in the fourth class (Burkitt’s lymphoma). We see in the table above that we have 29 observations for class 2 and 25 for class 4.\nPerforming a standard two-sample \\(t\\)-test on the \\(11\\)th gene produces a test-statistic of \\(-2.09\\) and an associated \\(p\\)-value of \\(0.0412\\), suggesting modest evidence of a difference in mean expression levels between the two cancer types.\n\n\nCode\nx &lt;- as.matrix(x)\nx1 &lt;- x[which(y == 2), ]\nx2 &lt;- x[which(y == 4), ]\nn1 &lt;- nrow(x1)\nn2 &lt;- nrow(x2)\nt.out &lt;- t.test(x1[, 11], x2[, 11], var.equal = TRUE)\nTT &lt;- t.out$statistic\nTT\n\n\n        t \n-2.093633 \n\n\nCode\nt.out$p.value\n\n\n[1] 0.04118644\n\n\nHowever, this \\(p\\)-value relies on the assumption that under the null hypothesis of no difference between the two groups, the test statistic follows a \\(t\\)-distribution with \\(29+25-2=52\\) degrees of freedom. Instead of using this theoretical null distribution, we can randomly split the 54 patients into two groups of 29 and 25, and compute a new test statistic. Under the null hypothesis of no difference between the groups, this new test statistic should have the same distribution as our original one. Repeating this process 10,000 times allows us to approximate the null distribution of the test statistic. We compute the fraction of the time that our observed test statistic exceeds the test statistics obtained via re-sampling.\n\n\nCode\nset.seed(1)\nB &lt;- 10000\nTbs &lt;- rep(NA, B) #initializing an empty vector for the T statistic\nfor (b in 1:B) {\n   dat &lt;- sample(c(x1[, 11], x2[, 11])) #reshufle the 11th gene for the two types of cancer\n   Tbs[b] &lt;- t.test(dat[1:n1], \n                    dat[(n1 + 1):(n1 + n2)],\n        var.equal = TRUE\n      )$statistic\n}\nmean((abs(Tbs) &gt;= abs(TT))) #calculate how many T's are higher than the original\n\n\n[1] 0.0416\n\n\nThis fraction, \\(0.0416\\), is our re-sampling-based \\(p\\)-value. It is almost identical to the \\(p\\)-value of \\(0.0412\\) obtained using the theoretical null distribution. This means that the assumptions we made in the original test hold for this data\nWe can plot a histogram of the re-sampling-based test statistics.\n\n\nCode\nhist(Tbs, breaks = 100, xlim = c(-4.2, 4.2), main = \"\",\n    xlab = \"Null Distribution of Test Statistic\", col = 7)\n\nlines(seq(-4.2, 4.2, len = 1000),\n      dt(seq(-4.2, 4.2, len = 1000),\n      df = (n1 + n2 - 2)\n      ) * 1000, col = 2, lwd = 3)\nabline(v = TT, col = 4, lwd = 2)\ntext(TT + 0.5, 350, paste(\"T = \", round(TT, 4), sep = \"\"),\n    col = 4)\n\n\n\n\n\n\n\n\n\nIn yellow we have the resampling null distribution and in red the theoretical \\(t\\) distribution for mean 0, sd 1 and 52 degrees of freedom.\nThe re-sampling-based null distribution is almost identical to the theoretical null distribution, which is displayed in red, so it is to no surprise that the theoretical \\(p\\)-value from the test is almost the same as the resampling \\(p\\)-value. This just confirms that it is reasonable to use the \\(t\\)-distribution here.\nFinally, we implement the plug-in re-sampling FDR approach outlined. Depending on the speed of your computer, calculating the FDR for all 2,308 genes in the Khan dataset may take a while. Hence, we will illustrate the approach on a random subset of 100 genes. For each gene, we first compute the observed test statistic, and then produce \\(10{,}000\\) re-sampled test statistics. This may take a few minutes to run. If you are in a rush, then you could set B equal to a smaller value (e.g. B = 500).\n\n\nCode\nm &lt;- 100\nB &lt;- 1000\nset.seed(1)\nindex &lt;- sample(ncol(x1), m)\nTs &lt;- rep(NA, m)\nTs.star &lt;- matrix(NA, ncol = m, nrow = B)\nfor (j in 1:m) {\n  k &lt;- index[j]\n  Ts[j] &lt;- t.test(x1[, k], x2[, k],\n        var.equal = TRUE\n      )$statistic\n  for (b in 1:B) {\n    dat &lt;- sample(c(x1[, k], x2[, k]))\n    Ts.star[b, j] &lt;- t.test(dat[1:n1],\n                           dat[(n1 + 1):(n1 + n2)], \n                           var.equal = TRUE\n       )$statistic\n  }\n}\n\n\nNext, we compute the number of rejected null hypotheses \\(R\\), the estimated number of false positives \\(\\widehat{V}\\), and the estimated FDR, for a range of threshold values \\(c\\). The threshold values are chosen using the absolute values of the test statistics from the \\(100\\) genes.\n\n\nCode\ncs &lt;- sort(abs(Ts))\nFDRs &lt;- Rs &lt;- Vs &lt;- rep(NA, m)\nfor (j in 1:m) {\n  R &lt;- sum(abs(Ts) &gt;= cs[j])\n  V &lt;- sum(abs(Ts.star) &gt;= cs[j]) / B\n  Rs[j] &lt;- R\n  Vs[j] &lt;- V\n  FDRs[j] &lt;- V / R\n}\n\n\nNow, for any given FDR, we can find the genes that will be rejected. For example, with the FDR controlled at 0.1, we reject 15 of the 100 null hypotheses. On average, we would expect about one or two of these genes (i.e. \\(10\\%\\) of 15) to be false discoveries.\nAt an FDR of \\(0.2\\), we can reject the null hypothesis for \\(28\\) genes, of which we expect around six to be false discoveries. The variable index is needed here since we restricted our analysis to just \\(100\\) randomly-selected genes.\n\n\nCode\nmax(Rs[FDRs &lt;= .1])\n\n\n[1] 15\n\n\nCode\nsort(index[abs(Ts) &gt;= min(cs[FDRs &lt; .1])])\n\n\n [1]   29  465  501  554  573  729  733 1301 1317 1640 1646 1706 1799 1942 2159\n\n\nCode\nmax(Rs[FDRs &lt;= .2])\n\n\n[1] 28\n\n\nCode\nsort(index[abs(Ts) &gt;= min(cs[FDRs &lt; .2])])\n\n\n [1]   29   40  287  361  369  465  501  554  573  679  729  733  990 1069 1073\n[16] 1301 1317 1414 1639 1640 1646 1706 1799 1826 1942 1974 2087 2159\n\n\n\n\nCode\nplot(Rs, FDRs, xlab = \"Number of Rejections\", type = \"l\",\n    ylab = \"False Discovery Rate\", col = 4, lwd = 3)\n\n\n\n\n\n\n\n\n\nAs noted in the chapter, much more efficient implementations of the re-sampling approach to FDR calculation are available, using e.g. the samr package in R."
  },
  {
    "objectID": "polynomialReg.html",
    "href": "polynomialReg.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Polynomial regression is an extension of linear regression that models the relationship between the independent variable𝑥and the dependent variable𝑦as an𝑛th-degree polynomial. In linear regression, the relationship between \\(x\\) and \\(y\\) is modeled as a straight line: \\(y=\\beta_0+\\beta_1X\\) In polynomial regression, the relationship is modeled as a polynomial. For example for a single variable \\(x\\): \\(y=\\beta_0+\\beta_1x+\\beta_2x^2+\\dots+\\beta_nx^n\\). It can capture more complex patterns in the data that linear regression might miss. By adjusting the degree of the polynomial, you can model different types of curves. To choose the degree of polynomial to use, we can use cross-validation for example.\nCode\n# Sample data\nset.seed(123)\nx &lt;- runif(100, 0, 10)\ny &lt;- 1 + 2 * x + 3 * x^2 + rnorm(100, 0, 10)  # True relationship is quadratic\n\n# Fit a polynomial regression model\nmodel &lt;- lm(y ~ poly(x, 2, raw = TRUE))\n\n# Summary of the model\nsummary(model)\n\n\n\nCall:\nlm(formula = y ~ poly(x, 2, raw = TRUE))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.9557  -6.2448  -0.4255   5.7489  22.2627 \n\nCoefficients:\n                        Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)               2.5565     3.0731   0.832               0.408    \npoly(x, 2, raw = TRUE)1   0.9521     1.4170   0.672               0.503    \npoly(x, 2, raw = TRUE)2   3.0952     0.1366  22.665 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.718 on 97 degrees of freedom\nMultiple R-squared:  0.9897,    Adjusted R-squared:  0.9895 \nF-statistic:  4647 on 2 and 97 DF,  p-value: &lt; 0.00000000000000022\n\n\nCode\n# Plot the data and the polynomial fit\nggplot(data.frame(x, y), aes(x, y)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ poly(x, 2, raw = TRUE), se = TRUE) +\n  labs(title = \"Polynomial Regression Fit\", x = \"x\", y = \"y\")\npoly(x, 2, raw = TRUE): Fits a polynomial of degree 2 (quadratic) to the data. High-degree polynomials can overfit the data, capturing noise rather than the underlying relationship. It’s important to choose the polynomial degree carefully.\nIn this case of polynomial regression we are not interested in the coefficients as we were in linear models, we are more interested in the fitted function values for a value \\(x_0\\)\n\\[\n\\hat{f}(x_0)= \\hat\\beta_0+\\hat\\beta_1x_0+\\hat\\beta_2x_0^2+\\hat\\beta_3x_0^3\n\\]"
  },
  {
    "objectID": "polynomialReg.html#linear-splines.",
    "href": "polynomialReg.html#linear-splines.",
    "title": "Polynomial Regression",
    "section": "5.1 Linear Splines.",
    "text": "5.1 Linear Splines.\nA linear spline with knots at k=1…k=n is a piecewise linear polynomial continuous at each knot. ### Cubic Splines A cubic spline is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot. ### Natural Splines A natural cubic spline is a cubic spline with two extra constraints at each end (boundary) of the data, and the constraints make the function extrapolate linearly beyond the boundary knots, this is reducing the standard errors at the ends. For fitting cubic splines in R we use the function bs() you give it the knots as arguments. For natural splines we use the function ns() both in package splines\nknot placement: one strategy is to decide on the total number of knots K, and then place them uniformly across different quantiles of the variable x. That results in a certain number of parameters. So a cubic spline with k knots gets k+4 parameters or degrees of freedom. If you use a natural spline with k knots, it only has k degrees of freedom because you get back two degrees of freedom for the two constraints on each of the boundaries."
  },
  {
    "objectID": "polynomialReg.html#smoothing-spline",
    "href": "polynomialReg.html#smoothing-spline",
    "title": "Polynomial Regression",
    "section": "5.2 Smoothing spline",
    "text": "5.2 Smoothing spline\nThere is a special type of spline called the smoothing spline with a knot at every unique value of x. We are not going to get into mathematical details here, you can check in the book, but in R we can use the function smooth.spline() and it will fit a smoothing spline.\nIn spline regression, the data is divided into segments, and a polynomial is fitted to each segment. The polynomials are connected at points called knots, ensuring smooth transitions.\nWhile LOESS also uses polynomials, it doesn’t have explicit segments or knots. Instead, the fitting is done continuously across the entire range of the data, with the local fits blending smoothly due to the weighted scheme.\n\nSplines\nThe library to work with splines is splines. Regression splines can be fit by constructing an appropriate matrix of basis functions. The bs() function generates the entire matrix of basis functions for splines with the specified set of knots. By default, cubic splines are produced.\nFitting wage to age using a regression spline is simple:\n\n\nCode\nlibrary(splines)\nfit = lm(wage ~ bs(age,knots=c(25,40,60)),data=Wage)\npred &lt;- predict(fit, newdata = list(age = age.grid), se = T)\nplot(Wage$age,Wage$wage,col='darkgrey', ylim =c(0,300))\nlines(age.grid, predict(fit, list(age=age.grid)), col = 'blue',lwd=2)\nabline(v= c(25,40,60), lty=2, col='red')\nlines(age.grid, pred$fit + 2 * pred$se, lty = \"dashed\")\nlines(age.grid, pred$fit - 2 * pred$se, lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nThe advantage is that they don’t vary so much at the boundaries.\nHere we have pre specified knots at ages \\(25\\), \\(40\\), and \\(60\\). This produces a spline with six basis functions. (Recall that a cubic spline with three knots has seven degrees of freedom; these degrees of freedom are used up by an intercept, plus six basis functions).\nWe could also use the df option to produce a spline with knots at uniform quantiles of the data.\n\n\nCode\ndim(bs(Wage$age, knots = c(25, 40, 60)))\n\n\n[1] 3000    6\n\n\nCode\ndim(bs(Wage$age, df = 6))\n\n\n[1] 3000    6\n\n\nCode\nattr(bs(Wage$age, df = 6), \"knots\")\n\n\n[1] 33.75 42.00 51.00\n\n\nIn this case R chooses knots at ages \\(33.8, 42.0\\), and \\(51.0\\), which correspond to the 25th, 50th, and 75th percentiles of age. The function bs() also has a degree argument, so we can fit splines of any degree, rather than the default degree of 3 (which yields a cubic spline).\nIn order to instead fit a natural spline, we use the ns() function. Here we fit a natural spline with four degrees of freedom.\n\n\nCode\nfit2 &lt;- lm(wage ~ ns(age, df = 4), data = Wage)\npred2 &lt;- predict(fit2, newdata = list(age = age.grid),se = T)\nplot(Wage$age, Wage$wage, col = \"gray\")\nlines(age.grid, pred2$fit, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nAs with the bs() function, we could instead specify the knots directly using the knots option.\nSmoothing splines In order to fit a smoothing spline, we use the smooth.spline() function. Here we fit a smoothing spline with 16 degrees of freedom, and as we can see, it overfits a bit.\n\n\nCode\nplot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = \"darkgrey\")\ntitle(\"Smoothing Spline\")\nfit &lt;- smooth.spline(Wage$age, Wage$wage, df = 16)\nlines(fit, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nWhat we can do instead is use cross validation leave one out to decide the smoothing parameter for us automatically:\n\n\nCode\nplot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = \"darkgrey\")\n title(\"Smoothing Spline\")\n\n fit2 &lt;- smooth.spline(Wage$age, Wage$wage, cv = TRUE)\n\nlines(fit, col = \"red\", lwd = 2)\nlines(fit2, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"16 DF\", \"6.8 DF\"),\n    col = c(\"red\", \"blue\"), lty = 1, lwd = 2, cex = .8)\n\n\n\n\n\n\n\n\n\nCode\nfit2\n\n\nCall:\nsmooth.spline(x = Wage$age, y = Wage$wage, cv = TRUE)\n\nSmoothing Parameter  spar= 0.6988943  lambda= 0.02792303 (12 iterations)\nEquivalent Degrees of Freedom (Df): 6.794596\nPenalized Criterion (RSS): 75215.9\nPRESS(l.o.o. CV): 1593.383\n\n\nNotice that in the first call to smooth.spline(), we specified df = 16. The function then determines which value of \\(\\lambda\\) leads to \\(16\\) degrees of freedom. In the second call to smooth.spline(), we select the smoothness level by cross-validation. This results in a value of \\(\\lambda\\) that yields 6.8 degrees of freedom.\nLocal Regression\nIn order to perform local regression, we use the loess() function.\n\n\nCode\nplot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = \"darkgrey\")\n\ntitle(\"Local Regression\")\n\nfit &lt;- loess(wage ~ age, span = .2, data = Wage)\nfit2 &lt;- loess(wage ~ age, span = .5, data = Wage)\n\nlines(age.grid, predict(fit, data.frame(age = age.grid)),\n    col = \"red\", lwd = 2)\nlines(age.grid, predict(fit2, data.frame(age = age.grid)),\n    col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Span = 0.2\", \"Span = 0.5\"),\n    col = c(\"red\", \"blue\"), lty = 1, lwd = 2, cex = .8)\n\n\n\n\n\n\n\n\n\nHere we have performed local linear regression using spans of \\(0.2\\) and \\(0.5\\): that is, each neighborhood consists of 20 % or 50 % of the observations. The larger the span, the smoother the fit. The locfit library can also be used for fitting local regression models in R."
  },
  {
    "objectID": "supportVectorMachines.html",
    "href": "supportVectorMachines.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Support vector machines (SVM) are another way of doing classification problems."
  },
  {
    "objectID": "supportVectorMachines.html#what-is-a-hyperplane",
    "href": "supportVectorMachines.html#what-is-a-hyperplane",
    "title": "Support Vector Machines",
    "section": "1.1 What is a hyperplane?",
    "text": "1.1 What is a hyperplane?\nA hyperplane in \\(p\\) dimensions is a flat affine subspace of dimensions p-1. in \\(p\\) = 2 dimensions a hyperplane is a line. If \\(\\beta_0=0\\) the hyperplane goes through the origin, otherwise not. The vector \\(\\beta = (\\beta_1,\\beta_2,\\dots, \\beta_p)\\) is called the normal vector and it points in a direction orthogonal to the surface of a hyperplane.\nImagine you have two classes plotted in a graph. You may be able to draw a line that separates one class from the other, but if you can do that, it is probable that you can draw other lines in different angles that also separates the two classes. What we want to find is that line that creates the biggest margin between the line and the points, so the line is at the maximum possible distance from points of the two classes.\n\n\n\n\n\n\n\n\n\nOften, the data is not separable by a line, typically when \\(n\\) is bigger than \\(p\\)\n\n\nCode\nset.seed(70)\nn &lt;- 70  # Number of points per class\nclass1 &lt;- data.frame(x = rnorm(n, mean = 2.5, sd = 1), \n  y = rnorm(n, mean = 2.5, sd = 1), class = \"Class 1\")\nclass2 &lt;- data.frame(x = rnorm(n, mean = 4, sd = 1), \n  y = rnorm(n, mean = 4, sd = 1), class = \"Class 2\")\ndata &lt;- rbind(class1, class2)\n# Plot the data with separating lines\nggplot(data, aes(x = x, y = y, color = class)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"Class 1\" = \"blue\", \"Class 2\" = \"red\")) +\n \n  labs(title = \"Synthetic Data with Separating Lines\",\n       x = \"Feature X\",\n       y = \"Feature Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nwhen this is is the case we allow some points to trespass their margin, so we create what we call a Soft margin.\nOne important point of the support vector machine is that it is measuring euclidean distance and treats all units as the same, so the variables should be standardized."
  },
  {
    "objectID": "supportVectorMachines.html#support-vector-machine-for-more-than-two-classes.",
    "href": "supportVectorMachines.html#support-vector-machine-for-more-than-two-classes.",
    "title": "Support Vector Machines",
    "section": "3.1 Support Vector Machine for more than two classes.",
    "text": "3.1 Support Vector Machine for more than two classes.\nWhen we have more than two classes SVM takes two different approaches: OVA (one versus all): it compares each class against all others one by one, so it’s always comparing really two classes, but one of them is an aggregation of all the others. OVO (One versus One): it fits all pairwise classifiers \\(\\binom{K}{2}\\) and then classify a point \\(x\\) to the class that wins the most pairwise competitions."
  },
  {
    "objectID": "supportVectorMachines.html#support-vector-classifier",
    "href": "supportVectorMachines.html#support-vector-classifier",
    "title": "Support Vector Machines",
    "section": "5.1 Support Vector Classifier",
    "text": "5.1 Support Vector Classifier\n\n5.1.1 Linear support vector machine\nThe e1071 library contains implementations for a number of statistical learning methods. In particular, the svm() function can be used to fit a support vector classifier when the argument kernel = \"linear\" is used. A cost argument allows us to specify the cost of a violation to the margin. When the cost argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin. When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.\nWe now use the svm() function to fit the support vector classifier for a given value of the cost parameter. Here we demonstrate the use of this function on a two-dimensional example so that we can plot the resulting decision boundary. We begin by generating the observations, which belong to two classes, and checking whether the classes are linearly separable.\n\n\nCode\nset.seed(1)\nx &lt;- matrix(rnorm(20 * 2), ncol = 2)\ny &lt;- c(rep(-1, 10), rep(1, 10))\nx[y == 1, ] &lt;- x[y == 1, ] + 1\nplot(x, col = (3 - y))\n\n\n\n\n\n\n\n\n\nThey are not sepparable. Next, we fit the support vector classifier. Note that in order for the svm() function to perform classification (as opposed to SVM-based regression), we must encode the response as a factor variable. We now create a data frame with the response coded as a factor.\n\n\nCode\ndat &lt;- data.frame(x = x, y = as.factor(y))\n\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", \n    cost = 10, scale = FALSE)\n\n\nThe argument scale = FALSE tells the svm() function not to scale each feature to have mean zero or standard deviation one; depending on the application, one might prefer to use scale = TRUE.\nWe can now plot the support vector classifier obtained:\n\n\nCode\nplot(svmfit, dat)\n\n\n\n\n\n\n\n\n\nNote that the two arguments to the SVM plot() function are the output of the call to svm(), as well as the data used in the call to svm(). The region of feature space that will be assigned to the \\(-1\\) class is shown in light yellow, and the region that will be assigned to the \\(+1\\) class is shown in red. The decision boundary between the two classes is linear (because we used the argument kernel = \"linear\"), though due to the way in which the plotting function is implemented in this library the decision boundary looks somewhat jagged in the plot. (Note that here the second feature is plotted on the \\(x\\)-axis and the first feature is plotted on the \\(y\\)-axis, in contrast to the behavior of the usual plot() function in R.) The support vectors are those data points that fall in the boundary or in the wrong side of the boundary The support vectors are plotted as crosses and the remaining observations are plotted as circles; we see here that there are seven support vectors. We can determine their identities as follows:\n\n\nCode\nsvmfit$index\n\n\n[1]  1  2  5  7 14 16 17\n\n\nWe can obtain some basic information about the support vector classifier fit using the summary() command:\n\n\nCode\nsummary(svmfit)\n\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10, scale = FALSE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  7\n\n ( 4 3 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\nThis tells us, for instance, that a linear kernel was used with cost = 10, and that there were seven support vectors, four in one class and three in the other.\nWe can also make our own plot: The first thing we will do is make a grid of values for X1 and X2. We will write a function to do that, in case we want to reuse it. It uses the handy function expand.grid(), and produces coordinates of n*n points on a lattice covering the domain of x. Having made the lattice, we make a prediction at each point on the laticce. We then plot the lattice, color-coded accordingly to the classification. Now we can see the decision boundary.\n\n\nCode\nmake.grid= function(x, n=75){\n  grange =apply(x,2,range) #get the range for each col\n  x1=seq(from=grange[1,1],to=grange[2,1], length=n) #creates n points between the range of the first col\n  x2=seq(from=grange[1,2],to=grange[2,2], length=n)#creates n points between the range of the second col\n  expand.grid(x.1=x1,x.2=x2)\n}\nxgrid =make.grid(x)\nygrid = predict(svmfit,xgrid)\nplot(xgrid,col=c(\"red\",\"blue\")[as.numeric(ygrid)],pch=20,cex=.2)\npoints(x,col=y+3, pch=19)\npoints(x[svmfit$index,], pch=5, cex=2) #support vector points\n\n\n\n\n\n\n\n\n\nThe svm function is not too friendly, in that we have to do some work to get back the linear coefficients, as described in the text. Probably the reason is that this only makes sense for linear kernels, and the function is more general. Here we will use a formula to extract the coefficients; for those interested in where this comes from, you can look in chapter 12 of the book “Elements of Statistical Learning”. We extract the linear coefficients, and then using simple algebra, we include the decision boundary and the two margins.\n\n\nCode\nbeta=drop(t(svmfit$coefs)%*%x[svmfit$index,])\nbeta0 =svmfit$rho\nplot(xgrid,col=c(\"red\", \"blue\")[as.numeric(ygrid)],pch=20,cex=.2)\npoints(x,col=y+3,pch=19)\npoints(x[svmfit$index,], pch=5, cex=2) \nabline(beta0/beta[2],-beta[1]/beta[2]) #decision boundary based on intersec and slope calculated with the coefficients\nabline((beta0-1)/beta[2],-beta[1]/beta[2],lty=2) #margin\nabline((beta0+1)/beta[2],-beta[1]/beta[2],lty=2) #margin\n\n\n\n\n\n\n\n\n\nWhat if we instead used a smaller value of the cost parameter?\n\n\nCode\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", \n    cost = 0.1, scale = FALSE)\nplot(svmfit, dat)\n\n\n\n\n\n\n\n\n\nCode\nsvmfit$index\n\n\n [1]  1  2  3  4  5  7  9 10 12 13 14 15 16 17 18 20\n\n\nNow that a smaller value of the cost parameter is being used, we obtain a larger number of support vectors, because the margin is now wider. Unfortunately, the svm() function does not explicitly output the coefficients of the linear decision boundary obtained when the support vector classifier is fit, nor does it output the width of the margin.\nCross Validation\nThe e1071 library includes a built-in function, tune(), to perform cross-validation. By default, tune() performs ten-fold cross-validation on a set of models of interest. In order to use this function, we pass in relevant information about the set of models that are under consideration. The following command indicates that we want to compare SVMs with a linear kernel, using a range of values of the cost parameter.\n\n\nCode\nset.seed(1)\ntune.out &lt;- tune(svm, y ~ ., data = dat, kernel = \"linear\", \n    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n\n\nWe can easily access the cross-validation errors for each of these models using the summary() command:\n\n\nCode\nsummary(tune.out)\n\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n  0.1\n\n- best performance: 0.05 \n\n- Detailed performance results:\n     cost error dispersion\n1   0.001  0.55  0.4377975\n2   0.010  0.55  0.4377975\n3   0.100  0.05  0.1581139\n4   1.000  0.15  0.2415229\n5   5.000  0.15  0.2415229\n6  10.000  0.15  0.2415229\n7 100.000  0.15  0.2415229\n\n\nWe see that cost = 0.1 results in the lowest cross-validation error rate. The tune() function stores the best model obtained, which can be accessed as follows:\n\n\nCode\nbestmod &lt;- tune.out$best.model\nsummary(bestmod)\n\n\n\nCall:\nbest.tune(METHOD = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, \n    0.01, 0.1, 1, 5, 10, 100)), kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.1 \n\nNumber of Support Vectors:  16\n\n ( 8 8 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\nThe predict() function can be used to predict the class label on a set of test observations, at any given value of the cost parameter. We begin by generating a test data set.\n\n\nCode\nxtest &lt;- matrix(rnorm(20 * 2), ncol = 2)\nytest &lt;- sample(c(-1, 1), 20, rep = TRUE)\nxtest[ytest == 1, ] &lt;- xtest[ytest == 1, ] + 1\ntestdat &lt;- data.frame(x = xtest, y = as.factor(ytest))\n\n\nNow we predict the class labels of these test observations. Here we use the best model obtained through cross-validation in order to make predictions.\n\n\nCode\nypred &lt;- predict(bestmod, testdat)\ntable(predict = ypred, truth = testdat$y)\n\n\n       truth\npredict -1 1\n     -1  9 1\n     1   2 8\n\n\nThus, with this value of cost, 17 of the test observations are correctly classified. What if we had instead used cost = 0.01?\n\n\nCode\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", \n    cost = .01, scale = FALSE)\nypred &lt;- predict(svmfit, testdat)\ntable(predict = ypred, truth = testdat$y)\n\n\n       truth\npredict -1  1\n     -1 11  6\n     1   0  3\n\n\nIn this case three additional observations are misclassified.\nNow consider a situation in which the two classes are linearly separable. Then we can find a separating hyperplane using the svm() function. We first further separate the two classes in our simulated data so that they are linearly separable:\n\n\nCode\nx[y == 1, ] &lt;- x[y == 1, ] + 0.5\nplot(x, col = (y + 5) / 2, pch = 19)\n\n\n\n\n\n\n\n\n\nNow the observations are just barely linearly separable. We fit the support vector classifier and plot the resulting hyperplane, using a very large value of cost so that no observations are misclassified.\n\n\nCode\ndat &lt;- data.frame(x = x, y = as.factor(y))\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", \n    cost = 1e5)\nsummary(svmfit)\n\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 100000)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  100000 \n\nNumber of Support Vectors:  3\n\n ( 1 2 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\nCode\nplot(svmfit, dat)\n\n\n\n\n\n\n\n\n\nNo training errors were made and only three support vectors were used. However, we can see from the figure that the margin is very narrow (because the observations that are not support vectors, indicated as circles, are very close to the decision boundary). It seems likely that this model will perform poorly on test data. We now try a smaller value of cost:\n\n\nCode\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"linear\", cost = 1)\nsummary(svmfit)\n\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  7\n\n ( 4 3 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n\nCode\nplot(svmfit, dat)\n\n\n\n\n\n\n\n\n\nUsing cost = 1, we misclassify a training observation, but we also obtain a much wider margin and make use of seven support vectors. It seems likely that this model will perform better on test data than the model with cost = 1e5.\n\n\n5.1.2 Non Linear SVM\nIn order to fit an SVM using a non-linear kernel, we once again use the svm() function. However, now we use a different value of the parameter kernel. To fit an SVM with a polynomial kernel we use kernel = \"polynomial\", and to fit an SVM with a radial kernel we use kernel = \"radial\". In the former case we also use the degree argument to specify a degree for the polynomial kernel (this is \\(d\\) in (9.22)), and in the latter case we use gamma to specify a value of \\(\\gamma\\) for the radial basis kernel.\nWe first generate some data with a non-linear class boundary, as follows:\n\n\nCode\nset.seed(1)\nx &lt;- matrix(rnorm(200 * 2), ncol = 2)\nx[1:100, ] &lt;- x[1:100, ] + 2\nx[101:150, ] &lt;- x[101:150, ] - 2\ny &lt;- c(rep(1, 150), rep(2, 50))\ndat &lt;- data.frame(x = x, y = as.factor(y))\n\n\nPlotting the data makes it clear that the class boundary is indeed non-linear:\n\n\nCode\nplot(x, col = y)\n\n\n\n\n\n\n\n\n\nThe data is randomly split into training and testing groups. We then fit the training data using the svm() function with a radial kernel and \\(\\gamma=1\\):\n\n\nCode\ntrain &lt;- sample(200, 100)\nsvmfit &lt;- svm(y ~ ., data = dat[train, ], kernel = \"radial\",  \n    gamma = 1, cost = 1)\nplot(svmfit, dat[train, ])\n\n\n\n\n\n\n\n\n\nThe plot shows that the resulting SVM has a decidedly non-linear boundary. The summary() function can be used to obtain some information about the SVM fit:\n\n\nCode\nsummary(svmfit)\n\n\n\nCall:\nsvm(formula = y ~ ., data = dat[train, ], kernel = \"radial\", gamma = 1, \n    cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  31\n\n ( 16 15 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 1 2\n\n\nWe can see from the figure that there are a fair number of training errors in this SVM fit. If we increase the value of cost, we can reduce the number of training errors. However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data.\n\n\nCode\nsvmfit &lt;- svm(y ~ ., data = dat[train, ], kernel = \"radial\", \n    gamma = 1, cost = 1e5)\nplot(svmfit, dat[train, ])\n\n\n\n\n\n\n\n\n\nWe can perform cross-validation using tune() to select the best choice of \\(\\gamma\\) and cost for an SVM with a radial kernel:\n\n\nCode\nset.seed(1)\ntune.out &lt;- tune(svm, y ~ ., data = dat[train, ], \n    kernel = \"radial\", \n    ranges = list(\n      cost = c(0.1, 1, 10, 100, 1000),\n      gamma = c(0.5, 1, 2, 3, 4)\n    )\n  )\nsummary(tune.out)\n\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.07 \n\n- Detailed performance results:\n     cost gamma error dispersion\n1     0.1   0.5  0.26 0.15776213\n2     1.0   0.5  0.07 0.08232726\n3    10.0   0.5  0.07 0.08232726\n4   100.0   0.5  0.14 0.15055453\n5  1000.0   0.5  0.11 0.07378648\n6     0.1   1.0  0.22 0.16193277\n7     1.0   1.0  0.07 0.08232726\n8    10.0   1.0  0.09 0.07378648\n9   100.0   1.0  0.12 0.12292726\n10 1000.0   1.0  0.11 0.11005049\n11    0.1   2.0  0.27 0.15670212\n12    1.0   2.0  0.07 0.08232726\n13   10.0   2.0  0.11 0.07378648\n14  100.0   2.0  0.12 0.13165612\n15 1000.0   2.0  0.16 0.13498971\n16    0.1   3.0  0.27 0.15670212\n17    1.0   3.0  0.07 0.08232726\n18   10.0   3.0  0.08 0.07888106\n19  100.0   3.0  0.13 0.14181365\n20 1000.0   3.0  0.15 0.13540064\n21    0.1   4.0  0.27 0.15670212\n22    1.0   4.0  0.07 0.08232726\n23   10.0   4.0  0.09 0.07378648\n24  100.0   4.0  0.13 0.14181365\n25 1000.0   4.0  0.15 0.13540064\n\n\nTherefore, the best choice of parameters involves cost = 1 and gamma = 0.5. We can view the test set predictions for this model by applying the predict() function to the data. Notice that to do this we subset the dataframe dat using -train as an index set.\n\n\nCode\ntable(\n    true = dat[-train, \"y\"], \n    pred = predict(\n      tune.out$best.model, newdata = dat[-train, ]\n      )\n    )\n\n\n    pred\ntrue  1  2\n   1 67 10\n   2  2 21\n\n\n\\(12 \\%\\) of test observations are misclassified by this SVM.\n\nNon linear SVM classification example\nWe use the ESL.mixture data from Elements of Statistical Learning book\n\n\nCode\nload(\"data/ESL.mixture.rda\")\nnames(ESL.mixture)\n\n\n[1] \"x\"        \"y\"        \"xnew\"     \"prob\"     \"marginal\" \"px1\"      \"px2\"     \n[8] \"means\"   \n\n\nThe data is two dimensional.\n\n\nCode\nplot(ESL.mixture$x, col=ESL.mixture$y+1)\n\n\n\n\n\n\n\n\n\nCode\ndat =data.frame(y=factor(ESL.mixture$y), ESL.mixture$x)\nfit=svm(factor(y) ~., data=dat, scale=F,kernell =\"radial\", cost=5)\n\n\nNow we are going to create a grid, as before, and make predictions on the grid. These data have the grid points for each variable included on the data object loaded, so we don’t have to create the grid directly, we can just expand it\n\n\nCode\nxgrid =expand.grid(X1 =ESL.mixture$px1, X2=ESL.mixture$px2)\nygrid = predict(fit, xgrid)\nplot(xgrid,col=as.numeric(ygrid),pch=20,cex=.2)\npoints(ESL.mixture$x,col=ESL.mixture$y+1, pch=19,cex=1)\n\n\n\n\n\n\n\n\n\nWE can go further and hae the predict function produce the actual function estimates at each of our grid points. We can include the actual decision boundary on the plot by making use of the contour function. On the dataframe is also ‘prob’, which is the true probability of class 1 for these data, at the gridpoints. If we plot its 0.5 contour, that will give us the Bayes decision Boundary, which is the best one could ever do.\n\n\nCode\nfunc= predict(fit, xgrid,decision.values=T)\nfunc=attributes(func)$decision\nxgrid =expand.grid(X1 =ESL.mixture$px1, X2=ESL.mixture$px2)\nygrid = predict(fit,xgrid)\nplot(xgrid,col=as.numeric(ygrid), pch=20, cex=.2)\npoints(ESL.mixture$x,col=ESL.mixture$y+1, pch=19,cex=1)\n\ncontour(ESL.mixture$px1,ESL.mixture$px2, matrix(func,69,99), level=0, add=T)"
  },
  {
    "objectID": "supportVectorMachines.html#roc-curves",
    "href": "supportVectorMachines.html#roc-curves",
    "title": "Support Vector Machines",
    "section": "5.2 ROC Curves",
    "text": "5.2 ROC Curves\nThe ROCR package can be used to produce ROC curves. We first write a short function to plot an ROC curve given a vector containing a numerical score for each observation, pred, and a vector containing the class label for each observation, truth.\n\n\nCode\nrocplot &lt;- function(pred, truth, ...) {\n  predob &lt;- prediction(pred, truth)\n  perf &lt;- performance(predob, \"tpr\", \"fpr\")\n  plot(perf, ...)\n}\n\n\nSVMs and support vector classifiers output class labels for each observation. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. For instance, in the case of a support vector classifier, the fitted value for an observation \\(X= (X_1, X_2, \\ldots, X_p)^T\\) takes the form \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + \\cdots + \\hat{\\beta}_p X_p\\). In essence, the sign of the fitted value determines on which side of the decision boundary the observation lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero then it is assigned to the other. In order to obtain the fitted values for a given SVM model fit, we use decision.values = TRUE when fitting svm(). Then the predict() function will output the fitted values.\n\n\nCode\nsvmfit.opt &lt;- svm(y ~ ., data = dat[train, ], \n    kernel = \"radial\", gamma = 2, cost = 1, \n    decision.values = T)\nfitted &lt;- attributes(\n    predict(svmfit.opt, dat[train, ], decision.values = TRUE)\n  )$decision.values\n\n\nNow we can produce the ROC plot. Note we use the negative of the fitted values so that negative values correspond to class 1 and positive values to class 2.\n\n\nCode\nrocplot(-fitted, dat[train, \"y\"], main = \"Training Data\")\n\n\n\n\n\n\n\n\n\nSVM appears to be producing accurate predictions. By increasing \\(\\gamma\\) we can produce a more flexible fit and generate further improvements in accuracy.\n\n\nCode\nrocplot(-fitted, dat[train, \"y\"], main = \"Training Data\")\nsvmfit.flex &lt;- svm(y ~ ., data = dat[train, ], \n    kernel = \"radial\", gamma = 50, cost = 1, \n    decision.values = T)\nfitted &lt;- attributes(\n    predict(svmfit.flex, dat[train, ], decision.values = T)\n  )$decision.values\nrocplot(-fitted, dat[train, \"y\"], add = T, col = \"red\")\n\n\n\n\n\n\n\n\n\nHowever, these ROC curves are all on the training data. We are really more interested in the level of prediction accuracy on the test data. When we compute the ROC curves on the test data, the model with \\(\\gamma=2\\) appears to provide the most accurate results.\n\n\nCode\nfitted &lt;- attributes(\n    predict(svmfit.opt, dat[-train, ], decision.values = T)\n  )$decision.values\nrocplot(-fitted, dat[-train, \"y\"], main = \"Test Data\")\nfitted &lt;- attributes(\n    predict(svmfit.flex, dat[-train, ], decision.values = T)\n  )$decision.values\nrocplot(-fitted, dat[-train, \"y\"], add = T, col = \"red\")"
  },
  {
    "objectID": "supportVectorMachines.html#svm-with-multiple-classes",
    "href": "supportVectorMachines.html#svm-with-multiple-classes",
    "title": "Support Vector Machines",
    "section": "5.3 SVM with Multiple Classes",
    "text": "5.3 SVM with Multiple Classes\nIf the response is a factor containing more than two levels, then the svm() function will perform multi-class classification using the one-versus-one approach. We explore that setting here by generating a third class of observations.\n\n\nCode\nset.seed(1)\nx &lt;- rbind(x, matrix(rnorm(50 * 2), ncol = 2))\ny &lt;- c(y, rep(0, 50))\nx[y == 0, 2] &lt;- x[y == 0, 2] + 2\ndat &lt;- data.frame(x = x, y = as.factor(y))\npar(mfrow = c(1, 1))\nplot(x, col = (y + 1))\n\n\n\n\n\n\n\n\n\nWe now fit an SVM to the data:\n\n\nCode\nsvmfit &lt;- svm(y ~ ., data = dat, kernel = \"radial\", \n    cost = 10, gamma = 1)\nplot(svmfit, dat)"
  },
  {
    "objectID": "supportVectorMachines.html#support-vector-regression",
    "href": "supportVectorMachines.html#support-vector-regression",
    "title": "Support Vector Machines",
    "section": "5.4 Support vector regression",
    "text": "5.4 Support vector regression\nThe e1071 library can also be used to perform support vector regression, if the response vector that is passed in to svm() is numerical rather than a factor."
  },
  {
    "objectID": "supportVectorMachines.html#application-to-gene-expression-data",
    "href": "supportVectorMachines.html#application-to-gene-expression-data",
    "title": "Support Vector Machines",
    "section": "5.5 Application to Gene Expression Data",
    "text": "5.5 Application to Gene Expression Data\nWe now examine the Khan data set, which consists of a number of tissue samples corresponding to four distinct types of small round blue cell tumors. For each tissue sample, gene expression measurements are available. The data set consists of training data, xtrain and ytrain, and testing data, xtest and ytest.\nWe examine the dimension of the data:\n\n\nCode\nlibrary(ISLR2)\nnames(Khan)\n\n\n[1] \"xtrain\" \"xtest\"  \"ytrain\" \"ytest\" \n\n\nCode\ndim(Khan$xtrain)\n\n\n[1]   63 2308\n\n\nCode\ndim(Khan$xtest)\n\n\n[1]   20 2308\n\n\nCode\nlength(Khan$ytrain)\n\n\n[1] 63\n\n\nCode\nlength(Khan$ytest)\n\n\n[1] 20\n\n\nThis data set consists of expression measurements for \\(2{,}308\\) genes. The training and test sets consist of \\(63\\) and \\(20\\) observations respectively.\n\n\nCode\ntable(Khan$ytrain)\n\n\n\n 1  2  3  4 \n 8 23 12 20 \n\n\nCode\ntable(Khan$ytest)\n\n\n\n1 2 3 4 \n3 6 6 5 \n\n\nWe will use a support vector approach to predict cancer subtype using gene expression measurements. In this data set, there are a very large number of features relative to the number of observations. This suggests that we should use a linear kernel, because the additional flexibility that will result from using a polynomial or radial kernel is unnecessary.\n\n\nCode\ndat &lt;- data.frame(\n    x = Khan$xtrain, \n    y = as.factor(Khan$ytrain)\n  )\nout &lt;- svm(y ~ ., data = dat, kernel = \"linear\", \n    cost = 10)\nsummary(out)\n\n\n\nCall:\nsvm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  58\n\n ( 20 20 11 7 )\n\n\nNumber of Classes:  4 \n\nLevels: \n 1 2 3 4\n\n\nCode\ntable(out$fitted, dat$y)\n\n\n   \n     1  2  3  4\n  1  8  0  0  0\n  2  0 23  0  0\n  3  0  0 12  0\n  4  0  0  0 20\n\n\nWe see that there are no training errors. In fact, this is not surprising, because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes. We are most interested not in the support vector classifier’s performance on the training observations, but rather its performance on the test observations.\n\n\nCode\ndat.te &lt;- data.frame(\n    x = Khan$xtest, \n    y = as.factor(Khan$ytest))\npred.te &lt;- predict(out, newdata = dat.te)\ntable(pred.te, dat.te$y)\n\n\n       \npred.te 1 2 3 4\n      1 3 0 0 0\n      2 0 6 2 0\n      3 0 0 4 0\n      4 0 0 0 5\n\n\nWe see that using cost = 10 yields two test set errors on this data."
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "Survival Analisys",
    "section": "",
    "text": "Survival analysis concerns a special kind of outcome variable: the time until an event occurs. For example, suppose that we have conducted a five-year medical study, in which patients have been treated for cancer. We would like to fit a model to predict patient survival time, using features such a baseline measurements or type of treatment. Sounds like a regression problem, but there is an important complication: some of the patients have survived until the end of the study. Such a patient’s survival time is said to be censored. We do not want to discard this subset of surviving patients, since the fact that they survived at least 5 years amounts to valuable information.\nThe applications of survival analysis extend far beyond medicine. For example, consider a company that wishes to model churn, the event when customers cancel subscription to a service. The company might collect data on customers over some period of time, in order to predict each customer’s time to cancellation.\nFor each individual, we suppose that there is a true failure or event time T, as well as a true censoring time C. The survival time represents the time at which the event of interest occurs. The censoring is the time at which censoring occurs, for example the time at which the patient drops out of the study or the study ends. For each observation we compute the min of those times, either the event or the censoring time. If the event occurs before censoring, then we observe the true survival time T, if censoring occurs before the event, then we observe C. We observe a status indicator \\(\\delta = 1\\ if\\  T\\leq C\\) \\(\\delta = 0\\ if\\  T &gt; C\\) so in our dataset we observe n pairs (Y,\\(\\delta\\)) where Y is the time to T or C.\nSuppose that a number of patients drop out of a cancer study early because they are very sick. An Analysis that does not take into consideration the reason why the patients dropped out will likely overestimate the true average survival time. Similarly, suppose that males are more likely to drop out of the study than females, then a comparison of male and female survival times may wrongly suggest that males survive longer than females.\nIn general, we need to assume that, conditional on the features, the event time T is independent of the censoring time C, and the above examples violate that assumption. There is no way of statistically checking if this assumption is right, you will have to think about those variables yourself and find out the reasons why observations are censored.\nThe survival curve is the probability that a true survival time \\(T\\) is higher than a specified time \\(t\\). \\[\nS(t)=Pr(T&gt;t)\n\\tag{1}\\]\nThis decreasing function quantifies the probability of surviving past time \\(t\\) For example, suppose that a company is interested in modeling customer churn. Let \\(T\\) represent the time that a customer cancels a subscription to the service, then \\(S(t)\\) represents the probability that a customer cancels later than time \\(t\\). The larger the value of \\(S(t)\\), the less likely that the customer will cancel before time \\(t\\).\nLet’s consider the BrainCancer dataset, which contains the survival times for patients with primary brain tumors undergoing treatments. The predictors are gtv (gross tumor volume), sex, diagnosis (type of cancer), loc (location of the tumor), ki (Karnofsky index), and stereo (stereotactic method). 53 of the 88 patients were still alive at the end of the study.\nSuppose we’d like to estimate the probability of surviving past 20 months. \\(S(20)=Pr(T&gt;20)\\). The first approach that would come to our minds would be to just count the proportion of patients that survived past 20 months, and that is 48/88 = 55%. However, this is not right, 17 of the 40 patients who did not survive to 20 months were actually censored, and this way of doing the analysis implicitly assumes they died, hence this probability is an underestimate."
  },
  {
    "objectID": "survival.html#harrels-concordance-index-the-c-index.",
    "href": "survival.html#harrels-concordance-index-the-c-index.",
    "title": "Survival Analisys",
    "section": "3.1 Harrel’s concordance index (the C-index).",
    "text": "3.1 Harrel’s concordance index (the C-index).\nThis is a method for assessing a fitted survival model on a test set. For each observation, we calculate the estimated risk score, so with the coefficients we got from the model we calculate the estimated risk score, called eta hat of i \\(\\hat\\eta_i = \\hat\\beta_1x_{i1}+\\dots+\\hat\\beta_px{ip}\\) Then for each pairs of observations, we compute the proportion that we got right from the model, this is, if they had a lower risk, they lived longer than their counterpart in the pair.\n\\[\nC = \\frac{\\sum_{i&lt;j} \\delta_{ij} \\cdot \\mathbb{I}(\\hat{T}_i &lt; \\hat{T}_j)}{\\sum_{i&lt;j} \\delta_{ij}}\n\\tag{4}\\]\nWhen we are comparing a pair that has one censored observation, we cannot really say if they lived longer or not, so we only count up over those that we can compare.\nIf we get a c-index of 0.733, roughly speaking that means that given two random observations, the model can predict with 73.3% accuracy which one will survive longer.\n\n\nOther considerations of survival analysis\n\n\nThere is not a single type of censoring, there is right censoring, left censoring and interval censoring.\n\nRight censoring: This occurs when the event of interest has not happened by the end of the study period or when a subject leaves the study before experiencing the event. Example: If a patient is still alive at the end of the study or drops out, the exact survival time is unknown but is known to be greater than the last observed time.\nLeft censoring: This happens when the event of interest has already occurred before the start of the study or before a subject’s entry into the study. Example: If you’re studying the time to a certain disease and some individuals already had the disease before the study began, their exact time to event is unknown but is known to be less than the first observed time.\nInterval censoring: This occurs when the event of interest is known to have occurred within a specific time interval, but the exact time is unknown. Example: If follow-up visits are scheduled annually and a patient was healthy at their last visit but found to have developed a disease at the next visit, the exact time of disease onset is unknown but is known to have occurred within that year.\n\nThere are considerations to be made about the time scale, for example, use if calendar time or patient’s age\nTime-dependent covariates: we measure certain predictors on the same patient over time"
  },
  {
    "objectID": "tree.html#predictions",
    "href": "tree.html#predictions",
    "title": "Tree Based Methods",
    "section": "2.1 Predictions",
    "text": "2.1 Predictions\nWe predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs."
  },
  {
    "objectID": "tree.html#pruning",
    "href": "tree.html#pruning",
    "title": "Tree Based Methods",
    "section": "2.2 Pruning",
    "text": "2.2 Pruning\nIf we leave the tree to have as many nodes as possible, it will produce a tree that may produce good predictions on the training set, but it is likely to overfit the data, leading to poor test set performance. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.\nOne possible alternative to the process described above is to grow the tree only so long as the decrease in the RSS due to each split exceeds some threshold. This strategy will result in smaller trees, but it is too short sighted: a seemingly worthless split early on in the tree might be followed by a very good split (that is, a split that leads to a large reduction in RSS later on)\nA better strategy is to grow a very large tree \\(T_0\\) and then prune it back in order to obtain a subtree. A strategy called cost complexity pruning or weakest link pruning is used to do this.\nWe consider a sequence of trees indexed by a non negative tuning parameter \\(\\alpha\\). For each value of alpha there corresponds a subtree \\(T \\subset T_0\\) such that the expression below is as small as possible:\n\\[\n\\sum^{|T|}_{m=1}\\sum_{i:x_i\\epsilon R_m}(y_i-\\hat y_{Rm})^2+\\alpha|T|\n\\]\nwhere \\(|T|\\) indicates the number of terminal nodes of the tree. \\(R_m\\) is the rectangle or region (the subset of predictor space) corresponding to the mth terminal node, and \\(\\hat y_{Rm}\\) is the mean of the training observations in the region. So we want the sum of squares in the region to be as small as possible, but we put a penalty on the number of terminals by adding the \\(\\alpha |T|\\). The concept is similar to the lasso. The tuning parameter alpha controls a trade-off between the subtree’s complexity and its fit to the training data. We select an optimal value for alpha using cross validation. We then return to the full dataset and obtain the subtree corresponding to that value of alpha.\nWe are going to do this manually here to show how the model overfits as more splits are added: the training MSE decreases, but the test MSE increases. The cross validation will help us choose the correct number of nodes to use:\nRandomly divided the data set in half and built large regression tree on training data and varied α to create subtrees with different numbers of terminal nodes Finally, performed 6-fold cross-validation to estimate the cross-validated MSE of the trees as a function of α\n\n\nCode\nset.seed(123) \ntrainIndex &lt;- sample(c(TRUE,FALSE), nrow(Hitters), TRUE)\n\ntrain &lt;- Hitters[trainIndex,] \ntest &lt;- Hitters[!trainIndex,]\n\ntree_model &lt;- tree(Salary ~ Hits + Years, data = train)\n\n# Perform cross-validation \ncv_tree &lt;- cv.tree(tree_model, FUN = prune.tree, K = 6)\n\n# Extract the tree size and corresponding deviance\n(tree_sizes &lt;- cv_tree$size)\n\n\n[1] 11 10  8  7  6  4  3  2  1\n\n\nCode\ntree_deviances &lt;- cv_tree$dev\n\n# Calculate training and test MSE for each tree size\ntrain_mse_list &lt;- c()\ntest_mse_list &lt;- c()\nfor (size in tree_sizes) { \n  pruned_tree &lt;- prune.tree(tree_model, best = size) \n  # Skip tree sizes that result in a single node \nif (nrow(pruned_tree$frame) &gt; 1) { \n  #calculate MSE\n  train_pred &lt;- predict(pruned_tree, train) \n  train_mse &lt;- mean((train_pred - train$Salary)^2) \n  test_pred &lt;- predict(pruned_tree, test) \n  test_mse &lt;- mean((test_pred - test$Salary)^2) \n  } \nelse { \n  train_mse &lt;- NA \n  test_mse &lt;- NA } \ntrain_mse_list &lt;- c(train_mse_list, train_mse) \ntest_mse_list &lt;- c(test_mse_list, test_mse)\n}\n\n# Calculate cross-validated MSE (average deviance from cross-validation)\ncv_mse &lt;- tree_deviances / length(train$Salary)\n\nresults &lt;- data.frame(\n  TreeSize = rep(tree_sizes, 3),\n  MSE = c(train_mse_list, test_mse_list, cv_mse),\n  Data = rep(c(\"Training\", \"Test\", \"Cross-Validation\"), each = length(tree_sizes))\n)\n\nggplot(results, aes(x = TreeSize, y = MSE, color = Data)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"MSE vs Tree Size\", x = \"Tree Size\", y = \"MSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLet’s see how the tree on the hits dataset looks without manual pruning. We are letting the software choose the right number of nodes:\n\n\nCode\ntree.Hitters &lt;- tree(Salary ~ Hits + Years, Hitters)\n\nplot(tree.Hitters, pretty=0)\n# Add text labels to the plot \ntext(tree.Hitters, pretty = 0)\n\n\n\n\n\n\n\n\n\nThe tree function grows a large, unpruned tree. This tree is typically overgrown and will have more terminal nodes than necessary to capture the underlying structure of the data. By default, the tree function will grow the tree until the decrease in deviance (a measure of model fit) is not statistically significant at each split.\nCross-Validation and Pruning: The process we followed to determine the optimal number of terminal nodes involves:\nGrowing a large tree: This is similar to what the tree function does by default.\nPruning the tree: We use cross-validation to determine the optimal size of the tree that minimizes the mean squared error (MSE). This involves pruning the large tree to a smaller, more optimal size.\n\n\nCode\ncv_tree &lt;- cv.tree(tree_model, FUN = prune.tree, K = 6)\n\n# Determine the tree size with the minimum deviance \noptimal_size &lt;- tree_sizes[which.min(tree_deviances)] \n\n# Prune the tree to the optimal size \npruned_tree &lt;- prune.tree(tree_model, best = optimal_size)\nsummary(pruned_tree)\n\n\n\nRegression tree:\nsnip.tree(tree = tree_model, nodes = c(2L, 7L, 6L))\nNumber of terminal nodes:  3 \nResidual mean deviance:  89150 = 11320000 / 127 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-759.30 -148.30  -70.32    0.00  111.80 1091.00 \n\n\n\nFitting Regression trees\nHere we fit a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data.\n\n\nCode\nset.seed(1)\ntrain &lt;- sample(1:nrow(Boston), nrow(Boston) / 2)\ntree.boston &lt;- tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)\n\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n\n\nNotice that the output of summary() indicates that only four of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.\n\n\nCode\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n\n\n\n\n\n\n\n\n\nThe variable lstat measures the percentage of individuals with {lower socioeconomic status}, while the variable rm corresponds to the average number of rooms. The tree indicates that larger values of rm, or lower values of lstat, correspond to more expensive houses. For example, the tree predicts a median house price of \\(45{,}400\\) for homes in census tracts in which rm &gt;= 7.553.\nIt is worth noting that we could have fit a much bigger tree, by passing control = tree.control(nobs = length(train), mindev = 0) into the tree() function.\nNow we use the cv.tree() function to see whether pruning the tree will improve performance.\n\n\nCode\ncv.boston &lt;- cv.tree(tree.boston)\nplot(cv.boston$size, cv.boston$dev, type = \"b\")\n\n\n\n\n\n\n\n\n\nIn this case, the most complex tree under consideration is selected by cross-validation. However, if we wish to prune the tree, we could do so as follows, using the prune.tree() function:\n\n\nCode\nprune.boston &lt;- prune.tree(tree.boston, best = 5)\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\n\n\n\n\n\n\n\n\n\nIn keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.\n\n\nCode\nyhat &lt;- predict(tree.boston, newdata = Boston[-train, ])\nboston.test &lt;- Boston[-train, \"medv\"]\nplot(yhat, boston.test)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nCode\nmean((yhat - boston.test)^2)\n\n\n[1] 35.28688\n\n\nIn other words, the test set MSE associated with the regression tree is \\(35.29\\). The square root of the MSE is therefore around \\(5.941\\), indicating that this model leads to test predictions that are (on average) within approximately \\(5{,}941\\) of the true median home value for the census tract.\n\n\nRegression trees using rpart\nWe will build a Regression Tree to predict the median house value (medv) in the Boston dataset based on various features such as crime rate, number of rooms, and distance to employment centers.\nWe will use the library rpart\n\n\nCode\n#split 70% of the data for training\ntrain_index &lt;- sample(1:nrow(Boston), 0.7*nrow(Boston))\ntrain_boston &lt;- Boston[train_index,]\ntest_boston &lt;- Boston[-train_index,]\n\n# Train a Regression Tree \nrt_model &lt;- rpart(medv ~ ., data = train_boston, method = \"anova\")\nrt_model\n\n\nn= 354 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 354 30392.2700 22.78164  \n   2) rm&lt; 6.92 299 11489.1900 20.03612  \n     4) lstat&gt;=14.405 118  2454.7150 15.15169  \n       8) nox&gt;=0.603 70   969.6979 12.92143  \n        16) crim&gt;=11.343 24   117.3763  9.36250 *\n        17) crim&lt; 11.343 46   389.7383 14.77826 *\n       9) nox&lt; 0.603 48   629.0592 18.40417 *\n     5) lstat&lt; 14.405 181  4383.9540 23.22044  \n      10) rm&lt; 6.543 139  2917.2830 21.95540  \n        20) dis&gt;=1.6156 132  1161.2110 21.53788 *\n        21) dis&lt; 1.6156 7  1299.1540 29.82857 *\n      11) rm&gt;=6.543 42   508.0279 27.40714 *\n   3) rm&gt;=6.92 55  4396.6570 37.70727  \n     6) rm&lt; 7.433 34  1254.4960 32.72059  \n      12) lstat&gt;=8.355 7   303.9600 26.30000 *\n      13) lstat&lt; 8.355 27   587.1541 34.38519 *\n     7) rm&gt;=7.433 21   927.8124 45.78095 *\n\n\nwe can visualize the trained regression tree:\n\n\nCode\n# Plot the regression tree\nplot(rt_model, margin = 0.1)  # decrease margin for readability\ntext(rt_model, cex=0.8)       # decrease font size for readability\n\n\n\n\n\n\n\n\n\nNote that the returned tree in rt_model has already been pruned by default. If you want to see what was going on under the hood, we can force rpart() to grow the “bushy” tree by setting the cost complexity \\(\\alpha\\) =0 (called cp)\n\n\nCode\nbushy_tree &lt;- rpart(\n    formula = medv ~ .,\n    data    = train_boston,\n    method  = \"anova\", \n    control = list(cp = 0) # set the cost complexity parameter to 0 \n)\nplot(bushy_tree, margin = 0.1)  # decrease margin for readability\ntext(bushy_tree, cex=0.8)       # decrease font size for readability\n\n\n\n\n\n\n\n\n\nTo see if pruning is needed let’s investigate the cross-validated error as a function of the complexity parameter\n\n\nCode\n# Plot the cross-validation results\nplotcp(bushy_tree)\n\n\n\n\n\n\n\n\n\nHere’s how to interpret the plot:\nX-val Relative Error (Y-axis): This represents the relative error obtained during cross-validation. Lower values indicate a better fit to the data. Tree Size (Top Axis): The top axis indicates the number of splits in the tree. The more splits, the larger the tree. For example, a tree size of 3 corresponds to a tree with three splits (four terminal nodes). Complexity Parameter (cp, X-axis): cp controls the complexity of the tree. Smaller values of cp allow more splits, resulting in a larger tree. As cp decreases from left to right, the tree becomes more complex. We call this in lecture. Error Bars: The vertical lines through the circles represent one standard error above and below the relative error. The error bars help to identify a “simpler” tree using the “1-SE rule”: the smallest tree with an error within one standard error of the minimum error. Dotted Line: The dotted line indicates the minimum cross-validated relative error plus one standard error. This is used in the “1-SE rule” to choose a tree size that balances error minimization and simplicity.\n1-SE rule Although there are larger trees that obtain smaller errors (the optimal tree is the size of tree 0.0011025, 23, 0.1304064, 0.2664683, 0.0478719), after around 8 to 9 splits, the relative error levels off, and further reductions in cp lead to only small changes in the error. Breiman (1984) suggested that in actual practice, it’s common to instead use the smallest tree within 1 standard error (SE) of the minimum CV error (this is called the 1-SE rule).\n\n\nCode\nbushy_tree$cptable\n\n\n             CP nsplit rel error    xerror       xstd\n1  0.4773063463      0 1.0000000 1.0095307 0.09977729\n2  0.1530165655      1 0.5226937 0.5847799 0.06238429\n3  0.0728589564      2 0.3696771 0.4351669 0.05392774\n4  0.0315423308      3 0.2968181 0.3434853 0.04935675\n5  0.0281636620      4 0.2652758 0.3359006 0.05034302\n6  0.0152204273      5 0.2371121 0.2966194 0.04955162\n7  0.0150340383      6 0.2218917 0.2941126 0.04994947\n8  0.0119563793      7 0.2068577 0.2968220 0.05266061\n9  0.0089230707      8 0.1949013 0.2943669 0.05263854\n10 0.0065782220      9 0.1859782 0.2884455 0.05138245\n11 0.0065592854     10 0.1794000 0.2823226 0.05021831\n12 0.0056365460     11 0.1728407 0.2753692 0.04762677\n13 0.0054386848     12 0.1672042 0.2740205 0.04743106\n14 0.0048069868     13 0.1617655 0.2723186 0.04746668\n15 0.0046267274     14 0.1569585 0.2709677 0.04743892\n16 0.0020743515     15 0.1523318 0.2564032 0.04733876\n17 0.0015681140     16 0.1502574 0.2569260 0.04687491\n18 0.0014857807     17 0.1486893 0.2539595 0.04548517\n19 0.0014715296     18 0.1472035 0.2540556 0.04548391\n20 0.0014408212     19 0.1457320 0.2542478 0.04548749\n21 0.0013844826     20 0.1442912 0.2539065 0.04548853\n22 0.0012821977     21 0.1429067 0.2528417 0.04549317\n23 0.0012635486     22 0.1416245 0.2546839 0.04556077\n24 0.0012319423     23 0.1403609 0.2546839 0.04556077\n25 0.0011324155     24 0.1391290 0.2524949 0.04544864\n26 0.0008419152     25 0.1379966 0.2498942 0.04546188\n27 0.0006833371     26 0.1371547 0.2467744 0.04466656\n28 0.0006826785     27 0.1364713 0.2466098 0.04466813\n29 0.0006795252     28 0.1357887 0.2466098 0.04466813\n30 0.0003224561     29 0.1351091 0.2457964 0.04469268\n31 0.0000000000     30 0.1347867 0.2449862 0.04469271\n\n\nMore specifically, we’d choose the smallest tree where the error is within one standard error of the minimum error (above the dotted line). In this case, we could use a tree with 8 terminal nodes and reasonably expect to experience similar results within a small margin of error.\nTo prune using the 1-SE rule:\n\n\nCode\n# Step 1: Find the minimum cross-validated error and its corresponding standard error\nmin_xerror &lt;- min(bushy_tree$cptable[, \"xerror\"])\nmin_xstd &lt;- bushy_tree$cptable[which.min(bushy_tree$cptable[, \"xerror\"]), \"xstd\"]\n\n# Step 2: Calculate the threshold for the 1-SE rule\nthreshold &lt;- min_xerror + min_xstd\n\n# Step 3: Find the smallest tree where the xerror is within the threshold\n# We use which() to find all indices where xerror is within the threshold\n# Then we choose the smallest cp that satisfies this condition\noptimal_cp_1se &lt;- bushy_tree$cptable[which(bushy_tree$cptable[, \"xerror\"] &lt;= threshold)[1], \"CP\"]\n\n# Step 4: Prune the tree using the optimal cp from the 1-SE rule\npruned_model &lt;- prune(bushy_tree, cp = optimal_cp_1se)\n\n# Step 5: Plot the pruned tree\nplot(pruned_model, margin = 0.1)\ntext(pruned_model, cex = 0.8)\n\n\n\n\n\n\n\n\n\nNotice that the pruned tree in Figure 3 is essentially the same as the tree obtained in Figure 1 when using the default settings; however, we achieve this result with significantly less effort. It’s worth noting that slight differences may appear due to the randomness introduced by the cross-validation process during the creation of the splits.\nEvaluating the Regression Tree Going back to the rt_model, we evaluate the regression tree’s performance by predicting on the test set and calculating metrics such as Mean Squared Error (MSE) and R-squared.\n\n\nCode\n# Predict the median house value on the test data\npredictions_reg &lt;- predict(rt_model, newdata = test_boston)\n\n# Calculate Mean Squared Error\nmse &lt;- mean((predictions_reg - test_boston$medv)^2)\nmse\n\n\n[1] 18.39567"
  },
  {
    "objectID": "tree.html#gini-index-and-deviance",
    "href": "tree.html#gini-index-and-deviance",
    "title": "Tree Based Methods",
    "section": "3.1 Gini index and Deviance",
    "text": "3.1 Gini index and Deviance\nIn decision tree algorithms, the Gini Index is used as a criterion to measure the purity or impurity of a split in the dataset. The goal is to create nodes that contain observations of mostly one class, leading to better classification accuracy. The Gini index is defined by\n\\[\nG = \\sum^K_{k=1} \\hat p_{mk}(1-\\hat p_{mk})\n\\tag{1}\\]\nK is the number of classes\nthe binomial variance formula is: \\[\nvar(X)=n \\times p \\times (1-p)\n\\tag{2}\\]\nso this is a measure of the variability of that region.\nIf the Gini index is really small, that means that one class is favored and all the rest are really small. If the region is pure, the Gini will be 0.\nAn alternative is the deviance or cross-entropy and this is based on the binomial log likelihook. It behaves similarly to the Gini index and both give similar results. \\[\nD = -\\sum^K_{k=1} \\hat p_{mk}log \\ \\hat p_{mk}\n\\]{eq-crossEntropy}\nLet’s look at an example with the Heart data. These data have a binary response called HD. Yes indicates presence of heart disease and No means no heart disease. There are 13 predictors.\nThe unpruned tree looks like this:\n\n\nCode\nheart&lt;- kmed::heart\nheart$hd &lt;- case_when(heart$class == 0 ~\"No\", \n                      TRUE ~ \"Yes\")\nheart_tree &lt;- heart %&gt;% \n  mutate_if(is.character, as.factor) \n\nset.seed(417)\nindex &lt;- sample(nrow(heart), size = nrow(heart)*0.80)\nheart_train_tree &lt;- heart_tree[index,] #take 80%\nheart_test_tree &lt;- heart_tree[-index,] #take 20%\n\ntree.heart &lt;- tree(hd ~ . -class, data=heart_train_tree)\nsummary(tree.heart)\n\n\n\nClassification tree:\ntree(formula = hd ~ . - class, data = heart_train_tree)\nVariables actually used in tree construction:\n[1] \"thal\"     \"ca\"       \"age\"      \"trestbps\" \"cp\"       \"exang\"    \"sex\"     \n[8] \"oldpeak\"  \"chol\"    \nNumber of terminal nodes:  20 \nResidual mean deviance:  0.41 = 88.97 / 217 \nMisclassification error rate: 0.08017 = 19 / 237 \n\n\nCode\nplot(tree.heart)\ntext(tree.heart, pretty=0)\n\n\n\n\n\n\n\n\n\nLet’s do cross-validation now:\n\n\nCode\nset.seed(7)\ncv.heart &lt;- cv.tree(tree.heart, FUN = prune.misclass)\n\npar(mfrow = c(1, 2))\nplot(cv.heart$size, cv.heart$dev, type = \"b\")\nplot(cv.heart$k, cv.heart$dev, type = \"b\")\n\n\n\n\n\ncross validation indicates that the best number of nodes is 6\n\n\nCode\nprune.heart &lt;- prune.misclass(tree.heart, best = 6)\nplot(prune.heart)\ntext(prune.heart, pretty = 0)\n\n\n\n\n\n\n\n\n\n\nAdvantages and disadvantages of Trees\nPros: - Trees are very easy to explain to people. - Some people believe that decision trees more closely mirror human decision-making than do regression and classification approaches. - Trees can be displayed graphically. - Trees can easily handle qualitative predictors without the need to create dummy variables.\nCons: - Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen already.\n\n\nFitting classification trees\nWe will have a look at the Carseats data using the tree package\nWe want a response variable that is qualitative so we are going to turn sales into a binary response. Let’s see what value can be appropriate:\n\n\nCode\nhist(Carseats$Sales)\n\n\n\n\n\n\n\n\n\nWe create a binary response variable ‘high’ for high sales:ifelse() function creates a variable, called High, which takes on a value of Yes if the Sales variable exceeds \\(8\\),and takes on a value of No otherwise.\n\n\nCode\nCarseats$High &lt;- factor(ifelse(Carseats$Sales &lt;= 8, \"No\", \"Yes\"))\n\n\nCorrelation Analysis Correlation heatmap (Pearson)\n\n\nCode\n# convert factor features to numeric for correlation analysis\ncarseats_num &lt;- Carseats %&gt;% \n     mutate(High = ifelse(High == \"No\", 0 , 1), \n            Urban = ifelse(Urban == \"No\", 0, 1), \n            US = ifelse(US == \"No\", 0, 1), \n            ShelveLoc = case_when(\n                 ShelveLoc == 'Bad' ~ 1, \n                 ShelveLoc == \"Medium\" ~ 2, \n                 TRUE ~ 3\n            ))\n\ncarseats_num\n\n\n    Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1    9.50       138     73          11        276   120         1  42        17\n2   11.22       111     48          16        260    83         3  65        10\n3   10.06       113     35          10        269    80         2  59        12\n4    7.40       117    100           4        466    97         2  55        14\n5    4.15       141     64           3        340   128         1  38        13\n6   10.81       124    113          13        501    72         1  78        16\n7    6.63       115    105           0         45   108         2  71        15\n8   11.85       136     81          15        425   120         3  67        10\n9    6.54       132    110           0        108   124         2  76        10\n10   4.69       132    113           0        131   124         2  76        17\n11   9.01       121     78           9        150   100         1  26        10\n12  11.96       117     94           4        503    94         3  50        13\n13   3.98       122     35           2        393   136         2  62        18\n14  10.96       115     28          11         29    86         3  53        18\n15  11.17       107    117          11        148   118         3  52        18\n16   8.71       149     95           5        400   144         2  76        18\n17   7.58       118     32           0        284   110         3  63        13\n18  12.29       147     74          13        251   131         3  52        10\n19  13.91       110    110           0        408    68         3  46        17\n20   8.73       129     76          16         58   121         2  69        12\n21   6.41       125     90           2        367   131         2  35        18\n22  12.13       134     29          12        239   109         3  62        18\n23   5.08       128     46           6        497   138         2  42        13\n24   5.87       121     31           0        292   109         2  79        10\n25  10.14       145    119          16        294   113         1  42        12\n26  14.90       139     32           0        176    82         3  54        11\n27   8.33       107    115          11        496   131         3  50        11\n28   5.27        98    118           0         19   107         2  64        17\n29   2.99       103     74           0        359    97         1  55        11\n30   7.81       104     99          15        226   102         1  58        17\n31  13.55       125     94           0        447    89         3  30        12\n32   8.25       136     58          16        241   131         2  44        18\n33   6.20       107     32          12        236   137         3  64        10\n34   8.77       114     38          13        317   128         3  50        16\n35   2.67       115     54           0        406   128         2  42        17\n36  11.07       131     84          11         29    96         2  44        17\n37   8.89       122     76           0        270   100         3  60        18\n38   4.95       121     41           5        412   110         2  54        10\n39   6.59       109     73           0        454   102         2  65        15\n40   3.24       130     60           0        144   138         1  38        10\n41   2.07       119     98           0         18   126         1  73        17\n42   7.96       157     53           0        403   124         1  58        16\n43  10.43        77     69           0         25    24         2  50        18\n44   4.12       123     42          11         16   134         2  59        13\n45   4.16        85     79           6        325    95         2  69        13\n46   4.56       141     63           0        168   135         1  44        12\n47  12.44       127     90          14         16    70         2  48        15\n48   4.38       126     98           0        173   108         1  55        16\n49   3.91       116     52           0        349    98         1  69        18\n50  10.61       157     93           0         51   149         3  32        17\n51   1.42        99     32          18        341   108         1  80        16\n52   4.42       121     90           0        150   108         1  75        16\n53   7.91       153     40           3        112   129         1  39        18\n54   6.92       109     64          13         39   119         2  61        17\n55   4.90       134    103          13         25   144         2  76        17\n56   6.85       143     81           5         60   154         2  61        18\n57  11.91       133     82           0         54    84         2  50        17\n58   0.91        93     91           0         22   117         1  75        11\n59   5.42       103     93          15        188   103         1  74        16\n60   5.21       118     71           4        148   114         2  80        13\n61   8.32       122    102          19        469   123         1  29        13\n62   7.32       105     32           0        358   107         2  26        13\n63   1.82       139     45           0        146   133         1  77        17\n64   8.47       119     88          10        170   101         2  61        13\n65   7.80       100     67          12        184   104         2  32        16\n66   4.90       122     26           0        197   128         2  55        13\n67   8.85       127     92           0        508    91         2  56        18\n68   9.01       126     61          14        152   115         2  47        16\n69  13.39       149     69          20        366   134         3  60        13\n70   7.99       127     59           0        339    99         2  65        12\n71   9.46        89     81          15        237    99         3  74        12\n72   6.50       148     51          16        148   150         2  58        17\n73   5.52       115     45           0        432   116         2  25        15\n74  12.61       118     90          10         54   104         3  31        11\n75   6.20       150     68           5        125   136         2  64        13\n76   8.55        88    111          23        480    92         1  36        16\n77  10.64       102     87          10        346    70         2  64        15\n78   7.70       118     71          12         44    89         2  67        18\n79   4.43       134     48           1        139   145         2  65        12\n80   9.14       134     67           0        286    90         1  41        13\n81   8.01       113    100          16        353    79         1  68        11\n82   7.52       116     72           0        237   128         3  70        13\n83  11.62       151     83           4        325   139         3  28        17\n84   4.42       109     36           7        468    94         1  56        11\n85   2.23       111     25           0         52   121         1  43        18\n86   8.47       125    103           0        304   112         2  49        13\n87   8.70       150     84           9        432   134         2  64        15\n88  11.70       131     67           7        272   126         3  54        16\n89   6.56       117     42           7        144   111         2  62        10\n90   7.95       128     66           3        493   119         2  45        16\n91   5.33       115     22           0        491   103         2  64        11\n92   4.81        97     46          11        267   107         2  80        15\n93   4.53       114    113           0         97   125         2  29        12\n94   8.86       145     30           0         67   104         2  55        17\n95   8.39       115     97           5        134    84         1  55        11\n96   5.58       134     25          10        237   148         2  59        13\n97   9.48       147     42          10        407   132         3  73        16\n98   7.45       161     82           5        287   129         1  33        16\n99  12.49       122     77          24        382   127         3  36        16\n100  4.88       121     47           3        220   107         1  56        16\n101  4.11       113     69          11         94   106         2  76        12\n102  6.20       128     93           0         89   118         2  34        18\n103  5.30       113     22           0         57    97         2  65        16\n104  5.07       123     91           0        334    96         1  78        17\n105  4.62       121     96           0        472   138         2  51        12\n106  5.55       104    100           8        398    97         2  61        11\n107  0.16       102     33           0        217   139         2  70        18\n108  8.55       134    107           0        104   108         2  60        12\n109  3.47       107     79           2        488   103         1  65        16\n110  8.98       115     65           0        217    90         2  60        17\n111  9.00       128     62           7        125   116         2  43        14\n112  6.62       132    118          12        272   151         2  43        14\n113  6.67       116     99           5        298   125         3  62        12\n114  6.01       131     29          11        335   127         1  33        12\n115  9.31       122     87           9         17   106         2  65        13\n116  8.54       139     35           0         95   129         2  42        13\n117  5.08       135     75           0        202   128         2  80        10\n118  8.80       145     53           0        507   119         2  41        12\n119  7.57       112     88           2        243    99         2  62        11\n120  7.37       130     94           8        137   128         2  64        12\n121  6.87       128    105          11        249   131         2  63        13\n122 11.67       125     89          10        380    87         1  28        10\n123  6.88       119    100           5         45   108         2  75        10\n124  8.19       127    103           0        125   155         3  29        15\n125  8.87       131    113           0        181   120         3  63        14\n126  9.34        89     78           0        181    49         2  43        15\n127 11.27       153     68           2         60   133         3  59        16\n128  6.52       125     48           3        192   116         2  51        14\n129  4.96       133    100           3        350   126         1  55        13\n130  4.47       143    120           7        279   147         1  40        10\n131  8.41        94     84          13        497    77         2  51        12\n132  6.50       108     69           3        208    94         2  77        16\n133  9.54       125     87           9        232   136         3  72        10\n134  7.62       132     98           2        265    97         1  62        12\n135  3.67       132     31           0        327   131         2  76        16\n136  6.44        96     94          14        384   120         2  36        18\n137  5.17       131     75           0         10   120         1  31        18\n138  6.52       128     42           0        436   118         2  80        11\n139 10.27       125    103          12        371   109         2  44        10\n140 12.30       146     62          10        310    94         2  30        13\n141  6.03       133     60          10        277   129         2  45        18\n142  6.53       140     42           0        331   131         1  28        15\n143  7.44       124     84           0        300   104         2  77        15\n144  0.53       122     88           7         36   159         1  28        17\n145  9.09       132     68           0        264   123         3  34        11\n146  8.77       144     63          11         27   117         2  47        17\n147  3.90       114     83           0        412   131         1  39        14\n148 10.51       140     54           9        402   119         3  41        16\n149  7.56       110    119           0        384    97         2  72        14\n150 11.48       121    120          13        140    87         2  56        11\n151 10.49       122     84           8        176   114         3  57        10\n152 10.77       111     58          17        407   103         3  75        17\n153  7.64       128     78           0        341   128         3  45        13\n154  5.93       150     36           7        488   150         2  25        17\n155  6.89       129     69          10        289   110         2  50        16\n156  7.71        98     72           0         59    69         2  65        16\n157  7.49       146     34           0        220   157         3  51        16\n158 10.21       121     58           8        249    90         2  48        13\n159 12.53       142     90           1        189   112         3  39        10\n160  9.32       119     60           0        372    70         1  30        18\n161  4.67       111     28           0        486   111         2  29        12\n162  2.93       143     21           5         81   160         2  67        12\n163  3.63       122     74           0        424   149         2  51        13\n164  5.68       130     64           0         40   106         1  39        17\n165  8.22       148     64           0         58   141         2  27        13\n166  0.37       147     58           7        100   191         1  27        15\n167  6.71       119     67          17        151   137         2  55        11\n168  6.71       106     73           0        216    93         2  60        13\n169  7.30       129     89           0        425   117         2  45        10\n170 11.48       104     41          15        492    77         3  73        18\n171  8.01       128     39          12        356   118         2  71        10\n172 12.49        93    106          12        416    55         2  75        15\n173  9.03       104    102          13        123   110         3  35        16\n174  6.38       135     91           5        207   128         2  66        18\n175  0.00       139     24           0        358   185         2  79        15\n176  7.54       115     89           0         38   122         2  25        12\n177  5.61       138    107           9        480   154         2  47        11\n178 10.48       138     72           0        148    94         2  27        17\n179 10.66       104     71          14         89    81         2  25        14\n180  7.78       144     25           3         70   116         2  77        18\n181  4.94       137    112          15        434   149         1  66        13\n182  7.43       121     83           0         79    91         2  68        11\n183  4.74       137     60           4        230   140         1  25        13\n184  5.32       118     74           6        426   102         2  80        18\n185  9.95       132     33           7         35    97         2  60        11\n186 10.07       130    100          11        449   107         2  64        10\n187  8.68       120     51           0         93    86         2  46        17\n188  6.03       117     32           0        142    96         1  62        17\n189  8.07       116     37           0        426    90         2  76        15\n190 12.11       118    117          18        509   104         2  26        15\n191  8.79       130     37          13        297   101         2  37        13\n192  6.67       156     42          13        170   173         3  74        14\n193  7.56       108     26           0        408    93         2  56        14\n194 13.28       139     70           7         71    96         3  61        10\n195  7.23       112     98          18        481   128         2  45        11\n196  4.19       117     93           4        420   112         1  66        11\n197  4.10       130     28           6        410   133         1  72        16\n198  2.52       124     61           0        333   138         2  76        16\n199  3.62       112     80           5        500   128         2  69        10\n200  6.42       122     88           5        335   126         2  64        14\n201  5.56       144     92           0        349   146         2  62        12\n202  5.94       138     83           0        139   134         2  54        18\n203  4.10       121     78           4        413   130         1  46        10\n204  2.05       131     82           0        132   157         1  25        14\n205  8.74       155     80           0        237   124         2  37        14\n206  5.68       113     22           1        317   132         2  28        12\n207  4.97       162     67           0         27   160         2  77        17\n208  8.19       111    105           0        466    97         1  61        10\n209  7.78        86     54           0        497    64         1  33        12\n210  3.02        98     21          11        326    90         1  76        11\n211  4.36       125     41           2        357   123         1  47        14\n212  9.39       117    118          14        445   120         2  32        15\n213 12.04       145     69          19        501   105         2  45        11\n214  8.23       149     84           5        220   139         2  33        10\n215  4.83       115    115           3         48   107         2  73        18\n216  2.34       116     83          15        170   144         1  71        11\n217  5.73       141     33           0        243   144         2  34        17\n218  4.34       106     44           0        481   111         2  70        14\n219  9.70       138     61          12        156   120         2  25        14\n220 10.62       116     79          19        359   116         3  58        17\n221 10.59       131    120          15        262   124         2  30        10\n222  6.43       124     44           0        125   107         2  80        11\n223  7.49       136    119           6        178   145         2  35        13\n224  3.45       110     45           9        276   125         2  62        14\n225  4.10       134     82           0        464   141         2  48        13\n226  6.68       107     25           0        412    82         1  36        14\n227  7.80       119     33           0        245   122         3  56        14\n228  8.69       113     64          10         68   101         2  57        16\n229  5.40       149     73          13        381   163         1  26        11\n230 11.19        98    104           0        404    72         2  27        18\n231  5.16       115     60           0        119   114         1  38        14\n232  8.09       132     69           0        123   122         2  27        11\n233 13.14       137     80          10         24   105         3  61        15\n234  8.65       123     76          18        218   120         2  29        14\n235  9.43       115     62          11        289   129         3  56        16\n236  5.53       126     32           8         95   132         2  50        17\n237  9.32       141     34          16        361   108         2  69        10\n238  9.62       151     28           8        499   135         2  48        10\n239  7.36       121     24           0        200   133         3  73        13\n240  3.89       123    105           0        149   118         1  62        16\n241 10.31       159     80           0        362   121         2  26        18\n242 12.01       136     63           0        160    94         2  38        12\n243  4.68       124     46           0        199   135         2  52        14\n244  7.82       124     25          13         87   110         2  57        10\n245  8.78       130     30           0        391   100         2  26        18\n246 10.00       114     43           0        199    88         3  57        10\n247  6.90       120     56          20        266    90         1  78        18\n248  5.04       123    114           0        298   151         1  34        16\n249  5.36       111     52           0         12   101         2  61        11\n250  5.05       125     67           0         86   117         1  65        11\n251  9.16       137    105          10        435   156         3  72        14\n252  3.72       139    111           5        310   132         1  62        13\n253  8.31       133     97           0         70   117         2  32        16\n254  5.64       124     24           5        288   122         2  57        12\n255  9.58       108    104          23        353   129         3  37        17\n256  7.71       123     81           8        198    81         1  80        15\n257  4.20       147     40           0        277   144         2  73        10\n258  8.67       125     62          14        477   112         2  80        13\n259  3.47       108     38           0        251    81         1  72        14\n260  5.12       123     36          10        467   100         1  74        11\n261  7.67       129    117           8        400   101         1  36        10\n262  5.71       121     42           4        188   118         2  54        15\n263  6.37       120     77          15         86   132         2  48        18\n264  7.77       116     26           6        434   115         2  25        17\n265  6.95       128     29           5        324   159         3  31        15\n266  5.31       130     35          10        402   129         1  39        17\n267  9.10       128     93          12        343   112         3  73        17\n268  5.83       134     82           7        473   112         1  51        12\n269  6.53       123     57           0         66   105         2  39        11\n270  5.01       159     69           0        438   166         2  46        17\n271 11.99       119     26           0        284    89         3  26        10\n272  4.55       111     56           0        504   110         2  62        16\n273 12.98       113     33           0         14    63         3  38        12\n274 10.04       116    106           8        244    86         2  58        12\n275  7.22       135     93           2         67   119         2  34        11\n276  6.67       107    119          11        210   132         2  53        11\n277  6.93       135     69          14        296   130         2  73        15\n278  7.80       136     48          12        326   125         2  36        16\n279  7.22       114    113           2        129   151         3  40        15\n280  3.42       141     57          13        376   158         2  64        18\n281  2.86       121     86          10        496   145         1  51        10\n282 11.19       122     69           7        303   105         3  45        16\n283  7.74       150     96           0         80   154         3  61        11\n284  5.36       135    110           0        112   117         2  80        16\n285  6.97       106     46          11        414    96         1  79        17\n286  7.60       146     26          11        261   131         2  39        10\n287  7.53       117    118          11        429   113         2  67        18\n288  6.88        95     44           4        208    72         1  44        17\n289  6.98       116     40           0         74    97         2  76        15\n290  8.75       143     77          25        448   156         2  43        17\n291  9.49       107    111          14        400   103         2  41        11\n292  6.64       118     70           0        106    89         1  39        17\n293 11.82       113     66          16        322    74         3  76        15\n294 11.28       123     84           0         74    89         3  59        10\n295 12.66       148     76           3        126    99         3  60        11\n296  4.21       118     35          14        502   137         2  79        10\n297  8.21       127     44          13        160   123         3  63        18\n298  3.07       118     83          13        276   104         1  75        10\n299 10.98       148     63           0        312   130         3  63        15\n300  9.40       135     40          17        497    96         2  54        17\n301  8.57       116     78           1        158    99         2  45        11\n302  7.41        99     93           0        198    87         2  57        16\n303  5.28       108     77          13        388   110         1  74        14\n304 10.01       133     52          16        290    99         2  43        11\n305 11.93       123     98          12        408   134         3  29        10\n306  8.03       115     29          26        394   132         2  33        13\n307  4.78       131     32           1         85   133         2  48        12\n308  5.90       138     92           0         13   120         1  61        12\n309  9.24       126     80          19        436   126         2  52        10\n310 11.18       131    111          13         33    80         1  68        18\n311  9.53       175     65          29        419   166         2  53        12\n312  6.15       146     68          12        328   132         1  51        14\n313  6.80       137    117           5        337   135         1  38        10\n314  9.33       103     81           3        491    54         2  66        13\n315  7.72       133     33          10        333   129         3  71        14\n316  6.39       131     21           8        220   171         3  29        14\n317 15.63       122     36           5        369    72         3  35        10\n318  6.41       142     30           0        472   136         3  80        15\n319 10.08       116     72          10        456   130         3  41        14\n320  6.97       127     45          19        459   129         2  57        11\n321  5.86       136     70          12        171   152         2  44        18\n322  7.52       123     39           5        499    98         2  34        15\n323  9.16       140     50          10        300   139         3  60        15\n324 10.36       107    105          18        428   103         2  34        12\n325  2.66       136     65           4        133   150         1  53        13\n326 11.70       144     69          11        131   104         2  47        11\n327  4.69       133     30           0        152   122         2  53        17\n328  6.23       112     38          17        316   104         2  80        16\n329  3.15       117     66           1         65   111         1  55        11\n330 11.27       100     54           9        433    89         3  45        12\n331  4.99       122     59           0        501   112         1  32        14\n332 10.10       135     63          15        213   134         2  32        10\n333  5.74       106     33          20        354   104         2  61        12\n334  5.87       136     60           7        303   147         2  41        10\n335  7.63        93    117           9        489    83         1  42        13\n336  6.18       120     70          15        464   110         2  72        15\n337  5.17       138     35           6         60   143         1  28        18\n338  8.61       130     38           0        283   102         2  80        15\n339  5.97       112     24           0        164   101         2  45        11\n340 11.54       134     44           4        219   126         3  44        15\n341  7.50       140     29           0        105    91         1  43        16\n342  7.38        98    120           0        268    93         2  72        10\n343  7.81       137    102          13        422   118         2  71        10\n344  5.99       117     42          10        371   121         1  26        14\n345  8.43       138     80           0        108   126         3  70        13\n346  4.81       121     68           0        279   149         3  79        12\n347  8.97       132    107           0        144   125         2  33        13\n348  6.88        96     39           0        161   112         3  27        14\n349 12.57       132    102          20        459   107         3  49        11\n350  9.32       134     27          18        467    96         2  49        14\n351  8.64       111    101          17        266    91         2  63        17\n352 10.44       124    115          16        458   105         2  62        16\n353 13.44       133    103          14        288   122         3  61        17\n354  9.45       107     67          12        430    92         2  35        12\n355  5.30       133     31           1         80   145         2  42        18\n356  7.02       130    100           0        306   146         3  42        11\n357  3.58       142    109           0        111   164         3  72        12\n358 13.36       103     73           3        276    72         2  34        15\n359  4.17       123     96          10         71   118         1  69        11\n360  3.13       130     62          11        396   130         1  66        14\n361  8.77       118     86           7        265   114         3  52        15\n362  8.68       131     25          10        183   104         2  56        15\n363  5.25       131     55           0         26   110         1  79        12\n364 10.26       111     75           1        377   108         3  25        12\n365 10.50       122     21          16        488   131         3  30        14\n366  6.53       154     30           0        122   162         2  57        17\n367  5.98       124     56          11        447   134         2  53        12\n368 14.37        95    106           0        256    53         3  52        17\n369 10.71       109     22          10        348    79         3  74        14\n370 10.26       135    100          22        463   122         2  36        14\n371  7.68       126     41          22        403   119         1  42        12\n372  9.08       152     81           0        191   126         2  54        16\n373  7.80       121     50           0        508    98         2  65        11\n374  5.58       137     71           0        402   116         2  78        17\n375  9.44       131     47           7         90   118         2  47        12\n376  7.90       132     46           4        206   124         2  73        11\n377 16.27       141     60          19        319    92         3  44        11\n378  6.81       132     61           0        263   125         2  41        12\n379  6.11       133     88           3        105   119         2  79        12\n380  5.81       125    111           0        404   107         1  54        15\n381  9.64       106     64          10         17    89         2  68        17\n382  3.90       124     65          21        496   151         1  77        13\n383  4.95       121     28          19        315   121         2  66        14\n384  9.35        98    117           0         76    68         2  63        10\n385 12.85       123     37          15        348   112         3  28        12\n386  5.87       131     73          13        455   132         2  62        17\n387  5.32       152    116           0        170   160         2  39        16\n388  8.67       142     73          14        238   115         2  73        14\n389  8.14       135     89          11        245    78         1  79        16\n390  8.44       128     42           8        328   107         2  35        12\n391  5.47       108     75           9         61   111         2  67        12\n392  6.10       153     63           0         49   124         1  56        16\n393  4.53       129     42          13        315   130         1  34        13\n394  5.57       109     51          10         26   120         2  30        17\n395  5.35       130     58          19        366   139         1  33        16\n396 12.57       138    108          17        203   128         3  33        14\n397  6.14       139     23           3         37   120         2  55        11\n398  7.41       162     26          12        368   159         2  40        18\n399  5.94       100     79           7        284    95         1  50        12\n400  9.71       134     37           0         27   120         3  49        16\n    Urban US High\n1       1  1    1\n2       1  1    1\n3       1  1    1\n4       1  1    0\n5       1  0    0\n6       0  1    1\n7       1  0    0\n8       1  1    1\n9       0  0    0\n10      0  1    0\n11      0  1    1\n12      1  1    1\n13      1  0    0\n14      1  1    1\n15      1  1    1\n16      0  0    1\n17      1  0    0\n18      1  1    1\n19      0  1    1\n20      1  1    1\n21      1  1    0\n22      0  1    1\n23      1  0    0\n24      1  0    0\n25      1  1    1\n26      0  0    1\n27      0  1    1\n28      1  0    0\n29      1  1    0\n30      1  1    0\n31      1  0    1\n32      1  1    1\n33      0  1    0\n34      1  1    1\n35      1  1    0\n36      0  1    1\n37      0  0    1\n38      1  1    0\n39      1  0    0\n40      0  0    0\n41      0  0    0\n42      1  0    0\n43      1  0    1\n44      1  1    0\n45      1  1    0\n46      1  1    0\n47      0  1    1\n48      1  0    0\n49      1  0    0\n50      1  0    1\n51      1  1    0\n52      1  0    0\n53      1  1    0\n54      1  1    0\n55      0  1    0\n56      1  1    0\n57      1  0    1\n58      1  0    0\n59      1  1    0\n60      1  0    0\n61      1  1    1\n62      0  0    0\n63      1  1    0\n64      1  1    1\n65      0  1    0\n66      0  0    0\n67      1  0    1\n68      1  1    1\n69      1  1    1\n70      1  0    0\n71      1  1    1\n72      0  1    0\n73      1  0    0\n74      0  1    1\n75      0  1    0\n76      0  1    1\n77      1  1    1\n78      0  1    0\n79      1  1    0\n80      1  0    1\n81      1  1    1\n82      1  0    0\n83      1  1    1\n84      1  1    0\n85      0  0    0\n86      0  0    1\n87      1  0    1\n88      0  1    1\n89      1  1    0\n90      0  0    0\n91      0  0    0\n92      1  1    0\n93      1  0    0\n94      1  0    1\n95      1  1    1\n96      1  1    0\n97      0  1    1\n98      1  1    0\n99      0  1    1\n100     0  1    0\n101     0  1    0\n102     1  0    0\n103     0  0    0\n104     1  1    0\n105     1  0    0\n106     1  1    0\n107     0  0    0\n108     1  0    1\n109     1  0    0\n110     0  0    1\n111     1  1    1\n112     1  1    0\n113     1  1    0\n114     1  1    0\n115     1  1    1\n116     1  0    1\n117     0  0    0\n118     1  0    1\n119     1  1    0\n120     1  1    0\n121     1  1    0\n122     1  1    1\n123     1  1    0\n124     0  1    1\n125     1  0    1\n126     0  0    1\n127     1  1    1\n128     1  1    0\n129     1  1    0\n130     0  1    0\n131     1  1    1\n132     1  0    0\n133     1  1    1\n134     1  1    0\n135     1  0    0\n136     0  1    0\n137     0  0    0\n138     1  0    0\n139     1  1    1\n140     0  1    1\n141     1  1    0\n142     1  0    0\n143     1  0    0\n144     1  1    0\n145     0  0    1\n146     1  1    1\n147     1  0    0\n148     0  1    1\n149     0  1    0\n150     1  1    1\n151     0  1    1\n152     0  1    1\n153     0  0    0\n154     0  1    0\n155     0  1    0\n156     1  0    0\n157     1  0    0\n158     0  1    1\n159     0  1    1\n160     0  0    1\n161     0  0    0\n162     0  1    0\n163     1  0    0\n164     0  0    0\n165     0  1    1\n166     1  1    0\n167     1  1    0\n168     1  0    0\n169     1  0    0\n170     1  1    1\n171     1  1    1\n172     1  1    1\n173     1  1    1\n174     1  1    0\n175     0  0    0\n176     1  0    0\n177     0  1    0\n178     1  1    1\n179     0  1    1\n180     1  1    0\n181     1  1    0\n182     1  0    0\n183     1  0    0\n184     1  1    0\n185     0  1    1\n186     1  1    1\n187     0  0    1\n188     1  0    0\n189     1  0    1\n190     0  1    1\n191     0  1    1\n192     1  1    0\n193     0  0    0\n194     1  1    1\n195     1  1    0\n196     1  1    0\n197     1  1    0\n198     1  0    0\n199     1  1    0\n200     1  1    0\n201     0  0    0\n202     1  0    0\n203     0  1    0\n204     1  0    0\n205     1  0    1\n206     1  0    0\n207     1  1    0\n208     0  0    1\n209     1  0    0\n210     0  1    0\n211     0  1    0\n212     1  1    1\n213     1  1    1\n214     1  1    1\n215     1  1    0\n216     1  1    0\n217     1  0    0\n218     0  0    0\n219     1  1    1\n220     1  1    1\n221     1  1    1\n222     1  0    0\n223     1  1    0\n224     1  1    0\n225     0  0    0\n226     1  0    0\n227     1  0    0\n228     1  1    1\n229     0  1    0\n230     0  0    1\n231     0  0    0\n232     0  0    1\n233     1  1    1\n234     0  1    1\n235     0  1    1\n236     1  1    0\n237     1  1    1\n238     1  1    1\n239     1  0    0\n240     1  1    0\n241     1  0    1\n242     1  0    1\n243     0  0    0\n244     1  1    0\n245     1  0    1\n246     0  1    1\n247     1  1    0\n248     1  0    0\n249     1  1    0\n250     1  0    0\n251     1  1    1\n252     1  1    0\n253     1  0    1\n254     0  1    0\n255     1  1    1\n256     1  1    0\n257     1  0    0\n258     1  1    1\n259     0  0    0\n260     0  1    0\n261     1  1    0\n262     1  1    0\n263     1  1    0\n264     1  1    0\n265     1  1    0\n266     1  1    0\n267     0  1    1\n268     0  1    0\n269     1  0    0\n270     1  0    0\n271     1  0    1\n272     1  0    0\n273     1  0    1\n274     1  1    1\n275     1  1    0\n276     1  1    0\n277     1  1    0\n278     1  1    0\n279     0  1    0\n280     1  1    0\n281     1  1    0\n282     0  1    1\n283     1  0    0\n284     0  0    0\n285     0  0    0\n286     1  1    0\n287     0  1    0\n288     1  1    0\n289     0  0    0\n290     1  1    1\n291     0  1    1\n292     1  0    0\n293     1  1    1\n294     1  0    1\n295     1  1    1\n296     0  1    0\n297     1  1    1\n298     1  1    0\n299     1  0    1\n300     0  1    1\n301     1  1    1\n302     1  1    0\n303     1  1    0\n304     1  1    1\n305     1  1    1\n306     1  1    1\n307     1  1    0\n308     1  0    0\n309     1  1    1\n310     1  1    1\n311     1  1    1\n312     1  1    0\n313     1  1    0\n314     1  0    1\n315     1  1    0\n316     1  1    0\n317     1  1    1\n318     0  0    0\n319     0  1    1\n320     0  1    0\n321     1  1    0\n322     1  0    0\n323     1  1    1\n324     1  1    1\n325     1  1    0\n326     1  1    1\n327     1  0    0\n328     1  1    0\n329     1  1    0\n330     1  1    1\n331     0  0    0\n332     1  1    1\n333     1  1    0\n334     1  1    0\n335     1  1    0\n336     1  1    0\n337     1  0    0\n338     1  0    1\n339     1  0    0\n340     1  1    1\n341     1  0    0\n342     0  0    0\n343     0  1    0\n344     1  1    0\n345     0  1    1\n346     1  0    0\n347     0  0    1\n348     0  0    0\n349     1  1    1\n350     0  1    1\n351     0  1    1\n352     0  1    1\n353     1  1    1\n354     0  1    1\n355     1  1    0\n356     1  0    0\n357     1  0    0\n358     1  1    1\n359     1  1    0\n360     1  1    0\n361     0  1    1\n362     0  1    1\n363     1  1    0\n364     1  0    1\n365     1  1    1\n366     0  0    0\n367     0  1    0\n368     1  0    1\n369     0  1    1\n370     1  1    1\n371     1  1    0\n372     1  0    1\n373     0  0    0\n374     1  0    0\n375     1  1    1\n376     1  0    0\n377     1  1    1\n378     0  0    0\n379     1  1    0\n380     1  0    0\n381     1  1    1\n382     1  1    0\n383     1  1    0\n384     1  0    1\n385     1  1    1\n386     1  1    0\n387     1  0    0\n388     0  1    1\n389     1  1    1\n390     1  1    1\n391     1  1    0\n392     1  0    0\n393     1  1    0\n394     0  1    0\n395     1  1    0\n396     1  1    1\n397     0  1    0\n398     1  1    0\n399     1  1    0\n400     1  1    1\n\n\nCode\ncarseats_num %&gt;% \n  dlookr::correlate() %&gt;% \n  plot()\n\n\n\n\n\n\n\n\n\nWe now use the tree() function to fit a classification tree in order to predict High using all variables but Sales. The syntax of the tree() function is quite similar to that of the lm() function.\n\n\nCode\ntree.carseats &lt;- tree(High ~ . - Sales, Carseats)\n\n\nThe summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.\n\n\nCode\nsummary(tree.carseats)\n\n\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n\n\nWe see that the training error rate is \\(9\\%\\). For classification trees, the deviance reported in the output of summary() is given by\n\\[\n-2 \\sum_m \\sum_k n_{mk} \\log \\hat{p}_{mk},\n\\]\nwhere \\(n_{mk}\\) is the number of observations in the \\(m\\)th terminal node that belong to the \\(k\\)th class. This is closely related to the entropy. A small deviance indicates a tree that provides a good fit to the (training) data. The residual mean deviance reported is simply the deviance divided by \\(n-|{T}_0|\\), which in this case is \\(400-27=373\\).\nOne of the most attractive properties of trees is that they can be graphically displayed. We use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category.\n\n\nCode\nplot(tree.carseats)\ntext(tree.carseats, pretty = 0)\n\n\n\n\n\n\n\n\n\nThe most important indicator of Sales appears to be shelving location, since the first branch differentiates Good locations from Bad and Medium locations.\nIf we just type the name of the tree object, R prints output corresponding to each branch of the tree. R displays the split criterion (e.g. Price &lt; 92.5), the number of observations in that branch, the deviance, the overall prediction for the branch (Yes or No), and the fraction of observations in that branch that take on values of Yes and No. Branches that lead to terminal nodes are indicated using asterisks.\n\n\nCode\ntree.carseats\n\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 400 541.500 No ( 0.59000 0.41000 )  \n    2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  \n      4) Price &lt; 92.5 46  56.530 Yes ( 0.30435 0.69565 )  \n        8) Income &lt; 57 10  12.220 No ( 0.70000 0.30000 )  \n         16) CompPrice &lt; 110.5 5   0.000 No ( 1.00000 0.00000 ) *\n         17) CompPrice &gt; 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *\n        9) Income &gt; 57 36  35.470 Yes ( 0.19444 0.80556 )  \n         18) Population &lt; 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *\n         19) Population &gt; 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *\n      5) Price &gt; 92.5 269 299.800 No ( 0.75465 0.24535 )  \n       10) Advertising &lt; 13.5 224 213.200 No ( 0.81696 0.18304 )  \n         20) CompPrice &lt; 124.5 96  44.890 No ( 0.93750 0.06250 )  \n           40) Price &lt; 106.5 38  33.150 No ( 0.84211 0.15789 )  \n             80) Population &lt; 177 12  16.300 No ( 0.58333 0.41667 )  \n              160) Income &lt; 60.5 6   0.000 No ( 1.00000 0.00000 ) *\n              161) Income &gt; 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n             81) Population &gt; 177 26   8.477 No ( 0.96154 0.03846 ) *\n           41) Price &gt; 106.5 58   0.000 No ( 1.00000 0.00000 ) *\n         21) CompPrice &gt; 124.5 128 150.200 No ( 0.72656 0.27344 )  \n           42) Price &lt; 122.5 51  70.680 Yes ( 0.49020 0.50980 )  \n             84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *\n             85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  \n              170) Price &lt; 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *\n              171) Price &gt; 109.5 24  32.600 No ( 0.58333 0.41667 )  \n                342) Age &lt; 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *\n                343) Age &gt; 49.5 11   6.702 No ( 0.90909 0.09091 ) *\n           43) Price &gt; 122.5 77  55.540 No ( 0.88312 0.11688 )  \n             86) CompPrice &lt; 147.5 58  17.400 No ( 0.96552 0.03448 ) *\n             87) CompPrice &gt; 147.5 19  25.010 No ( 0.63158 0.36842 )  \n              174) Price &lt; 147 12  16.300 Yes ( 0.41667 0.58333 )  \n                348) CompPrice &lt; 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *\n                349) CompPrice &gt; 152.5 5   5.004 No ( 0.80000 0.20000 ) *\n              175) Price &gt; 147 7   0.000 No ( 1.00000 0.00000 ) *\n       11) Advertising &gt; 13.5 45  61.830 Yes ( 0.44444 0.55556 )  \n         22) Age &lt; 54.5 25  25.020 Yes ( 0.20000 0.80000 )  \n           44) CompPrice &lt; 130.5 14  18.250 Yes ( 0.35714 0.64286 )  \n             88) Income &lt; 100 9  12.370 No ( 0.55556 0.44444 ) *\n             89) Income &gt; 100 5   0.000 Yes ( 0.00000 1.00000 ) *\n           45) CompPrice &gt; 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *\n         23) Age &gt; 54.5 20  22.490 No ( 0.75000 0.25000 )  \n           46) CompPrice &lt; 122.5 10   0.000 No ( 1.00000 0.00000 ) *\n           47) CompPrice &gt; 122.5 10  13.860 No ( 0.50000 0.50000 )  \n             94) Price &lt; 125 5   0.000 Yes ( 0.00000 1.00000 ) *\n             95) Price &gt; 125 5   0.000 No ( 1.00000 0.00000 ) *\n    3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  \n      6) Price &lt; 135 68  49.260 Yes ( 0.11765 0.88235 )  \n       12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  \n         24) Price &lt; 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n         25) Price &gt; 109 9  11.460 No ( 0.66667 0.33333 ) *\n       13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *\n      7) Price &gt; 135 17  22.070 No ( 0.64706 0.35294 )  \n       14) Income &lt; 46 6   0.000 No ( 1.00000 0.00000 ) *\n       15) Income &gt; 46 11  15.160 Yes ( 0.45455 0.54545 ) *\n\n\nIn order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. The predict() function can be used for this purpose. In the case of a classification tree, the argument type = \"class\" instructs R to return the actual class prediction. This approach leads to correct predictions for around \\(77 \\%\\) of the locations in the test data set.\n\n\nCode\nset.seed(2)\ntrain &lt;- sample(1:nrow(Carseats), 200)\nCarseats.test &lt;- Carseats[-train, ]\n\ntree.carseats &lt;- tree(High ~ . - Sales, Carseats,\n    subset = train)\n\ntree.pred &lt;- predict(tree.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, Carseats.test$High)\n\n\n         \ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n\n\nCode\n(104 + 50) / 200\n\n\n[1] 0.77\n\n\n(If you re-run the predict() function then you might get slightly different results, due to “ties”: for instance, this can happen when the training observations corresponding to a terminal node are evenly split between Yes and No response values.)\nNext, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration.\nWe use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance.\nThe cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to \\(\\alpha\\)).\n\n\nCode\nset.seed(7)\ncv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\n\nCode\ncv.carseats\n\n\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n\nCode\nplot(cv.carseats)\n\n\n\n\n\n\n\n\n\nDespite its name, dev corresponds to the number of cross-validation errors. The tree with 9 terminal nodes results in only 74 cross-validation errors. We plot the error rate as a function of both size and k.\n\n\nCode\npar(mfrow = c(1, 2))\nplot(cv.carseats$size, cv.carseats$dev, type = \"b\")\nplot(cv.carseats$k, cv.carseats$dev, type = \"b\")\n\n\n\n\n\n\n\n\n\nWe now apply the prune.misclass() function in order to prune the tree to obtain the nine-node tree.\n\n\nCode\nprune.carseats &lt;- prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n\n\n\n\n\n\n\n\n\nHow well does this pruned tree perform on the test data set? Once again, we apply the predict() function.\n\n\nCode\ntree.pred &lt;- predict(prune.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, Carseats.test$High)\n\n\n         \ntree.pred No Yes\n      No  97  25\n      Yes 20  58\n\n\nCode\n(97 + 58) / 200\n\n\n[1] 0.775\n\n\nNow \\(77.5 \\%\\) of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also slightly improved the classification accuracy.\nIf we increase the value of best, we obtain a larger pruned tree with lower classification accuracy:\n\n\nCode\nprune.carseats &lt;- prune.misclass(tree.carseats, best = 14)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n\n\n\n\n\n\n\n\n\nCode\ntree.pred &lt;- predict(prune.carseats, Carseats.test,\n    type = \"class\")\n\ntable(tree.pred, Carseats.test$High)\n\n\n         \ntree.pred  No Yes\n      No  102  31\n      Yes  15  52\n\n\nCode\n(102 + 52) / 200\n\n\n[1] 0.77\n\n\n\n\nClassification using rpart\nNow we are going to use a different library rpart to create classification trees. We will use the Iris dataset.\nData splitting\n\n\nCode\nset.seed(800)\ntrainIndex &lt;- sample(1:nrow(iris), 0.7 * nrow(iris)) \ntrainData &lt;- iris[trainIndex, ] \ntestData &lt;- iris[-trainIndex,]\n\n\nWe will build a Classification Tree to classify the species of iris flowers based on their sepal and petal measurements.\n\n\nCode\n# Train a Classification Tree \nct_model &lt;- rpart(Species ~., data = trainData, method = \"class\")\nct_model\n\n\nn= 105 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 105 67 virginica (0.3238095 0.3142857 0.3619048)  \n  2) Petal.Length&lt; 2.45 34  0 setosa (1.0000000 0.0000000 0.0000000) *\n  3) Petal.Length&gt;=2.45 71 33 virginica (0.0000000 0.4647887 0.5352113)  \n    6) Petal.Width&lt; 1.75 37  4 versicolor (0.0000000 0.8918919 0.1081081) *\n    7) Petal.Width&gt;=1.75 34  0 virginica (0.0000000 0.0000000 1.0000000) *\n\n\nCode\nplot(ct_model, margin = 0.1)  # Increase the margin parameter for more space\ntext(ct_model,  cex = 0.8)\n\n\n\n\n\n\n\n\n\nPredictions on the test dataset:\n\n\nCode\n# Predict the species on the test data\npredictions &lt;- predict(ct_model, newdata = testData, type = \"class\")\n\n# Evaluate the model with a confusion matrix\nconf_matrix &lt;- caret::confusionMatrix(predictions, testData$Species)\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         16         1\n  virginica       0          1        11\n\nOverall Statistics\n                                              \n               Accuracy : 0.9556              \n                 95% CI : (0.8485, 0.9946)    \n    No Information Rate : 0.3778              \n    P-Value [Acc &gt; NIR] : 0.000000000000000261\n                                              \n                  Kappa : 0.9326              \n                                              \n Mcnemar's Test P-Value : NA                  \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9412           0.9167\nSpecificity                 1.0000            0.9643           0.9697\nPos Pred Value              1.0000            0.9412           0.9167\nNeg Pred Value              1.0000            0.9643           0.9697\nPrevalence                  0.3556            0.3778           0.2667\nDetection Rate              0.3556            0.3556           0.2444\nDetection Prevalence        0.3556            0.3778           0.2667\nBalanced Accuracy           1.0000            0.9527           0.9432\n\n\nIn addition to providing the confusion matrix, the confusionMatrix() function provides some associated statistics; interpreting the results of a confusion matrix . We can see all these per class using:\n\n\nCode\nstats_by_class &lt;- conf_matrix$byClass\nround(stats_by_class, 3) # round to read easier\n\n\n                  Sensitivity Specificity Pos Pred Value Neg Pred Value\nClass: setosa           1.000       1.000          1.000          1.000\nClass: versicolor       0.941       0.964          0.941          0.964\nClass: virginica        0.917       0.970          0.917          0.970\n                  Precision Recall    F1 Prevalence Detection Rate\nClass: setosa         1.000  1.000 1.000      0.356          0.356\nClass: versicolor     0.941  0.941 0.941      0.378          0.356\nClass: virginica      0.917  0.917 0.917      0.267          0.244\n                  Detection Prevalence Balanced Accuracy\nClass: setosa                    0.356             1.000\nClass: versicolor                0.378             0.953\nClass: virginica                 0.267             0.943\n\n\nIn a multi-class classification problem, there is a separate metric for each class because we treat each class as the “positive” class while considering all other classes as “negative”. This approach provides a detailed evaluation of the model’s performance for each individual class. Example, calculating precision for versicolor: Precision, also known as the Positive Predictive Value (PPV) is calculated as: \\[\nPrecision = \\frac{True \\ Positives}{True\\ Positives+ False\\ Positives}\n\\] In our case\n\n\nCode\nconf_matrix$table\n\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         16         1\n  virginica       0          1        11\n\n\nVersicolor: True Positives: 16, False Positives: 1. Precision = 16/17 =0.94 (note that selecting a different split in the train/test dataset will give you different trees and different results)\nPruning The rpart function performs cross-validation to determine the optimal tree size based on the cp (cross-complexity \\(\\alpha\\)) parameter. The goal is to find the smallest tree with the lowest cross-validated error. You can visualize the cross-validated error as a function of the complexity parameter using plotcp(). This plot helps identify the optimal cp value\n\n\nCode\nplotcp(ct_model)\n\n\n\n\n\n\n\n\n\nalternative we can access the table of numbers using:\n\n\nCode\nprintcp(ct_model)\n\n\n\nClassification tree:\nrpart(formula = Species ~ ., data = trainData, method = \"class\")\n\nVariables actually used in tree construction:\n[1] Petal.Length Petal.Width \n\nRoot node error: 67/105 = 0.6381\n\nn= 105 \n\n       CP nsplit rel error   xerror     xstd\n1 0.50746      0  1.000000 1.059701 0.071565\n2 0.43284      1  0.492537 0.582090 0.073898\n3 0.01000      2  0.059701 0.074627 0.032570\n\n\nThis output helps you understand how the cross-validated error changes as the tree size increases and help discern the optimal tree size.\nWe can get the value programatically:\n\n\nCode\n# Prune the tree based on the optimal cp\noptimal_cp &lt;- ct_model$cptable[which.min(ct_model$cptable[,\"xerror\"]), \"CP\"]\noptimal_cp\n\n\n[1] 0.01\n\n\nIn our case, the optimal cp value is 0.01 which corresponds to our largest tree with 3 terminal nodes. In our case, this suggests that we don’t need to prune our tree. If we did require pruning, we could easily achieve this:\n\n\nCode\npruned_model &lt;- prune(ct_model, cp = optimal_cp)\n\nplot(pruned_model, margin = 0.1)  # Increase the margin parameter for more space\ntext(pruned_model,  cex = 0.8)"
  },
  {
    "objectID": "tree.html#tuning-parameters-for-boosting",
    "href": "tree.html#tuning-parameters-for-boosting",
    "title": "Tree Based Methods",
    "section": "6.1 Tuning parameters for boosting",
    "text": "6.1 Tuning parameters for boosting\nThe number of trees B. Unlike bagging and random forest, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross validation to select B.\nThe shrinkage parameter lambda, a small, positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small lambda can require using a very large value of B in order to achieve performance.\nThe number of splits d in each tree, which controls the complexity of the boosted ensemble. Often d=1 works well, in which each tree is a stump, consisting of a single split and resulting in an additive model. More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables. So with d=1 no interaction between parameters is allowed, and d=2 is a pairwise interaction.\n\nRegression with Boosting\nHere we use the gbm package, and within it the gbm() function, to fit boosted regression trees to the Boston data set. We run gbm() with the option distribution = \"gaussian\" since this is a regression problem; if it were a binary classification problem, we would use distribution = \"bernoulli\". The argument n.trees = 5000 indicates that we want \\(5000\\) trees, and the option interaction.depth = 4 limits the depth of each tree.\nExample of use: Train a gradient boosting model for regression gbm_model &lt;- gbm( formula = medv ~ ., # The model formula data = train_boston, # Training data distribution = “gaussian”, # Loss function for regression (gaussian for squared error) n.trees = 100, # Number of trees to build interaction.depth = 3, # Maximum depth of each tree shrinkage = 0.01, # Learning rate (step size reduction) n.minobsinnode = 10, # Minimum number of observations in the terminal nodes cv.folds = 5 # Number of cross-validation folds for evaluation )\n\n\nCode\nset.seed(42)\nboost.boston &lt;- gbm(\n  medv ~ ., \n  data = Boston[train, ],\n  distribution = \"gaussian\",\n  n.trees = 5000,\n  interaction.depth = 4)\n\n\nThe summary() function produces a relative influence plot and also outputs the relative influence statistics.\n\n\nCode\nsummary(boost.boston)\n\n\n\n\n\n\n\n\n\n\n\n            var     rel.inf\nrm           rm 44.40567454\nlstat     lstat 30.10922797\ncrim       crim  5.51512326\nnox         nox  5.40206684\ndis         dis  4.51780767\nage         age  4.35951069\nptratio ptratio  2.09717861\ntax         tax  1.60055211\nindus     indus  1.03276811\nrad         rad  0.78491069\nzn           zn  0.14283068\nchas       chas  0.03234884\n\n\n\n\nWe see that lstat and rm are by far the most important variables. We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.\n\n\nCode\nplot(boost.boston, i = \"rm\")\nplot(boost.boston, i = \"lstat\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now use the boosted model to predict medv on the test set:\n\n\nCode\nyhat.boost &lt;- predict(boost.boston,\n    newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)\n\n\n[1] 17.55734\n\n\nThe test MSE obtained is \\(18.39\\): this is superior to the test MSE of random forests and bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter \\(\\lambda\\). The default value is \\(0.001\\), but this is easily modified. Here we take \\(\\lambda=0.2\\).\n\n\nCode\nboost.boston &lt;- gbm(medv ~ ., data = Boston[train, ],\n    distribution = \"gaussian\", n.trees = 5000,\n    interaction.depth = 4, shrinkage = 0.2, verbose = F)\nyhat.boost &lt;- predict(boost.boston,\n    newdata = Boston[-train, ], n.trees = 5000)\nmean((yhat.boost - boston.test)^2)\n\n\n[1] 17.1414\n\n\nIn this case, using \\(\\lambda=0.2\\) leads to a lower test MSE than \\(\\lambda=0.001\\).\nWe will have to use cross validation to select the best parameters for the number of trees, the depth tuning parameter and shrinkage parameter and fit our model according to the results. This, makes boosting a more time consuming method than random forest, that requires very little tuning, but if one is willing to do the job, usually boosting outperform random forest.\nWe are not going to do this in this exercise.\n\n\nCode\nn_trees &lt;- seq(from=100, to=5000, by =100)\n\npredmat &lt;- predict(boost.boston, newdata=Boston[-train,],n.trees=n_trees)\nerr&lt;- with(Boston[-train,], apply((predmat - medv)^2,2,mean))\nn_trees[which.min(err)]\n\n\n[1] 300\n\n\nCode\nplot(n_trees, err, pch=19, ylab=\"MSE\", xlab=\"N trees\", main= \"Boosting Test Error\")\n\n\n\n\n\nlet’s do cross validation:\n\n\nCode\n#cross-validation\nboston.boost.cv &lt;- gbm(medv~., data = Boston[train,], \n  distribution = \"gaussian\", n.trees=5000, \n  interaction.depth=4, shrinkage = 0.2, verbose=F, cv.folds=10)\n\n\n#find the best prediction\nbestTreeForPrediction &lt;- gbm.perf(boston.boost.cv)\n\n\n\n\n\nwe now predict using this tree:\n\n\nCode\nyhat.boost = predict(boston.boost.cv, newdata = Boston[-train,],n.trees = bestTreeForPrediction)\nround(mean((yhat.boost-boston.test)^2),2)\n\n\n[1] 19.05\n\n\n\n\nClassification with Boosting\n\n\nCode\n# Train a boosting model\ngbm_model &lt;- gbm(Species ~ .,\n  data = trainData, \n  distribution = \"multinomial\",\n  n.trees = 100, \n  interaction.depth = 3, \n  shrinkage = 0.1, \n  cv.folds = 5)\nsummary(gbm_model)\n\n\n\n\n\n\n\n\n\n                      var   rel.inf\nPetal.Length Petal.Length 56.150324\nPetal.Width   Petal.Width 33.938916\nSepal.Width   Sepal.Width  5.621333\nSepal.Length Sepal.Length  4.289426\n\n\nTo evaluate our model let’s test it in our test set:\n\n\nCode\n# Make predictions on the test set\ngbm_pred &lt;- predict(gbm_model, \n  newdata = testData, \n  n.trees = 100, \n  type = \"response\")\ngbm_pred_class &lt;- colnames(gbm_pred)[apply(gbm_pred, 1, which.max)]\n\n# Confusion matrix\nboost_reg_conf_mat = caret::confusionMatrix(factor(gbm_pred_class), testData$Species)\nboost_reg_conf_mat\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         16          0         0\n  versicolor      0         16         0\n  virginica       0          1        12\n\nOverall Statistics\n                                               \n               Accuracy : 0.9778               \n                 95% CI : (0.8823, 0.9994)     \n    No Information Rate : 0.3778               \n    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022\n                                               \n                  Kappa : 0.9664               \n                                               \n Mcnemar's Test P-Value : NA                   \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9412           1.0000\nSpecificity                 1.0000            1.0000           0.9697\nPos Pred Value              1.0000            1.0000           0.9231\nNeg Pred Value              1.0000            0.9655           1.0000\nPrevalence                  0.3556            0.3778           0.2667\nDetection Rate              0.3556            0.3556           0.2667\nDetection Prevalence        0.3556            0.3556           0.2889\nBalanced Accuracy           1.0000            0.9706           0.9848\n\n\nFor each species:\n\n\nCode\nboost_reg_conf_mat$byClass\n\n\n                  Sensitivity Specificity Pos Pred Value Neg Pred Value\nClass: setosa       1.0000000    1.000000      1.0000000      1.0000000\nClass: versicolor   0.9411765    1.000000      1.0000000      0.9655172\nClass: virginica    1.0000000    0.969697      0.9230769      1.0000000\n                  Precision    Recall       F1 Prevalence Detection Rate\nClass: setosa     1.0000000 1.0000000 1.000000  0.3555556      0.3555556\nClass: versicolor 1.0000000 0.9411765 0.969697  0.3777778      0.3555556\nClass: virginica  0.9230769 1.0000000 0.960000  0.2666667      0.2666667\n                  Detection Prevalence Balanced Accuracy\nClass: setosa                0.3555556         1.0000000\nClass: versicolor            0.3555556         0.9705882\nClass: virginica             0.2888889         0.9848485"
  },
  {
    "objectID": "unsupervisedLearning.html",
    "href": "unsupervisedLearning.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(ISLR2)\nThis chapter will focus on unsupervised learning, a set of statistical tools intended for the setting in which we have only a set of features X1,X2, . . . ,Xp measured on n observations. We are not interested in prediction, because we do not have an associated response variable Y . Rather, the goal is to discover interesting things about the measurements on X1,X2, . . . ,Xp. Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. In this chapter, we will focus on two particular types of unsupervised learning: principal components analysis, a tool used for data visualization or data pre-processing before supervised techniques are applied, and clustering, a broad class of methods for discovering unknown subgroups in data. Unsupervised learning is often performed as part of an exploratory data analysis. Examples of goals of unsupervised learning can be: An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. A search engine might choose which search results to display to a particular individual based on the click histories of other individuals with similar search patterns. These statistical learning tasks, and many more, can be performed via unsupervised learning techniques."
  },
  {
    "objectID": "unsupervisedLearning.html#proportion-variance-explained",
    "href": "unsupervisedLearning.html#proportion-variance-explained",
    "title": "Unsupervised Learning",
    "section": "1.1 Proportion Variance Explained",
    "text": "1.1 Proportion Variance Explained\nTo understand the strength of each component, we are interested in knowing the proportion of variance explained (PVE) by each one. The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as \\[\n\\sum^p_{j=1}Var(X_j) = \\sum^p_{j=1}\\frac{1}{n}\\sum^n_{i=1}x^2_{ij}\n\\] and the variance explained by the \\(m\\)th principal component is : \\[\nVar(Z_m) = \\frac{1}{n}\\sum^n_{i=1}z^2_{im}\n\\] and \\[\n\\sum^p_{j=1}Var(X_j) = \\sum ^M_{m=1} Var(Z_m)\n\\] therefore, the PVE of the \\(m\\)th principal component is given by the positive quantity between 0 and 1, because we can talk about the proportion of variance explained by looking at the variance of an individual z relative to the sum of the variances of all of the components. And that gives you an idea of the importance of each of the components.\n\nPrincipal Components\nWe are going to take a look at the USArrests data set, which is part of the base R package as an example for this lecture. For each of the fifty states in the United States, the data set constrains the number of arrest per 100k residents for each of three crimes: Assault, Murder and Rape. We also record the percentage of the population in each state living in urban areas (UrbanPop). The principal component score vectors have length \\(n=50\\) and the principal component loading vectors have length \\(p=4\\). We standardize each variable to have mean 0 and standard deviation 1\nThe columns of the data set contain the four variables.\n\n\nCode\nnames(USArrests)\n\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n\nWe first briefly examine the data. We notice that the variables have vastly different means.\n\n\nCode\napply(USArrests, 2, mean)\n\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nNote that the apply() function allows us to apply a function—in this case, the mean() function—to each row or column of the data set. The second input here denotes whether we wish to compute the mean of the rows, \\(1\\), or the columns, \\(2\\). We see that there are on average three times as many rapes as murders, and more than eight times as many assaults as rapes. We can also examine the variances of the four variables using the apply() function.\n\n\nCode\napply(USArrests, 2, var)\n\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n\nCode\napply(USArrests, 2, sd)\n\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\nNot surprisingly, the variables also have vastly different variances: The UrbanPop variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes in each state per 100,000 individuals. If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the Assault variable, since it has by far the largest mean and variance. Thus, it is important to standardize the variables to have mean zero and standard deviation one before performing PCA.\nWe now perform principal components analysis using the prcomp() function, which is one of several functions in R that perform PCA.\n\n\nCode\npr.out &lt;- prcomp(USArrests, scale = TRUE)\n\n\nBy default, the prcomp() function centers the variables to have mean zero. By using the option scale = TRUE, we scale the variables to have standard deviation one. The output from prcomp() contains a number of useful quantities.\n\n\nCode\nnames(pr.out)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nThe center and scale components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.\n\n\nCode\npr.out$center\n\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nCode\npr.out$scale\n\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\nThe rotation matrix provides the principal component loadings; each column of pr.out$rotation contains the corresponding principal component loading vector.\n\n\nCode\npr.out$rotation\n\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\nWe see that there are four distinct principal components. This is to be expected because there are in general \\(\\min(n-1,p)\\) informative principal components in a data set with \\(n\\) observations and \\(p\\) variables.\nThe sign of the principal components is irrelevant, so being negative does not mean anything because we are measuring variance.\nWe see that the first principal component is mainly loaded with the crime stats, while the second one is more loaded by the urban population.\nUsing the prcomp() function, we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors. Rather the \\(50 \\times 4\\) matrix x has as its columns the principal component score vectors. That is, the \\(k\\)th column is the \\(k\\)th principal component score vector.\n\n\nCode\ndim(pr.out$x)\n\n\n[1] 50  4\n\n\nWe can plot the first two principal components as follows:\n\n\nCode\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\n\n\n\nThe scale = 0 argument to biplot() ensures that the arrows are scaled to represent the loadings; other values for scale give slightly different biplots with different interpretations.\nIn this case the component was negative, it had negative loading. Ans so negative scores in that component means negative times negative is positive so the state of Florida is high crime while being in the end of the negative x axis.\nThe Y axis is showing the second component so New Yersey has a high urban population while Arkansas is on the lower side.\nWe can alter that to see a more human readable representation:\n\n\nCode\npr.out$rotation = -pr.out$rotation\npr.out$x = -pr.out$x\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\n\n\n\nThe prcomp() function also outputs the standard deviation of each principal component. For instance, on the USArrests data set, we can access these standard deviations as follows:\n\n\nCode\npr.out$sdev\n\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\n\nThe variance explained by each principal component is obtained by squaring these:\n\n\nCode\npr.var &lt;- pr.out$sdev^2\npr.var\n\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n\nTo compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components:\n\n\nCode\npve &lt;- pr.var / sum(pr.var)\npve\n\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n\n\nWe see that the first principal component explains \\(62.0 \\%\\) of the variance in the data, the next principal component explains \\(24.7 \\%\\) of the variance, and so forth. We can plot the PVE explained by each component, as well as the cumulative PVE, as follows:\n\n\nCode\npar(mfrow = c(1, 2))\nplot(pve, xlab = \"Principal Component\",\n    ylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\n    type = \"b\")\nplot(cumsum(pve), xlab = \"Principal Component\",\n    ylab = \"Cumulative Proportion of Variance Explained\",\n    ylim = c(0, 1), type = \"b\")\n\n\n\n\n\n\n\n\n\nThe result is shown in Figure 12.3. Note that the function cumsum() computes the cumulative sum of the elements of a numeric vector."
  },
  {
    "objectID": "unsupervisedLearning.html#k-means-clustering",
    "href": "unsupervisedLearning.html#k-means-clustering",
    "title": "Unsupervised Learning",
    "section": "3.1 k-means clustering",
    "text": "3.1 k-means clustering\nLet’s define \\(C_1,\\dots,C_k\\) clusters that holds the indices of each of the observations pertaining to that cluster. There is no overlap between the clusters, so one observations can only pertain to one clusters, and all the observations are registered in one of the clusters. The idea behind \\(K\\)-meand clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation (WCV) for cluster \\(C_k\\) is a measure WCV(\\(C_k\\)) of the amount by which the observations within a cluster differ from each other. WE want to minimize:\n\\[\n\\begin{equation}\n\\begin{aligned}\n& \\text{minimize}_{C_1, \\ldots, C_k} \\left\\{ \\sum_{k=1}^{K} WCV(C_k) \\right\\}\n\\end{aligned}\n\\end{equation}\n\\] Most of the time to use euclidean distance for this: we define WCV to be the pair wise squared distance between each pair of observations in the cluster.\nFirst we randomly assign a number from 1 to \\(k\\) to each of the observations. These serve as initial cluster assignments for the observations. Next we iterate until the cluster assignments stop changing: - For each of the \\(K\\) clusters, compute the cluster centroid. The \\(k\\)th cluster centroid is the vector of the \\(p\\) feature means for the observations in the \\(k\\)th cluster. - Assign each observation to the cluster whose centroid is closest (euclidean distance)\nThis method is guaranteed to decrease the value of the objective at each step, however, it is not guaranteed to give the global minimum.\n\n\n\n\n\n\n\n\n\nWhen the function is not convex and has more than one minimum, it is possible to get different solutions for the algorithm when we run it because the starting points are random, so they may pick up different valleys.\n\n\\(K\\)-Means Clustering\nThe function kmeans() performs \\(K\\)-means clustering in R. We begin with a simple simulated example in which there truly are two clusters in the data: the first 25 observations have a mean shift relative to the next 25 observations.\n\n\nCode\nset.seed(2)\nx &lt;- matrix(rnorm(50 * 2), ncol = 2)\nx[1:25, 1] &lt;- x[1:25, 1] + 3\nx[1:25, 2] &lt;- x[1:25, 2] - 4\nhead(x)\n\n\n         [,1]      [,2]\n[1,] 2.103085 -4.838287\n[2,] 3.184849 -1.933699\n[3,] 4.587845 -4.562247\n[4,] 1.869624 -2.724284\n[5,] 2.919748 -5.047573\n[6,] 3.132420 -5.965878\n\n\nCode\nplot(x)\n\n\n\n\n\n\n\n\n\nWe now perform \\(K\\)-means clustering with \\(K=2\\).\n\n\nCode\nkm.out &lt;- kmeans(x, 2, nstart = 20)\n\n\nThe cluster assignments of the 50 observations are contained in km.out$cluster.\n\n\nCode\nkm.out$cluster\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nThe \\(K\\)-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to kmeans(). We can plot the data, with each observation colored according to its cluster assignment.\n\n\nCode\nplot(x, col = (km.out$cluster + 1),\n    main = \"K-Means Clustering Results with K = 2\",\n    xlab = \"\", ylab = \"\", pch = 20, cex = 2)\n\n\n\n\n\n\n\n\n\nHere the observations can be easily plotted because they are two-dimensional. If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors.\nIn this example, we knew that there really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters. We could instead have performed \\(K\\)-means clustering on this example with \\(K=3\\).\n\n\nCode\nset.seed(4)\nkm.out &lt;- kmeans(x, 3, nstart = 20)\nkm.out\n\n\nK-means clustering with 3 clusters of sizes 17, 23, 10\n\nCluster means:\n        [,1]        [,2]\n1  3.7789567 -4.56200798\n2 -0.3820397 -0.08740753\n3  2.3001545 -2.69622023\n\nClustering vector:\n [1] 1 3 1 3 1 1 1 3 1 3 1 3 1 3 1 3 1 1 1 1 1 3 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 3 2 3 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 25.74089 52.67700 19.56137\n (between_SS / total_SS =  79.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nCode\nplot(x, col = (km.out$cluster + 1),\n    main = \"K-Means Clustering Results with K = 3\",\n    xlab = \"\", ylab = \"\", pch = 20, cex = 2)\n\n\n\n\n\n\n\n\n\nWhen \\(K=3\\), \\(K\\)-means clustering splits up the two clusters.\nTo run the kmeans() function in R with multiple initial cluster assignments, we use the nstart argument. If a value of nstart greater than one is used, then \\(K\\)-means clustering will be performed using multiple random assignments in Step~1 of Algorithm, and the kmeans() function will report only the best results. Here we compare using nstart = 1 to nstart = 20.\n\n\nCode\nset.seed(4)\nkm.out &lt;- kmeans(x, 3, nstart = 1)\nkm.out$tot.withinss\n\n\n[1] 104.3319\n\n\nCode\nkm.out &lt;- kmeans(x, 3, nstart = 20)\nkm.out$tot.withinss\n\n\n[1] 97.97927\n\n\nNote that km.out$tot.withinss is the total within-cluster sum of squares, which we seek to minimize by performing \\(K\\)-means clustering. The individual within-cluster sum-of-squares are contained in the vector km.out$withinss.\nWe strongly recommend always running \\(K\\)-means clustering with a large value of nstart, such as 20 or 50, since otherwise an undesirable local optimum may be obtained.\nWhen performing \\(K\\)-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the set.seed() function. This way, the initial cluster assignments in Step~1 can be replicated, and the \\(K\\)-means output will be fully reproducible."
  },
  {
    "objectID": "unsupervisedLearning.html#hierarchical-clustering",
    "href": "unsupervisedLearning.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "3.2 Hierarchical Clustering",
    "text": "3.2 Hierarchical Clustering\n\\(k\\)-means clustering requires us to pre-specify the number of clusters \\(K\\), this can be a disadvantage. Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of \\(K\\). In this section we describe bottom-up or agglomerative clustering. This is the most common type of hierarchical clustering, and refers to the fact that a dendogram is built starting from the leaves and combining clusters up to the trunk.\nIt first find the closest pair of observations and then it looks for the next closest pair, and it continues like this except that it does not necessarily join only pairs of observations, it may also join an observation with an existing cluster or two clusters together. To join clusters together it can use four techniques, complete linkage would be using the distance from the points in each cluster that are the furthest apart, while single linkage does the opposite and measures the distance from the closest points in each cluster. Average linkage will use the mean between each pair of points in each clusters and centroid linkage calculates the centroid of each cluster and measure the distance between the centroids of the clusters. The most used methods are complete and average\nWhen we view the dendogram, the height of the lines is proportional to the distances between the points, so the smallest the distance, the smallest the lines in the dendogram.\nChoice of Dissimilarity Measure\nSo far we have talked about euclidean distance. An alternative is correlation-based distance which considers two observations to be similar if their features are highly correlated. This is an unusual use of correlation, which is normally computed between variables, here it is computed between observations profiles for each pair of observations. This is often used when features are actually measurements at different times, so you can think of each measurement for an individual as a series of points over time in a time series.\nPractical considerations As in principal components and \\(K\\)-means, scaling variables matters. Should the observations of features be standardized in some way? For instance, maybe the variables should be centered to have mean zero and scaled to have standard deviation one. If the variables are not in the same units, you should standardize them. If they are in the same unit, it is useful to do both, a trial standardizing them and another one leaving them as they are.\nEven though in hierarchical clustering you don’t choose the number of clusters to begin with, at some point you will need to look at the dendogram and choose the division that makes sense to you. There is no mathematical process that can do that choice for you.\n\nHierarchical clustering\nThe hclust() function implements hierarchical clustering in R. In the following example we use the data from the previous lab to plot the hierarchical clustering dendrogram using complete, single, and average linkage clustering, with Euclidean distance as the dissimilarity measure. We begin by clustering observations using complete linkage. The dist() function is used to compute the \\(50 \\times 50\\) inter-observation Euclidean distance matrix.\n\n\nCode\nhc.complete &lt;- hclust(dist(x), method = \"complete\")\nplot(hc.complete)\n\n\n\n\n\n\n\n\n\nWe could just as easily perform hierarchical clustering with average or single linkage instead:\n\n\nCode\nhc.average &lt;- hclust(dist(x), method = \"average\")\n\n\nhc.single &lt;- hclust(dist(x), method = \"single\")\n\n\nThe numbers at the bottom of the plot identify each observation.\n\n\nCode\npar(mfrow = c(1, 3))\nplot(hc.complete, main = \"Complete Linkage\",\n    xlab = \"\", sub = \"\", cex = .9)\nplot(hc.average, main = \"Average Linkage\",\n    xlab = \"\", sub = \"\", cex = .9)\nplot(hc.single, main = \"Single Linkage\",\n    xlab = \"\", sub = \"\", cex = .9)\n\n\n\n\n\n\n\n\n\nTo determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the cutree() function: The second argument to cutree() is the number of clusters we wish to obtain.\n\n\nCode\ncutree(hc.complete, 2)\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nCode\ncutree(hc.average, 2)\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2 2 2 2 2\n[39] 2 2 2 2 2 1 2 1 2 2 2 2\n\n\nCode\ncutree(hc.single, 2)\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nFor this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage identifies one point as belonging to its own cluster. A more sensible answer is obtained when four clusters are selected, although there are still two singletons.\n\n\nCode\ncutree(hc.single, 4)\n\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3\n[39] 3 3 3 4 3 3 3 3 3 3 3 3\n\n\nTo scale the variables before performing hierarchical clustering of the observations, we use the scale() function:\n\n\nCode\nxsc &lt;- scale(x)\nplot(hclust(dist(xsc), method = \"complete\"),\n    main = \"Hierarchical Clustering with Scaled Features\")\n\n\n\n\n\n\n\n\n\nCorrelation-based distance can be computed using the as.dist() function, which converts an arbitrary square symmetric matrix into a form that the hclust() function recognizes as a distance matrix. However, this only makes sense for data with at least three features since the absolute correlation between any two observations with measurements on two features is always 1. Hence, we will cluster a three-dimensional data set. This data set does not contain any true clusters.\n\n\nCode\nx &lt;- matrix(rnorm(30 * 3), ncol = 3)\ndd &lt;- as.dist(1 - cor(t(x)))\n\nplot(hclust(dd, method = \"complete\"),\n    main = \"Complete Linkage with Correlation-Based Distance\",\n    xlab = \"\", sub = \"\")"
  },
  {
    "objectID": "unsupervisedLearning.html#pca-on-the-nci60-data",
    "href": "unsupervisedLearning.html#pca-on-the-nci60-data",
    "title": "Unsupervised Learning",
    "section": "4.1 PCA on the NCI60 Data",
    "text": "4.1 PCA on the NCI60 Data\nWe first perform PCA on the data after scaling the variables (genes) to have standard deviation one, although one could reasonably argue that it is better not to scale the genes.\n\n\nCode\npr.out &lt;- prcomp(nci.data, scale = TRUE)\n\n\nWe now plot the first few principal component score vectors, in order to visualize the data. The observations (cell lines) corresponding to a given cancer type will be plotted in the same color, so that we can see to what extent the observations within a cancer type are similar to each other. We first create a simple function that assigns a distinct color to each element of a numeric vector. The function will be used to assign a color to each of the \\(64\\) cell lines, based on the cancer type to which it corresponds.\n\n\nCode\nCols &lt;- function(vec) {\n   cols &lt;- rainbow(length(unique(vec)))\n   return(cols[as.numeric(as.factor(vec))])\n }\n\n\nNote that the rainbow() function takes as its argument a positive integer, and returns a vector containing that number of distinct colors. We now can plot the principal component score vectors.\nLet’s take a look first at the first 10 rows and columns of the principal components data:\n\n\nCode\npr.out$x[1:10,1:10]\n\n\n          PC1       PC2         PC3         PC4        PC5         PC6\nV1  -19.68245  3.527748  -9.7354382   0.8177816 -12.511081   7.4129037\nV2  -22.90812  6.390938 -13.3725378  -5.5911088  -7.972471   3.6860385\nV3  -27.24077  2.445809  -3.5053437   1.3311502 -12.466296  17.2088846\nV4  -42.48098 -9.691742  -0.8830921  -3.4180227 -41.938370  27.0251739\nV5  -54.98387 -5.158121 -20.9291076 -15.7253986 -10.361364  12.8891587\nV6  -26.96488  6.727122 -21.6422924 -13.7323153   7.934827   0.7073495\nV7  -31.19930  3.833763 -30.1164573 -41.3381143  10.343814 -16.8686721\nV8  -22.15718 10.314872 -18.6100363  -6.8972287  -5.484858  11.6386342\nV9  -14.17784 15.983080 -19.6024756  -6.5140900  -3.773111  -7.9587522\nV10 -29.51480 23.805015  -5.8397059   9.9421666   3.424825  11.6025539\n            PC7         PC8         PC9       PC10\nV1  -14.0794288   3.1728943 -21.7662819  20.209601\nV2  -10.0636687   7.2355250 -22.1630786  13.016862\nV3  -10.2736408   2.6503302   0.2330832   6.308100\nV4  -17.3832231   0.5499935 -14.1524751 -15.843953\nV5  -12.4990865 -32.2580875   7.8348685  10.097440\nV6  -27.7843016 -31.0436472  10.8657699  -2.298071\nV7  -23.5116274   0.9405339 -14.1310690   7.703723\nV8  -11.6761995 -22.6670094   3.6494171   4.991671\nV9   13.0186059  -7.0386030  -0.9060585   7.901416\nV10  -0.5383865  -7.9571655  19.8738758  27.706370\n\n\nthe first and second principal components\n\n\nCode\npr.out$x[1:10, 1:2]\n\n\n          PC1       PC2\nV1  -19.68245  3.527748\nV2  -22.90812  6.390938\nV3  -27.24077  2.445809\nV4  -42.48098 -9.691742\nV5  -54.98387 -5.158121\nV6  -26.96488  6.727122\nV7  -31.19930  3.833763\nV8  -22.15718 10.314872\nV9  -14.17784 15.983080\nV10 -29.51480 23.805015\n\n\nthe first and third principal components\n\n\nCode\npr.out$x[1:10, c(1, 3)]\n\n\n          PC1         PC3\nV1  -19.68245  -9.7354382\nV2  -22.90812 -13.3725378\nV3  -27.24077  -3.5053437\nV4  -42.48098  -0.8830921\nV5  -54.98387 -20.9291076\nV6  -26.96488 -21.6422924\nV7  -31.19930 -30.1164573\nV8  -22.15718 -18.6100363\nV9  -14.17784 -19.6024756\nV10 -29.51480  -5.8397059\n\n\n\n\nCode\npar(mfrow = c(1, 2))\nplot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19,\n    xlab = \"Z1\", ylab = \"Z2\")\n\nplot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19,\n    xlab = \"Z1\", ylab = \"Z3\")\nlegend(\"topright\", legend = unique(nci.labs), col = unique(Cols(nci.labs)), pch = 19)\n\n\n\n\n\n\n\n\n\nOn the whole, cell lines corresponding to a single cancer type do tend to have similar values on the first few principal component score vectors. This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels.\nWe can obtain a summary of the proportion of variance explained (PVE) of the first few principal components using the summary() method for a prcomp object (we have truncated the printout):\n\n\nCode\nsummary(pr.out)\n\n\nImportance of components:\n                           PC1      PC2      PC3      PC4      PC5      PC6\nStandard deviation     27.8535 21.48136 19.82046 17.03256 15.97181 15.72108\nProportion of Variance  0.1136  0.06756  0.05752  0.04248  0.03735  0.03619\nCumulative Proportion   0.1136  0.18115  0.23867  0.28115  0.31850  0.35468\n                            PC7      PC8      PC9     PC10     PC11     PC12\nStandard deviation     14.47145 13.54427 13.14400 12.73860 12.68672 12.15769\nProportion of Variance  0.03066  0.02686  0.02529  0.02376  0.02357  0.02164\nCumulative Proportion   0.38534  0.41220  0.43750  0.46126  0.48482  0.50646\n                           PC13     PC14     PC15     PC16     PC17     PC18\nStandard deviation     11.83019 11.62554 11.43779 11.00051 10.65666 10.48880\nProportion of Variance  0.02049  0.01979  0.01915  0.01772  0.01663  0.01611\nCumulative Proportion   0.52695  0.54674  0.56590  0.58361  0.60024  0.61635\n                           PC19    PC20     PC21    PC22    PC23    PC24\nStandard deviation     10.43518 10.3219 10.14608 10.0544 9.90265 9.64766\nProportion of Variance  0.01594  0.0156  0.01507  0.0148 0.01436 0.01363\nCumulative Proportion   0.63229  0.6479  0.66296  0.6778 0.69212 0.70575\n                          PC25    PC26    PC27   PC28    PC29    PC30    PC31\nStandard deviation     9.50764 9.33253 9.27320 9.0900 8.98117 8.75003 8.59962\nProportion of Variance 0.01324 0.01275 0.01259 0.0121 0.01181 0.01121 0.01083\nCumulative Proportion  0.71899 0.73174 0.74433 0.7564 0.76824 0.77945 0.79027\n                          PC32    PC33    PC34    PC35    PC36    PC37    PC38\nStandard deviation     8.44738 8.37305 8.21579 8.15731 7.97465 7.90446 7.82127\nProportion of Variance 0.01045 0.01026 0.00988 0.00974 0.00931 0.00915 0.00896\nCumulative Proportion  0.80072 0.81099 0.82087 0.83061 0.83992 0.84907 0.85803\n                          PC39    PC40    PC41   PC42    PC43   PC44    PC45\nStandard deviation     7.72156 7.58603 7.45619 7.3444 7.10449 7.0131 6.95839\nProportion of Variance 0.00873 0.00843 0.00814 0.0079 0.00739 0.0072 0.00709\nCumulative Proportion  0.86676 0.87518 0.88332 0.8912 0.89861 0.9058 0.91290\n                         PC46    PC47    PC48    PC49    PC50    PC51    PC52\nStandard deviation     6.8663 6.80744 6.64763 6.61607 6.40793 6.21984 6.20326\nProportion of Variance 0.0069 0.00678 0.00647 0.00641 0.00601 0.00566 0.00563\nCumulative Proportion  0.9198 0.92659 0.93306 0.93947 0.94548 0.95114 0.95678\n                          PC53    PC54    PC55    PC56    PC57   PC58    PC59\nStandard deviation     6.06706 5.91805 5.91233 5.73539 5.47261 5.2921 5.02117\nProportion of Variance 0.00539 0.00513 0.00512 0.00482 0.00438 0.0041 0.00369\nCumulative Proportion  0.96216 0.96729 0.97241 0.97723 0.98161 0.9857 0.98940\n                          PC60    PC61    PC62    PC63      PC64\nStandard deviation     4.68398 4.17567 4.08212 4.04124 1.951e-14\nProportion of Variance 0.00321 0.00255 0.00244 0.00239 0.000e+00\nCumulative Proportion  0.99262 0.99517 0.99761 1.00000 1.000e+00\n\n\nUsing the plot() function, we can also plot the variance explained by the first few principal components.\n\n\nCode\nplot(pr.out)\n\n\n\n\n\n\n\n\n\nNote that the height of each bar in the bar plot is given by squaring the corresponding element of pr.out$sdev. However, it is more informative to plot the PVE of each principal component (i.e. a scree plot) and the cumulative PVE of each principal component. This can be done with just a little work.\n\n\nCode\npve &lt;- 100 * pr.out$sdev^2 / sum(pr.out$sdev^2)\npar(mfrow = c(1, 2))\nplot(pve,  type = \"o\", ylab = \"PVE\",\n    xlab = \"Principal Component\", col = \"blue\")\nplot(cumsum(pve), type = \"o\", ylab = \"Cumulative PVE\",\n    xlab = \"Principal Component\", col = \"brown3\")\n\n\n\n\n\n\n\n\n\n(Note that the elements of pve can also be computed directly from the summary, summary(pr.out)$importance[2, ], and the elements of cumsum(pve) are given by summary(pr.out)$importance[3, ].) We see that together, the first seven principal components explain around \\(40 \\%\\) of the variance in the data. This is not a huge amount of the variance. However, looking at the scree plot, we see that while each of the first seven principal components explain a substantial amount of variance, there is a marked decrease in the variance explained by further principal components. That is, there is an elbow in the plot after approximately the seventh principal component. This suggests that there may be little benefit to examining more than seven or so principal components (though even examining seven principal components may be difficult).\nTo see the loadings (here we only plot the first 10 rows of the first 10 PC:\n\n\nCode\n# Loadings\nabs(pr.out$rotation[1:10,1:10])\n\n\n           PC1          PC2         PC3         PC4         PC5         PC6\n1  0.010682370 0.0013244060 0.008503514 0.003524094 0.010126893 0.028903572\n2  0.002312078 0.0016752664 0.010256593 0.002603645 0.011400802 0.011242885\n3  0.005879750 0.0062894339 0.010055415 0.010681458 0.010264980 0.018449224\n4  0.003278071 0.0026661375 0.008361513 0.007475761 0.011248268 0.005553433\n5  0.007677535 0.0025080967 0.013820836 0.009509144 0.004094756 0.002731900\n6  0.002266671 0.0096779335 0.010818283 0.012751147 0.007196820 0.010587841\n7  0.008588379 0.0083617883 0.013556547 0.003419593 0.008980825 0.013541507\n8  0.004975711 0.0092527209 0.011433476 0.008848041 0.006268111 0.011404868\n9  0.004528170 0.0006385224 0.018869684 0.006181702 0.015130504 0.002502444\n10 0.008123112 0.0144309013 0.014013737 0.001973625 0.004247080 0.017522834\n            PC7         PC8          PC9        PC10\n1  0.0078245907 0.002031284 0.0021958070 0.011169672\n2  0.0018828759 0.002363234 0.0062465551 0.036961814\n3  0.0073309428 0.002452734 0.0006017505 0.021233368\n4  0.0038679774 0.021100577 0.0304323937 0.021461746\n5  0.0039558134 0.001171506 0.0215235298 0.011597183\n6  0.0104728454 0.002937423 0.0039773824 0.007049164\n7  0.0069567564 0.013385537 0.0009339968 0.019437655\n8  0.0002185486 0.008698437 0.0063185521 0.003604981\n9  0.0014271945 0.007366758 0.0073619558 0.012737682\n10 0.0236414654 0.005169351 0.0154250042 0.008311270\n\n\nInterpreting Loadings: The loadings tell you which variables (genes) are most influential in each principal component. High absolute values of loadings indicate variables that strongly influence that principal component. By examining the loadings and the variance explained by each principal component, you can understand which variables (genes) are driving the differences in your dataset."
  },
  {
    "objectID": "unsupervisedLearning.html#clustering-the-observations-of-the-nci60-data",
    "href": "unsupervisedLearning.html#clustering-the-observations-of-the-nci60-data",
    "title": "Unsupervised Learning",
    "section": "4.2 Clustering the Observations of the NCI60 Data",
    "text": "4.2 Clustering the Observations of the NCI60 Data\nWe now proceed to hierarchically cluster the cell lines in the NCI data, with the goal of finding out whether or not the observations cluster into distinct types of cancer. To begin, we standardize the variables to have mean zero and standard deviation one. As mentioned earlier, this step is optional and should be performed only if we want each gene to be on the same scale.\n\n\nCode\nsd.data &lt;- scale(nci.data)\n\n\nWe now perform hierarchical clustering of the observations using complete, single, and average linkage. Euclidean distance is used as the dissimilarity measure.\n\n\nCode\n#par(mfrow = c(1, 3))\ndata.dist &lt;- dist(sd.data)\nplot(hclust(data.dist), xlab = \"\", sub = \"\", ylab = \"\",\n    labels = nci.labs, main = \"Complete Linkage\")\n\n\n\n\n\n\n\n\n\nCode\nplot(hclust(data.dist, method = \"average\"),\n    labels = nci.labs, main = \"Average Linkage\",\n    xlab = \"\", sub = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\n\nCode\nplot(hclust(data.dist, method = \"single\"),\n    labels = nci.labs,  main = \"Single Linkage\",\n    xlab = \"\", sub = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\n\nWe see that the choice of linkage certainly does affect the results obtained. Typically, single linkage will tend to yield trailing clusters: very large clusters onto which individual observations attach one-by-one. On the other hand, complete and average linkage tend to yield more balanced, attractive clusters. For this reason, complete and average linkage are generally preferred to single linkage. Clearly cell lines within a single cancer type do tend to cluster together, although the clustering is not perfect. We will use complete linkage hierarchical clustering for the analysis that follows.\nWe can cut the dendrogram at the height that will yield a particular number of clusters, say four:\n\n\nCode\nhc.out &lt;- hclust(dist(sd.data))\nhc.clusters &lt;- cutree(hc.out, 4)\ntable(hc.clusters, nci.labs)\n\n\n           nci.labs\nhc.clusters BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro\n          1      2   3     2           0           0        0           0\n          2      3   2     0           0           0        0           0\n          3      0   0     0           1           1        6           0\n          4      2   0     5           0           0        0           1\n           nci.labs\nhc.clusters MCF7D-repro MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN\n          1           0        8     8       6        2     8       1\n          2           0        0     1       0        0     1       0\n          3           0        0     0       0        0     0       0\n          4           1        0     0       0        0     0       0\n\n\nThere are some clear patterns. All the leukemia cell lines fall in cluster \\(3\\), while the breast cancer cell lines are spread out over three different clusters. We can plot the cut on the dendrogram that produces these four clusters:\n\n\nCode\npar(mfrow = c(1, 1))\nplot(hc.out, labels = nci.labs)\nabline(h = 139, col = \"red\")\n\n\n\n\n\n\n\n\n\nThe abline() function draws a straight line on top of any existing plot in~R. The argument h = 139 plots a horizontal line at height \\(139\\) on the dendrogram; this is the height that results in four distinct clusters. It is easy to verify that the resulting clusters are the same as the ones we obtained using cutree(hc.out, 4).\nPrinting the output of hclust gives a useful brief summary of the object:\n\n\nCode\nhc.out\n\n\n\nCall:\nhclust(d = dist(sd.data))\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 64 \n\n\nWe claimed earlie that \\(K\\)-means clustering and hierarchical clustering with the dendrogram cut to obtain the same number of clusters can yield very different results.\nHow do these NCI hierarchical clustering results compare to what we get if we perform \\(K\\)-means clustering with \\(K=4\\)?\n\n\nCode\nset.seed(2)\nkm.out &lt;- kmeans(sd.data, 4, nstart = 20)\nkm.clusters &lt;- km.out$cluster\ntable(km.clusters, hc.clusters)\n\n\n           hc.clusters\nkm.clusters  1  2  3  4\n          1 11  0  0  9\n          2 20  7  0  0\n          3  9  0  0  0\n          4  0  0  8  0\n\n\nWe see that the four clusters obtained using hierarchical clustering and \\(K\\)-means clustering are somewhat different. Cluster \\(4\\) in \\(K\\)-means clustering is identical to cluster \\(3\\) in hierarchical clustering. However, the other clusters differ: for instance, cluster \\(2\\) in \\(K\\)-means clustering contains a portion of the observations assigned to cluster 1 by hierarchical clustering, as well as all of the observations assigned to cluster \\(2\\) by hierarchical clustering.\nRather than performing hierarchical clustering on the entire data matrix, we can simply perform hierarchical clustering on the first few principal component score vectors, as follows:\n\n\nCode\nhc.out &lt;- hclust(dist(pr.out$x[, 1:5]))\nplot(hc.out, labels = nci.labs,\n    main = \"Hier. Clust. on First Five Score Vectors\")\n\n\n\n\n\n\n\n\n\nCode\ntable(cutree(hc.out, 4), nci.labs)\n\n\n   nci.labs\n    BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro MCF7D-repro\n  1      0   2     7           0           0        2           0           0\n  2      5   3     0           0           0        0           0           0\n  3      0   0     0           1           1        4           0           0\n  4      2   0     0           0           0        0           1           1\n   nci.labs\n    MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN\n  1        1     8       5        2     7       0\n  2        7     1       1        0     2       1\n  3        0     0       0        0     0       0\n  4        0     0       0        0     0       0\n\n\nNot surprisingly, these results are different from the ones that we obtained when we performed hierarchical clustering on the full data set. Sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data. In this situation, we might view the principal component step as one of denoising the data. We could also perform \\(K\\)-means clustering on the first few principal component score vectors rather than the full data set. :::"
  },
  {
    "objectID": "probability.html#equal-tailed-posterior-intervals",
    "href": "probability.html#equal-tailed-posterior-intervals",
    "title": "Probability",
    "section": "15.1 Equal-Tailed Posterior Intervals",
    "text": "15.1 Equal-Tailed Posterior Intervals\nIn Bayesian statistics, equal-tailed posterior intervals (also known as credible intervals) are used to indicate the range within which a parameter is likely to lie, given the observed data and prior information. These intervals provide a measure of uncertainty for the parameter estimates.\nAn equal-tailed posterior interval for a parameter \\(\\theta\\) is an interval \\([a, b]\\) such that the probability of \\(\\theta\\) lying below the interval is equal to the probability of \\(\\theta\\) lying above the interval. In other words:\n\\[\nP(\\theta &lt; a \\mid \\text{data}) = P(\\theta &gt; b \\mid \\text{data}) = \\frac{\\alpha}{2}\n\\]\nwhere \\(\\alpha\\) is the significance level (e.g., \\(\\alpha = 0.05\\) for a 95% credible interval).\nTo calculate an equal-tailed posterior interval:\n\nDetermine the Posterior Distribution: Obtain the posterior distribution \\(p(\\theta \\mid \\text{data})\\) based on the observed data and prior information.\nFind the Interval Limits: Identify the values \\(a\\) and \\(b\\) such that the cumulative probability at \\(a\\) is \\(\\frac{\\alpha}{2}\\) and the cumulative probability at \\(b\\) is \\(1 - \\frac{\\alpha}{2}\\).\n\nSuppose we have a posterior distribution for \\(\\theta\\) given by \\(p(\\theta \\mid \\text{data})\\), and we want to calculate a 95% equal-tailed posterior interval. We need to find values \\(a\\) and \\(b\\) such that:\n\\[\nP(\\theta &lt; a \\mid \\text{data}) = 0.025 \\quad \\text{and} \\quad P(\\theta &gt; b \\mid \\text{data}) = 0.025\n\\]\nThe interval \\([a, b]\\) will then contain 95% of the posterior probability. If we want an interval that covers 95% of the probability, the tails will be \\(\\frac{1-0.95}{2}=0.025\\) so we calculate the probability of \\(theta\\) being less than that number.\n\nEqual tailed posterior interval.\nWe can think again about flipping a coin. He flipped a coin once and it came up head. We don’t know if the coin is fair or not.\nWe already calculated the posterior distribution for 𝜃 given𝑋=1 (one head observed) is:\n\\[\nP(\\theta \\mid x=1)= 2\\theta\n\\]\nWe want to find the 95% equal-tailed posterior interval for \\(\\theta\\).\nLower Limit We need to find \\(a\\) such that \\(P(\\theta \\leq a \\mid x=1) =0.025\\) The cumulative distribution function (CDF) for the posterior distribution is:\n\\[\nP(\\theta\\leq a \\mid x=1)=\\int_0^a 2\\theta d\\theta = a^2\n\\]We want \\(q\\) such that \\(q^2=0.025\\):\\[\nq = \\sqrt{0.025} \\approx 0.158\n\\]So, the lower limit \\(a\\) is approximately 0.158.\nUpper limit We need to find \\(b\\) such that \\(P(\\theta \\geq b \\mid x=1) =0.025\\) This is equivalent to finding \\(b\\) such that𝑃(𝜃≤𝑏∣𝑋=1)=0.975 : \\[\n\\int_0^b 2\\theta \\, d\\theta = b^2 = 0.975\n\\] \\[b = \\sqrt{0.975} \\approx 0.987\\]Therefore, the 95% equal-tailed posterior interval for 𝜃 is[0.158,0.987].\nTo verify, we can check that the probability of 𝜃 lying within this interval is indeed 0.95:\n\\[\nP(\\sqrt{0.025}\\leq 0\\leq \\sqrt{0.925}) = P(.158 \\leq 0 \\leq .987)= .95\n\\] Using the CDF: \\[\nP(\\theta \\leq 0.987 \\mid X=1) = (0.987)^2 \\approx 0.975\n\\]\n\n\nCode\n# Define the posterior distribution function\nposterior &lt;- function(theta) {\n  return(2 * theta)\n}\n\n# Define the lower and upper bounds of the interval\nlower_bound &lt;- 0\nupper_bound &lt;- 0.987\n\n# Compute the integral using the integrate() function\nresult &lt;- integrate(posterior, lower = lower_bound, upper = upper_bound)\n\n# Extract the value of the integral\nprobability &lt;- result$value\n\n# Print the result\ncat(\"P( θ &lt;= 0.987) =\", probability, \"\\n\")\n\n\nP( θ &lt;= 0.987) = 0.974169 \n\n\n\\[\nP(\\theta \\leq 0.158 \\mid X=1) = (0.158)^2 \\approx 0.025\n\\]\n\n\nCode\n# Define the posterior distribution function\nposterior &lt;- function(theta) {\n  return(2 * theta)\n}\n\n# Define the lower and upper bounds of the interval\nlower_bound &lt;- 0\nupper_bound &lt;- 0.158\n\n# Compute the integral using the integrate() function\nresult &lt;- integrate(posterior, lower = lower_bound, upper = upper_bound)\n\n# Extract the value of the integral\nprobability &lt;- result$value\n\n# Print the result\ncat(\"P( θ &lt;= 0.158) =\", probability, \"\\n\")\n\n\nP( θ &lt;= 0.158) = 0.024964 \n\n\nSo: \\(P(0.158 \\leq \\theta \\leq 0.987) = 0.975 - 0.025 = 0.95\\)\n\nAn equal-tailed posterior interval provides a range of values for the parameter \\(\\theta\\) that is consistent with the observed data and prior information. Unlike frequentist confidence intervals, credible intervals directly represent the probability of the parameter lying within the interval, given the data and prior information.\nEqual-tailed posterior intervals are a useful tool in Bayesian analysis for quantifying uncertainty and providing plausible ranges for parameter estimates. These intervals are derived from the posterior distribution and give a direct probability interpretation, making them intuitive and informative."
  },
  {
    "objectID": "probability.html#normalizing-constants-and-proportionality",
    "href": "probability.html#normalizing-constants-and-proportionality",
    "title": "Probability",
    "section": "15.1 Normalizing Constants and Proportionality",
    "text": "15.1 Normalizing Constants and Proportionality\nThe full expression for a posterior distribution of some parameter \\(\\theta\\) is given by\n\\[\nf(\\theta|x) = \\frac{f(x|\\theta)f(\\theta)}{\\int f(x|\\theta)f(\\theta)d\\theta}.\n\\]\nAs we will see in coming lessons, it is often more convenient to work with the numerator only: \\(f(\\theta|x) \\propto f(x|\\theta)f(\\theta)\\), which is the likelihood times the prior. The symbol \\(\\propto\\) stands for “is proportional to.” We can multiply a function of \\(\\theta\\) by any constant and maintain proportionality. For example, if \\(f(\\theta) = 5\\theta\\), then \\(f(\\theta) \\propto \\theta\\). However, \\(f(\\theta)\\) is not proportional to \\(\\theta + 1\\). We maintain proportionality only by modifying constants which are multiplied by the entire function \\(f(\\theta)\\). Hence \\(5(\\theta + 1) \\propto \\theta + 1\\).\nThe reason we can write \\(f(\\theta|x) \\propto f(x|\\theta)f(\\theta)\\) is because the denominator \\(\\int f(x|\\theta)f(\\theta)d\\theta\\) is free of \\(\\theta\\). It is just a normalizing constant. Therefore, we can ignore any multiplicative terms not involving \\(\\theta\\). For example, if \\(\\theta \\sim N(\\mu, \\sigma^2)\\), then\n\\[\nf(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(\\theta - \\mu)^2}{2\\sigma^2} \\right),\n\\]\nClearly, the expression in (1) does not integrate to 1 (it integrates to \\(\\sqrt{2\\pi\\sigma^2}\\)). Although it is not a PDF, it is proportional to the \\(N(\\mu, \\sigma^2)\\) PDF and can be normalized to represent the \\(N(\\mu, \\sigma^2)\\) distribution only. Likewise, the posterior \\(f(\\theta|x)\\) maintains its uniqueness as long as we specify it up to a proportionality constant.\nTo evaluate posterior quantities such as posterior probabilities, we will eventually need to find the normalizing constant. If the integral required is not tractable, we can often still simulate draws from the posterior and approximate posterior quantities. In some cases, we can identify \\(f(x|\\theta)f(\\theta)\\) as being proportional to the PDF of some known distribution. This will be a major topic of Lesson 6.\nRemember also that in the posterior distribution of \\(\\theta\\), we are treating \\(x\\) as a known constant."
  },
  {
    "objectID": "probability.html#probability-density-function-pdf",
    "href": "probability.html#probability-density-function-pdf",
    "title": "Probability",
    "section": "10.1 Probability Density Function (PDF)",
    "text": "10.1 Probability Density Function (PDF)\nThe Probability Density Function (PDF) is a function that describes the likelihood of a continuous random variable taking on a particular value. The PDF, denoted as $ f(x) $, represents the density of the probability distribution at each point $ x $. For a random variable $ X $ with a PDF $ f(x) $, the probability that $ X $ falls within a specific interval \\([a, b]\\) is given by the integral of $ f(x) $ over that interval:\n\\[P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx\\] A few important properties of the PDF:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\nThe area under the entire curve of the PDF is equal to 1:\n\n\\[\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\]\nThe PDF provides a way to understand the distribution and density of probabilities across different values of a continuous random variable."
  },
  {
    "objectID": "probability.html#maximum-likelihood-estimation",
    "href": "probability.html#maximum-likelihood-estimation",
    "title": "Probability",
    "section": "11.1 Maximum Likelihood Estimation",
    "text": "11.1 Maximum Likelihood Estimation\nThe likelihood function is used to estimate the parameters \\(\\theta\\) that are most likely to have generated the observed data. This is done through the method of Maximum Likelihood Estimation (MLE), which finds the parameter values that maximize the likelihood function.\nTheta (\\(\\theta\\)) is the value of the probability that has the highest likelihood for the data we have observed.\n\nLikelihood calculation (binomial distribution)\nLet’s consider an example where we are trying to infer the death rate of patients admitted to a specific hospital for heart attacks. Suppose we have the following data:\n\nNumber of patients admitted for heart attacks: 400\nNumber of patients who died: 72\n\nWe want to infer the death rate \\(p\\) for the whole region based on this data.\nIn this example, we are dealing with a binomial distribution, where each patient admitted for a heart attack can either survive or die. The probability of a patient dying is denoted by \\(p\\), and the probability of a patient surviving is \\(1-p\\).\nIn a binomial distribution, the probability of observing exactly \\(k\\) successes (deaths, in this case) out of \\(n\\) trials (patients) is given by the binomial probability formula: \\[P(X = k \\mid n, p) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nIn this case, the likelihood function can be expressed as:\n\\[\nL(p) = P(X=72 \\mid n=400, p) = \\binom {400}{72} p^{72}(1-p)^{328}\n\\] The binomial coefficient \\(\\binom{400}{72}\\) represents the number of ways to choose 72 out of 100 patients.\nTo find the Maximum Likelihood Estimate (MLE) of \\(p\\), we need to maximize the likelihood function \\(L(p)\\). This is typically done by taking the derivative of the log-likelihood function with respect to \\(p\\), setting it to zero, and solving for \\(p\\).\n\n\nCode\n# Data\nn &lt;- 400\nx &lt;- 72\n\n# Define the likelihood function\nlikelihood &lt;- function(p) {\n  choose(n, x) * p^x * (1 - p)^(n - x)\n}\n\n# Generate a sequence of p values for plotting\np &lt;- seq(0, 1, by = 0.01)\nlikelihood_values &lt;- sapply(p, likelihood)\n\n# Find the MLE by maximizing the likelihood function\nmle &lt;- p[which.max(likelihood_values)]\n\n# Plot the likelihood function\ndata &lt;- data.frame(\n  p = p,\n  Likelihood = likelihood_values\n)\n\nggplot(data, aes(x = p, y = Likelihood)) +\n  geom_line() +\n  geom_vline(xintercept = mle, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Likelihood Function\",\n       x = \"Survival Rate (p)\",\n       y = \"Likelihood\") +\n  annotate(\"text\", x = mle, y = max(likelihood_values), label = paste(\"MLE: p =\", round(mle, 2)), vjust = 2) +\n  theme_minimal()\n#define the logLikelihood function\nloglike = function(p){\n  x*log(p)+(n-x)*log(1-p)\n}\n\nplot(p,loglike(p), main = \"log-likelihood plot\")\nabline(v= mle)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s what the MLE tells us in this case:\nObserved Data: We have data from a specific hospital where 400 patients were admitted for heart attacks, and 72 of them survived.\nParameter of Interest: The survival rate 𝑝 for the whole region.\nLikelihood Function: The likelihood function represents how likely it is to observe 72 survivors out of 400 patients given different values of 𝑝.\nMLE: The value of 𝑝 that maximizes the likelihood function is the MLE. In this case, the MLE of 𝑝 is 0.18, which means that the estimated survival rate for the whole region based on this data is 18%.\nSo, the MLE provides the most likely value of the survival rate given the observed data.\n\n\nIs it a fair coin?\nWe know our brother has a loaded coin, that falls heads 75% of the times. He makes a bet with us that the coin is going to fall heads and tells us that he is not using the loaded coin. We don’t trust him so we agree that he will flip the coin five times to prove that is not loaded. It falls 2 heads and 3 tails. With this information you need to decide if it is the loaded coin or a fair coin and how confident you are so you can decide how much money you want to bet.\nWe start by defining our parameter \\(\\theta\\) to decide if it is a fair coin\\[\n\\theta = \\{ \\text fair, loaded\\}\n\\]Our data is a binomial of 5 trials \\(X \\sim \\text binomial (5,p)\\). So we can write our likelihood:\n\\[\nf(x \\mid \\theta) = \\begin{cases}  \\binom{5}{x} (\\frac{1}{2})^5 \\text { if fair} \\\\\n\\binom{5}{x} (0.7)^x (0.3)^{5-x} \\text  { if loaded}\n\\end{cases}\n\\] we can write the same using indicator functions: \\[\nf(x \\mid \\theta) = \\binom{5}{x} \\left[ \\left(\\frac{1}{2}\\right)^5 I_{\\{\\theta = \\text{fair}\\}} + (0.7)^x (0.3)^{5-x} I_{\\{\\theta = \\text{loaded}\\}} \\right]\n\\] In our trials we have observed \\(x=2\\) what is our likelihood?\nFor \\(\\theta = \\text{fair}\\):\n\\[\nf(x=2 \\mid \\text{fair}) = \\binom{5}{2} \\left(\\frac{1}{2}\\right)^5 = 10 \\cdot \\left(\\frac{1}{32}\\right) = \\frac{10}{32} = \\frac{5}{16} = 0.3125\n\\]\nFor \\(\\theta = \\text{loaded}\\):\n\\[\nf(x=2 \\mid \\text{loaded}) = \\binom{5}{2} (0.7)^2 (0.3)^3 = 10 \\cdot 0.49 \\cdot 0.027 = 10 \\cdot 0.01323 = 0.1323\n\\]\n\n\nCode\n# Define the binomial coefficient function\nchoose &lt;- function(n, x) {\n  factorial(n) / (factorial(x) * factorial(n - x))\n}\n\n# Values for x and n\nx &lt;- 2\nn &lt;- 5\n\n# Case 1: Fair\nprob_fair &lt;- choose(n, x) * (1/2)^5\ncat(\"f(2 | fair) =\", prob_fair, \"\\n\")\n\n\nf(2 | fair) = 0.3125 \n\n\nCode\n# Case 2: Loaded\nprob_loaded &lt;- choose(n, x) * (0.7)^x * (0.3)^(n-x)\ncat(\"f(2 | loaded) =\", prob_loaded, \"\\n\")\n\n\nf(2 | loaded) = 0.1323 \n\n\ngiving this data, it is most likely that the coin is fair.\nTo know how sure are we of this result, we need to do more calculations.\nYou need to ask yourself, what is the probability that the coin is fair, given that we have observed two heads \\(P(\\theta = fair \\mid x=2)\\)\nUsing the frequentist approach\n\nNull hypothesis is that the coin is fair.\n\nAlternative hypothesis is that the coin is loaded.\n\nWe can use a binomial test, which is suitable for small sample sizes.\nThe number of heads observed (𝑥=2) out of 5 flips (𝑛=5) can be tested against the expected number of heads for both hypotheses. Under the null hypothesis (𝐻0), the probability of getting 2 heads in 5 flips with a fair coin is 0.31 as we saw already:\n\\[\nf(x=2 \\mid \\text{fair}) = \\binom{5}{2} \\left(\\frac{1}{2}\\right)^5 = 10 \\cdot \\left(\\frac{1}{32}\\right) = \\frac{10}{32} = \\frac{5}{16} = 0.3125\n\\] We then compare this to the observed frequency of 2 heads. Typically, we use a significance level (𝛼), such as 0.05. If the \\(p\\)-value is less than or equal to 𝛼, we reject the null hypothesis in favor of the alternative hypothesis.\n\n\nCode\n# Perform a binomial test\nresult &lt;- binom.test(2, 5, p = 0.5, alternative = \"two.sided\")\n\n# Print the result\nprint(result)\n\n\n\n    Exact binomial test\n\ndata:  2 and 5\nnumber of successes = 2, number of trials = 5, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.05274495 0.85336720\nsample estimates:\nprobability of success \n                   0.4 \n\n\np-value: The \\(p\\)-value of 1 suggests that there is no evidence to reject the null hypothesis. This means the observed result (2 heads out of 5) is very consistent with what we would expect if the coin were fair (with a probability of 0.5 for heads).\nConfidence Interval: The 95% confidence interval for the true probability of success (getting heads) ranges from approximately 0.053 to 0.853. This wide interval indicates a high level of uncertainty due to the small sample size (only 5 flips).\nSample Estimate: The estimated probability of success (getting heads) from the observed data is 0.4. This is simply the proportion of heads observed (2 heads out of 5 flips).\nThis does not really gives us a lot of useful information.\nNow we can use Bayes to calculate posterior probability.: Calculate the posterior probability that the coin is fair given the observed data.\nAssume we don’t have prior information and both coins have equal prior probability, 𝑃(fair)=𝑃(loaded)=0.5.\nUsing Bayes’ theorem: \\[\nP(\\text{fair} \\mid X = 2) = \\frac{P(X = 2 \\mid \\text{fair}) \\cdot P(\\text{fair})}{P(X = 2 \\mid \\text{fair}) \\cdot P(\\text{fair}) + P(X = 2 \\mid \\text{loaded}) \\cdot P(\\text{loaded})}\n\\]\nPlugging in the values: \\[ P(\\text{fair} \\mid X = 2) = \\frac{0.3125 \\cdot 0.5}{0.3125 \\cdot 0.5 + 0.1323 \\cdot 0.5} = \\frac{0.15625}{0.15625 + 0.06615} = \\frac{0.15625}{0.2224} \\approx 0.703\n\\]\nSo, with the observed data, we are approximately 70.3% confident that the coin is fair.\nNow let’s assume that our brother has played this trick with us many times and we calculate that based on our experience, before tossing the coin, the probability that he is using a loaded coin is 0.6\nIf we know that the prior probability for the loaded coin is 𝑃(loaded)=0.6 then the prior probability for the fair coin is 𝑃(fair)=0.4. .\nUsing Bayes’ theorem:\\[ P(\\text{fair} \\mid X = 2) = \\frac{P(X = 2 \\mid \\text{fair}) \\cdot P(\\text{fair})}{P(X = 2 \\mid \\text{fair}) \\cdot P(\\text{fair}) + P(X = 2 \\mid \\text{loaded}) \\cdot P(\\text{loaded})}\n\\]\nPlugging in the values:\\[\nP(X = 2 \\mid \\text{fair}) = 0.3125 \\quad \\text{(calculated earlier)} \\]\\[ P(X = 2 \\mid \\text{loaded}) = 0.1323 \\quad \\text{(calculated earlier)}\n\\]\nNow substituting these into Bayes’ theorem:\\[\nP(\\text{fair} \\mid X = 2) = \\frac{0.3125 \\cdot 0.4}{0.3125 \\cdot 0.4 + 0.1323 \\cdot 0.6} = \\frac{0.125}{0.125 + 0.07938} = \\frac{0.125}{0.20438} \\approx 0.6114\n\\]\nSo, with the updated prior probability, we are approximately 61.14% confident that the coin is fair, or \\(1-0.6114=0.3886\\) probability that the coin is loaded.\nHow many flip would we need to be sure that coin is fair with 90% confident?\nTo determine the number of initial flips needed to be sure with 90% certainty, we need to calculate the posterior probability for various numbers of flips until we achieve a posterior probability of 90%.\nThis involves calculating probabilities for multiple scenarios. Let’s use a script to find the minimum number of flips required:\n\n\nCode\n# Define the binomial coefficient function\nchoose &lt;- function(n, x) {\n  factorial(n) / (factorial(x) * factorial(n - x))\n}\n\n# Probability functions\nprob_fair &lt;- function(x, n) {\n  choose(n, x) * (1/2)^n\n}\n\nprob_loaded &lt;- function(x, n) {\n  choose(n, x) * (0.7)^x * (0.3)^(n-x)\n}\n\n# Function to calculate posterior probability for 'fair' coin\nposterior_prob_fair &lt;- function(x, n) {\n  p_fair = prob_fair(x, n)\n  p_loaded = prob_loaded(x, n)\n  p_fair / (p_fair + p_loaded)\n}\n\n# Find the minimum number of flips for 90% certainty\nn &lt;- 5\nwhile(TRUE) {\n  x &lt;- 0:n\n  probs &lt;- sapply(x, function(k) posterior_prob_fair(k, n))\n  if (max(probs) &gt;= 0.9) break\n  n &lt;- n + 1\n}\n\ncat(\"Minimum number of flips for 90% certainty:\", n, \"\\n\")\n\n\nMinimum number of flips for 90% certainty: 5 \n\n\nSurprisingly, this is the same number of tosses we had before. In the previous calculation, we assumed both coins (fair and loaded) have equal prior probabilities, meaning 𝑃(fair)=𝑃(loaded)=0.5. The posterior probability of 70.3% was calculated for this specific observed outcome (2 heads out of 5) based on the prior probabilities and likelihoods.\nHowever, the R script’s loop is designed to find the minimum number of flips needed to achieve 90% posterior probability for any outcome. This means we’re looking for the number of flips that would allow us to be 90% certain in at least one scenario, not specifically for the observed 2 heads in 5 flips.\nThus, it is possible for the same number of flips (5) to yield different posterior probabilities depending on the observed outcomes. The loop in the R script checks various outcomes until it finds one that achieves the 90% certainty threshold.\nThe calculated 70.3% certainty is specific to the observed outcome of 2 heads in 5 flips.\nThe R script’s result of 5 flips indicates that there exists an outcome within 5 flips that would give us 90% certainty.\nSuppose now that your brother has a third coin which comes up tails 70% of the time. Again, you don’t know which coin your brother has brought to you, so you are going to test it by flipping it 4 times, were X counts the number of heads.\nLet \\(\\theta\\) identify the coin so that there are three possibilities: - \\(\\theta\\) = fair - \\(\\theta\\) = loaded Heads - \\(\\theta\\) = loaded Tails\n\\(x= heads\\)\n\\[\nf(x \\mid \\theta) = \\begin{cases}  \\binom{5}{x} (\\frac{1}{2})^5 \\text { if fair} \\\\\n\\binom{5}{x} (0.7)^x (0.3)^{5-x} \\text  { if loaded heads}\\\\\n\\binom{5}{x} (0.3)^x (0.7)^{5-x} \\text  { if loaded tails}\n\\end{cases}\n\\]\nYour prior is: - \\(P(\\theta =fair) = 0.4\\) - \\(P(\\theta =loadedH) = 0.3\\) - \\(P(\\theta =loadedT) = 0.3\\)\nOur prior probability that the coin is loaded is still 0.6, but we do not know which loaded coin it is, so we split the probability evenly between the two options\nWhat is the form of the likelihood now that we have three options? Likelihood:\n\\[\nL(\\theta) = P(X=x_1, X= x_2, \\cdots, X=x_n \\mid \\theta)\n\\]the probability function for a binomial distribution:\\[\nf(x\\mid p) = \\binom{n}{x}p^x(1-p)^{n-x}\n\\]\n\\(n=4\\) substituting:\n\\[f(x \\mid \\theta) = \\binom{4}{x} 0.5^x \\times 0.5^{4-x} I_{fair}  + \\binom{4}{x} 0.7^x \\times 0.3^{4-x} I_{fair.heads}  +\\binom{4}{x} 0.3^x \\times 0.7^{4-x} I_{fair.tails}\\]\n\\[f(x \\mid \\theta) = \\binom{4}{x} 0.5^x \\times 0.5^{4-x} \\times 0.4 + \\binom{4}{x} 0.7^x \\times 0.3^{4-x} \\times 0.3 +\\binom{4}{x} 0.3^x \\times 0.7^{4-x}  \\times 0.3\\]\nSuppose you flip the coin four times and it comes up heads twice. What is the posterior probability that this is the fair coin\nTo determine the posterior probability that the coin is fair, we can use Bayes’ theorem.\n\\[\nP(\\theta=fair \\mid x=2)= \\frac{P(X=2\\mid \\theta = fair)P(\\theta=fair)}{P(x=2)}\n\\]\nWhere \\(P(x=2)\\) is\n\\[P(x=2) =\nP(X=2\\mid \\theta = fair)P(\\theta=fair) + \\\\\nP(X=2\\mid \\theta = loaded.heads)P(\\theta=loaded.heads) + \\\\\nP(X=2\\mid \\theta = loaded.tails)P(\\theta=loaded.tails)\n\\] Given the following parameters: - \\(P(\\theta = \\text{fair}) = 0.4\\) - \\(P(\\theta = \\text{loaded heads}) = 0.3\\) - \\(P(\\theta = \\text{loaded tails}) = 0.3\\)\nFirst, we calculate \\(P(X = 2 \\mid \\theta = \\text{fair})\\):\n\\[P(X = 2 \\mid \\theta = \\text{fair}) = \\binom{4}{2} (0.5)^2 (0.5)^{2} = \\frac{4!}{2!2!} (0.5)^4 = 6 \\times 0.0625 = 0.375\\]\nNext, we calculate \\(P(X = 2 \\mid \\theta = \\text{loaded heads})\\):\n\\[P(X = 2 \\mid \\theta = \\text{loaded heads}) = \\binom{4}{2} (0.7)^2 (0.3)^2 = \\frac{4!}{2!2!} (0.7)^2 (0.3)^2 = 6 \\times 0.49 \\times 0.09 = 6 \\times 0.0441 = 0.2646\\]And \\(P(X = 2 \\mid \\theta = \\text{loaded tails})\\):\n\\[P(X = 2 \\mid \\theta = \\text{loaded tails}) = \\binom{4}{2} (0.3)^2 (0.7)^2 = \\frac{4!}{2!2!} (0.3)^2 (0.7)^2 = 6 \\times 0.09 \\times 0.49 = 6 \\times 0.0441 = 0.2646\\]\nNow, we substitute:\n\\[\nP(\\theta=fair \\mid x=2)= \\frac{0.375\\times 0.4} {0.375\\times 0.4+0.2646 \\times 0.3+0.2646\\times 0.3} =0.486\n\\]In code:\n\n\nCode\n# Given parameters\nP_fair &lt;- 0.4\nP_loaded_heads &lt;- 0.3\nP_loaded_tails &lt;- 0.3\n\n# Likelihoods\nP_X_2_given_fair &lt;- choose(4, 2) * (0.5^2) * (0.5^2)\nP_X_2_given_loaded_heads &lt;- choose(4, 2) * (0.7^2) * (0.3^2)\nP_X_2_given_loaded_tails &lt;- choose(4, 2) * (0.3^2) * (0.7^2)\n\n# Marginal likelihood\nP_X_2 &lt;- (P_X_2_given_fair * P_fair) + \n         (P_X_2_given_loaded_heads * P_loaded_heads) + \n         (P_X_2_given_loaded_tails * P_loaded_tails)\n\n# Posterior probability\nP_fair_given_X_2 &lt;- (P_X_2_given_fair * P_fair) / P_X_2\n\n# Display the result\nP_fair_given_X_2\n\n\n[1] 0.4858142"
  },
  {
    "objectID": "probability.html#priors",
    "href": "probability.html#priors",
    "title": "Probability",
    "section": "4.1 Priors",
    "text": "4.1 Priors\nA prior in Bayesian statistics represents your initial beliefs or assumptions about a parameter before seeing any data. It incorporates any prior knowledge or expertise you might have. Priors can be expressed in various forms, such as:\nNon-informative (or flat) priors: These are used when you have no strong prior beliefs. They spread the probability evenly across all possible values of the parameter.\nInformative priors: These incorporate specific prior knowledge or beliefs. For example, if you have historical data or expert opinions, you can use these to inform your prior.\nBayesian probability adjust the probabilities based on updated information, let’s see this with an example:\n\nEstimating the Size of Atacama\nThe country of Chile is divided administratively into 15 regions. The size of the country is 756,096 square kilometers. How big do you think the region of Atacama is? Let’s define the following events:\n\n(A_1): The event that Atacama is less than 10,000 square kilometers.\n(A_2): The event that Atacama is between 10,000 and 50,000 square kilometers.\n(A_3): The event that Atacama is between 50,000 and 100,000 quare kilometers.\n(A_4): The event that Atacama is more than 100,000 square kilometers.\n\nWe initially assign probabilities to these events based on the average size of Chile’s regions (756096/15 = 50406):\n\n(A_1): P((A_1)) = 5%\n(A_2): P((A_2)) = 45%\n(A_3): P((A_3)) = 45%\n(A_4): P((A_4)) = 5%\n\nGiven that Atacama is the fourth largest region:\n\n(A_1): P((A_1)) = 1%\n(A_2): P((A_2)) = 14%\n(A_3): P((A_3)) = 60%\n(A_4): P((A_4)) = 25%\n\nGiven that the smallest region (Santiago Metropolitan) has an area of 15,403 square kilometers:\n\n(A_1): P((A_1)) = 0%\n(A_2): P((A_2)) = 15%\n(A_3): P((A_3)) = 60%\n(A_4): P((A_4)) = 25%\n\nGiven that the third largest region, Aysén del General Carlos Ibáñez del Campo, has an area of 108,494 square kilometers:\n\n(A_1): P((A_1)) = 0%\n(A_2): P((A_2)) = 10%\n(A_3): P((A_3)) = 65%\n(A_4): P((A_4)) = 25%"
  },
  {
    "objectID": "probability.html#posterior-predictive-distribution",
    "href": "probability.html#posterior-predictive-distribution",
    "title": "Probability",
    "section": "15.2 Posterior predictive distribution",
    "text": "15.2 Posterior predictive distribution\nWhat is our posterior predictive distribution after we have observed some data?\n\nCoin toss: posterior predictive distribution.\nWe can think again about flipping a coin, we don’t know if it is a fair coin, so we don’t know what the probability comes up head is. We observe, after one flip, a head. We want to ask, what’s our predicted distribution for the second flip, given that we saw a head on the first flip?\n\\[\nf(x_2 \\mid x_1) = \\int f(x_2\\mid \\theta, x_1) \\times \\text{posterior distribution for }\\theta \\times d\\theta\n\\]\n\\[\nf(x_2 \\mid x_1) = \\int f(x_2\\mid \\theta, x_1) \\times \\ f(\\theta \\mid x_1) \\times d\\theta\n\\]\nWe assume that the second toss is independent from the first toss, so \\(f(x_2\\mid \\theta, x_1) = f(x_2 \\mid \\theta)\\)\nSubstituting:\n\\[\nf(x_2 \\mid x_1) = \\int f(x_2 \\mid \\theta) \\times \\ f(\\theta \\mid x_1) \\times d\\theta\n\\]\nNow we cannot keep on assuming that theta follows an uniform distribution where all the probabilities of getting heads are equal, because we have already observed one head, so it is more likely now to think that the coin has 0.5 probability or more of falling heads.\n\\[\nf(x_2 \\mid x_1) = \\int_0^1 \\theta^{x_2} (1-\\theta)^{1-x_2} \\times f(\\theta \\mid x_1) \\times d\\theta\n\\]\nWe know, because we calculated above that the posterior is \\(2\\theta\\)\n\\[\nf(x_2 \\mid x_1) = \\int_0^1 \\theta^{x_2} (1-\\theta)^{1-x_2} \\times 2\\theta \\times d\\theta = \\int_0^1 2\\theta ^{x_2+1}(1-\\theta)^{1-x_2} \\times d\\theta\n\\]"
  },
  {
    "objectID": "probability.html#bernoulli-likelihood",
    "href": "probability.html#bernoulli-likelihood",
    "title": "Probability",
    "section": "20.2 Bernoulli Likelihood",
    "text": "20.2 Bernoulli Likelihood\nA Bernoulli trial is a simple experiment where there are only two possible outcomes, often labeled as 1 (success) and 0 (failure). An example of this is flipping a coin, where you might label heads as 1 and tails as 0.\nThe probability of success (1) is denoted by \\(\\theta\\). Thus, the likelihood function, which tells us how probable a given outcome is, looks like this:\n\\[\nP(y \\mid \\theta) = \\theta^y (1 - \\theta)^{1 - y}\n\\] for n independent trials: \\[\nP(y_1,y_2, \\cdots, y_n \\mid \\theta) = \\theta \\sum_{y_i}(1-\\theta)^{n-\\sum_{y_i}}\n\\] ## Uniform Prior\nA prior distribution represents our beliefs about \\(\\theta\\) before we observe any data. A uniform prior is the simplest form and indicates that we have no preference for any particular value of \\(\\theta\\) between 0 and 1. Mathematically, it’s written as:\n\\[\nP(\\theta) = 1 \\quad \\text{for } 0 \\leq \\theta \\leq 1\n\\] In fact, the uniform distribution, is a beta one one. And any beta distribution, is conjugate for the Bernoulli distribution. Any beta prior, will give a beta posterior.\nAssuming uniform prior over \\(\\theta\\), which is a special case of Beta distribution \\(\\theta \\sim Beta(1,1)\\)"
  },
  {
    "objectID": "probability.html#uniform-prior",
    "href": "probability.html#uniform-prior",
    "title": "Bayesian Statistics with Bernoulli Likelihood and Beta Posterior",
    "section": "20.2 Uniform Prior",
    "text": "20.2 Uniform Prior\nA prior distribution represents our beliefs about \\(\\theta\\) before we observe any data. A uniform prior is the simplest form and indicates that we have no preference for any particular value of \\(\\theta\\) between 0 and 1. Mathematically, it’s written as:\n\\[\nP(\\theta) = 1 \\quad \\text{for } 0 \\leq \\theta \\leq 1\n\\]"
  },
  {
    "objectID": "probability.html#posterior-distribution",
    "href": "probability.html#posterior-distribution",
    "title": "Probability",
    "section": "20.3 Posterior Distribution",
    "text": "20.3 Posterior Distribution\nBayes’ theorem combines our prior beliefs with the observed data to update our belief about \\(\\theta\\):\n\\[\nP(\\theta \\mid \\mathbf{y}) = \\frac{P(\\mathbf{y} \\mid \\theta) P(\\theta)}{P(\\mathbf{y})}\n\\]\nWhere \\(\\mathbf{y}\\) represents the observed data.\nGiven a uniform prior and Bernoulli likelihood, the posterior distribution turns out to be a Beta distribution. The Beta distribution is particularly convenient because it’s a conjugate prior for the Bernoulli likelihood, meaning the posterior distribution is also a Beta distribution.\nThe posterior distribution follows from the conjugacy of the Beta distribution: \\[\nP(\\theta \\mid data) \\sim Beta(\\alpha + \\sum y_i, \\beta+ n-\\sum y_i)\n\\] where \\(\\alpha\\) and \\(\\beta\\) are the parameters of the prior. Since the prior is Beta(1,1), we set \\(\\alpha=1\\) and \\(\\beta=1\\) so\nWhen we start with a uniform prior (which is \\(\\text{Beta}(1, 1)\\)) and observe \\(n\\) Bernoulli trials with \\(y\\) successes, the posterior becomes:\n\\[\nBeta(1 + \\sum y_i, 1+n -\\sum Y_i)\n\\]\n\nIn practice:\nReturn to the example of flipping a coin with unknown probability of heads \\(\\theta\\). If we use a Bernoulli likehood for each coin flip, ie.\n\\[\nP(y \\mid \\theta) = \\theta^y (1 - \\theta)^{1 - y} I_{\\{ 0 \\leq \\theta \\leq 1\\}}\n\\] and a uniform prior for \\(\\theta\\), what is the posterior distribution for \\(\\theta\\) if we observe the following sequence (Heads, Heads, Tails)\nSol: \\[\nBeta(1 + \\sum y_i, 1+n -\\sum Y_i) = Beta(1 +2, 1+ 3-2) = Beta(3,2)\n\\]\nSuppose we perform 10 Bernoulli trials and observe 7 successes:\n\nPrior: \\(\\text{Beta}(1, 1)\\)\nPosterior: \\(\\text{Beta}(1 + 7, 1 + (10 - 7)) = \\text{Beta}(8, 4)\\)\n\nThis posterior distribution now reflects our updated belief about \\(\\theta\\) after observing the data.\nWe repeat the example of flipping a coin with unkown probability of heads theta, but with a different prior. If we use a Bernoulli likelihood for each coin flip$ P(y_i ) = ^y_i (1 - )^{1 - y} I_{{0 }}$ and Beta(5,5) prior. What is the posterior distribution for \\(\\theta\\) if we observe the following sequence: (H,H,T)\n\\[\nP(\\theta \\mid data) \\sim Beta(\\alpha + \\sum y_i, \\beta+ n-\\sum y_i)\n\\] \\[\nP(\\theta \\mid data) \\sim Beta(5 + 2, 5+ 3-2)= Beta (7,6)\n\\]"
  },
  {
    "objectID": "probability.html#beta-distribution",
    "href": "probability.html#beta-distribution",
    "title": "Probability",
    "section": "10.5 Beta Distribution",
    "text": "10.5 Beta Distribution\nThe Beta distribution is a continuous probability distribution defined on the interval [0, 1]. It’s characterized by two shape parameters, \\(\\alpha\\) and \\(\\beta\\), which determine the shape of the distribution. The probability density function (PDF) of the Beta distribution is given by:\n\\[\n\\text{Beta}(\\theta; \\alpha, \\beta) = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\nWhere \\(B(\\alpha, \\beta)\\) is a normalization constant, specifically the Beta function.\n\n10.5.1 Real-Life Examples\n\nProportion of Successes:\nSuppose you are interested in modeling the probability of success for a new drug. Before conducting trials, you have some prior belief about the drug’s success rate. As you collect data from trials, you update this belief. The Beta distribution is a natural choice for representing your belief about the success rate because it’s defined on the interval [0, 1] and can be updated with new data easily.\nQuality Control:\nIn manufacturing, you might use the Beta distribution to model the proportion of defective items in a batch. Based on prior knowledge and observed data (e.g., inspections), you can use the Beta distribution to update your estimate of the defect rate.\nUser Behavior:\nIn web analytics, the Beta distribution can model the probability of a user clicking on an ad. Based on past click data, you can update your belief about the click-through rate using the Beta distribution.\n\n\n\n10.5.2 Plotting Beta Distribution in R\nHere’s some R code that demonstrates how to plot the Beta distribution for different values of \\(\\alpha\\) and \\(\\beta\\):\n\n\nCode\n# Define a function to plot the Beta distribution\nplot_beta &lt;- function(alpha, beta) {\n  # Create a sequence of theta values\n  theta &lt;- seq(0, 1, length.out = 100)\n  \n  # Calculate the Beta density values\n  density &lt;- dbeta(theta, alpha, beta)\n  \n  # Create a data frame for plotting\n  df &lt;- data.frame(theta, density)\n  \n  # Plot the Beta distribution using ggplot2\n  ggplot(df, aes(x = theta, y = density)) +\n    geom_line(color = \"blue\") +\n    labs(title = paste(\"Beta Distribution (α =\", alpha, \", β =\", beta, \")\"),\n         x = \"θ\",\n         y = \"Density\")\n}\n\n# Example: Plotting Beta distribution for different values of alpha and beta\npar(mfrow = c(2, 2))\nplot_beta(2, 2)  # Symmetric distribution\n\n\n\n\n\n\n\n\n\nCode\nplot_beta(5, 2)  # Skewed distribution\n\n\n\n\n\n\n\n\n\nCode\nplot_beta(2, 5)  # Another skewed distribution\n\n\n\n\n\n\n\n\n\nCode\nplot_beta(1, 1)  # Uniform distribution\n\n\n\n\n\n\n\n\n\nIn Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial, and geometric distributions."
  },
  {
    "objectID": "probability.html#intuition",
    "href": "probability.html#intuition",
    "title": "Bayesian Statistics with Bernoulli Likelihood and Beta Posterior",
    "section": "20.5 Intuition",
    "text": "20.5 Intuition\n\nPrior Belief: We start with a uniform prior, indicating we believe all values of \\(\\theta\\) (0 to 1) are equally likely.\nCollect Data: We then collect data from Bernoulli trials (e.g., coin flips).\nUpdate Belief: Using Bayes’ theorem, we update our belief about \\(\\theta\\) based on the observed data.\n\nThe Beta distribution becomes more peaked as we gather more data, indicating increased confidence in our estimate of \\(\\theta\\)."
  },
  {
    "objectID": "probability.html#example",
    "href": "probability.html#example",
    "title": "Bayesian Statistics with Bernoulli Likelihood and Beta Posterior",
    "section": "20.6 Example",
    "text": "20.6 Example\nSuppose we perform 10 Bernoulli trials and observe 7 successes:\n\nPrior: \\(\\text{Beta}(1, 1)\\)\nPosterior: \\(\\text{Beta}(1 + 7, 1 + (10 - 7)) = \\text{Beta}(8, 4)\\)\n\nThis posterior distribution now reflects our updated belief about \\(\\theta\\) after observing the data."
  },
  {
    "objectID": "probability.html#real-life-examples",
    "href": "probability.html#real-life-examples",
    "title": "Bayesian Statistics with Bernoulli Likelihood and Beta Posterior",
    "section": "11.1 Real-Life Examples",
    "text": "11.1 Real-Life Examples\n\nProportion of Successes:\nSuppose you are interested in modeling the probability of success for a new drug. Before conducting trials, you have some prior belief about the drug’s success rate. As you collect data from trials, you update this belief. The Beta distribution is a natural choice for representing your belief about the success rate because it’s defined on the interval [0, 1] and can be updated with new data easily.\nQuality Control:\nIn manufacturing, you might use the Beta distribution to model the proportion of defective items in a batch. Based on prior knowledge and observed data (e.g., inspections), you can use the Beta distribution to update your estimate of the defect rate.\nUser Behavior:\nIn web analytics, the Beta distribution can model the probability of a user clicking on an ad. Based on past click data, you can update your belief about the click-through rate using the Beta distribution."
  },
  {
    "objectID": "probability.html#plotting-beta-distribution-in-r",
    "href": "probability.html#plotting-beta-distribution-in-r",
    "title": "Bayesian Statistics with Bernoulli Likelihood and Beta Posterior",
    "section": "11.2 Plotting Beta Distribution in R",
    "text": "11.2 Plotting Beta Distribution in R\nHere’s some R code that demonstrates how to plot the Beta distribution for different values of \\(\\alpha\\) and \\(\\beta\\):\n# Load necessary library\nlibrary(ggplot2)\n\n# Define a function to plot the Beta distribution\nplot_beta &lt;- function(alpha, beta) {\n  # Create a sequence of theta values\n  theta &lt;- seq(0, 1, length.out = 100)\n  \n  # Calculate the Beta density values\n  density &lt;- dbeta(theta, alpha, beta)\n  \n  # Create a data frame for plotting\n  df &lt;- data.frame(theta, density)\n  \n  # Plot the Beta distribution using ggplot2\n  ggplot(df, aes(x = theta, y = density)) +\n    geom_line(color = \"blue\") +\n    labs(title = paste(\"Beta Distribution (α =\", alpha, \", β =\", beta, \")\"),\n         x = \"θ\",\n         y = \"Density\")\n}\n\n# Example: Plotting Beta distribution for different values of alpha and beta\nplot_beta(2, 2)  # Symmetric distribution\nplot_beta(5, 2)  # Skewed distribution\nplot_beta(2, 5)  # Another skewed distribution\nplot_beta(1, 1)  # Uniform distribution"
  },
  {
    "objectID": "probability.html#beta-distribution-1",
    "href": "probability.html#beta-distribution-1",
    "title": "Probability",
    "section": "20.1 Beta Distribution",
    "text": "20.1 Beta Distribution\nThe Beta distribution is defined as:\n\\[\n\\text{Beta}(\\theta; \\alpha, \\beta) = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\nWhere \\(\\alpha\\) and \\(\\beta\\) are parameters that shape the distribution and \\(B(\\alpha, \\beta)\\) is a normalization constant."
  },
  {
    "objectID": "matrix.html#matrix-multiplication",
    "href": "matrix.html#matrix-multiplication",
    "title": "Matrix Multiplication Example",
    "section": "1.3 Matrix Multiplication",
    "text": "1.3 Matrix Multiplication\nTo multiply a \\(3 \\times 4\\) matrix $ A$ with a \\(4 \\times 2\\) matrix \\(B\\), we follow the rule that each row of \\(A\\) interacts with each column of \\(B\\) using the dot product.\nGiven Matrices:\n\\[\nA = \\begin{pmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4}\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\nb_{1,1} & b_{1,2} \\\\\nb_{2,1} & b_{2,2} \\\\\nb_{3,1} & b_{3,2} \\\\\nb_{4,1} & b_{4,2}\n\\end{pmatrix}\n\\]\nMatrix Product:\nMatrix multiplication is performed by taking the dot product of rows from the first matrix (𝐴) with columns of the second matrix (𝐵). The key steps are:\nCheck compatibility: Ensure the number of columns in 𝐴 matches the number of rows in 𝐵.\nDot Product Computation: Each element in the resulting matrix is calculated by multiplying corresponding entries from a row of 𝐴 and a column of𝐵, summing the results.\nMatrix multiplication follows a systematic approach where: - Each row of matrix \\(A\\) is multiplied by each column of matrix \\(B\\). - The resulting matrix has dimensions \\(m \\times p\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\)."
  },
  {
    "objectID": "matrix.html#given-matrices-1",
    "href": "matrix.html#given-matrices-1",
    "title": "Matrix Multiplication Example",
    "section": "1.4 Given Matrices:",
    "text": "1.4 Given Matrices:\n\\[\nA = \\begin{pmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4}\n\\end{pmatrix}\n\\]\n\\[\nB = \\begin{pmatrix}\nb_{1,1} & b_{1,2} \\\\\nb_{2,1} & b_{2,2} \\\\\nb_{3,1} & b_{3,2} \\\\\nb_{4,1} & b_{4,2}\n\\end{pmatrix}\n\\]\nThese matrices are compatible for multiplication because $ A$ has 4 columns, matching $ B$’s 4 rows.\nThe resulting $3 ) matrix $ C$ is computed as follows:\n\\[\nC = A B =\n\\begin{pmatrix}\na_{1,1} \\cdot b_{1,1} + a_{1,2} \\cdot b_{2,1} + a_{1,3} \\cdot b_{3,1} + a_{1,4} \\cdot b_{4,1} &\na_{1,1} \\cdot b_{1,2} + a_{1,2} \\cdot b_{2,2} + a_{1,3} \\cdot b_{3,2} + a_{1,4} \\cdot b_{4,2} \\\\\na_{2,1} \\cdot b_{1,1} + a_{2,2} \\cdot b_{2,1} + a_{2,3} \\cdot b_{3,1} + a_{2,4} \\cdot b_{4,1} &\na_{2,1} \\cdot b_{1,2} + a_{2,2} \\cdot b_{2,2} + a_{2,3} \\cdot b_{3,2} + a_{2,4} \\cdot b_{4,2} \\\\\na_{3,1} \\cdot b_{1,1} + a_{3,2} \\cdot b_{2,1} + a_{3,3} \\cdot b_{3,1} + a_{3,4} \\cdot b_{4,1} &\na_{3,1} \\cdot b_{1,2} + a_{3,2} \\cdot b_{2,2} + a_{3,3} \\cdot b_{3,2} + a_{3,4} \\cdot b_{4,2}\n\\end{pmatrix}\n\\]\nEach element in $ C$ is derived from the dot product of a row in $ A$ and a column in $ B$.\nFor example, the top-left element of $ C$ (i.e., $ c_{1,1}$) is calculated as:\n\\[\nc_{1,1} = a_{1,1} \\cdot b_{1,1} + a_{1,2} \\cdot b_{2,1} + a_{1,3} \\cdot b_{3,1} + a_{1,4} \\cdot b_{4,1}\n\\]\nLikewise, every position in $ C$ follows the same logic.\nThis structure ensures clarity in understanding how matrix multiplication works systematically.\nExercise Given two matrices ( A ) and ( B ):\n\\[\nA = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n\\]\nThe result of multiplying matrix ( A ) by matrix ( B ) is:\n\\[\nAB = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 2 \\cdot 0 + 3 \\cdot (-1) \\\\\n4 \\cdot 1 + 5 \\cdot 0 + 6 \\cdot (-1) \\\\\n7 \\cdot 1 + 8 \\cdot 0 + 9 \\cdot (-1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n-2 \\\\\n-2 \\\\\n-2\n\\end{pmatrix}\n\\] and in r we use %*%\n\n\nCode\nX&lt;- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)\nX\n\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3   -2    1\n[3,]    2    1   -1\n\n\nCode\nbeta&lt;- c(3,2,1)\nX%*%beta\n\n\n     [,1]\n[1,]    6\n[2,]    6\n[3,]    7\n\n\ndue to the way the matrices multiplication work, we can only multiply two matrices if the number of rows in one matrix is equal to the number of columns in the other matrix.\nGiven two matrices ( A ) and ( B ):\n\\[\nA = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{pmatrix}\n\\]\nThe result of multiplying matrix ( A ) by matrix ( B ) is:\n\\[\nAB = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 7 + 2 \\cdot 10 & 1 \\cdot 8 + 2 \\cdot 11 & 1 \\cdot 9 + 2 \\cdot 12 \\\\\n3 \\cdot 7 + 4 \\cdot 10 & 3 \\cdot 8 + 4 \\cdot 11 & 3 \\cdot 9 + 4 \\cdot 12 \\\\\n5 \\cdot 7 + 6 \\cdot 10 & 5 \\cdot 8 + 6 \\cdot 11 & 5 \\cdot 9 + 6 \\cdot 12\n\\end{pmatrix}\n= \\begin{pmatrix}\n27 & 30 & 33 \\\\\n61 & 68 & 75 \\\\\n95 & 106 & 117\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "matrix.html#given-matrices",
    "href": "matrix.html#given-matrices",
    "title": "Matrix Algebra",
    "section": "1.3 Given Matrices:",
    "text": "1.3 Given Matrices:\n\\[\nA = \\begin{pmatrix}\na_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\\\\na_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\\\\na_{3,1} & a_{3,2} & a_{3,3} & a_{3,4}\n\\end{pmatrix}\n\\]\n\\[\nB = \\begin{pmatrix}\nb_{1,1} & b_{1,2} \\\\\nb_{2,1} & b_{2,2} \\\\\nb_{3,1} & b_{3,2} \\\\\nb_{4,1} & b_{4,2}\n\\end{pmatrix}\n\\]\nThese matrices are compatible for multiplication because \\(A\\) has 4 columns, matching \\(B\\)’s 4 rows.\nThe resulting \\(3 \\times 2\\) matrix \\(C\\) is computed as follows:\n\\[\nC = A B =\n\\begin{pmatrix}\na_{1,1} \\cdot b_{1,1} + a_{1,2} \\cdot b_{2,1} + a_{1,3} \\cdot b_{3,1} + a_{1,4} \\cdot b_{4,1} &\na_{1,1} \\cdot b_{1,2} + a_{1,2} \\cdot b_{2,2} + a_{1,3} \\cdot b_{3,2} + a_{1,4} \\cdot b_{4,2} \\\\\na_{2,1} \\cdot b_{1,1} + a_{2,2} \\cdot b_{2,1} + a_{2,3} \\cdot b_{3,1} + a_{2,4} \\cdot b_{4,1} &\na_{2,1} \\cdot b_{1,2} + a_{2,2} \\cdot b_{2,2} + a_{2,3} \\cdot b_{3,2} + a_{2,4} \\cdot b_{4,2} \\\\\na_{3,1} \\cdot b_{1,1} + a_{3,2} \\cdot b_{2,1} + a_{3,3} \\cdot b_{3,1} + a_{3,4} \\cdot b_{4,1} &\na_{3,1} \\cdot b_{1,2} + a_{3,2} \\cdot b_{2,2} + a_{3,3} \\cdot b_{3,2} + a_{3,4} \\cdot b_{4,2}\n\\end{pmatrix}\n\\]\nEach element in \\(C\\) is derived from the dot product of a row in \\(A\\) and a column in \\(B\\).\nFor example, the top-left element of \\(C\\) (i.e., \\(c_{1,1}\\)) is calculated as:\n\\[\nc_{1,1} = a_{1,1} \\cdot b_{1,1} + a_{1,2} \\cdot b_{2,1} + a_{1,3} \\cdot b_{3,1} + a_{1,4} \\cdot b_{4,1}\n\\]\nLikewise, every position in \\(C\\) follows the same logic.\nThis structure ensures clarity in understanding how matrix multiplication works systematically.\nExercise Given two matrices ( A ) and ( B ):\n\\[\nA = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n\\]\nThe result of multiplying matrix ( A ) by matrix ( B ) is:\n\\[\nAB = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 2 \\cdot 0 + 3 \\cdot (-1) \\\\\n4 \\cdot 1 + 5 \\cdot 0 + 6 \\cdot (-1) \\\\\n7 \\cdot 1 + 8 \\cdot 0 + 9 \\cdot (-1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n-2 \\\\\n-2 \\\\\n-2\n\\end{pmatrix}\n\\]\nand in r we use %*%\n\n\nCode\nX&lt;- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)\nX\n\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3   -2    1\n[3,]    2    1   -1\n\n\nCode\nbeta&lt;- c(3,2,1)\nX%*%beta\n\n\n     [,1]\n[1,]    6\n[2,]    6\n[3,]    7\n\n\ndue to the way the matrices multiplication work, we can only multiply two matrices if the number of rows in one matrix is equal to the number of columns in the other matrix.\nGiven two matrices ( A ) and ( B ):\n\\[\nA = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{pmatrix}\n\\]\nThe result of multiplying matrix ( A ) by matrix ( B ) is:\n\\[\nAB = \\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n5 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 7 + 2 \\cdot 10 & 1 \\cdot 8 + 2 \\cdot 11 & 1 \\cdot 9 + 2 \\cdot 12 \\\\\n3 \\cdot 7 + 4 \\cdot 10 & 3 \\cdot 8 + 4 \\cdot 11 & 3 \\cdot 9 + 4 \\cdot 12 \\\\\n5 \\cdot 7 + 6 \\cdot 10 & 5 \\cdot 8 + 6 \\cdot 11 & 5 \\cdot 9 + 6 \\cdot 12\n\\end{pmatrix}\n= \\begin{pmatrix}\n27 & 30 & 33 \\\\\n61 & 68 & 75 \\\\\n95 & 106 & 117\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "matrix.html#determinant",
    "href": "matrix.html#determinant",
    "title": "Matrix Algebra",
    "section": "6.1 Determinant",
    "text": "6.1 Determinant\nThe determinant of a square matrix is a scalar value that provides important properties of the matrix. It is denoted as det(A) for a matrix A. A matrix is invertible if and only if its determinant is non-zero.\nFor a 2×2 matrix:\n\\[\nA = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\]\nThe determinant is:\n\\[\n\\text{det}(A) = ad - bc\n\\] In r we use det formula to calculate it.\n\n\nCode\n# Define the matrix\nA &lt;- matrix(c(1, 0, 1, 2, 4, 0, 3, 5, 6), nrow = 3, byrow = TRUE)\n\n# Compute the determinant\ndet_A &lt;- det(A)"
  },
  {
    "objectID": "matrix.html#adjoint",
    "href": "matrix.html#adjoint",
    "title": "Matrix Algebra",
    "section": "6.2 Adjoint",
    "text": "6.2 Adjoint\nThe adjoint (or adjugate) of a matrix is the transpose of the cofactor matrix. For a 2×2 matrix:\n\\[\nA = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\]\nThe adjoint is:\n\\[\n\\text{adj}(A) = \\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "matrix.html#cofactor-matrix",
    "href": "matrix.html#cofactor-matrix",
    "title": "Matrix Algebra",
    "section": "6.3 Cofactor Matrix",
    "text": "6.3 Cofactor Matrix\nTo compute the adjoint of a matrix, we first need the cofactor matrix.\nThe cofactor of an element \\(a_{ij}\\) in a matrix is calculated as:\n\\[\nC_{ij} = (-1)^{i+j} \\cdot M_{ij}\n\\]\nWhere: - \\(M_{ij}\\) is the minor of the element \\(a_{ij}\\), i.e., the determinant of the submatrix formed by removing the \\(i\\)-th row and \\(j\\)-th column from the original matrix. - \\((-1)^{i+j}\\) gives the correct sign based on the position.\nThe cofactor matrix is the matrix of all \\(C_{ij}\\) values."
  },
  {
    "objectID": "matrix.html#adjoint-of-a-matrix-general-case",
    "href": "matrix.html#adjoint-of-a-matrix-general-case",
    "title": "Matrix Algebra",
    "section": "6.4 Adjoint of a Matrix (General Case)",
    "text": "6.4 Adjoint of a Matrix (General Case)\nThe adjoint of a matrix is the transpose of its cofactor matrix:\n\\[\n\\text{adj}(A) = \\text{Cofactor}(A)^T\n\\]\nThis method works for any square matrix, not just 2×2."
  },
  {
    "objectID": "matrix.html#calculating-inverse-of-a-matrix-manually",
    "href": "matrix.html#calculating-inverse-of-a-matrix-manually",
    "title": "Matrix Algebra",
    "section": "6.5 Calculating Inverse of a Matrix manually",
    "text": "6.5 Calculating Inverse of a Matrix manually\n\\[\nA = \\begin{bmatrix}\n2 & 3 \\\\\n1 & 4\n\\end{bmatrix}\n\\]\n\n\nCode\n# Define the matrix\nA &lt;- matrix(c(2, 1, 3, 4), nrow = 2, byrow = TRUE)\n\n# Compute the determinant\ndet_A &lt;- det(A)\n\n# Compute the adjoint manually\nadj_A &lt;- matrix(c(4, -3, -1, 2), nrow = 2, byrow = TRUE)\n\n# Compute the inverse using the formula\nA_inv &lt;- (1 / det_A) * adj_A\n\n# Display the result\nA_inv\n\n\n     [,1] [,2]\n[1,]  0.8 -0.6\n[2,] -0.2  0.4\n\n\nIn r there is no formula to calculate the adjoint of a matrix directly, so if you need to calculate the adjoint of a matrix of more than 2x2, you can use the package matlib\n\n\nCode\n# Define a matrix\nA &lt;- matrix(c(1, 0, 1, 2, 4, 0, 3, 5, 6), nrow = 3, byrow = TRUE)\n\n# Compute the adjoint\nadj_A &lt;- matlib::adjoint(A)\n\n# Display the result\nadj_A\n\n\n     [,1] [,2] [,3]\n[1,]   24    5   -4\n[2,]  -12    3    2\n[3,]   -2   -5    4"
  },
  {
    "objectID": "matrix.html#calculating-inverse-of-a-matrix-using-software",
    "href": "matrix.html#calculating-inverse-of-a-matrix-using-software",
    "title": "Matrix Algebra",
    "section": "6.6 Calculating Inverse of a Matrix using software",
    "text": "6.6 Calculating Inverse of a Matrix using software\nWe rarely need to get the adjoint outside of the scope of calculating the inverse of a matrix, and r gives us a formula for directly calculating the inverse of a matrix, the determinant and the adjoint are calculated internally\n\nExample: Adjoint and Inverse of a 3×3 Matrix in R\nLet’s compute the inverse of:\n\\[\nA = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 4 & 5 \\\\\n1 & 0 & 6\n\\end{bmatrix}\n\\]\n\n\nCode\n# Define the matrix\nA &lt;- matrix(c(1, 0, 1, 2, 4, 0, 3, 5, 6), nrow = 3, byrow = TRUE)\n\n# Compute the inverse using solve (R handles adjoint and cofactors internally)\nA_inv &lt;- solve(A)\n\n# Display the result\nA_inv\n\n\n            [,1]       [,2]        [,3]\n[1,]  1.09090909  0.2272727 -0.18181818\n[2,] -0.54545455  0.1363636  0.09090909\n[3,] -0.09090909 -0.2272727  0.18181818\n\n\n\nIn r we use the function solve to get the inverse, and we use it to solve equations: it gives us the values for a, b and c to resolve the system of equations:\n\\[\\begin{align*}\na + b + c &= 6 \\\\\n3a - 2b + c &= 2 \\\\\n2a + b - c &= 1\n\\end{align*}\\]\n\\[\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n3 & -2 & 1 \\\\\n2 & 1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\na \\\\\nb \\\\\nc\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 \\\\\n2 \\\\\n1\n\\end{pmatrix}\n\\]\n\n\nCode\nX &lt;- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)\ny &lt;- matrix(c(6,2,1),3,1)\nsolve(X)%*%y\n\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n\n\n\nExample\nA small factory produces two products: Chairs and Tables. Each product requires a certain amount of wood and labor hours:\n\nA Chair requires 2 units of wood and 3 hours of labor.\nA Table requires 5 units of wood and 2 hours of labor.\n\nThe factory has available resources of:\n\n40 units of wood\n30 hours of labor\n\nWe want to determine how many Chairs (x) and Tables (y) the factory can produce using all available resources.\nStep 1: Represent the System as Equations\nWe can write the constraints as:\n\\[\n\\begin{aligned}\n2x + 5y &= 40 \\quad \\text{(wood constraint)} \\\\\n3x + 2y &= 30 \\quad \\text{(labor constraint)}\n\\end{aligned}\n\\]\nStep 2: Matrix Form\nThis system can be written in matrix form:\n\\[\nAX = B\n\\]\nWhere:\n\\[\nA = \\begin{bmatrix} 2 & 5 \\\\ 3 & 2 \\end{bmatrix}, \\quad\nX = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 40 \\\\ 30 \\end{bmatrix}\n\\]\nStep 3: Solve in R\n\n\nCode\n# Coefficient matrix A\nA &lt;- matrix(c(2, 3, 5, 2), nrow = 2, byrow = TRUE)\n\n# Resource vector B\nB &lt;- matrix(c(40, 30), nrow = 2)\n\n# Solve for X (number of chairs and tables)\nX &lt;- solve(A, B)\n\n# Display the result\nX\n\n\n           [,1]\n[1,]  0.9090909\n[2,] 12.7272727"
  },
  {
    "objectID": "matrix.html#mathematical-background",
    "href": "matrix.html#mathematical-background",
    "title": "Matrix Algebra",
    "section": "2.1 Mathematical Background",
    "text": "2.1 Mathematical Background\nThe cosine similarity between two vectors a and b is calculated as:\n\\[\n\\text{cosine similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}\n\\]\nWhere: - ( ) is the dot product of vectors a and b - ( || ) and ( || ) are the magnitudes (Euclidean norms) of vectors a and b - ( ) is the angle between the vectors\n\\[\n\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i = a_1b_1 + a_2b_2 + \\dots + a_nb_n\n\\] \\[\n\\|\\vec{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2} = \\sqrt{\\sum_{i=1}^{n} v_i^2}\n\\]\n\n\nCode\nv1 &lt;- c(3, 4) \nv2 &lt;- c(4, 5)\nplot(0, 0, xlim = c(0, 6), ylim = c(0, 6), type = \"n\", xlab = \"X-axis\", ylab = \"Y-axis\", main = \"Similar Vectors\") \ngrid() \narrows(0, 0, v1[1], v1[2], col = \"red\", lwd = 2)\narrows(0, 0, v2[1], v2[2], col = \"blue\", lwd = 2) \ntext(v1[1], v1[2], \"v1\", pos = 3, col = \"red\")\ntext(v2[1], v2[2], \"v2\", pos = 3, col = \"blue\")\n\n\n\n\n\nThese vectors point in nearly the same direction, forming a small angle between them. This results in a cosine similarity close to 1, indicating high similarity.\n\n\nCode\nv1 &lt;- c(4, 0)\nv2 &lt;- c(0, 4)\n\nplot(0, 0, xlim = c(0, 5), ylim = c(0, 5), type = \"n\",\n     xlab = \"X-axis\", ylab = \"Y-axis\", main = \"Unrelated Vectors (Orthogonal)\")\ngrid()\narrows(0, 0, v1[1], v1[2], col = \"red\", lwd = 2)\narrows(0, 0, v2[1], v2[2], col = \"blue\", lwd = 2)\ntext(v1[1], v1[2], \"v1\", pos = 3, col = \"red\")\ntext(v2[1], v2[2], \"v2\", pos = 3, col = \"blue\")\n\n\n\n\n\nThese vectors are perpendicular to each other, forming a 90° angle. Their cosine similarity is 0, meaning they are completely unrelated in direction.\n\n\nCode\nv1 &lt;- c(3, 4)\nv2 &lt;- c(-3, -4)\n\nplot(0, 0, xlim = c(-5, 5), ylim = c(-5, 5), type = \"n\",\n     xlab = \"X-axis\", ylab = \"Y-axis\", main = \"Opposite Vectors\")\ngrid()\narrows(0, 0, v1[1], v1[2], col = \"red\", lwd = 2)\narrows(0, 0, v2[1], v2[2], col = \"blue\", lwd = 2)\ntext(v1[1], v1[2], \"v1\", pos = 3, col = \"red\")\ntext(v2[1], v2[2], \"v2\", pos = 3, col = \"blue\")\n\n\n\n\n\nThese vectors point in exactly opposite directions, forming a 180° angle. Their cosine similarity is -1, indicating they are completely dissimilar in direction."
  },
  {
    "objectID": "matrix.html#r-implementation",
    "href": "matrix.html#r-implementation",
    "title": "Matrix Algebra",
    "section": "2.2 R Implementation",
    "text": "2.2 R Implementation\n\n\nCode\n# Define vectors\na &lt;- c(1, 2, 3)\nb &lt;- c(4, 5, 6)\n\n# Dot product\ndot_product &lt;- sum(a * b)\n\n# Magnitudes\nmag_a &lt;- sqrt(sum(a^2))\nmag_b &lt;- sqrt(sum(b^2))\n\n# Cosine similarity\ncos_sim &lt;- dot_product / (mag_a * mag_b)\ncos_sim\n\n\n[1] 0.9746318\n\n\n\n\nCode\n# Define vectors\nv1 &lt;- c(3, 4)\nv2 &lt;- c(4, 2)\n\n# Calculate angle\ndot &lt;- sum(v1 * v2)\nmag1 &lt;- sqrt(sum(v1^2))\nmag2 &lt;- sqrt(sum(v2^2))\nangle &lt;- acos(dot / (mag1 * mag2)) * 180 / pi\n\n# Plot\nplot(0, 0, xlim = c(0, 5), ylim = c(0, 5), type = \"n\",\n     xlab = \"X-axis\", ylab = \"Y-axis\", main = \"Vectors and Angle\")\ngrid()\narrows(0, 0, v1[1], v1[2], col = \"red\", lwd = 2)\narrows(0, 0, v2[1], v2[2], col = \"blue\", lwd = 2)\ntext(v1[1], v1[2], \"v1\", pos = 3, col = \"red\")\ntext(v2[1], v2[2], \"v2\", pos = 3, col = \"blue\")\ntext(1.5, 1.5, paste0(\"θ = \", round(angle, 2), \"°\"), col = \"darkgreen\")"
  },
  {
    "objectID": "matrix.html#residual-sum-of-squares",
    "href": "matrix.html#residual-sum-of-squares",
    "title": "Matrix Algebra",
    "section": "9.1 Residual sum of squares",
    "text": "9.1 Residual sum of squares\nWriting it this way we can calculate the values to minimize the residual sum of squares (RSS). The RSS equation now looks like this:\n\\[\n(Y - X\\beta)^T(Y - X\\beta)\n\\]"
  },
  {
    "objectID": "matrix.html#least-squares-estimator-lse-lse-is-a-method-used-to-estimate-the-parameters-of-a-linear-model-by-minimizing-the-sum-of-the-squared-differences-errors-between-observed-and-predicted-values.",
    "href": "matrix.html#least-squares-estimator-lse-lse-is-a-method-used-to-estimate-the-parameters-of-a-linear-model-by-minimizing-the-sum-of-the-squared-differences-errors-between-observed-and-predicted-values.",
    "title": "Matrix Algebra",
    "section": "9.2 Least Squares Estimator (LSE) LSE is a method used to estimate the parameters of a linear model by minimizing the sum of the squared differences (errors) between observed and predicted values.",
    "text": "9.2 Least Squares Estimator (LSE) LSE is a method used to estimate the parameters of a linear model by minimizing the sum of the squared differences (errors) between observed and predicted values.\nTo find the \\(\\hat{\\beta}\\) that minimizes this we solve by taking the derivative: \\[\n2X^T(Y-X\\hat{\\beta})=0\\\\\nX^TX\\hat{\\beta}=X^TY\\\\\n\\hat{\\beta}= (X^TX^{-1}X^TY)\n\\]\nIn r:\n\n\nCode\nx= father.son$fheight\ny= father.son$sheight\nX&lt;- cbind(1,x)\nbetahat &lt;- solve(t(X)%*%X)%*%t(X)%*%y\nbetahat\n\n\n       [,1]\n  33.886604\nx  0.514093\n\n\nCode\n# or equivalent code:\nbetahat &lt;- solve(crossprod((X)))%*%crossprod(X,y)\nbetahat\n\n\n       [,1]\n  33.886604\nx  0.514093\n\n\nso now with \\(\\hat{\\beta}\\) we can draw the linear model line.\n\n\nCode\nintercept = betahat[1,1]\nslope= betahat[2, 1]\n\nplot(x,y)\nabline(intercept, slope, col = \"blue\")"
  }
]