<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.7.31" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />


<title>Understanding Matrices and Logistic Regression in Neural Networks – Mastering Statistics: Fundamentals of Data Analysis.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>


<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="custom-style.css" />
</head>

<body>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="/index.html">
    <span class="navbar-title">Mastering Statistics: Fundamentals of Data Analysis.</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
  aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation"
  onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="/index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="/MathBackground.html"> 
<span class="menu-text">Math background</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="/probability.html"> 
<span class="menu-text">Probability</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="/exploratoryAnalysis.html"> 
<span class="menu-text">Exploratory Analysis</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-supervised-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false" >
 <span class="menu-text">Supervised Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-supervised-learning">    
        <li>
    <a class="dropdown-item" href="/Inferencial.html">
 <span class="dropdown-text">Statistical Tests</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/linearModels.html">
 <span class="dropdown-text">Linear Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/modelAccuracy.html">
 <span class="dropdown-text">Assessing Model Accuracy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/classification.html">
 <span class="dropdown-text">Classification problems</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/polynomialReg.html">
 <span class="dropdown-text">Polynomial Regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/survival.html">
 <span class="dropdown-text">Survival Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/tree.html">
 <span class="dropdown-text">Tree Based Methods</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/supportVectorMachines.html">
 <span class="dropdown-text">Support Vector Machines</span></a>
  </li>  
        <li class="dropdown-header">
 Deep Learning with TensorFlow</li>
        <li>
    <a class="dropdown-item" href="/installingTensorFlow.html">
 <span class="dropdown-text">Installing TensorFlow</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/DeepLearning.html">
 <span class="dropdown-text">Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/deepLearningTorch.html">
 <span class="dropdown-text">Deep Learning Lab with Torch</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="/unsupervisedLearning.html"> 
<span class="menu-text">Unsupervised Learning</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-matrices" role="link" data-bs-toggle="dropdown" aria-expanded="false" >
 <span class="menu-text">Matrices</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-matrices">    
        <li>
    <a class="dropdown-item" href="/matrix.html">
 <span class="dropdown-text">Matrix Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="/logisticRegressionMatrixCalculation.html">
 <span class="dropdown-text">Matrices in Logistic Regression in Neural Networks</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Understanding Matrices and Logistic Regression in Neural Networks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#understanding-matrices-in-neural-networks" id="toc-understanding-matrices-in-neural-networks"><span class="header-section-number">1</span> Understanding Matrices in Neural Networks</a>
  <ul>
  <li><a href="#traditional-logistic-regression-notation" id="toc-traditional-logistic-regression-notation"><span class="header-section-number">1.0.1</span> <strong>Traditional Logistic Regression Notation</strong></a></li>
  </ul></li>
  <li><a href="#logistic-regression-model" id="toc-logistic-regression-model"><span class="header-section-number">2</span> Logistic Regression Model</a></li>
  <li><a href="#forward-propagation-loss-function" id="toc-forward-propagation-loss-function"><span class="header-section-number">3</span> Forward Propagation &amp; Loss Function</a></li>
  <li><a href="#updating-weights-with-gradient-descent" id="toc-updating-weights-with-gradient-descent"><span class="header-section-number">4</span> Updating Weights with Gradient Descent</a></li>
  <li><a href="#implementation-in-r" id="toc-implementation-in-r"><span class="header-section-number">5</span> Implementation in R</a></li>
  <li><a href="#cat-non-cat-exercise" id="toc-cat-non-cat-exercise"><span class="header-section-number">6</span> Cat /Non-cat exercise</a></li>
  </ul>
</nav>
<p>In this document, we explore the foundations of matrices in neural networks, logistic regression, forward propagation, and updating weights using stochastic gradient descent. We will implement a basic example in <code>R</code> to reinforce learning.</p>
<section id="understanding-matrices-in-neural-networks" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Understanding Matrices in Neural Networks</h1>
<p>Matrices enable efficient mathematical operations in neural networks. For a simple model:</p>
<p><span class="math display">\[
Z = XW
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(X\)</span> is the <strong>input matrix</strong> (containing feature values).</p></li>
<li><p><span class="math inline">\(W\)</span> is the <strong>weight matrix</strong> (containing learned coefficients).</p></li>
<li><p><span class="math inline">\(Z\)</span> is the <strong>output before activation</strong>.</p></li>
</ul>
<section id="traditional-logistic-regression-notation" class="level3" data-number="1.0.1">
<h3 data-number="1.0.1"><span class="header-section-number">1.0.1</span> <strong>Traditional Logistic Regression Notation</strong></h3>
<p>In standard <strong>logistic regression</strong>, the equation is expressed using individual predictor variables:</p>
<p><span class="math display">\[
Z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span> is the <strong>intercept (bias term)</strong>.</p></li>
<li><p><span class="math inline">\(\beta_1, \beta_2, \dots, \beta_n\)</span> are the <strong>weights (coefficients) assigned to each feature</strong>.</p></li>
<li><p><span class="math inline">\(x_1, x_2, \dots, x_n\)</span> are the <strong>input feature values</strong> (e.g., time spent, pages visited).</p></li>
<li><p><span class="math inline">\(Z\)</span> is the <strong>linear combination before applying the activation function</strong>.</p></li>
</ul>
<p>Once <span class="math inline">\(Z\)</span> is computed, the logistic function (<strong>sigmoid activation</strong>) is applied:</p>
<div class="important-formula">
<p><span class="math display">\[
\sigma(Z) = \frac{1}{1 + e^{-Z}}
\]</span>{eq-sigmoid-function}</p>
</div>
<p>Both forms describe the same underlying concept:</p>
<ul>
<li><p>The <strong>matrix form</strong> (<span class="math inline">\(Z = XW\)</span>) is compact and efficient, especially when handling multiple samples.</p></li>
<li><p>The <strong>traditional logistic regression form</strong> explicitly shows the relationship between individual features and their respective weights.</p></li>
</ul>
<p>Both representations lead to the same outcome: a <strong>probability prediction</strong> via the sigmoid function.</p>
<p>The term <span class="math inline">\(Z\)</span> in the sigmoid activation function is most commonly referred as the weighted sum of imputs (plus bias)</p>
<p>Herer’s a breakdown: In a single neuron, before an activation function is applied, a neuron takes multiple inputs, multiplies each input by a corresponding weight, sums these weighted inputs, and then adds the bias term. So <span class="math inline">\(Z\)</span> represents the linear combination of the inputs to a neuron before any any non-linear transformation is applied.</p>
<p><strong>The concept around activation functions</strong> 1. Introducing Non-linearity: this is the most important reason for activation functions. Imagine a neural network without activation functions. Each neuron would simply perform a linear transformation (weighted sum +bias). If you stack multiple layers of linear transformations, the entire network would still be a single linear transformation. Real world data is almost never linearly separable. Problems like image recognition, natural language or complex pattern detection involve highly non-linear relationships. Activation functions introduce non-linearity, allowing neural network to learn and approximate complex, non-linear functions and relationships in the data. You can think of an activation function as determining whether a neuron should “activate” or “fire” and pass its signal to the next layer. It transforms the raw, unbounded weighted sum (Z) into an output that is typically within a specific range, often interpreted as probability or a strength of activation. Sigmoid maps any real number (Z) to a value between 0 and 1. This makes it ideal for output layers in binary classification problems, where the output can be interpreted as a probability.</p>
</section>
</section>
<section id="logistic-regression-model" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Logistic Regression Model</h1>
<p>Logistic regression predicts probabilities using the <strong>sigmoid function</strong>: The sigmoid function is a mathematical function that outputs values between 0 and 1, making it ideal for logistic regression, where we interpret the result as a probability.</p>
<p><span id="eq-sigmoid"><span class="math display">\[
\sigma(Z) = \frac{1}{1 + e^{-Z}}
\tag{1}\]</span></span></p>
<ul>
<li><p><span class="math inline">\(Z\)</span> is the input value (can be any real number)</p></li>
<li><p><span class="math inline">\(e\)</span> is Euler’s number</p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(Z\)</span> is large and positive,<span class="math inline">\(\sigma(z)\)</span> approaches 1 (strong positive probability)</p>
<p>If <span class="math inline">\(z\)</span> is large and negative,<span class="math inline">\(\sigma(z)\)</span> approaches 0 (strong negative probability)</p>
<p>when <span class="math inline">\(z=0\)</span>, <span class="math inline">\(\sigma(z)=0.5)\)</span>, meaning neutral probability</p>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/chunk1-1.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="960" /></p>
</figure>
</div>
</div>
</div>
</section>
<section id="forward-propagation-loss-function" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Forward Propagation &amp; Loss Function</h1>
<p>Predictions (<span class="math inline">\(\hat{y}\)</span>) are made using: <span class="math display">\[\hat{y} = \sigma(Z)\]</span> The <strong>binary cross-entropy loss function</strong> quantifies the error between the estimate and the real output provided to the model: <span id="eq-loss-function"><span class="math display">\[
L = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\tag{2}\]</span></span></p>
</section>
<section id="updating-weights-with-gradient-descent" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Updating Weights with Gradient Descent</h1>
<p>Since we don’t know the optimal values of <span class="math inline">\(W\)</span>, we start with random weights and iteratively update them to minimize the loss function. Now, let’s break down the equation further and clarify the partial derivatives.</p>
<p>Gradient loss function: <span id="eq-gradient-loss-function"><span class="math display">\[
dw =\frac{\partial L}{\partial W} = \frac{1}{m} X^T (A-Y)
\tag{3}\]</span></span></p>
<p>This term represents the gradient, or the slope, of the loss function <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(W\)</span>. Essentially, it tells us:</p>
<p>How much the loss function changes when we slightly change<span class="math inline">\(W\)</span> The direction we should move <span class="math inline">\(W\)</span> to minimize the loss.</p>
<p>Since <span class="math inline">\(L\)</span> depends on <span class="math inline">\(W\)</span> (because changing 𝑊affects predictions), we need to calculate the rate of change of <span class="math inline">\(L\)</span> concerning <span class="math inline">\(W\)</span>, which is where the partial derivative comes in.</p>
<p><em>Partial Derivatives Explained</em> A partial derivative calculates how one variable changes while keeping others constant. In our case: <span class="math display">\[
\frac{\partial L}{\partial W}
\]</span> measures how much the loss function changes if we make a small adjustment to <span class="math inline">\(W\)</span>. If we visualize the loss function as a mountain, the gradient tells us which direction leads us downhill the fastest (toward lower loss). The gradient gives the best direction for adjusting W, but how far we step in that direction is controlled by the learning rate <span class="math inline">\(\alpha\)</span></p>
<p><span class="math display">\[
W = W -\alpha \cdot \frac{\partial L}{\partial W}
\]</span> Where:</p>
<ul>
<li><p><span class="math inline">\(\alpha\)</span> is the <strong>learning rate</strong> (a small step size to prevent large jumps)</p></li>
<li><p><span class="math inline">\(\frac{\partial L}{\partial W}\)</span> is the <strong>gradient</strong></p></li>
</ul>
<p><span class="math inline">\(W\)</span> is updated gradually with each iteration.</p>
<p>For logistic regression, the gradient of the <em>binary cross-entropy loss function</em> with respect to <span class="math inline">\(W\)</span> is: <span class="math display">\[
\frac{\partial L}{\partial W} = \frac{1}{m}X^T(\hat y -y)
\]</span> Where:</p>
<ul>
<li><p><span class="math inline">\(X\)</span> is the input matrix (features)</p></li>
<li><p><span class="math inline">\(y\)</span> is the actual target values.</p></li>
<li><p><span class="math inline">\(\hat y = \sigma(XW)\)</span> is the predicted output after applying the sigmoid function.</p></li>
<li><p><span class="math inline">\(X^T(\hat y- y)\)</span> measures the error’s contribution to weight updates.</p></li>
</ul>
<div class="cell" data-layout-align="center">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Example loss reduction over iterations</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>iterations <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a>loss_values <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.05</span> <span class="sc">*</span> iterations)  <span class="co"># Simulated loss decreasing</span></span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># Create dataframe</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">iteration =</span> iterations, <span class="at">loss =</span> loss_values)</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># Plot loss reduction</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> iteration, <span class="at">y =</span> loss)) <span class="sc">+</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Loss Reduction Over Iterations&quot;</span>) <span class="sc">+</span></span>
<span id="cb1-12"><a href="#cb1-12"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Iteration&quot;</span>) <span class="sc">+</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Loss&quot;</span>) <span class="sc">+</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/chunk2-1.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="960" /></p>
</figure>
</div>
</div>
</div>
</section>
<section id="implementation-in-r" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Implementation in R</h1>
<p>Scenario: Predicting Whether Someone Will Buy a Product Imagine you’re running an online store, and you want to predict whether a customer will buy a product based on two simple features:</p>
<ul>
<li><p>Time spent on the website (in minutes)</p></li>
<li><p>Number of pages visited</p></li>
</ul>
<p>We’ll create a small dataset with these features and train a logistic regression model using gradient descent to predict whether a customer will buy the product (1) or not (0).</p>
<p>The traditional logistical regression formula would be: <span class="math display">\[
Z=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+\epsilon_i
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span> is the <strong>intercept (bias term)</strong>.</p></li>
<li><p><span class="math inline">\(\beta_1, \beta_2\)</span> are the <strong>weights (coefficients) assigned to each feature</strong>.</p></li>
<li><p><span class="math inline">\(x_{1i}, x_{2i}\)</span> are the <strong>feature values</strong> for the<span class="math inline">\(i\)</span>-th sample.</p></li>
<li><p><span class="math inline">\(\epsilon_i\)</span> is the <strong>error term</strong> accounting for noise in the data.</p></li>
</ul>
<p>Instead of writing the equation explicitly for each feature, we can use matrix multiplication:</p>
<p><span class="math display">\[
Z = XW + \epsilon
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(X\)</span> is the <strong>input matrix</strong> containing feature values.</p></li>
<li><p><span class="math inline">\(W\)</span> is the <strong>weight matrix</strong> (vector of coefficients).</p></li>
<li><p><span class="math inline">\(\epsilon\)</span> is the <strong>error term</strong>.</p></li>
</ul>
<p>Expanding this in <strong>matrix notation</strong>:</p>
<p><span class="math display">\[
\begin{bmatrix}
Z_1 \\
Z_2 \\
Z_3 \\
\vdots \\
Z_m
\end{bmatrix}=\begin{bmatrix}
1 &amp; x_{11} &amp; x_{21} \\
1 &amp; x_{12} &amp; x_{22} \\
1 &amp; x_{13} &amp; x_{23} \\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{1m} &amp; x_{2m}
\end{bmatrix} \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2
\end{bmatrix}+ \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\vdots \\ \epsilon_m
\end{bmatrix}
\]</span></p>
<p>Where:</p>
<ul>
<li><p>Each row in <span class="math inline">\(X\)</span> represents <strong>one sample</strong>, including a <strong>bias term (1), feature 1, and feature 2</strong>.</p></li>
<li><p>The column vector <span class="math inline">\(W\)</span> contains the <strong>learned parameters</strong> (<span class="math inline">\(\beta_0, \beta_1, \beta_2\)</span>).</p></li>
<li><p>The error term <span class="math inline">\(\epsilon\)</span> accounts for <strong>random noise in predictions</strong>.</p></li>
</ul>
<p>We have data from 5 customers, we will create a matrix for this: Each row in <span class="math inline">\(X\)</span> represents a customer. First column = Bias term (always 1). Second column = Time spent on the website. Third column = Number of pages visited. y contains whether the customer bought the product (1) or not (0).</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a> <span class="co"># Real-world dataset (Time spent &amp; Pages visited)</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(</span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>,</span>
<span id="cb2-4"><a href="#cb2-4"></a>  <span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">5</span>,</span>
<span id="cb2-5"><a href="#cb2-5"></a>  <span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">7</span>,</span>
<span id="cb2-6"><a href="#cb2-6"></a>  <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>,</span>
<span id="cb2-7"><a href="#cb2-7"></a>  <span class="dv">1</span>, <span class="dv">30</span>, <span class="dv">10</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>), <span class="at">ncol=</span><span class="dv">3</span>, <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Labels</span></span></code></pre></div>
</details>
</div>
<p>As a recapitulation, we use the <em>sigmoid</em> function to predict the expected output of any specific combination of minutes in the website and pages visited. We will create a function for this:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Sigmoid function</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>sigmoid <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb3-3"><a href="#cb3-3"></a>  <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z)))</span>
<span id="cb3-4"><a href="#cb3-4"></a>}</span></code></pre></div>
</details>
</div>
<p>The <em>loss function</em> will tell us how far our predicted value is from the actual value, we will also create a function for this:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Loss function (binary cross-entropy)</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>loss_function <span class="ot">&lt;-</span> <span class="cf">function</span>(y, y_hat) {</span>
<span id="cb4-3"><a href="#cb4-3"></a>  <span class="fu">return</span>(<span class="sc">-</span><span class="fu">mean</span>(y <span class="sc">*</span> <span class="fu">log</span>(y_hat) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> y) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> y_hat)))</span>
<span id="cb4-4"><a href="#cb4-4"></a>}</span></code></pre></div>
</details>
</div>
<p>Now we create a function to calculate the <em>gradient descent</em></p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>gradient_descent <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, <span class="at">learning_rate =</span> <span class="fl">0.01</span>, <span class="at">iterations =</span> <span class="dv">50</span>) {</span>
<span id="cb5-2"><a href="#cb5-2"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb5-3"><a href="#cb5-3"></a>  W <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">ncol</span>(X))  <span class="co"># Initialize weights randomly</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb5-5"><a href="#cb5-5"></a>    Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W   <span class="co"># Compute Z values for all samples</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)  <span class="co"># Apply sigmoid function to all samples</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>    gradient <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> (y_hat <span class="sc">-</span> y) <span class="sc">/</span> m  <span class="co"># Compute gradient for weight updates</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> gradient  <span class="co"># Update weights</span></span>
<span id="cb5-9"><a href="#cb5-9"></a>  }</span>
<span id="cb5-10"><a href="#cb5-10"></a>  <span class="fu">return</span> (W)</span>
<span id="cb5-11"><a href="#cb5-11"></a>}</span></code></pre></div>
</details>
</div>
<p>Just for learning purposes, we will create a new version of the gradient descent function where we store the results of each iteration, so we can visualize it later:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Gradient Descent Implementation (Tracking Progress)</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>gradient_descent_progress <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, <span class="at">learning_rate =</span> <span class="fl">0.01</span>, <span class="at">iterations =</span> <span class="dv">50</span>) {</span>
<span id="cb6-3"><a href="#cb6-3"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb6-4"><a href="#cb6-4"></a>  W <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">ncol</span>(X))  <span class="co"># Initialize weights randomly</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>  progress <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Iteration =</span> <span class="fu">integer</span>(), </span>
<span id="cb6-6"><a href="#cb6-6"></a>                         <span class="at">Weight1 =</span> <span class="fu">numeric</span>(), <span class="at">Weight2 =</span> <span class="fu">numeric</span>(),</span>
<span id="cb6-7"><a href="#cb6-7"></a>                         <span class="at">Z1 =</span> <span class="fu">numeric</span>(), <span class="at">Z2 =</span> <span class="fu">numeric</span>(), <span class="at">Z3 =</span> <span class="fu">numeric</span>(), <span class="at">Z4 =</span> <span class="fu">numeric</span>(), <span class="at">Z5 =</span> <span class="fu">numeric</span>(),</span>
<span id="cb6-8"><a href="#cb6-8"></a>                         <span class="at">y_hat1 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat2 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat3 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat4 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat5 =</span> <span class="fu">numeric</span>(),</span>
<span id="cb6-9"><a href="#cb6-9"></a>                         <span class="at">Grad1 =</span> <span class="fu">numeric</span>(), <span class="at">Grad2 =</span> <span class="fu">numeric</span>())  <span class="co"># Tracking all five samples</span></span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb6-12"><a href="#cb6-12"></a>    Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W   <span class="co"># Compute Z values for all samples</span></span>
<span id="cb6-13"><a href="#cb6-13"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)  <span class="co"># Apply sigmoid function to all samples</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>    gradient <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> (y_hat <span class="sc">-</span> y) <span class="sc">/</span> m  <span class="co"># Compute gradient for weight updates</span></span>
<span id="cb6-15"><a href="#cb6-15"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> gradient  <span class="co"># Update weights</span></span>
<span id="cb6-16"><a href="#cb6-16"></a></span>
<span id="cb6-17"><a href="#cb6-17"></a>    <span class="co"># Store results for all five samples</span></span>
<span id="cb6-18"><a href="#cb6-18"></a>    progress <span class="ot">&lt;-</span> <span class="fu">rbind</span>(progress, <span class="fu">data.frame</span>(</span>
<span id="cb6-19"><a href="#cb6-19"></a>      <span class="at">Iteration =</span> i,</span>
<span id="cb6-20"><a href="#cb6-20"></a>      <span class="at">Weight1 =</span> W[<span class="dv">2</span>], <span class="at">Weight2 =</span> W[<span class="dv">3</span>],</span>
<span id="cb6-21"><a href="#cb6-21"></a>      <span class="at">Z1 =</span> Z[<span class="dv">1</span>], <span class="at">Z2 =</span> Z[<span class="dv">2</span>], <span class="at">Z3 =</span> Z[<span class="dv">3</span>], <span class="at">Z4 =</span> Z[<span class="dv">4</span>], <span class="at">Z5 =</span> Z[<span class="dv">5</span>],</span>
<span id="cb6-22"><a href="#cb6-22"></a>      <span class="at">y_hat1 =</span> y_hat[<span class="dv">1</span>], <span class="at">y_hat2 =</span> y_hat[<span class="dv">2</span>], <span class="at">y_hat3 =</span> y_hat[<span class="dv">3</span>], <span class="at">y_hat4 =</span> y_hat[<span class="dv">4</span>], <span class="at">y_hat5 =</span> y_hat[<span class="dv">5</span>],</span>
<span id="cb6-23"><a href="#cb6-23"></a>      <span class="at">Grad1 =</span> gradient[<span class="dv">2</span>], <span class="at">Grad2 =</span> gradient[<span class="dv">3</span>]</span>
<span id="cb6-24"><a href="#cb6-24"></a>    ))</span>
<span id="cb6-25"><a href="#cb6-25"></a>  }</span>
<span id="cb6-26"><a href="#cb6-26"></a>  </span>
<span id="cb6-27"><a href="#cb6-27"></a>  <span class="fu">return</span>(progress)</span>
<span id="cb6-28"><a href="#cb6-28"></a>}</span></code></pre></div>
</details>
</div>
<p>At the beginning, the weights are random.</p>
<p>Over each iteration, the values change towards better predictions.</p>
<ul>
<li><p><span class="math inline">\(Z\)</span> values show the raw linear transformation before activation.</p></li>
<li><p><span class="math inline">\(\hat{y}\)</span> (y_hat) tracks how probabilities evolve as weights adjust.</p></li>
<li><p>Gradient values indicate how weights update to minimize the loss.</p></li>
</ul>
<p>If you increase iterations, you’ll see further refinement.</p>
<div class="cell" data-layout-align="center">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource r cell-code code-overflow-scroll number-lines"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Train Model</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb7-3"><a href="#cb7-3"></a>progress_df <span class="ot">&lt;-</span> <span class="fu">gradient_descent_progress</span>(X, y)</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="fu">print</span>(<span class="fu">head</span>(progress_df))</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  Iteration   Weight1   Weight2       Z1       Z2       Z3       Z4       Z5
1         1 0.6888648 0.5675639 4.843405 13.58712 18.24565 2.162957 26.98937
2         2 0.6753759 0.5618110 4.760556 13.35190 17.93135 2.126398 26.52269
3         3 0.6619085 0.5560680 4.677836 13.11703 17.61753 2.089897 26.05672
4         4 0.6484633 0.5503354 4.595251 12.88254 17.30422 2.053458 25.59151
5         5 0.6350416 0.5446137 4.512808 12.64845 16.99143 2.017082 25.12707
6         6 0.6216444 0.5389033 4.430512 12.41477 16.67920 1.980774 24.66346
     y_hat1    y_hat2    y_hat3    y_hat4 y_hat5    Grad1     Grad2
1 0.9921814 0.9999987 1.0000000 0.8968733      1 1.350927 0.5762460
2 0.9915118 0.9999984 1.0000000 0.8934425      1 1.348884 0.5752916
3 0.9907866 0.9999980 1.0000000 0.8899173      1 1.346747 0.5742961
4 0.9900013 0.9999975 1.0000000 0.8862965      1 1.344512 0.5732572
5 0.9891514 0.9999968 1.0000000 0.8825790      1 1.342173 0.5721731
6 0.9882318 0.9999959 0.9999999 0.8787636      1 1.339725 0.5710413</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/progres-plot-1.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="960" /></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/progres-plot-2.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="960" /></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/progres-plot-3.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="960" /></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/progres-plot-4.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="960" /></p>
</figure>
</div>
</div>
</div>
<p><strong>Understanding the Learned Weights</strong></p>
<p>After running gradient descent, we obtained the following learned weights:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="co"># Gradient Descent Implementation (Track Progress)</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>gradient_descent <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, <span class="at">learning_rate =</span> <span class="fl">0.01</span>, <span class="at">iterations =</span> <span class="dv">50</span>) {</span>
<span id="cb9-4"><a href="#cb9-4"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb9-5"><a href="#cb9-5"></a>  W <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">ncol</span>(X))  <span class="co"># Initialize weights randomly</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb9-7"><a href="#cb9-7"></a>    Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W   <span class="co"># Compute Z values for all samples</span></span>
<span id="cb9-8"><a href="#cb9-8"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)  <span class="co"># Apply sigmoid function to all samples</span></span>
<span id="cb9-9"><a href="#cb9-9"></a>    gradient <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> (y_hat <span class="sc">-</span> y) <span class="sc">/</span> m  <span class="co"># Compute gradient for weight updates</span></span>
<span id="cb9-10"><a href="#cb9-10"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> gradient  <span class="co"># Update weights</span></span>
<span id="cb9-11"><a href="#cb9-11"></a>  }</span>
<span id="cb9-12"><a href="#cb9-12"></a>  <span class="fu">return</span> (W)</span>
<span id="cb9-13"><a href="#cb9-13"></a>}</span>
<span id="cb9-14"><a href="#cb9-14"></a>W <span class="ot">&lt;-</span> <span class="fu">gradient_descent</span>(X, y)</span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="fu">print</span>(W)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,] 0.01567795
[2,] 0.09666904
[3,] 0.31485151</code></pre>
</div>
</div>
<p>These correspond to:</p>
<ul>
<li><p><span class="math inline">\(W_0\)</span> <strong>Bias term</strong> (Intercept).</p></li>
<li><p><span class="math inline">\(W_1\)</span> <strong>Effect of Time Spent on probability of purchasing</strong>.</p></li>
<li><p><span class="math inline">\(W_2\)</span> <strong>Effect of Pages Visited on probability of purchasing</strong>.A positive value increases probability, while a negative value decreases probability.</p></li>
</ul>
<p>To predict whether a new customer will buy a product, we use the following equation:</p>
<p><span class="math display">\[
Z = W_0 + W_1 \cdot \text{Time Spent} + W_2 \cdot \text{Pages Visited}
\]</span></p>
<p>Once we calculate<span class="math inline">\(Z\)</span>, we apply the <strong>sigmoid activation function</strong>:</p>
<p><span class="math display">\[
\sigma(Z) = \frac{1}{1 + e^{-Z}}
\]</span></p>
<p>where <span class="math inline">\(\sigma(Z)\)</span> represents the <strong>probability</strong> that the customer will buy.</p>
<p><strong>Example Calculation</strong> Let’s take a new customer who spends <strong>12 minutes</strong> on the website and visits <strong>4 pages</strong>. We calculate <span class="math inline">\(Z\)</span> as:</p>
<p><span class="math display">\[
Z = 0.01567795 + (0.09666904 \times 12) + (0.31485151 \times 4)
\]</span></p>
<p>Applying the <em>learned weights</em>, the prediction follows:</p>
<p><span class="math display">\[
Z = 2.435112
\]</span></p>
<p>Applying the <em>sigmoid</em> function:</p>
<p><span class="math display">\[
\sigma(2.435112) = \frac{1}{1 + e^{-2.435112}}
\]</span></p>
<p>Approximating:</p>
<p><span class="math display">\[
\sigma(2.435112) \approx 0.91
\]</span></p>
<p>Thus, the model predicts <strong>91% probability</strong> that this customer <strong>will buy</strong> the product.</p>
<p>Let’s see the predictions calculated in <code>r</code> over the same data we used for training:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Function to predict probability based on learned weights</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>predict_probability <span class="ot">&lt;-</span> <span class="cf">function</span>(X, W) {</span>
<span id="cb11-3"><a href="#cb11-3"></a>  <span class="fu">return</span>(<span class="fu">sigmoid</span>(X <span class="sc">%*%</span> W))</span>
<span id="cb11-4"><a href="#cb11-4"></a>}</span></code></pre></div>
</details>
</div>
<div class="cell" data-layout-align="center">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Get predictions</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>predicted_probs <span class="ot">&lt;-</span> <span class="fu">predict_probability</span>(X, W)</span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="co"># Create dataframe for plotting</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">TimeSpent =</span> X[,<span class="dv">2</span>], <span class="at">PagesVisited =</span> X[,<span class="dv">3</span>], <span class="at">Probability =</span> predicted_probs)</span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="co"># Plot results</span></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> TimeSpent, <span class="at">y =</span> PagesVisited, <span class="at">color =</span> Probability)) <span class="sc">+</span></span>
<span id="cb12-9"><a href="#cb12-9"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>  <span class="fu">scale_color_gradient</span>(<span class="at">low =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">high =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Logistic Regression: Probability of Purchase&quot;</span>) <span class="sc">+</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Time Spent on Website (minutes)&quot;</span>) <span class="sc">+</span></span>
<span id="cb12-13"><a href="#cb12-13"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Number of Pages Visited&quot;</span>) <span class="sc">+</span></span>
<span id="cb12-14"><a href="#cb12-14"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/chunk%206-1.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="960" /></p>
</figure>
</div>
</div>
</div>
<p>We apply gradient descent to learn optimal weights and predict whether future customers will buy by minimizing the loss function iteratively.</p>
</section>
<section id="cat-non-cat-exercise" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Cat /Non-cat exercise</h1>
<p>We are going to use the cat/non-cat dataset from <code>kaggle</code> package to see how to use these maths to find out if a given image is a cat or not a cat. The cat dataset comes in <code>hdf5</code>format so we will need to install a couple of libraries to load it.</p>
<p>Now we load the datasets for our model. You can download the data from <a href="@https://www.kaggle.com/datasets/sagar2522/cat-vs-non-cat?resource=download">cat/non-cat</a></p>
<p><strong>Data Loading and Preprocessing for Image Classification</strong></p>
<p>Before we can feed our “cat/non-cat” image data into our logistic regression model, we need to load it and transform it into a format that our matrix-based gradient descent implementation can efficiently process. Image data, especially raw pixel values, requires several crucial preprocessing steps to make it suitable for machine learning algorithms.</p>
<ol type="1">
<li><strong>Loading the HDF5 Dataset</strong></li>
</ol>
<p>Our image dataset is stored in HDF5 (<code>.h5</code>) files, a format optimized for storing large arrays of numerical data. The dataset is conveniently split into two files: <code>train_catvsnoncat.h5</code> for training and <code>test_catvsnoncat.h5</code> for evaluation. Within each file, the image pixel data is typically stored under a key like <code>train_set_x</code> (or <code>test_set_x</code>), and their corresponding labels under <code>train_set_y</code> (or <code>test_set_y</code>).</p>
<p>We use the <code>h5read()</code> function from the <code>rhdf5</code> package to extract these specific datasets from the HDF5 files.</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb13-2"><a href="#cb13-2"></a>test_data_file_path <span class="ot">&lt;-</span> <span class="fu">file.path</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;test_catvsnoncat.h5&quot;</span>)</span>
<span id="cb13-3"><a href="#cb13-3"></a>train_data_file_path <span class="ot">&lt;-</span> <span class="fu">file.path</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;train_catvsnoncat.h5&quot;</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="co"># Load training and test data</span></span>
<span id="cb13-6"><a href="#cb13-6"></a>train_dataset <span class="ot">&lt;-</span> <span class="fu">h5read</span>(train_data_file_path, <span class="st">&quot;train_set_x&quot;</span>)</span>
<span id="cb13-7"><a href="#cb13-7"></a>train_labels <span class="ot">&lt;-</span> <span class="fu">h5read</span>(train_data_file_path, <span class="st">&quot;train_set_y&quot;</span>)</span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="co"># Load test data</span></span>
<span id="cb13-10"><a href="#cb13-10"></a>test_dataset <span class="ot">&lt;-</span> <span class="fu">h5read</span>(test_data_file_path, <span class="st">&quot;test_set_x&quot;</span>)</span>
<span id="cb13-11"><a href="#cb13-11"></a>test_labels <span class="ot">&lt;-</span> <span class="fu">h5read</span>(test_data_file_path, <span class="st">&quot;test_set_y&quot;</span>)</span></code></pre></div>
</details>
</div>
<p>Raw image data is inherently multi-dimensional (e.g., <code>64 pixels height x 64 pixels width x 3 color channels for RGB</code>).</p>
<p>By looking at the dimensions of the datasets we can see that we have 209 images in the train dataset and 50 in the test dataset:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="fu">dim</span>(test_dataset)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  3 64 64 50</code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="fu">dim</span>(train_dataset)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1]   3  64  64 209</code></pre>
</div>
</div>
<p>If we want to extract the data for the first cat image (the first cat image is the third image in the train dataset), for example:<br />
</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Extract the third image (which is a 3D array)</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>first_cat_image_raw <span class="ot">&lt;-</span> train_dataset[,,,<span class="dv">3</span>]</span>
<span id="cb18-3"><a href="#cb18-3"></a><span class="fu">dim</span>(first_cat_image_raw)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  3 64 64</code></pre>
</div>
</div>
<p>This will give us a 3X64x64 matrix</p>
<p>To get just the first channel (Red) for the first image:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb20"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># Extract the Red channel (1st channel)</span></span>
<span id="cb20-2"><a href="#cb20-2"></a>first_cat_image_red_channel_raw <span class="ot">&lt;-</span> train_dataset[<span class="dv">1</span>,,,<span class="dv">3</span>]</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="fu">dim</span>(first_cat_image_red_channel_raw)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 64 64</code></pre>
</div>
</div>
<p>Le’t see the first 10 pixels of that channel:<br />
</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb22"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>first_cat_image_red_channel_raw[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]   52   5f   5e   60   63   5e   62   65   67    6d
 [2,]   59   68   61   62   63   60   62   69   69    6e
 [3,]   64   6e   6e   65   6d   67   6a   73   70    70
 [4,]   6a   74   77   6f   74   72   6d   79   77    73
 [5,]   76   78   7a   72   7d   7d   71   77   7c    75
 [6,]   79   7e   80   76   7c   81   7c   7a   81    7c
 [7,]   77   7e   83   7d   7b   7f   83   80   86    85
 [8,]   85   80   84   81   7c   80   85   83   87    88
 [9,]   84   82   83   88   82   7f   87   86   86    88
[10,]   82   7d   7e   83   85   7f   84   8c   8a    89</code></pre>
</div>
</div>
<p>We can see that we have hexadecimal values, so we will need to convert them to numeric:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb24"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Convert character (hex) values to numeric integers ---</span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="co"># &#39;strtoi()&#39; converts string representations of numbers in a given base </span></span>
<span id="cb24-3"><a href="#cb24-3"></a><span class="co">#(here, 16 for hexadecimal) to integers. We apply it directly to the entire array, preserving its dimensions.</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>original_train_dims <span class="ot">&lt;-</span> <span class="fu">dim</span>(train_dataset)</span>
<span id="cb24-5"><a href="#cb24-5"></a>original_test_dims <span class="ot">&lt;-</span> <span class="fu">dim</span>(test_dataset)</span>
<span id="cb24-6"><a href="#cb24-6"></a></span>
<span id="cb24-7"><a href="#cb24-7"></a>train_dataset <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">strtoi</span>(<span class="fu">as.vector</span>(train_dataset), <span class="at">base =</span> <span class="dv">16</span>), <span class="at">dim =</span> original_train_dims)</span>
<span id="cb24-8"><a href="#cb24-8"></a>test_dataset <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">strtoi</span>(<span class="fu">as.vector</span>(test_dataset), <span class="at">base =</span> <span class="dv">16</span>), <span class="at">dim =</span> original_test_dims)</span></code></pre></div>
</details>
</div>
<p>Let’s see the Red channel of the image again to see if its numeric now:<br />
</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb25"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="fu">dim</span>(train_dataset)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1]   3  64  64 209</code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb27"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a>first_cat_image_red_channel_raw <span class="ot">&lt;-</span> train_dataset[<span class="dv">1</span>,,,<span class="dv">3</span>]</span>
<span id="cb27-2"><a href="#cb27-2"></a>first_cat_image_red_channel_raw[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]   82   95   94   96   99   94   98  101  103   109
 [2,]   89  104   97   98   99   96   98  105  105   110
 [3,]  100  110  110  101  109  103  106  115  112   112
 [4,]  106  116  119  111  116  114  109  121  119   115
 [5,]  118  120  122  114  125  125  113  119  124   117
 [6,]  121  126  128  118  124  129  124  122  129   124
 [7,]  119  126  131  125  123  127  131  128  134   133
 [8,]  133  128  132  129  124  128  133  131  135   136
 [9,]  132  130  131  136  130  127  135  134  134   136
[10,]  130  125  126  131  133  127  132  140  138   137</code></pre>
</div>
</div>
<p>Now we can display the Red channel as a grayscale image</p>
<div class="cell" data-layout-align="center">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb29"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a> <span class="co"># Normalize pixel values (0-255 to 0-1) for plotting</span></span>
<span id="cb29-2"><a href="#cb29-2"></a>first_cat_image_red_channel_normalized <span class="ot">&lt;-</span> first_cat_image_red_channel_raw <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="co"># For a single channel, higher values will appear brighter.</span></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="fu">plot</span>(<span class="fu">as.raster</span>(first_cat_image_red_channel_normalized),</span>
<span id="cb29-5"><a href="#cb29-5"></a>     <span class="at">main =</span> <span class="st">&quot;First Image: Red Channel Only (Grayscale)&quot;</span>)</span></code></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/redchannelcat-1.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" width="384" /></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li><strong>Flattening the Images</strong></li>
</ol>
<p>As we have seen, the image data is multi-dimensional, however, our logistic regression model, which operates on linear combinations of features, expects each image to be represented as a single, flat vector of features.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="images/imagepixelvector.png" class="margin-caption img-fluid quarto-figure quarto-figure-center" /></p>
</figure>
</div>
<p>This step transforms each 3D image array into a 1D vector by concatenating all its pixel values. We then stack these individual vectors to form a 2D matrix, where each row corresponds to a single image (sample) and each column represents a specific pixel feature. This conversion makes the data compatible with standard matrix multiplication operations like <span class="math inline">\(Z = XW\)</span>, where <span class="math inline">\(X\)</span> is a <code>(samples x features)</code> matrix.</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb30"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Flattening the images:</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="co"># Each image (accessed by the 4th dimension) is converted to a vector.</span></span>
<span id="cb30-3"><a href="#cb30-3"></a><span class="co"># &#39;t()&#39; transposes the result to get samples as rows, features (pixels) as columns.</span></span>
<span id="cb30-4"><a href="#cb30-4"></a>X_train <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(train_dataset, <span class="dv">4</span>, as.vector))</span>
<span id="cb30-5"><a href="#cb30-5"></a>X_test <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(test_dataset, <span class="dv">4</span>, as.vector))</span>
<span id="cb30-6"><a href="#cb30-6"></a><span class="fu">dim</span>(X_train)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1]   209 12288</code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb32"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="fu">dim</span>(X_test)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1]    50 12288</code></pre>
</div>
</div>
<ol start="3" type="1">
<li><strong>Converting Labels to a Matrix (Column Vector)</strong></li>
</ol>
<p>The labels (0 for non-cat, 1 for cat) are initially loaded as simple numerical vectors. For consistency and robustness in matrix operations within our gradient descent functions (e.g., calculating <code>y_hat - y</code>), it’s good practice to explicitly convert these label vectors into single-column matrices. This ensures that matrix multiplication and subtraction behave as expected without unexpected R vector recycling rules.</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb34"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a><span class="co"># Converting labels to a matrix (column vector):</span></span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="co"># Ensures y_train and y_test are treated as column matrices for consistent</span></span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="co"># matrix operations later in the gradient descent.</span></span>
<span id="cb34-4"><a href="#cb34-4"></a>y_train <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(train_labels)</span>
<span id="cb34-5"><a href="#cb34-5"></a>y_test <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(test_labels)</span>
<span id="cb34-6"><a href="#cb34-6"></a><span class="fu">dim</span>(y_train)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 209   1</code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb36"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a><span class="fu">dim</span>(y_test)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 50  1</code></pre>
</div>
</div>
<p><strong>4. Normalizing Pixel Values</strong></p>
<p>Image pixel intensities range from 0 to 255. This step involves dividing all pixel values by 255 (the max value), scaling them down to a standardized range between 0 and 1.</p>
<p>This normalization is critical for several reasons:</p>
<ul>
<li><p><strong>Numerical Stability:</strong> Machine learning algorithms, especially those that rely on gradient descent, perform much better and converge more reliably when input features are on a similar, small scale. Large input values can lead to extremely large intermediate calculations (<span class="math inline">\(Z\)</span> values) and gradients, potentially causing numerical overflow or instability.</p></li>
<li><p><strong>Faster Convergence:</strong> Scaling features to a consistent range helps the optimization algorithm find the optimal weights more efficiently. The sigmoid activation function, in particular, has a very flat gradient for very large or very small inputs; normalizing helps keep inputs within the active range of the sigmoid, where gradients are stronger and learning is more effective.</p></li>
</ul>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb38"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a>X_train <span class="ot">&lt;-</span> X_train<span class="sc">/</span><span class="dv">255</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>X_test <span class="ot">&lt;-</span> X_test<span class="sc">/</span><span class="dv">255</span></span></code></pre></div>
</details>
</div>
<p><strong>5. Adding a Bias Term (Intercept)</strong></p>
<p>Finally, we add an extra column of <code>1</code>s to the leftmost side of our feature matrices (<code>X_train</code> and <code>X_test</code>).</p>
<p>This column represents the <strong>bias term</strong> (or intercept) for our logistic regression model. In the matrix multiplication <span class="math inline">\(Z = XW\)</span>, if <span class="math inline">\(X\)</span> includes this column of ones, the first element of the weight vector <span class="math inline">\(W\)</span> will correspond to <span class="math inline">\(\beta_0\)</span> (the intercept). This allows the model to learn a baseline probability (or a baseline activation) even if all other feature values are zero. It effectively shifts the decision boundary, giving the model more flexibility to fit the data.</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb39"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># Prepends a column of &#39;1&#39;s to the feature matrices. This allows the model</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="co"># to learn an intercept, which is a baseline prediction independent of features.</span></span>
<span id="cb39-3"><a href="#cb39-3"></a>X_train <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X_train)</span>
<span id="cb39-4"><a href="#cb39-4"></a>X_test <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X_test)</span>
<span id="cb39-5"><a href="#cb39-5"></a></span>
<span id="cb39-6"><a href="#cb39-6"></a><span class="co"># --- Display Final Dimensions for Verification ---</span></span>
<span id="cb39-7"><a href="#cb39-7"></a><span class="co"># These outputs confirm the shape of your data matrices after preprocessing.</span></span>
<span id="cb39-8"><a href="#cb39-8"></a><span class="fu">cat</span>(<span class="st">&quot;--- Final Data Dimensions (after preprocessing) ---</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--- Final Data Dimensions (after preprocessing) ---</code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb41"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of X_train (samples x (features + bias)):&quot;</span>, <span class="fu">dim</span>(X_train), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions of X_train (samples x (features + bias)): 209 12289 </code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb43"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of y_train (samples x 1):&quot;</span>, <span class="fu">dim</span>(y_train), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions of y_train (samples x 1): 209 1 </code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb45"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of X_test (samples x (features + bias)):&quot;</span>, <span class="fu">dim</span>(X_test), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions of X_test (samples x (features + bias)): 50 12289 </code></pre>
</div>
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb47"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of y_test (samples x 1):&quot;</span>, <span class="fu">dim</span>(y_test), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions of y_test (samples x 1): 50 1 </code></pre>
</div>
</div>
<p><strong>Building the parts of our algorithm</strong></p>
<p>The main steps for building a neural network are:</p>
<ol type="1">
<li><p>Define the model structure (such as number of input features)</p></li>
<li><p>Initialize the model’s parameters</p></li>
<li><p>loop:</p>
<ol type="1">
<li><p>Calculate current loss (forward propagation)</p></li>
<li><p>Calculate current gradient (Backward propagation)</p></li>
<li><p>Update parameters (gradient descent)</p></li>
</ol></li>
<li><p>You often build 1-3 separately and integrate them into one function we call model.</p></li>
</ol>
<hr />
<p>Let’s start:</p>
<p>For one sample image <span class="math inline">\(x_i\)</span> :</p>
<p><span class="math display">\[
z_i= w^Tx_i+b
\]</span> the probability of belonging to the cat class will be calculated as:</p>
<p><span class="math display">\[
\hat{y_i} = a_i = sigmoid(z_i)
\]</span></p>
<p>where the formula for the <em>sigmoid</em> function is:</p>
<p><span class="math display">\[
\sigma(Z) = \frac{1}{1 + e^{-Z}}
\]</span></p>
<p>and loss function:<br />
<span class="math display">\[
L = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\]</span></p>
<p>so for each image <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[
L(a_i,y_i) = -y_i \log(a_i)- (1-y_i)log(1-a_i)
\]</span></p>
<p>The cost is then computed by summing over all training examples:</p>
<p><span class="math display">\[
J=\frac{1}{m} \sum^m_{i=1} L(a_1,y_i)
\]</span></p>
<p>This will:</p>
<ul>
<li><p>initialize the parameters of the model</p></li>
<li><p>learn the parameters for the model by minimizing the cost</p></li>
<li><p>Use the learned parameters to make predictions (on the test set)</p></li>
<li><p>Analyse the results.</p></li>
</ul>
<p><strong>Create the helper functions</strong></p>
<p>Implement <code>sigmoid()</code> where <span class="math inline">\(z\)</span> is a scalar or array of any size</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb49"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a><span class="co"># Sigmoid function</span></span>
<span id="cb49-2"><a href="#cb49-2"></a>sigmoid <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb49-3"><a href="#cb49-3"></a>  <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z)))</span>
<span id="cb49-4"><a href="#cb49-4"></a>}</span></code></pre></div>
</details>
</div>
<p>Initialize a initial weight vector with zeros:</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb50"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a>initialize_W_with_zeros<span class="ot">&lt;-</span> <span class="cf">function</span>(dim){</span>
<span id="cb50-2"><a href="#cb50-2"></a>  w <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>,dim,<span class="dv">1</span>)</span>
<span id="cb50-3"><a href="#cb50-3"></a>}</span></code></pre></div>
</details>
</div>
<p>Initialize bias as 0</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb51"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a>bias <span class="ot">=</span> <span class="dv">0</span></span></code></pre></div>
</details>
</div>
<p>The activated input will be the <em>sigmoid</em> for all the <span class="math inline">\(x_i\)</span> values:</p>
<p><span class="math display">\[
A= (a_1,a_2,\cdots,a_m)
\]</span></p>
<p>and <span class="math inline">\(a_1 = sigmoid (W^T x_i + b)\)</span></p>
<div id="formulaexplanation" class="callout-orange">
<p>When we apply the formula above in our code, we do it differently: <span class="math inline">\(XW+b\)</span> The difference between the mathematical notation <span class="math inline">\((W^T x_i + b)\)</span> and the R code <code>X %*%W +b</code> is due to how vector and matrix dimensions are conceptualized and applied in different contexts. Both approaches are mathematically equivalent: In many mathematical texts and derivations, <span class="math inline">\(W\)</span> (the weight vector) is typically represented as a column vector of dimensions (number_of_features X 1). <span class="math inline">\(x_i\)</span> (a single input sample) is also typically represented as a column vector. To compute the doc product for a single sample, you need to multiply a row vector by a column vector, therefore, you just transpose <span class="math inline">\(W\)</span> to get <span class="math inline">\(W^T\)</span> (a <span class="math inline">\(1 \times \text{number_of_features}\)</span> row vector), which can then be multiplied by <span class="math inline">\(x_i\)</span>. This results in a <span class="math inline">\(1 \times 1\)</span> scalar value for <span class="math inline">\(z_i\)</span> for that single sample.</p>
<p>In <code>R</code> and many other programming languages and machine language libraries, the standard convention for the input matrix <span class="math inline">\(X\)</span> is <em>Rows represent individual samples</em> and <em>Columns represent features.</em></p>
<p>So our <span class="math inline">\(X\)</span> matrix has dimensions (<em>number_of_samples</em> x <em>number_of_features</em>) (209 x 12288). Given this and our <span class="math inline">\(W\)</span> vector being a (<em>number_of_features</em> x 1) (12288 x 1), the matrix multiplication perfectly aligns. The result is a (number_of_samples x 1) matrix, where each row contains the linear combination (z) for one sample.</p>
</div>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb52"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1"></a><span class="co"># Computes the activated output (predicted probabilities A) given weights W, bias b, and input X.</span></span>
<span id="cb52-2"><a href="#cb52-2"></a><span class="co"># Returns: A vector/matrix of activated outputs (probabilities), (samples x 1)</span></span>
<span id="cb52-3"><a href="#cb52-3"></a></span>
<span id="cb52-4"><a href="#cb52-4"></a>activate <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X){</span>
<span id="cb52-5"><a href="#cb52-5"></a>  Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W <span class="sc">+</span> b</span>
<span id="cb52-6"><a href="#cb52-6"></a>  </span>
<span id="cb52-7"><a href="#cb52-7"></a>  <span class="co"># Apply the sigmoid activation function element-wise to Z</span></span>
<span id="cb52-8"><a href="#cb52-8"></a>  A <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)</span>
<span id="cb52-9"><a href="#cb52-9"></a>  </span>
<span id="cb52-10"><a href="#cb52-10"></a>  <span class="fu">return</span>(A)</span>
<span id="cb52-11"><a href="#cb52-11"></a>}</span></code></pre></div>
</details>
</div>
<p>Now we create a function to calculate the <em>binary cross-entropy loss</em>:</p>
<p><span class="math display">\[
L = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\]</span></p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb53"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a><span class="co"># Returns: A single scalar value representing the total cost.</span></span>
<span id="cb53-2"><a href="#cb53-2"></a>cost_function <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X, Y) {</span>
<span id="cb53-3"><a href="#cb53-3"></a>  <span class="co"># Calculate the number of samples (m) from the dimensions of X</span></span>
<span id="cb53-4"><a href="#cb53-4"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb53-5"><a href="#cb53-5"></a>  </span>
<span id="cb53-6"><a href="#cb53-6"></a>  <span class="co"># Calculate the activated output (predicted probabilities A) using the activate function</span></span>
<span id="cb53-7"><a href="#cb53-7"></a>  A <span class="ot">&lt;-</span> <span class="fu">activate</span>(W, b, X)</span>
<span id="cb53-8"><a href="#cb53-8"></a>  </span>
<span id="cb53-9"><a href="#cb53-9"></a>  <span class="co"># Calculate the binary cross-entropy loss </span></span>
<span id="cb53-10"><a href="#cb53-10"></a>  cost <span class="ot">&lt;-</span> <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>m) <span class="sc">*</span> <span class="fu">sum</span>(Y <span class="sc">*</span> <span class="fu">log</span>(A) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> Y) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> A))</span>
<span id="cb53-11"><a href="#cb53-11"></a>  </span>
<span id="cb53-12"><a href="#cb53-12"></a>  <span class="fu">return</span>(cost)</span>
<span id="cb53-13"><a href="#cb53-13"></a>}</span></code></pre></div>
</details>
</div>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(w\)</span> is the weights matrix, it will be an array of the same dimensions as features in the flattered dataset (<span class="math inline">\(64 \times 64 \times 3, 1\)</span>)</p></li>
<li><p><span class="math inline">\(b\)</span> is a scalar representing the bias</p></li>
<li><p><span class="math inline">\(X\)</span> is the data (our flattened array)</p></li>
<li><p><span class="math inline">\(Y\)</span> is the labels matrix</p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Understanding Cost
</div>
</div>
<div class="callout-body-container callout-body">
<p>In machine learning, a <strong>cost function</strong> is a mathematical measure of how well (or how poorly) a model performs relative to its task. It quantifies the “error” or “discrepancy” between the model’s predicted output and the actual true values (labels) in your training data.</p>
<p>Think of it as a scoring system:</p>
<ul>
<li><p><strong>Low Cost:</strong> Indicates that your model’s predictions are very close to the actual values.</p></li>
<li><p><strong>High Cost:</strong> Means your model’s predictions are significantly different from the actual values, indicating poor performance.</p></li>
</ul>
<p>The primary goal of training a machine learning model is to <strong>minimize this cost function</strong>. We do this by adjusting the model’s internal parameters (your weights <span class="math inline">\(W\)</span> and bias <span class="math inline">\(b\)</span>) during the learning process (<em>gradient descent</em>).</p>
<p><strong>The Binary Cross-Entropy Cost Function in Detail</strong></p>
<p>In our cat/non-cat example, we’re using a specific type of cost function called <strong>Binary Cross-Entropy (BCE) Loss</strong>, also known as <strong>Log Loss</strong>. This is the standard choice for binary classification problems (where there are only two possible outcomes, like cat/non-cat, 0/1, true/false).</p>
<p><span class="math display">\[
L(a_i,y_i) = -y_i \log(a_i)- (1-y_i)log(1-a_i)
\]</span></p>
<p>This single-example loss function behaves differently depending on the true label <span class="math inline">\(y_i\)</span>:</p>
<ol type="1">
<li><strong>When</strong> <span class="math inline">\(y_i=1\)</span> (it’s a cat): The formula simplifies to:</li>
</ol>
<p><span class="math display">\[
  L(a_i, 1) = -1 \cdot \log(a_i) - (1 - 1) \cdot \log(1-a_i)
\]</span> <span class="math display">\[
    L(a_i, 1) = -\log(a_i) -0
\]</span></p>
<ul>
<li><p>In this case, the cost is only influenced by <span class="math inline">\(\log(a_i)\)</span>.</p></li>
<li><p>If the model predicts a high probability for a cat (<span class="math inline">\(a_i\)</span> close to 1), <span class="math inline">\(\log(a_i)\)</span> will be close to 0 (since <span class="math inline">\(\log(1)=0\)</span>), and thus the loss <span class="math inline">\(\log(a_i)\)</span> will be close to 0. This is good!</p></li>
<li><p>If the model incorrectly predicts a low probability for a cat (<span class="math inline">\(a_i\)</span> close to 0), <span class="math inline">\(\log(a_i)\)</span> will be a large negative number (e.g., <span class="math inline">\(\log(0.01)≈−4.6\)</span>), and thus the loss <span class="math inline">\(-\log(a_i)\)</span> will be a large positive number (e.g., 4.6). This penalizes the model heavily for being confident but wrong.</p></li>
</ul>
<ol start="2" type="1">
<li><strong>When</strong> <span class="math inline">\(y_i=0\)</span> (it’s a non-cat): The formula simplifies to: <span class="math display">\[
L(a_i, 0) = -0 \cdot \log(a_i) - (1 - 0) \cdot \log(1 - a_i)
\]</span> <span class="math display">\[ L(a_i, 0) = -\log(1 - a_i) \]</span> Here, the cost is only influenced by <span class="math inline">\(\log(1−a_i)\)</span>.</li>
</ol>
<ul>
<li>If the model predicts a low probability for a cat (<span class="math inline">\(a_i\)</span>) close to 0, meaning <span class="math inline">\(1−a_i\)</span> is close to 1), <span class="math inline">\(\log(1−a_i)\)</span> will be close to 0, and the loss: <span class="math inline">\(-\log(1−a_i)\)</span>) will be close to 0.</li>
<li>If the model incorrectly predicts a high probability for a cat (<span class="math inline">\(a_i\)</span> close to 1, meaning <span class="math inline">\(1−a_i\)</span> is close to 0), <span class="math inline">\(\log(1−a_i)\)</span> will be a large negative number, and the loss <span class="math inline">\(-\log(1−a_i)\)</span>) will be a large positive number. Again, this penalizes confident wrong predictions.</li>
</ul>
<p><strong>The total Cost(</strong><span class="math inline">\(J\)</span><strong>)</strong>: The overall cost <span class="math inline">\(J\)</span> for the entire training set is the average of the individual losses over all <span class="math inline">\(m\)</span> training examples: <span class="math display">\[
   J= \frac{1}{m}\sum_{i=1}^m L(a_1,y_i)
   \]</span>Averaging the cost is representative of the model’s performance across the entire dataset, and it makes the cost function less sensitive to the size of the training set.</p>
<p><strong>Cost vs. Accuracy</strong> It’s important to distinguish between “cost” and “accuracy”:</p>
<ul>
<li>Cost (e.g., BCE Loss): This is a continuous value that measures the overall “error” of the model. It’s what the optimization algorithm directly tries to minimize. It’s often not directly interpretable as “percentage correct” but provides a nuanced measure of certainty and error.</li>
<li>Accuracy: This is a simple metric that measures the proportion of predictions that were exactly correct (e.g., 90% of images classified correctly). It’s easy for humans to understand, but it’s a discrete measure (either right or wrong).</li>
</ul>
</div>
</div>
<p>The logical next step in building a logistic regression model with gradient descent is to implement the “backward pass”, which involves calculating the gradients. <strong>These gradients tell us how much each weight (W) and the bias (b) contribute to the total cost, and in which direction they need to be adjusted to minimize that cost.</strong></p>
<p>We will create a propagate function. It will combine the forward pass (computing A and cost) with the backward pass (computing the gradients dW and db).</p>
<p>To update our weights (W) and bias (b) during gradient descenct, we need to know the derivative of the <em>cost function</em> with respect to these parameters. These derivatives are known as gradients</p>
<p>Gradient of the cost with respect to the weights (dW): <span class="math display">\[
dw = \frac{1}{m} X^T (A-Y)
\]</span> In <code>R</code> this computes to <code>(1/m) * t(X) %*% (A-Y)</code>.</p>
<p>For bias (db): <span class="math display">\[
db = \frac{1}{m} \sum_{i=1}^m (A_i-Y_i)
\]</span></p>
<p>in <code>R</code> this simplifies to <code>(1/m)* sum(A-Y)</code></p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb54"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1"></a><span class="co"># Returns: A list containing &#39;cost&#39;, &#39;dW&#39;, and &#39;db&#39;.</span></span>
<span id="cb54-2"><a href="#cb54-2"></a>propagate <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X, Y) {</span>
<span id="cb54-3"><a href="#cb54-3"></a>  <span class="co"># Get the number of samples (m)</span></span>
<span id="cb54-4"><a href="#cb54-4"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb54-5"><a href="#cb54-5"></a>  </span>
<span id="cb54-6"><a href="#cb54-6"></a>  <span class="co"># --- Forward Propagation ---</span></span>
<span id="cb54-7"><a href="#cb54-7"></a>  <span class="co"># 1. Calculate activated output (predicted probabilities A)</span></span>
<span id="cb54-8"><a href="#cb54-8"></a>  A <span class="ot">&lt;-</span> <span class="fu">activate</span>(W, b, X)</span>
<span id="cb54-9"><a href="#cb54-9"></a>  </span>
<span id="cb54-10"><a href="#cb54-10"></a>  <span class="co"># 2. Calculate the cost</span></span>
<span id="cb54-11"><a href="#cb54-11"></a>  cost <span class="ot">&lt;-</span> <span class="fu">cost_function</span>(W, b, X, Y) <span class="co"># Using the integrated cost_function</span></span>
<span id="cb54-12"><a href="#cb54-12"></a>  </span>
<span id="cb54-13"><a href="#cb54-13"></a>  <span class="co"># --- Backward Propagation (Calculate Gradients) ---</span></span>
<span id="cb54-14"><a href="#cb54-14"></a>  <span class="co"># 1. Calculate gradient for weights (dW)</span></span>
<span id="cb54-15"><a href="#cb54-15"></a> </span>
<span id="cb54-16"><a href="#cb54-16"></a>  dW <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>m) <span class="sc">*</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> (A <span class="sc">-</span> Y))</span>
<span id="cb54-17"><a href="#cb54-17"></a>  </span>
<span id="cb54-18"><a href="#cb54-18"></a>  <span class="co"># 2. Calculate gradient for bias (db)</span></span>
<span id="cb54-19"><a href="#cb54-19"></a>  <span class="co"># Sums all elements of (A - Y) and averages them.</span></span>
<span id="cb54-20"><a href="#cb54-20"></a>  db <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>m) <span class="sc">*</span> <span class="fu">sum</span>(A <span class="sc">-</span> Y)</span>
<span id="cb54-21"><a href="#cb54-21"></a>  </span>
<span id="cb54-22"><a href="#cb54-22"></a>  <span class="co"># Store gradients in a list</span></span>
<span id="cb54-23"><a href="#cb54-23"></a>  gradients <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">dW =</span> dW, <span class="at">db =</span> db)</span>
<span id="cb54-24"><a href="#cb54-24"></a>  </span>
<span id="cb54-25"><a href="#cb54-25"></a>  <span class="co"># Return results as a list</span></span>
<span id="cb54-26"><a href="#cb54-26"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">cost =</span> cost, <span class="at">gradients =</span> gradients))</span>
<span id="cb54-27"><a href="#cb54-27"></a>}</span></code></pre></div>
</details>
</div>
<p><strong>Optimization: Learning with Gradient descent</strong></p>
<p>Now that we have a function (propagate) that can calculate the cost and the gradients, we need to use those gradients to “learn”. This is done through an optimization algorithm called <em>Gradient Descent</em>.</p>
<p>The core idea is simple: we will iteratively adjust our parameters, <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span>, in the direction that minimally reduces the cost function <span class="math inline">\(J\)</span>. The gradients, <span class="math inline">\(dW\)</span> and <span class="math inline">\(db\)</span>, tell us the direction of the steepest ascent of the cost function, so to decrease the cost, we move in the opposite direction.</p>
<p>The update rules for the parameters are: <span class="math display">\[
W = W- \alpha \cdot dW
\]</span></p>
<p><span class="math display">\[
b = b- \alpha \cdot db
\]</span> Alpha is the learning rate, a crucial hyperparameter that controls how large of a step we take during each update. If <span class="math inline">\(\alpha\)</span> is too large, we might overshoot the optimal value. If it is too small, the training process will be very slow. Finding a good learning rate is a key part of training neural networks</p>
<p>Let’s create an <code>optimize()</code> function that performs this process for a specified number of iterations</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb55"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1"></a><span class="co"># This function optimizes W and b by running a gradient descent algorithm.</span></span>
<span id="cb55-2"><a href="#cb55-2"></a><span class="co">#</span></span>
<span id="cb55-3"><a href="#cb55-3"></a><span class="co"># Arguments:</span></span>
<span id="cb55-4"><a href="#cb55-4"></a><span class="co"># W              -- weights, a numerical matrix of size (num_features, 1)</span></span>
<span id="cb55-5"><a href="#cb55-5"></a><span class="co"># b              -- bias, a scalar</span></span>
<span id="cb55-6"><a href="#cb55-6"></a><span class="co"># X              -- data of size (num_samples, num_features)</span></span>
<span id="cb55-7"><a href="#cb55-7"></a><span class="co"># Y              -- true &quot;label&quot; vector (e.g., 0 for non-cat, 1 for cat)</span></span>
<span id="cb55-8"><a href="#cb55-8"></a><span class="co"># num_iterations -- number of iterations for the optimization loop</span></span>
<span id="cb55-9"><a href="#cb55-9"></a><span class="co"># learning_rate  -- learning rate of the gradient descent update rule</span></span>
<span id="cb55-10"><a href="#cb55-10"></a><span class="co"># print_cost     -- if TRUE, prints the cost every 100 iterations</span></span>
<span id="cb55-11"><a href="#cb55-11"></a><span class="co">#</span></span>
<span id="cb55-12"><a href="#cb55-12"></a><span class="co"># Returns:</span></span>
<span id="cb55-13"><a href="#cb55-13"></a><span class="co"># A list containing the final learned parameters (W, b) and a record of the costs.</span></span>
<span id="cb55-14"><a href="#cb55-14"></a></span>
<span id="cb55-15"><a href="#cb55-15"></a>optimize <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X, Y, num_iterations, learning_rate, <span class="at">print_cost =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb55-16"><a href="#cb55-16"></a>  </span>
<span id="cb55-17"><a href="#cb55-17"></a>  costs <span class="ot">&lt;-</span> <span class="fu">c</span>() <span class="co"># Vector to store the cost at each interval</span></span>
<span id="cb55-18"><a href="#cb55-18"></a>  </span>
<span id="cb55-19"><a href="#cb55-19"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb55-20"><a href="#cb55-20"></a>    <span class="co"># Calculate cost and gradient for the current parameters (W, b)</span></span>
<span id="cb55-21"><a href="#cb55-21"></a>    results <span class="ot">&lt;-</span> <span class="fu">propagate</span>(W, b, X, Y)</span>
<span id="cb55-22"><a href="#cb55-22"></a>    cost <span class="ot">&lt;-</span> results<span class="sc">$</span>cost</span>
<span id="cb55-23"><a href="#cb55-23"></a>    gradients <span class="ot">&lt;-</span> results<span class="sc">$</span>gradients</span>
<span id="cb55-24"><a href="#cb55-24"></a>    dW <span class="ot">&lt;-</span> gradients<span class="sc">$</span>dW</span>
<span id="cb55-25"><a href="#cb55-25"></a>    db <span class="ot">&lt;-</span> gradients<span class="sc">$</span>db</span>
<span id="cb55-26"><a href="#cb55-26"></a>    </span>
<span id="cb55-27"><a href="#cb55-27"></a>    <span class="co"># Update rule for W and b</span></span>
<span id="cb55-28"><a href="#cb55-28"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> dW</span>
<span id="cb55-29"><a href="#cb55-29"></a>    b <span class="ot">&lt;-</span> b <span class="sc">-</span> learning_rate <span class="sc">*</span> db</span>
<span id="cb55-30"><a href="#cb55-30"></a>    </span>
<span id="cb55-31"><a href="#cb55-31"></a>    <span class="co"># Record the cost every 100 iterations</span></span>
<span id="cb55-32"><a href="#cb55-32"></a>    <span class="cf">if</span> (i <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb55-33"><a href="#cb55-33"></a>      costs <span class="ot">&lt;-</span> <span class="fu">c</span>(costs, cost)</span>
<span id="cb55-34"><a href="#cb55-34"></a>      <span class="cf">if</span> (print_cost) {</span>
<span id="cb55-35"><a href="#cb55-35"></a>        <span class="fu">cat</span>(<span class="st">&quot;Cost after iteration&quot;</span>, i, <span class="st">&quot;:&quot;</span>, cost, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb55-36"><a href="#cb55-36"></a>      }</span>
<span id="cb55-37"><a href="#cb55-37"></a>    }</span>
<span id="cb55-38"><a href="#cb55-38"></a>  }</span>
<span id="cb55-39"><a href="#cb55-39"></a>  </span>
<span id="cb55-40"><a href="#cb55-40"></a>  <span class="co"># Return the learned parameters and tracked information</span></span>
<span id="cb55-41"><a href="#cb55-41"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb55-42"><a href="#cb55-42"></a>    <span class="at">W =</span> W,</span>
<span id="cb55-43"><a href="#cb55-43"></a>    <span class="at">b =</span> b,</span>
<span id="cb55-44"><a href="#cb55-44"></a>    <span class="at">costs =</span> costs,</span>
<span id="cb55-45"><a href="#cb55-45"></a>    <span class="at">gradients =</span> <span class="fu">list</span>(<span class="at">dW =</span> dW, <span class="at">db =</span> db)</span>
<span id="cb55-46"><a href="#cb55-46"></a>  ))</span>
<span id="cb55-47"><a href="#cb55-47"></a>}</span></code></pre></div>
</details>
</div>
<p><strong>Making predictions</strong> Once the model has been trained and we have our optimized parameters <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span>, the final step is to make predictions on new data. The prediction process involves two steps:</p>
<ol type="1">
<li><p>Calculate the predicted probabilities $\hatY = A = \sigma(XW+b)$ for a given dataset $X$.</p></li>
<li><p>Convert these probabilities into final predictions (0 or 1). A common convention is to classify an image as cat (1) if its corresponding probability in <span class="math inline">\(A\)</span> is greater than 0.5, and as non-cat (0) otherwise.</p></li>
</ol>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb56"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1"></a><span class="co"># Predicts whether the label is 0 or 1 using learned logistic regression parameters (W, b).</span></span>
<span id="cb56-2"><a href="#cb56-2"></a><span class="co">#</span></span>
<span id="cb56-3"><a href="#cb56-3"></a><span class="co"># Arguments:</span></span>
<span id="cb56-4"><a href="#cb56-4"></a><span class="co"># W -- weights, a numerical matrix of size (num_features, 1)</span></span>
<span id="cb56-5"><a href="#cb56-5"></a><span class="co"># b -- bias, a scalar</span></span>
<span id="cb56-6"><a href="#cb56-6"></a><span class="co"># X -- data of size (num_samples, num_features)</span></span>
<span id="cb56-7"><a href="#cb56-7"></a><span class="co">#</span></span>
<span id="cb56-8"><a href="#cb56-8"></a><span class="co"># Returns:</span></span>
<span id="cb56-9"><a href="#cb56-9"></a><span class="co"># Y_prediction -- a vector of size (num_samples, 1) containing all predictions (0/1) for the examples in X.</span></span>
<span id="cb56-10"><a href="#cb56-10"></a>predict <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X) {</span>
<span id="cb56-11"><a href="#cb56-11"></a>  </span>
<span id="cb56-12"><a href="#cb56-12"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb56-13"><a href="#cb56-13"></a>  Y_prediction <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, m, <span class="dv">1</span>)</span>
<span id="cb56-14"><a href="#cb56-14"></a>  </span>
<span id="cb56-15"><a href="#cb56-15"></a>  <span class="co"># Compute the activation (probabilities) for the input data X</span></span>
<span id="cb56-16"><a href="#cb56-16"></a>  A <span class="ot">&lt;-</span> <span class="fu">activate</span>(W, b, X)</span>
<span id="cb56-17"><a href="#cb56-17"></a>  </span>
<span id="cb56-18"><a href="#cb56-18"></a>  <span class="co"># Convert probabilities to actual predictions</span></span>
<span id="cb56-19"><a href="#cb56-19"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(A)) {</span>
<span id="cb56-20"><a href="#cb56-20"></a>    <span class="cf">if</span> (A[i, <span class="dv">1</span>] <span class="sc">&gt;</span> <span class="fl">0.5</span>) {</span>
<span id="cb56-21"><a href="#cb56-21"></a>      Y_prediction[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb56-22"><a href="#cb56-22"></a>    } <span class="cf">else</span> {</span>
<span id="cb56-23"><a href="#cb56-23"></a>      Y_prediction[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb56-24"><a href="#cb56-24"></a>    }</span>
<span id="cb56-25"><a href="#cb56-25"></a>  }</span>
<span id="cb56-26"><a href="#cb56-26"></a>  </span>
<span id="cb56-27"><a href="#cb56-27"></a>  <span class="fu">return</span>(Y_prediction)</span>
<span id="cb56-28"><a href="#cb56-28"></a>}</span></code></pre></div>
</details>
</div>
<p><strong>Integrating everything into a complete model</strong></p>
<p>We now have all the necessary components. The final step is to assemble them into a single, high level model function. This function will orchestrate the entire workflow: it will initialize the parameters, call the <code>optimize()</code> function to train them, and then use the <code>predict()</code> function to evaluate the model’s performance on both the training and the test data sets.</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb57"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1"></a><span class="co"># Builds the logistic regression model by calling the functions we&#39;ve implemented.</span></span>
<span id="cb57-2"><a href="#cb57-2"></a><span class="co">#</span></span>
<span id="cb57-3"><a href="#cb57-3"></a><span class="co"># Arguments:</span></span>
<span id="cb57-4"><a href="#cb57-4"></a><span class="co"># X_train, Y_train -- training set and its labels</span></span>
<span id="cb57-5"><a href="#cb57-5"></a><span class="co"># X_test, Y_test   -- test set and its labels</span></span>
<span id="cb57-6"><a href="#cb57-6"></a><span class="co"># num_iterations   -- hyperparameter for the number of training iterations</span></span>
<span id="cb57-7"><a href="#cb57-7"></a><span class="co"># learning_rate    -- hyperparameter for the optimization step</span></span>
<span id="cb57-8"><a href="#cb57-8"></a><span class="co"># print_cost       -- if TRUE, prints the cost during training</span></span>
<span id="cb57-9"><a href="#cb57-9"></a><span class="co">#</span></span>
<span id="cb57-10"><a href="#cb57-10"></a><span class="co"># Returns:</span></span>
<span id="cb57-11"><a href="#cb57-11"></a><span class="co"># A list containing information about the trained model.</span></span>
<span id="cb57-12"><a href="#cb57-12"></a>model <span class="ot">&lt;-</span> <span class="cf">function</span>(X_train, Y_train, X_test, Y_test, <span class="at">num_iterations =</span> <span class="dv">2000</span>, <span class="at">learning_rate =</span> <span class="fl">0.5</span>, <span class="at">print_cost =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb57-13"><a href="#cb57-13"></a>  </span>
<span id="cb57-14"><a href="#cb57-14"></a>  <span class="co"># 1. Initialize parameters with zeros</span></span>
<span id="cb57-15"><a href="#cb57-15"></a>  <span class="co"># The number of features corresponds to the number of columns in X_train</span></span>
<span id="cb57-16"><a href="#cb57-16"></a>  num_features <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X_train)</span>
<span id="cb57-17"><a href="#cb57-17"></a>  W <span class="ot">&lt;-</span> <span class="fu">initialize_W_with_zeros</span>(num_features)</span>
<span id="cb57-18"><a href="#cb57-18"></a>  b <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb57-19"><a href="#cb57-19"></a>  </span>
<span id="cb57-20"><a href="#cb57-20"></a>  <span class="co"># 2. Gradient Descent: Learn parameters by calling optimize()</span></span>
<span id="cb57-21"><a href="#cb57-21"></a>  optimization_results <span class="ot">&lt;-</span> <span class="fu">optimize</span>(W, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span>
<span id="cb57-22"><a href="#cb57-22"></a>  </span>
<span id="cb57-23"><a href="#cb57-23"></a>  W_final <span class="ot">&lt;-</span> optimization_results<span class="sc">$</span>W</span>
<span id="cb57-24"><a href="#cb57-24"></a>  b_final <span class="ot">&lt;-</span> optimization_results<span class="sc">$</span>b</span>
<span id="cb57-25"><a href="#cb57-25"></a>  costs <span class="ot">&lt;-</span> optimization_results<span class="sc">$</span>costs</span>
<span id="cb57-26"><a href="#cb57-26"></a>  </span>
<span id="cb57-27"><a href="#cb57-27"></a>  <span class="co"># 3. Predict on the training and test sets</span></span>
<span id="cb57-28"><a href="#cb57-28"></a>  Y_prediction_test <span class="ot">&lt;-</span> <span class="fu">predict</span>(W_final, b_final, X_test)</span>
<span id="cb57-29"><a href="#cb57-29"></a>  Y_prediction_train <span class="ot">&lt;-</span> <span class="fu">predict</span>(W_final, b_final, X_train)</span>
<span id="cb57-30"><a href="#cb57-30"></a>  </span>
<span id="cb57-31"><a href="#cb57-31"></a>  <span class="co"># 4. Calculate and print accuracies</span></span>
<span id="cb57-32"><a href="#cb57-32"></a>  train_accuracy <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(Y_prediction_train <span class="sc">-</span> Y_train)) <span class="sc">*</span> <span class="dv">100</span></span>
<span id="cb57-33"><a href="#cb57-33"></a>  test_accuracy <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(Y_prediction_test <span class="sc">-</span> Y_test)) <span class="sc">*</span> <span class="dv">100</span></span>
<span id="cb57-34"><a href="#cb57-34"></a>  </span>
<span id="cb57-35"><a href="#cb57-35"></a>  <span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">-------------------------------------------</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb57-36"><a href="#cb57-36"></a>  <span class="fu">cat</span>(<span class="st">&quot;Train Accuracy:&quot;</span>, train_accuracy, <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb57-37"><a href="#cb57-37"></a>  <span class="fu">cat</span>(<span class="st">&quot;Test Accuracy:&quot;</span>, test_accuracy, <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb57-38"><a href="#cb57-38"></a>  <span class="fu">cat</span>(<span class="st">&quot;-------------------------------------------</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb57-39"><a href="#cb57-39"></a>  </span>
<span id="cb57-40"><a href="#cb57-40"></a>  <span class="co"># Return a comprehensive list of model results</span></span>
<span id="cb57-41"><a href="#cb57-41"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb57-42"><a href="#cb57-42"></a>    <span class="at">costs =</span> costs,</span>
<span id="cb57-43"><a href="#cb57-43"></a>    <span class="at">Y_prediction_test =</span> Y_prediction_test,</span>
<span id="cb57-44"><a href="#cb57-44"></a>    <span class="at">Y_prediction_train =</span> Y_prediction_train,</span>
<span id="cb57-45"><a href="#cb57-45"></a>    <span class="at">W =</span> W_final,</span>
<span id="cb57-46"><a href="#cb57-46"></a>    <span class="at">b =</span> b_final,</span>
<span id="cb57-47"><a href="#cb57-47"></a>    <span class="at">learning_rate =</span> learning_rate,</span>
<span id="cb57-48"><a href="#cb57-48"></a>    <span class="at">num_iterations =</span> num_iterations</span>
<span id="cb57-49"><a href="#cb57-49"></a>  ))</span>
<span id="cb57-50"><a href="#cb57-50"></a>}</span></code></pre></div>
</details>
</div>
<p><strong>Training the model and analyzing the results</strong></p>
<p>Let’s call our <code>model()</code> function with our preprocessed data and see how well it performs. We will set the learning rate to <span class="math inline">\(0.005\)</span> and run for 2000 iterations.</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb58"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1"></a><span class="co"># Set hyperparameters</span></span>
<span id="cb58-2"><a href="#cb58-2"></a>learning_rate_val <span class="ot">&lt;-</span> <span class="fl">0.005</span></span>
<span id="cb58-3"><a href="#cb58-3"></a>num_iterations_val <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb58-4"><a href="#cb58-4"></a></span>
<span id="cb58-5"><a href="#cb58-5"></a><span class="co"># Run the model</span></span>
<span id="cb58-6"><a href="#cb58-6"></a>model_results <span class="ot">&lt;-</span> <span class="fu">model</span>(X_train, y_train, X_test, y_test, </span>
<span id="cb58-7"><a href="#cb58-7"></a>                       <span class="at">num_iterations =</span> num_iterations_val, </span>
<span id="cb58-8"><a href="#cb58-8"></a>                       <span class="at">learning_rate =</span> learning_rate_val, </span>
<span id="cb58-9"><a href="#cb58-9"></a>                       <span class="at">print_cost =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Cost after iteration 100 : 0.6454642 
Cost after iteration 200 : 0.4853574 
Cost after iteration 300 : 0.3779958 
Cost after iteration 400 : 0.3317399 
Cost after iteration 500 : 0.3035048 
Cost after iteration 600 : 0.2800795 
Cost after iteration 700 : 0.2602181 
Cost after iteration 800 : 0.2430976 
Cost after iteration 900 : 0.2281456 
Cost after iteration 1000 : 0.2149477 
Cost after iteration 1100 : 0.2031951 
Cost after iteration 1200 : 0.1926513 
Cost after iteration 1300 : 0.1831317 
Cost after iteration 1400 : 0.1744892 
Cost after iteration 1500 : 0.1666051 
Cost after iteration 1600 : 0.159382 
Cost after iteration 1700 : 0.1527393 
Cost after iteration 1800 : 0.1466091 
Cost after iteration 1900 : 0.1409344 
Cost after iteration 2000 : 0.1356662 

-------------------------------------------
Train Accuracy: 99.04306 %
Test Accuracy: 70 %
-------------------------------------------</code></pre>
</div>
</div>
<p><strong>Analyzing the learning curve</strong></p>
<p>A great way to check if our gradient descent is working correctly is to plot the cost against the number of iterations. If the model is learning, we should see the cost steadily decrease over time.</p>
<div class="cell">
<details open class="code-fold">
<summary>Code</summary>
<div class="sourceCode" id="cb60"><pre class="sourceCode numberSource r cell-code number-lines"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb60-2"><a href="#cb60-2"></a></span>
<span id="cb60-3"><a href="#cb60-3"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb60-4"><a href="#cb60-4"></a>cost_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb60-5"><a href="#cb60-5"></a>  <span class="at">iterations =</span> <span class="fu">seq</span>(<span class="dv">100</span>, num_iterations_val, <span class="at">by =</span> <span class="dv">100</span>),</span>
<span id="cb60-6"><a href="#cb60-6"></a>  <span class="at">cost =</span> model_results<span class="sc">$</span>costs</span>
<span id="cb60-7"><a href="#cb60-7"></a>)</span>
<span id="cb60-8"><a href="#cb60-8"></a></span>
<span id="cb60-9"><a href="#cb60-9"></a><span class="co"># Plot the cost</span></span>
<span id="cb60-10"><a href="#cb60-10"></a><span class="fu">ggplot</span>(cost_data, <span class="fu">aes</span>(<span class="at">x =</span> iterations, <span class="at">y =</span> cost)) <span class="sc">+</span></span>
<span id="cb60-11"><a href="#cb60-11"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb60-12"><a href="#cb60-12"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb60-13"><a href="#cb60-13"></a>  <span class="fu">labs</span>(</span>
<span id="cb60-14"><a href="#cb60-14"></a>    <span class="at">title =</span> <span class="st">&quot;Cost Function Decrease Over Iterations&quot;</span>,</span>
<span id="cb60-15"><a href="#cb60-15"></a>    <span class="at">x =</span> <span class="st">&quot;Number of Iterations&quot;</span>,</span>
<span id="cb60-16"><a href="#cb60-16"></a>    <span class="at">y =</span> <span class="st">&quot;Cost&quot;</span></span>
<span id="cb60-17"><a href="#cb60-17"></a>  ) <span class="sc">+</span></span>
<span id="cb60-18"><a href="#cb60-18"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img src="logisticRegressionMatrixCalculation_files/figure-html/unnamed-chunk-11-1.png" class="margin-caption img-fluid" width="384" /></p>
</figure>
</div>
</div>
</div>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl">Mastering Statistics: Fundamentals of Data Analysis.</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXItdGl0bGU=">Mastering Statistics: Fundamentals of Data Analysis.</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6SG9tZQ==">Home</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2luZGV4Lmh0bWw=">/index.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6TWF0aCBiYWNrZ3JvdW5k">Math background</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L01hdGhCYWNrZ3JvdW5kLmh0bWw=">/MathBackground.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6UHJvYmFiaWxpdHk=">Probability</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3Byb2JhYmlsaXR5Lmh0bWw=">/probability.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6RXhwbG9yYXRvcnkgQW5hbHlzaXM=">Exploratory Analysis</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2V4cGxvcmF0b3J5QW5hbHlzaXMuaHRtbA==">/exploratoryAnalysis.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6U3VwZXJ2aXNlZCBMZWFybmluZw==">Supervised Learning</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6U3RhdGlzdGljYWwgVGVzdHM=">Statistical Tests</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L0luZmVyZW5jaWFsLmh0bWw=">/Inferencial.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6TGluZWFyIE1vZGVscw==">Linear Models</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2xpbmVhck1vZGVscy5odG1s">/linearModels.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6QXNzZXNzaW5nIE1vZGVsIEFjY3VyYWN5">Assessing Model Accuracy</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L21vZGVsQWNjdXJhY3kuaHRtbA==">/modelAccuracy.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6Q2xhc3NpZmljYXRpb24gcHJvYmxlbXM=">Classification problems</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2NsYXNzaWZpY2F0aW9uLmh0bWw=">/classification.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6UG9seW5vbWlhbCBSZWdyZXNzaW9u">Polynomial Regression</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3BvbHlub21pYWxSZWcuaHRtbA==">/polynomialReg.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6U3Vydml2YWwgQW5hbHlzaXM=">Survival Analysis</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3N1cnZpdmFsLmh0bWw=">/survival.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6VHJlZSBCYXNlZCBNZXRob2Rz">Tree Based Methods</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3RyZWUuaHRtbA==">/tree.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6U3VwcG9ydCBWZWN0b3IgTWFjaGluZXM=">Support Vector Machines</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3N1cHBvcnRWZWN0b3JNYWNoaW5lcy5odG1s">/supportVectorMachines.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6RGVlcCBMZWFybmluZyB3aXRoIFRlbnNvckZsb3c=">Deep Learning with TensorFlow</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6SW5zdGFsbGluZyBUZW5zb3JGbG93">Installing TensorFlow</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2luc3RhbGxpbmdUZW5zb3JGbG93Lmh0bWw=">/installingTensorFlow.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6RGVlcCBMZWFybmluZw==">Deep Learning</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L0RlZXBMZWFybmluZy5odG1s">/DeepLearning.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6RGVlcCBMZWFybmluZyBMYWIgd2l0aCBUb3JjaA==">Deep Learning Lab with Torch</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2RlZXBMZWFybmluZ1RvcmNoLmh0bWw=">/deepLearningTorch.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6VW5zdXBlcnZpc2VkIExlYXJuaW5n">Unsupervised Learning</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L3Vuc3VwZXJ2aXNlZExlYXJuaW5nLmh0bWw=">/unsupervisedLearning.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6TWF0cmljZXM=">Matrices</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6TWF0cml4IEFsZ2VicmE=">Matrix Algebra</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L21hdHJpeC5odG1s">/matrix.html</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6TWF0cmljZXMgaW4gTG9naXN0aWMgUmVncmVzc2lvbiBpbiBOZXVyYWwgTmV0d29ya3M=">Matrices in Logistic Regression in Neural Networks</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXI6L2xvZ2lzdGljUmVncmVzc2lvbk1hdHJpeENhbGN1bGF0aW9uLmh0bWw=">/logisticRegressionMatrixCalculation.html</span></p>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGF0aXRsZQ==">Understanding Matrices and Logistic Regression in Neural Networks – Mastering Statistics: Fundamentals of Data Analysis.</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU=">Understanding Matrices and Logistic Regression in Neural Networks – Mastering Statistics: Fundamentals of Data Analysis.</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZHRpdGxl">Understanding Matrices and Logistic Regression in Neural Networks – Mastering Statistics: Fundamentals of Data Analysis.</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGFzaXRlbmFtZQ==">Mastering Statistics: Fundamentals of Data Analysis.</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw=="></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZGRkZXNj"></span></p>
</div>
<!-- -->
<div class="quarto-embedded-source-code">
<div class="sourceCode" id="cb61" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines"><code class="sourceCode markdown"><span id="cb61-1"><a href="#cb61-1"></a><span class="co">---</span></span>
<span id="cb61-2"><a href="#cb61-2"></a><span class="an">title:</span><span class="co"> &quot;Understanding Matrices and Logistic Regression in Neural Networks&quot;</span></span>
<span id="cb61-3"><a href="#cb61-3"></a><span class="an">editor:</span><span class="co"> </span></span>
<span id="cb61-4"><a href="#cb61-4"></a><span class="co">  markdown: </span></span>
<span id="cb61-5"><a href="#cb61-5"></a><span class="co">    wrap: 72</span></span>
<span id="cb61-6"><a href="#cb61-6"></a><span class="co">---</span></span>
<span id="cb61-7"><a href="#cb61-7"></a></span>
<span id="cb61-8"><a href="#cb61-8"></a>quarto-executable-code-5450563D</span>
<span id="cb61-9"><a href="#cb61-9"></a></span>
<span id="cb61-10"><a href="#cb61-10"></a><span class="in">```r</span></span>
<span id="cb61-11"><a href="#cb61-11"></a><span class="co">#| echo: false</span></span>
<span id="cb61-12"><a href="#cb61-12"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb61-13"><a href="#cb61-13"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb61-14"><a href="#cb61-14"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb61-15"><a href="#cb61-15"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb61-16"><a href="#cb61-16"></a><span class="fu">options</span>(<span class="at">scipen=</span> <span class="dv">999</span>)</span>
<span id="cb61-17"><a href="#cb61-17"></a><span class="in">```</span></span>
<span id="cb61-18"><a href="#cb61-18"></a></span>
<span id="cb61-19"><a href="#cb61-19"></a>In this document, we explore the foundations of matrices in neural</span>
<span id="cb61-20"><a href="#cb61-20"></a>networks, logistic regression, forward propagation, and updating weights</span>
<span id="cb61-21"><a href="#cb61-21"></a>using stochastic gradient descent. We will implement a basic example in</span>
<span id="cb61-22"><a href="#cb61-22"></a><span class="in">`R`</span> to reinforce learning.</span>
<span id="cb61-23"><a href="#cb61-23"></a></span>
<span id="cb61-24"><a href="#cb61-24"></a><span class="fu"># Understanding Matrices in Neural Networks</span></span>
<span id="cb61-25"><a href="#cb61-25"></a></span>
<span id="cb61-26"><a href="#cb61-26"></a>Matrices enable efficient mathematical operations in neural networks.</span>
<span id="cb61-27"><a href="#cb61-27"></a>For a simple model:</span>
<span id="cb61-28"><a href="#cb61-28"></a></span>
<span id="cb61-29"><a href="#cb61-29"></a>$$</span>
<span id="cb61-30"><a href="#cb61-30"></a>Z = XW</span>
<span id="cb61-31"><a href="#cb61-31"></a>$$</span>
<span id="cb61-32"><a href="#cb61-32"></a></span>
<span id="cb61-33"><a href="#cb61-33"></a>where:</span>
<span id="cb61-34"><a href="#cb61-34"></a></span>
<span id="cb61-35"><a href="#cb61-35"></a><span class="ss">-   </span>$X$ is the **input matrix** (containing feature values).</span>
<span id="cb61-36"><a href="#cb61-36"></a></span>
<span id="cb61-37"><a href="#cb61-37"></a><span class="ss">-   </span>$W$ is the **weight matrix** (containing learned coefficients).</span>
<span id="cb61-38"><a href="#cb61-38"></a></span>
<span id="cb61-39"><a href="#cb61-39"></a><span class="ss">-   </span>$Z$ is the **output before activation**.</span>
<span id="cb61-40"><a href="#cb61-40"></a></span>
<span id="cb61-41"><a href="#cb61-41"></a><span class="fu">### **Traditional Logistic Regression Notation**</span></span>
<span id="cb61-42"><a href="#cb61-42"></a></span>
<span id="cb61-43"><a href="#cb61-43"></a>In standard **logistic regression**, the equation is expressed using</span>
<span id="cb61-44"><a href="#cb61-44"></a>individual predictor variables:</span>
<span id="cb61-45"><a href="#cb61-45"></a></span>
<span id="cb61-46"><a href="#cb61-46"></a>$$</span>
<span id="cb61-47"><a href="#cb61-47"></a>Z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n</span>
<span id="cb61-48"><a href="#cb61-48"></a>$$</span>
<span id="cb61-49"><a href="#cb61-49"></a></span>
<span id="cb61-50"><a href="#cb61-50"></a>where:</span>
<span id="cb61-51"><a href="#cb61-51"></a></span>
<span id="cb61-52"><a href="#cb61-52"></a><span class="ss">-   </span>$\beta_0$ is the **intercept (bias term)**.</span>
<span id="cb61-53"><a href="#cb61-53"></a></span>
<span id="cb61-54"><a href="#cb61-54"></a><span class="ss">-   </span>$\beta_1, \beta_2, \dots, \beta_n$ are the **weights (coefficients)</span>
<span id="cb61-55"><a href="#cb61-55"></a>    assigned to each feature**.</span>
<span id="cb61-56"><a href="#cb61-56"></a></span>
<span id="cb61-57"><a href="#cb61-57"></a><span class="ss">-   </span>$x_1, x_2, \dots, x_n$ are the **input feature values** (e.g., time</span>
<span id="cb61-58"><a href="#cb61-58"></a>    spent, pages visited).</span>
<span id="cb61-59"><a href="#cb61-59"></a></span>
<span id="cb61-60"><a href="#cb61-60"></a><span class="ss">-   </span>$Z$ is the **linear combination before applying the activation</span>
<span id="cb61-61"><a href="#cb61-61"></a>    function**.</span>
<span id="cb61-62"><a href="#cb61-62"></a></span>
<span id="cb61-63"><a href="#cb61-63"></a>Once $Z$ is computed, the logistic function (**sigmoid activation**) is</span>
<span id="cb61-64"><a href="#cb61-64"></a>applied:</span>
<span id="cb61-65"><a href="#cb61-65"></a></span>
<span id="cb61-66"><a href="#cb61-66"></a>::: important-formula</span>
<span id="cb61-67"><a href="#cb61-67"></a>$$</span>
<span id="cb61-68"><a href="#cb61-68"></a>\sigma(Z) = \frac{1}{1 + e^{-Z}}</span>
<span id="cb61-69"><a href="#cb61-69"></a>$${eq-sigmoid-function}</span>
<span id="cb61-70"><a href="#cb61-70"></a>:::</span>
<span id="cb61-71"><a href="#cb61-71"></a></span>
<span id="cb61-72"><a href="#cb61-72"></a>Both forms describe the same underlying concept:</span>
<span id="cb61-73"><a href="#cb61-73"></a></span>
<span id="cb61-74"><a href="#cb61-74"></a><span class="ss">-   </span>The **matrix form** ($Z = XW$) is compact and efficient, especially</span>
<span id="cb61-75"><a href="#cb61-75"></a>    when handling multiple samples.</span>
<span id="cb61-76"><a href="#cb61-76"></a></span>
<span id="cb61-77"><a href="#cb61-77"></a><span class="ss">-   </span>The **traditional logistic regression form** explicitly shows the</span>
<span id="cb61-78"><a href="#cb61-78"></a>    relationship between individual features and their respective</span>
<span id="cb61-79"><a href="#cb61-79"></a>    weights.</span>
<span id="cb61-80"><a href="#cb61-80"></a></span>
<span id="cb61-81"><a href="#cb61-81"></a>Both representations lead to the same outcome: a **probability</span>
<span id="cb61-82"><a href="#cb61-82"></a>prediction** via the sigmoid function.</span>
<span id="cb61-83"><a href="#cb61-83"></a></span>
<span id="cb61-84"><a href="#cb61-84"></a>The term $Z$ in the sigmoid activation function is most commonly</span>
<span id="cb61-85"><a href="#cb61-85"></a>referred as the weighted sum of imputs (plus bias)</span>
<span id="cb61-86"><a href="#cb61-86"></a></span>
<span id="cb61-87"><a href="#cb61-87"></a>Herer&#39;s a breakdown: In a single neuron, before an activation function</span>
<span id="cb61-88"><a href="#cb61-88"></a>is applied, a neuron takes multiple inputs, multiplies each input by a</span>
<span id="cb61-89"><a href="#cb61-89"></a>corresponding weight, sums these weighted inputs, and then adds the bias</span>
<span id="cb61-90"><a href="#cb61-90"></a>term. So $Z$ represents the linear combination of the inputs to a neuron</span>
<span id="cb61-91"><a href="#cb61-91"></a>before any any non-linear transformation is applied.</span>
<span id="cb61-92"><a href="#cb61-92"></a></span>
<span id="cb61-93"><a href="#cb61-93"></a>**The concept around activation functions** 1. Introducing</span>
<span id="cb61-94"><a href="#cb61-94"></a>Non-linearity: this is the most important reason for activation</span>
<span id="cb61-95"><a href="#cb61-95"></a>functions. Imagine a neural network without activation functions. Each</span>
<span id="cb61-96"><a href="#cb61-96"></a>neuron would simply perform a linear transformation (weighted sum</span>
<span id="cb61-97"><a href="#cb61-97"></a>+bias). If you stack multiple layers of linear transformations, the</span>
<span id="cb61-98"><a href="#cb61-98"></a>entire network would still be a single linear transformation. Real world</span>
<span id="cb61-99"><a href="#cb61-99"></a>data is almost never linearly separable. Problems like image</span>
<span id="cb61-100"><a href="#cb61-100"></a>recognition, natural language or complex pattern detection involve</span>
<span id="cb61-101"><a href="#cb61-101"></a>highly non-linear relationships. Activation functions introduce</span>
<span id="cb61-102"><a href="#cb61-102"></a>non-linearity, allowing neural network to learn and approximate complex,</span>
<span id="cb61-103"><a href="#cb61-103"></a>non-linear functions and relationships in the data. You can think of an</span>
<span id="cb61-104"><a href="#cb61-104"></a>activation function as determining whether a neuron should &quot;activate&quot; or</span>
<span id="cb61-105"><a href="#cb61-105"></a>&quot;fire&quot; and pass its signal to the next layer. It transforms the raw,</span>
<span id="cb61-106"><a href="#cb61-106"></a>unbounded weighted sum (Z) into an output that is typically within a</span>
<span id="cb61-107"><a href="#cb61-107"></a>specific range, often interpreted as probability or a strength of</span>
<span id="cb61-108"><a href="#cb61-108"></a>activation. Sigmoid maps any real number (Z) to a value between 0 and 1.</span>
<span id="cb61-109"><a href="#cb61-109"></a>This makes it ideal for output layers in binary classification problems,</span>
<span id="cb61-110"><a href="#cb61-110"></a>where the output can be interpreted as a probability.</span>
<span id="cb61-111"><a href="#cb61-111"></a></span>
<span id="cb61-112"><a href="#cb61-112"></a><span class="fu"># Logistic Regression Model</span></span>
<span id="cb61-113"><a href="#cb61-113"></a></span>
<span id="cb61-114"><a href="#cb61-114"></a>Logistic regression predicts probabilities using the **sigmoid</span>
<span id="cb61-115"><a href="#cb61-115"></a>function**: The sigmoid function is a mathematical function that outputs</span>
<span id="cb61-116"><a href="#cb61-116"></a>values between 0 and 1, making it ideal for logistic regression, where</span>
<span id="cb61-117"><a href="#cb61-117"></a>we interpret the result as a probability.</span>
<span id="cb61-118"><a href="#cb61-118"></a></span>
<span id="cb61-119"><a href="#cb61-119"></a>$$</span>
<span id="cb61-120"><a href="#cb61-120"></a>\sigma(Z) = \frac{1}{1 + e^{-Z}}</span>
<span id="cb61-121"><a href="#cb61-121"></a>$$ {#eq-sigmoid}</span>
<span id="cb61-122"><a href="#cb61-122"></a></span>
<span id="cb61-123"><a href="#cb61-123"></a><span class="ss">-   </span>$Z$ is the input value (can be any real number)</span>
<span id="cb61-124"><a href="#cb61-124"></a></span>
<span id="cb61-125"><a href="#cb61-125"></a><span class="ss">-   </span>$e$ is Euler&#39;s number</span>
<span id="cb61-126"><a href="#cb61-126"></a></span>
<span id="cb61-127"><a href="#cb61-127"></a>::: callout-tip</span>
<span id="cb61-128"><a href="#cb61-128"></a>If $Z$ is large and positive,$\sigma(z)$ approaches 1 (strong positive</span>
<span id="cb61-129"><a href="#cb61-129"></a>probability)</span>
<span id="cb61-130"><a href="#cb61-130"></a></span>
<span id="cb61-131"><a href="#cb61-131"></a>If $z$ is large and negative,$\sigma(z)$ approaches 0 (strong negative</span>
<span id="cb61-132"><a href="#cb61-132"></a>probability)</span>
<span id="cb61-133"><a href="#cb61-133"></a></span>
<span id="cb61-134"><a href="#cb61-134"></a>when $z=0$, $\sigma(z)=0.5)$, meaning neutral probability</span>
<span id="cb61-135"><a href="#cb61-135"></a>:::</span>
<span id="cb61-136"><a href="#cb61-136"></a></span>
<span id="cb61-137"><a href="#cb61-137"></a><span class="in">```{r chunk1, echo=FALSE, fig.align=&#39;center&#39;, fig.width=10, fig.asp=0.318}</span></span>
<span id="cb61-138"><a href="#cb61-138"></a><span class="co"># Define the sigmoid function</span></span>
<span id="cb61-139"><a href="#cb61-139"></a>sigmoid <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb61-140"><a href="#cb61-140"></a>  <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z)))</span>
<span id="cb61-141"><a href="#cb61-141"></a>}</span>
<span id="cb61-142"><a href="#cb61-142"></a></span>
<span id="cb61-143"><a href="#cb61-143"></a><span class="co"># Generate values for plotting</span></span>
<span id="cb61-144"><a href="#cb61-144"></a>z_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="at">by=</span><span class="fl">0.1</span>)  <span class="co"># Range from -10 to 10</span></span>
<span id="cb61-145"><a href="#cb61-145"></a>sigmoid_values <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(z_values)</span>
<span id="cb61-146"><a href="#cb61-146"></a></span>
<span id="cb61-147"><a href="#cb61-147"></a><span class="co"># Create a dataframe for plotting</span></span>
<span id="cb61-148"><a href="#cb61-148"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">z =</span> z_values, <span class="at">sigmoid =</span> sigmoid_values)</span>
<span id="cb61-149"><a href="#cb61-149"></a></span>
<span id="cb61-150"><a href="#cb61-150"></a><span class="co"># Plot the sigmoid function</span></span>
<span id="cb61-151"><a href="#cb61-151"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> z, <span class="at">y =</span> sigmoid)) <span class="sc">+</span></span>
<span id="cb61-152"><a href="#cb61-152"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb61-153"><a href="#cb61-153"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Sigmoid Function&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-154"><a href="#cb61-154"></a>  <span class="fu">xlab</span>(<span class="st">&quot;z&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-155"><a href="#cb61-155"></a>  <span class="fu">ylab</span>(<span class="st">&quot;σ(z)&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-156"><a href="#cb61-156"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb61-157"><a href="#cb61-157"></a></span>
<span id="cb61-158"><a href="#cb61-158"></a><span class="in">```</span></span>
<span id="cb61-159"><a href="#cb61-159"></a></span>
<span id="cb61-160"><a href="#cb61-160"></a><span class="fu"># Forward Propagation &amp; Loss Function</span></span>
<span id="cb61-161"><a href="#cb61-161"></a></span>
<span id="cb61-162"><a href="#cb61-162"></a>Predictions ($\hat{y}$) are made using: $$\hat{y} = \sigma(Z)$$ The</span>
<span id="cb61-163"><a href="#cb61-163"></a>**binary cross-entropy loss function** quantifies the error between the</span>
<span id="cb61-164"><a href="#cb61-164"></a>estimate and the real output provided to the model: $$</span>
<span id="cb61-165"><a href="#cb61-165"></a>L = -\frac{1}{m} \sum_{i=1}^{m} \left<span class="co">[</span><span class="ot"> y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right</span><span class="co">]</span></span>
<span id="cb61-166"><a href="#cb61-166"></a>$$ {#eq-loss-function}</span>
<span id="cb61-167"><a href="#cb61-167"></a></span>
<span id="cb61-168"><a href="#cb61-168"></a><span class="fu"># Updating Weights with Gradient Descent</span></span>
<span id="cb61-169"><a href="#cb61-169"></a></span>
<span id="cb61-170"><a href="#cb61-170"></a>Since we don&#39;t know the optimal values of $W$, we start with random</span>
<span id="cb61-171"><a href="#cb61-171"></a>weights and iteratively update them to minimize the loss function. Now,</span>
<span id="cb61-172"><a href="#cb61-172"></a>let&#39;s break down the equation further and clarify the partial</span>
<span id="cb61-173"><a href="#cb61-173"></a>derivatives.</span>
<span id="cb61-174"><a href="#cb61-174"></a></span>
<span id="cb61-175"><a href="#cb61-175"></a>Gradient loss function: $$</span>
<span id="cb61-176"><a href="#cb61-176"></a>dw =\frac{\partial L}{\partial W} = \frac{1}{m} X^T (A-Y)</span>
<span id="cb61-177"><a href="#cb61-177"></a>$$ {#eq-gradient-loss-function}</span>
<span id="cb61-178"><a href="#cb61-178"></a></span>
<span id="cb61-179"><a href="#cb61-179"></a>This term represents the gradient, or the slope, of the loss function</span>
<span id="cb61-180"><a href="#cb61-180"></a>$L$ with respect to $W$. Essentially, it tells us:</span>
<span id="cb61-181"><a href="#cb61-181"></a></span>
<span id="cb61-182"><a href="#cb61-182"></a>How much the loss function changes when we slightly change$W$ The</span>
<span id="cb61-183"><a href="#cb61-183"></a>direction we should move $W$ to minimize the loss.</span>
<span id="cb61-184"><a href="#cb61-184"></a></span>
<span id="cb61-185"><a href="#cb61-185"></a>Since $L$ depends on $W$ (because changing 𝑊affects predictions), we</span>
<span id="cb61-186"><a href="#cb61-186"></a>need to calculate the rate of change of $L$ concerning $W$, which is</span>
<span id="cb61-187"><a href="#cb61-187"></a>where the partial derivative comes in.</span>
<span id="cb61-188"><a href="#cb61-188"></a></span>
<span id="cb61-189"><a href="#cb61-189"></a>*Partial Derivatives Explained* A partial derivative calculates how one</span>
<span id="cb61-190"><a href="#cb61-190"></a>variable changes while keeping others constant. In our case: $$</span>
<span id="cb61-191"><a href="#cb61-191"></a>\frac{\partial L}{\partial W}</span>
<span id="cb61-192"><a href="#cb61-192"></a>$$ measures how much the loss function changes if we make a small</span>
<span id="cb61-193"><a href="#cb61-193"></a>adjustment to $W$. If we visualize the loss function as a mountain, the</span>
<span id="cb61-194"><a href="#cb61-194"></a>gradient tells us which direction leads us downhill the fastest (toward</span>
<span id="cb61-195"><a href="#cb61-195"></a>lower loss). The gradient gives the best direction for adjusting W, but</span>
<span id="cb61-196"><a href="#cb61-196"></a>how far we step in that direction is controlled by the learning rate</span>
<span id="cb61-197"><a href="#cb61-197"></a>$\alpha$</span>
<span id="cb61-198"><a href="#cb61-198"></a></span>
<span id="cb61-199"><a href="#cb61-199"></a>$$</span>
<span id="cb61-200"><a href="#cb61-200"></a>W = W -\alpha \cdot \frac{\partial L}{\partial W}</span>
<span id="cb61-201"><a href="#cb61-201"></a>$$ Where:</span>
<span id="cb61-202"><a href="#cb61-202"></a></span>
<span id="cb61-203"><a href="#cb61-203"></a><span class="ss">-   </span>$\alpha$ is the **learning rate** (a small step size to prevent</span>
<span id="cb61-204"><a href="#cb61-204"></a>    large jumps)</span>
<span id="cb61-205"><a href="#cb61-205"></a></span>
<span id="cb61-206"><a href="#cb61-206"></a><span class="ss">-   </span>$\frac{\partial L}{\partial W}$ is the **gradient**</span>
<span id="cb61-207"><a href="#cb61-207"></a></span>
<span id="cb61-208"><a href="#cb61-208"></a>$W$ is updated gradually with each iteration.</span>
<span id="cb61-209"><a href="#cb61-209"></a></span>
<span id="cb61-210"><a href="#cb61-210"></a>For logistic regression, the gradient of the *binary cross-entropy loss</span>
<span id="cb61-211"><a href="#cb61-211"></a>function* with respect to $W$ is: $$</span>
<span id="cb61-212"><a href="#cb61-212"></a> \frac{\partial L}{\partial W} = \frac{1}{m}X^T(\hat y -y)</span>
<span id="cb61-213"><a href="#cb61-213"></a>$$ Where:</span>
<span id="cb61-214"><a href="#cb61-214"></a></span>
<span id="cb61-215"><a href="#cb61-215"></a><span class="ss">-   </span>$X$ is the input matrix (features)</span>
<span id="cb61-216"><a href="#cb61-216"></a></span>
<span id="cb61-217"><a href="#cb61-217"></a><span class="ss">-   </span>$y$ is the actual target values.</span>
<span id="cb61-218"><a href="#cb61-218"></a></span>
<span id="cb61-219"><a href="#cb61-219"></a><span class="ss">-   </span>$\hat y = \sigma(XW)$ is the predicted output after applying the</span>
<span id="cb61-220"><a href="#cb61-220"></a>    sigmoid function.</span>
<span id="cb61-221"><a href="#cb61-221"></a></span>
<span id="cb61-222"><a href="#cb61-222"></a><span class="ss">-   </span>$X^T(\hat y- y)$ measures the error&#39;s contribution to weight</span>
<span id="cb61-223"><a href="#cb61-223"></a>    updates.</span>
<span id="cb61-224"><a href="#cb61-224"></a></span>
<span id="cb61-225"><a href="#cb61-225"></a><span class="in">```{r chunk2, fig.align=&#39;center&#39;, fig.width=10, fig.asp=0.318}</span></span>
<span id="cb61-226"><a href="#cb61-226"></a><span class="co"># Example loss reduction over iterations</span></span>
<span id="cb61-227"><a href="#cb61-227"></a>iterations <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb61-228"><a href="#cb61-228"></a>loss_values <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.05</span> <span class="sc">*</span> iterations)  <span class="co"># Simulated loss decreasing</span></span>
<span id="cb61-229"><a href="#cb61-229"></a></span>
<span id="cb61-230"><a href="#cb61-230"></a><span class="co"># Create dataframe</span></span>
<span id="cb61-231"><a href="#cb61-231"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">iteration =</span> iterations, <span class="at">loss =</span> loss_values)</span>
<span id="cb61-232"><a href="#cb61-232"></a></span>
<span id="cb61-233"><a href="#cb61-233"></a><span class="co"># Plot loss reduction</span></span>
<span id="cb61-234"><a href="#cb61-234"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> iteration, <span class="at">y =</span> loss)) <span class="sc">+</span></span>
<span id="cb61-235"><a href="#cb61-235"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb61-236"><a href="#cb61-236"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Loss Reduction Over Iterations&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-237"><a href="#cb61-237"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Iteration&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-238"><a href="#cb61-238"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Loss&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-239"><a href="#cb61-239"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb61-240"><a href="#cb61-240"></a></span>
<span id="cb61-241"><a href="#cb61-241"></a><span class="in">```</span></span>
<span id="cb61-242"><a href="#cb61-242"></a></span>
<span id="cb61-243"><a href="#cb61-243"></a><span class="fu"># Implementation in R</span></span>
<span id="cb61-244"><a href="#cb61-244"></a></span>
<span id="cb61-245"><a href="#cb61-245"></a>Scenario: Predicting Whether Someone Will Buy a Product Imagine you&#39;re</span>
<span id="cb61-246"><a href="#cb61-246"></a>running an online store, and you want to predict whether a customer will</span>
<span id="cb61-247"><a href="#cb61-247"></a>buy a product based on two simple features:</span>
<span id="cb61-248"><a href="#cb61-248"></a></span>
<span id="cb61-249"><a href="#cb61-249"></a><span class="ss">-   </span>Time spent on the website (in minutes)</span>
<span id="cb61-250"><a href="#cb61-250"></a></span>
<span id="cb61-251"><a href="#cb61-251"></a><span class="ss">-   </span>Number of pages visited</span>
<span id="cb61-252"><a href="#cb61-252"></a></span>
<span id="cb61-253"><a href="#cb61-253"></a>We&#39;ll create a small dataset with these features and train a logistic</span>
<span id="cb61-254"><a href="#cb61-254"></a>regression model using gradient descent to predict whether a customer</span>
<span id="cb61-255"><a href="#cb61-255"></a>will buy the product (1) or not (0).</span>
<span id="cb61-256"><a href="#cb61-256"></a></span>
<span id="cb61-257"><a href="#cb61-257"></a>The traditional logistical regression formula would be: $$</span>
<span id="cb61-258"><a href="#cb61-258"></a>Z=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+\epsilon_i</span>
<span id="cb61-259"><a href="#cb61-259"></a>$$</span>
<span id="cb61-260"><a href="#cb61-260"></a></span>
<span id="cb61-261"><a href="#cb61-261"></a>where:</span>
<span id="cb61-262"><a href="#cb61-262"></a></span>
<span id="cb61-263"><a href="#cb61-263"></a><span class="ss">-   </span>$\beta_0$ is the **intercept (bias term)**.</span>
<span id="cb61-264"><a href="#cb61-264"></a></span>
<span id="cb61-265"><a href="#cb61-265"></a><span class="ss">-   </span>$\beta_1, \beta_2$ are the **weights (coefficients) assigned to each</span>
<span id="cb61-266"><a href="#cb61-266"></a>    feature**.</span>
<span id="cb61-267"><a href="#cb61-267"></a></span>
<span id="cb61-268"><a href="#cb61-268"></a><span class="ss">-   </span>$x_{1i}, x_{2i}$ are the **feature values** for the$i$-th sample.</span>
<span id="cb61-269"><a href="#cb61-269"></a></span>
<span id="cb61-270"><a href="#cb61-270"></a><span class="ss">-   </span>$\epsilon_i$ is the **error term** accounting for noise in the data.</span>
<span id="cb61-271"><a href="#cb61-271"></a></span>
<span id="cb61-272"><a href="#cb61-272"></a>Instead of writing the equation explicitly for each feature, we can use</span>
<span id="cb61-273"><a href="#cb61-273"></a>matrix multiplication:</span>
<span id="cb61-274"><a href="#cb61-274"></a></span>
<span id="cb61-275"><a href="#cb61-275"></a>$$</span>
<span id="cb61-276"><a href="#cb61-276"></a>Z = XW + \epsilon</span>
<span id="cb61-277"><a href="#cb61-277"></a>$$</span>
<span id="cb61-278"><a href="#cb61-278"></a></span>
<span id="cb61-279"><a href="#cb61-279"></a>Where:</span>
<span id="cb61-280"><a href="#cb61-280"></a></span>
<span id="cb61-281"><a href="#cb61-281"></a><span class="ss">-   </span>$X$ is the **input matrix** containing feature values.</span>
<span id="cb61-282"><a href="#cb61-282"></a></span>
<span id="cb61-283"><a href="#cb61-283"></a><span class="ss">-   </span>$W$ is the **weight matrix** (vector of coefficients).</span>
<span id="cb61-284"><a href="#cb61-284"></a></span>
<span id="cb61-285"><a href="#cb61-285"></a><span class="ss">-   </span>$\epsilon$ is the **error term**.</span>
<span id="cb61-286"><a href="#cb61-286"></a></span>
<span id="cb61-287"><a href="#cb61-287"></a>Expanding this in **matrix notation**:</span>
<span id="cb61-288"><a href="#cb61-288"></a></span>
<span id="cb61-289"><a href="#cb61-289"></a>$$</span>
<span id="cb61-290"><a href="#cb61-290"></a>\begin{bmatrix}</span>
<span id="cb61-291"><a href="#cb61-291"></a>Z_1 <span class="sc">\\</span></span>
<span id="cb61-292"><a href="#cb61-292"></a>Z_2 <span class="sc">\\</span></span>
<span id="cb61-293"><a href="#cb61-293"></a>Z_3 <span class="sc">\\</span></span>
<span id="cb61-294"><a href="#cb61-294"></a>\vdots <span class="sc">\\</span></span>
<span id="cb61-295"><a href="#cb61-295"></a>Z_m</span>
<span id="cb61-296"><a href="#cb61-296"></a>\end{bmatrix}=\begin{bmatrix}</span>
<span id="cb61-297"><a href="#cb61-297"></a>1 &amp; x_{11} &amp; x_{21} <span class="sc">\\</span></span>
<span id="cb61-298"><a href="#cb61-298"></a>1 &amp; x_{12} &amp; x_{22} <span class="sc">\\</span></span>
<span id="cb61-299"><a href="#cb61-299"></a>1 &amp; x_{13} &amp; x_{23} <span class="sc">\\</span></span>
<span id="cb61-300"><a href="#cb61-300"></a>\vdots &amp; \vdots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb61-301"><a href="#cb61-301"></a>1 &amp; x_{1m} &amp; x_{2m}</span>
<span id="cb61-302"><a href="#cb61-302"></a>\end{bmatrix} \begin{bmatrix}</span>
<span id="cb61-303"><a href="#cb61-303"></a>\beta_0 <span class="sc">\\</span></span>
<span id="cb61-304"><a href="#cb61-304"></a>\beta_1 <span class="sc">\\</span></span>
<span id="cb61-305"><a href="#cb61-305"></a>\beta_2</span>
<span id="cb61-306"><a href="#cb61-306"></a>\end{bmatrix}+ \begin{bmatrix}</span>
<span id="cb61-307"><a href="#cb61-307"></a>\epsilon_1 <span class="sc">\\</span></span>
<span id="cb61-308"><a href="#cb61-308"></a>\epsilon_2 <span class="sc">\\</span></span>
<span id="cb61-309"><a href="#cb61-309"></a>\epsilon_3 <span class="sc">\\</span></span>
<span id="cb61-310"><a href="#cb61-310"></a>\vdots <span class="sc">\\</span> \epsilon_m</span>
<span id="cb61-311"><a href="#cb61-311"></a>\end{bmatrix}</span>
<span id="cb61-312"><a href="#cb61-312"></a>$$</span>
<span id="cb61-313"><a href="#cb61-313"></a></span>
<span id="cb61-314"><a href="#cb61-314"></a>Where:</span>
<span id="cb61-315"><a href="#cb61-315"></a></span>
<span id="cb61-316"><a href="#cb61-316"></a><span class="ss">-   </span>Each row in $X$ represents **one sample**, including a **bias term</span>
<span id="cb61-317"><a href="#cb61-317"></a>    (1), feature 1, and feature 2**.</span>
<span id="cb61-318"><a href="#cb61-318"></a></span>
<span id="cb61-319"><a href="#cb61-319"></a><span class="ss">-   </span>The column vector $W$ contains the **learned parameters**</span>
<span id="cb61-320"><a href="#cb61-320"></a>    ($\beta_0, \beta_1, \beta_2$).</span>
<span id="cb61-321"><a href="#cb61-321"></a></span>
<span id="cb61-322"><a href="#cb61-322"></a><span class="ss">-   </span>The error term $\epsilon$ accounts for **random noise in</span>
<span id="cb61-323"><a href="#cb61-323"></a>    predictions**.</span>
<span id="cb61-324"><a href="#cb61-324"></a></span>
<span id="cb61-325"><a href="#cb61-325"></a>We have data from 5 customers, we will create a matrix for this: Each</span>
<span id="cb61-326"><a href="#cb61-326"></a>row in $X$ represents a customer. First column = Bias term (always 1).</span>
<span id="cb61-327"><a href="#cb61-327"></a>Second column = Time spent on the website. Third column = Number of</span>
<span id="cb61-328"><a href="#cb61-328"></a>pages visited. y contains whether the customer bought the product (1) or</span>
<span id="cb61-329"><a href="#cb61-329"></a>not (0).</span>
<span id="cb61-330"><a href="#cb61-330"></a></span>
<span id="cb61-331"><a href="#cb61-331"></a><span class="in">```{r chunk3}</span></span>
<span id="cb61-332"><a href="#cb61-332"></a> <span class="co"># Real-world dataset (Time spent &amp; Pages visited)</span></span>
<span id="cb61-333"><a href="#cb61-333"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(</span>
<span id="cb61-334"><a href="#cb61-334"></a>  <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>,</span>
<span id="cb61-335"><a href="#cb61-335"></a>  <span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">5</span>,</span>
<span id="cb61-336"><a href="#cb61-336"></a>  <span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">7</span>,</span>
<span id="cb61-337"><a href="#cb61-337"></a>  <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>,</span>
<span id="cb61-338"><a href="#cb61-338"></a>  <span class="dv">1</span>, <span class="dv">30</span>, <span class="dv">10</span></span>
<span id="cb61-339"><a href="#cb61-339"></a>), <span class="at">ncol=</span><span class="dv">3</span>, <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb61-340"><a href="#cb61-340"></a></span>
<span id="cb61-341"><a href="#cb61-341"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Labels</span></span>
<span id="cb61-342"><a href="#cb61-342"></a><span class="in">```</span></span>
<span id="cb61-343"><a href="#cb61-343"></a></span>
<span id="cb61-344"><a href="#cb61-344"></a>As a recapitulation, we use the *sigmoid* function to predict the</span>
<span id="cb61-345"><a href="#cb61-345"></a>expected output of any specific combination of minutes in the website</span>
<span id="cb61-346"><a href="#cb61-346"></a>and pages visited. We will create a function for this:</span>
<span id="cb61-347"><a href="#cb61-347"></a></span>
<span id="cb61-348"><a href="#cb61-348"></a><span class="in">```{r sigmoid-function}</span></span>
<span id="cb61-349"><a href="#cb61-349"></a><span class="co"># Sigmoid function</span></span>
<span id="cb61-350"><a href="#cb61-350"></a>sigmoid <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb61-351"><a href="#cb61-351"></a>  <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z)))</span>
<span id="cb61-352"><a href="#cb61-352"></a>}</span>
<span id="cb61-353"><a href="#cb61-353"></a><span class="in">```</span></span>
<span id="cb61-354"><a href="#cb61-354"></a></span>
<span id="cb61-355"><a href="#cb61-355"></a>The *loss function* will tell us how far our predicted value is from the</span>
<span id="cb61-356"><a href="#cb61-356"></a>actual value, we will also create a function for this:</span>
<span id="cb61-357"><a href="#cb61-357"></a></span>
<span id="cb61-358"><a href="#cb61-358"></a><span class="in">```{r loss-function}</span></span>
<span id="cb61-359"><a href="#cb61-359"></a><span class="co"># Loss function (binary cross-entropy)</span></span>
<span id="cb61-360"><a href="#cb61-360"></a>loss_function <span class="ot">&lt;-</span> <span class="cf">function</span>(y, y_hat) {</span>
<span id="cb61-361"><a href="#cb61-361"></a>  <span class="fu">return</span>(<span class="sc">-</span><span class="fu">mean</span>(y <span class="sc">*</span> <span class="fu">log</span>(y_hat) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> y) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> y_hat)))</span>
<span id="cb61-362"><a href="#cb61-362"></a>}</span>
<span id="cb61-363"><a href="#cb61-363"></a><span class="in">```</span></span>
<span id="cb61-364"><a href="#cb61-364"></a></span>
<span id="cb61-365"><a href="#cb61-365"></a>Now we create a function to calculate the *gradient descent*</span>
<span id="cb61-366"><a href="#cb61-366"></a></span>
<span id="cb61-367"><a href="#cb61-367"></a><span class="in">```{r gradient-descent}</span></span>
<span id="cb61-368"><a href="#cb61-368"></a></span>
<span id="cb61-369"><a href="#cb61-369"></a>gradient_descent <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, <span class="at">learning_rate =</span> <span class="fl">0.01</span>, <span class="at">iterations =</span> <span class="dv">50</span>) {</span>
<span id="cb61-370"><a href="#cb61-370"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb61-371"><a href="#cb61-371"></a>  W <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">ncol</span>(X))  <span class="co"># Initialize weights randomly</span></span>
<span id="cb61-372"><a href="#cb61-372"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb61-373"><a href="#cb61-373"></a>    Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W   <span class="co"># Compute Z values for all samples</span></span>
<span id="cb61-374"><a href="#cb61-374"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)  <span class="co"># Apply sigmoid function to all samples</span></span>
<span id="cb61-375"><a href="#cb61-375"></a>    gradient <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> (y_hat <span class="sc">-</span> y) <span class="sc">/</span> m  <span class="co"># Compute gradient for weight updates</span></span>
<span id="cb61-376"><a href="#cb61-376"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> gradient  <span class="co"># Update weights</span></span>
<span id="cb61-377"><a href="#cb61-377"></a>  }</span>
<span id="cb61-378"><a href="#cb61-378"></a>  <span class="fu">return</span> (W)</span>
<span id="cb61-379"><a href="#cb61-379"></a>}</span>
<span id="cb61-380"><a href="#cb61-380"></a><span class="in">```</span></span>
<span id="cb61-381"><a href="#cb61-381"></a></span>
<span id="cb61-382"><a href="#cb61-382"></a>Just for learning purposes, we will create a new version of the gradient</span>
<span id="cb61-383"><a href="#cb61-383"></a>descent function where we store the results of each iteration, so we can</span>
<span id="cb61-384"><a href="#cb61-384"></a>visualize it later:</span>
<span id="cb61-385"><a href="#cb61-385"></a></span>
<span id="cb61-386"><a href="#cb61-386"></a><span class="in">```{r gradien-descent-progress}</span></span>
<span id="cb61-387"><a href="#cb61-387"></a></span>
<span id="cb61-388"><a href="#cb61-388"></a><span class="co"># Gradient Descent Implementation (Tracking Progress)</span></span>
<span id="cb61-389"><a href="#cb61-389"></a>gradient_descent_progress <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, <span class="at">learning_rate =</span> <span class="fl">0.01</span>, <span class="at">iterations =</span> <span class="dv">50</span>) {</span>
<span id="cb61-390"><a href="#cb61-390"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb61-391"><a href="#cb61-391"></a>  W <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">ncol</span>(X))  <span class="co"># Initialize weights randomly</span></span>
<span id="cb61-392"><a href="#cb61-392"></a>  progress <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Iteration =</span> <span class="fu">integer</span>(), </span>
<span id="cb61-393"><a href="#cb61-393"></a>                         <span class="at">Weight1 =</span> <span class="fu">numeric</span>(), <span class="at">Weight2 =</span> <span class="fu">numeric</span>(),</span>
<span id="cb61-394"><a href="#cb61-394"></a>                         <span class="at">Z1 =</span> <span class="fu">numeric</span>(), <span class="at">Z2 =</span> <span class="fu">numeric</span>(), <span class="at">Z3 =</span> <span class="fu">numeric</span>(), <span class="at">Z4 =</span> <span class="fu">numeric</span>(), <span class="at">Z5 =</span> <span class="fu">numeric</span>(),</span>
<span id="cb61-395"><a href="#cb61-395"></a>                         <span class="at">y_hat1 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat2 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat3 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat4 =</span> <span class="fu">numeric</span>(), <span class="at">y_hat5 =</span> <span class="fu">numeric</span>(),</span>
<span id="cb61-396"><a href="#cb61-396"></a>                         <span class="at">Grad1 =</span> <span class="fu">numeric</span>(), <span class="at">Grad2 =</span> <span class="fu">numeric</span>())  <span class="co"># Tracking all five samples</span></span>
<span id="cb61-397"><a href="#cb61-397"></a></span>
<span id="cb61-398"><a href="#cb61-398"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb61-399"><a href="#cb61-399"></a>    Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W   <span class="co"># Compute Z values for all samples</span></span>
<span id="cb61-400"><a href="#cb61-400"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)  <span class="co"># Apply sigmoid function to all samples</span></span>
<span id="cb61-401"><a href="#cb61-401"></a>    gradient <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> (y_hat <span class="sc">-</span> y) <span class="sc">/</span> m  <span class="co"># Compute gradient for weight updates</span></span>
<span id="cb61-402"><a href="#cb61-402"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> gradient  <span class="co"># Update weights</span></span>
<span id="cb61-403"><a href="#cb61-403"></a></span>
<span id="cb61-404"><a href="#cb61-404"></a>    <span class="co"># Store results for all five samples</span></span>
<span id="cb61-405"><a href="#cb61-405"></a>    progress <span class="ot">&lt;-</span> <span class="fu">rbind</span>(progress, <span class="fu">data.frame</span>(</span>
<span id="cb61-406"><a href="#cb61-406"></a>      <span class="at">Iteration =</span> i,</span>
<span id="cb61-407"><a href="#cb61-407"></a>      <span class="at">Weight1 =</span> W[<span class="dv">2</span>], <span class="at">Weight2 =</span> W[<span class="dv">3</span>],</span>
<span id="cb61-408"><a href="#cb61-408"></a>      <span class="at">Z1 =</span> Z[<span class="dv">1</span>], <span class="at">Z2 =</span> Z[<span class="dv">2</span>], <span class="at">Z3 =</span> Z[<span class="dv">3</span>], <span class="at">Z4 =</span> Z[<span class="dv">4</span>], <span class="at">Z5 =</span> Z[<span class="dv">5</span>],</span>
<span id="cb61-409"><a href="#cb61-409"></a>      <span class="at">y_hat1 =</span> y_hat[<span class="dv">1</span>], <span class="at">y_hat2 =</span> y_hat[<span class="dv">2</span>], <span class="at">y_hat3 =</span> y_hat[<span class="dv">3</span>], <span class="at">y_hat4 =</span> y_hat[<span class="dv">4</span>], <span class="at">y_hat5 =</span> y_hat[<span class="dv">5</span>],</span>
<span id="cb61-410"><a href="#cb61-410"></a>      <span class="at">Grad1 =</span> gradient[<span class="dv">2</span>], <span class="at">Grad2 =</span> gradient[<span class="dv">3</span>]</span>
<span id="cb61-411"><a href="#cb61-411"></a>    ))</span>
<span id="cb61-412"><a href="#cb61-412"></a>  }</span>
<span id="cb61-413"><a href="#cb61-413"></a>  </span>
<span id="cb61-414"><a href="#cb61-414"></a>  <span class="fu">return</span>(progress)</span>
<span id="cb61-415"><a href="#cb61-415"></a>}</span>
<span id="cb61-416"><a href="#cb61-416"></a><span class="in">```</span></span>
<span id="cb61-417"><a href="#cb61-417"></a></span>
<span id="cb61-418"><a href="#cb61-418"></a>At the beginning, the weights are random.</span>
<span id="cb61-419"><a href="#cb61-419"></a></span>
<span id="cb61-420"><a href="#cb61-420"></a>Over each iteration, the values change towards better predictions.</span>
<span id="cb61-421"><a href="#cb61-421"></a></span>
<span id="cb61-422"><a href="#cb61-422"></a><span class="ss">-   </span>$Z$ values show the raw linear transformation before activation.</span>
<span id="cb61-423"><a href="#cb61-423"></a></span>
<span id="cb61-424"><a href="#cb61-424"></a><span class="ss">-   </span>$\hat{y}$ (y_hat) tracks how probabilities evolve as weights adjust.</span>
<span id="cb61-425"><a href="#cb61-425"></a></span>
<span id="cb61-426"><a href="#cb61-426"></a><span class="ss">-   </span>Gradient values indicate how weights update to minimize the loss.</span>
<span id="cb61-427"><a href="#cb61-427"></a></span>
<span id="cb61-428"><a href="#cb61-428"></a>If you increase iterations, you&#39;ll see further refinement.</span>
<span id="cb61-429"><a href="#cb61-429"></a></span>
<span id="cb61-430"><a href="#cb61-430"></a><span class="in">```{r chunk4, fig.align=&#39;center&#39;, fig.width=10, fig.asp=0.318}</span></span>
<span id="cb61-431"><a href="#cb61-431"></a><span class="co">#| code-overflow: scroll</span></span>
<span id="cb61-432"><a href="#cb61-432"></a></span>
<span id="cb61-433"><a href="#cb61-433"></a><span class="co"># Train Model</span></span>
<span id="cb61-434"><a href="#cb61-434"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb61-435"><a href="#cb61-435"></a>progress_df <span class="ot">&lt;-</span> <span class="fu">gradient_descent_progress</span>(X, y)</span>
<span id="cb61-436"><a href="#cb61-436"></a></span>
<span id="cb61-437"><a href="#cb61-437"></a><span class="fu">print</span>(<span class="fu">head</span>(progress_df))</span>
<span id="cb61-438"><a href="#cb61-438"></a><span class="in">```</span></span>
<span id="cb61-439"><a href="#cb61-439"></a></span>
<span id="cb61-440"><a href="#cb61-440"></a><span class="in">```{r progres-plot, fig.align=&#39;center&#39;, fig.width=10, fig.asp=0.318, echo=FALSE}</span></span>
<span id="cb61-441"><a href="#cb61-441"></a></span>
<span id="cb61-442"><a href="#cb61-442"></a><span class="co"># Plot how weights change over iterations</span></span>
<span id="cb61-443"><a href="#cb61-443"></a><span class="fu">ggplot</span>(progress_df, <span class="fu">aes</span>(<span class="at">x =</span> Iteration)) <span class="sc">+</span></span>
<span id="cb61-444"><a href="#cb61-444"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> Weight1, <span class="at">color =</span> <span class="st">&quot;Weight1&quot;</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb61-445"><a href="#cb61-445"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> Weight2, <span class="at">color =</span> <span class="st">&quot;Weight2&quot;</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb61-446"><a href="#cb61-446"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Gradient Descent: Evolution of Weights&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-447"><a href="#cb61-447"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Iteration&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-448"><a href="#cb61-448"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Weight Value&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-449"><a href="#cb61-449"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb61-450"><a href="#cb61-450"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Weight1&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;Weight2&quot;</span> <span class="ot">=</span> <span class="st">&quot;red&quot;</span>))</span>
<span id="cb61-451"><a href="#cb61-451"></a></span>
<span id="cb61-452"><a href="#cb61-452"></a><span class="co"># Reshape progress_df to plot variables in separate plots</span></span>
<span id="cb61-453"><a href="#cb61-453"></a>long_df <span class="ot">&lt;-</span> tidyr<span class="sc">::</span><span class="fu">gather</span>(progress_df, <span class="at">key =</span> <span class="st">&quot;Variable&quot;</span>, <span class="at">value =</span> <span class="st">&quot;Value&quot;</span>, <span class="sc">-</span>Iteration)</span>
<span id="cb61-454"><a href="#cb61-454"></a></span>
<span id="cb61-455"><a href="#cb61-455"></a><span class="co"># **Plot 1: Gradient Evolution**</span></span>
<span id="cb61-456"><a href="#cb61-456"></a>gradient_df <span class="ot">&lt;-</span> long_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(Variable <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;Grad1&quot;</span>, <span class="st">&quot;Grad2&quot;</span>))</span>
<span id="cb61-457"><a href="#cb61-457"></a></span>
<span id="cb61-458"><a href="#cb61-458"></a><span class="fu">ggplot</span>(gradient_df, <span class="fu">aes</span>(<span class="at">x =</span> Iteration, <span class="at">y =</span> Value, <span class="at">color =</span> Variable)) <span class="sc">+</span></span>
<span id="cb61-459"><a href="#cb61-459"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span>  <span class="co"># Use points instead of lines</span></span>
<span id="cb61-460"><a href="#cb61-460"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Gradient Descent: Evolution of Gradients&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-461"><a href="#cb61-461"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Iteration&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-462"><a href="#cb61-462"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Gradient Value&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-463"><a href="#cb61-463"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb61-464"><a href="#cb61-464"></a></span>
<span id="cb61-465"><a href="#cb61-465"></a><span class="co"># **Plot 2: Predicted Probabilities (y_hat) Evolution**</span></span>
<span id="cb61-466"><a href="#cb61-466"></a>y_hat_df <span class="ot">&lt;-</span> long_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="fu">grepl</span>(<span class="st">&quot;y_hat&quot;</span>, Variable))</span>
<span id="cb61-467"><a href="#cb61-467"></a></span>
<span id="cb61-468"><a href="#cb61-468"></a><span class="fu">ggplot</span>(y_hat_df, <span class="fu">aes</span>(<span class="at">x =</span> Iteration, <span class="at">y =</span> Value, <span class="at">color =</span> Variable)) <span class="sc">+</span></span>
<span id="cb61-469"><a href="#cb61-469"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span>  <span class="co"># Use points instead of lines</span></span>
<span id="cb61-470"><a href="#cb61-470"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Gradient Descent: Evolution of Predicted Probabilities&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-471"><a href="#cb61-471"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Iteration&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-472"><a href="#cb61-472"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Probability (y_hat)&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-473"><a href="#cb61-473"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb61-474"><a href="#cb61-474"></a></span>
<span id="cb61-475"><a href="#cb61-475"></a><span class="co"># **Plot 3: Z Values Evolution**</span></span>
<span id="cb61-476"><a href="#cb61-476"></a>z_values_df <span class="ot">&lt;-</span> long_df <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="fu">grepl</span>(<span class="st">&quot;Z&quot;</span>, Variable))</span>
<span id="cb61-477"><a href="#cb61-477"></a></span>
<span id="cb61-478"><a href="#cb61-478"></a><span class="fu">ggplot</span>(z_values_df, <span class="fu">aes</span>(<span class="at">x =</span> Iteration, <span class="at">y =</span> Value, <span class="at">color =</span> Variable)) <span class="sc">+</span></span>
<span id="cb61-479"><a href="#cb61-479"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span>  <span class="co"># Use points instead of lines</span></span>
<span id="cb61-480"><a href="#cb61-480"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Gradient Descent: Evolution of Z Values&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-481"><a href="#cb61-481"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Iteration&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-482"><a href="#cb61-482"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Z Value&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-483"><a href="#cb61-483"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb61-484"><a href="#cb61-484"></a><span class="in">```</span></span>
<span id="cb61-485"><a href="#cb61-485"></a></span>
<span id="cb61-486"><a href="#cb61-486"></a>**Understanding the Learned Weights**</span>
<span id="cb61-487"><a href="#cb61-487"></a></span>
<span id="cb61-488"><a href="#cb61-488"></a>After running gradient descent, we obtained the following learned</span>
<span id="cb61-489"><a href="#cb61-489"></a>weights:</span>
<span id="cb61-490"><a href="#cb61-490"></a></span>
<span id="cb61-491"><a href="#cb61-491"></a><span class="in">```{r, chunk5}</span></span>
<span id="cb61-492"><a href="#cb61-492"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb61-493"><a href="#cb61-493"></a><span class="co"># Gradient Descent Implementation (Track Progress)</span></span>
<span id="cb61-494"><a href="#cb61-494"></a>gradient_descent <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, <span class="at">learning_rate =</span> <span class="fl">0.01</span>, <span class="at">iterations =</span> <span class="dv">50</span>) {</span>
<span id="cb61-495"><a href="#cb61-495"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb61-496"><a href="#cb61-496"></a>  W <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">ncol</span>(X))  <span class="co"># Initialize weights randomly</span></span>
<span id="cb61-497"><a href="#cb61-497"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>iterations) {</span>
<span id="cb61-498"><a href="#cb61-498"></a>    Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W   <span class="co"># Compute Z values for all samples</span></span>
<span id="cb61-499"><a href="#cb61-499"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)  <span class="co"># Apply sigmoid function to all samples</span></span>
<span id="cb61-500"><a href="#cb61-500"></a>    gradient <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> (y_hat <span class="sc">-</span> y) <span class="sc">/</span> m  <span class="co"># Compute gradient for weight updates</span></span>
<span id="cb61-501"><a href="#cb61-501"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> gradient  <span class="co"># Update weights</span></span>
<span id="cb61-502"><a href="#cb61-502"></a>  }</span>
<span id="cb61-503"><a href="#cb61-503"></a>  <span class="fu">return</span> (W)</span>
<span id="cb61-504"><a href="#cb61-504"></a>}</span>
<span id="cb61-505"><a href="#cb61-505"></a>W <span class="ot">&lt;-</span> <span class="fu">gradient_descent</span>(X, y)</span>
<span id="cb61-506"><a href="#cb61-506"></a><span class="fu">print</span>(W)</span>
<span id="cb61-507"><a href="#cb61-507"></a><span class="in">```</span></span>
<span id="cb61-508"><a href="#cb61-508"></a></span>
<span id="cb61-509"><a href="#cb61-509"></a>These correspond to:</span>
<span id="cb61-510"><a href="#cb61-510"></a></span>
<span id="cb61-511"><a href="#cb61-511"></a><span class="ss">-   </span>$W_0$ **Bias term** (Intercept).</span>
<span id="cb61-512"><a href="#cb61-512"></a></span>
<span id="cb61-513"><a href="#cb61-513"></a><span class="ss">-   </span>$W_1$ **Effect of Time Spent on probability of purchasing**.</span>
<span id="cb61-514"><a href="#cb61-514"></a></span>
<span id="cb61-515"><a href="#cb61-515"></a><span class="ss">-   </span>$W_2$ **Effect of Pages Visited on probability of purchasing**.A</span>
<span id="cb61-516"><a href="#cb61-516"></a>    positive value increases probability, while a negative value</span>
<span id="cb61-517"><a href="#cb61-517"></a>    decreases probability.</span>
<span id="cb61-518"><a href="#cb61-518"></a></span>
<span id="cb61-519"><a href="#cb61-519"></a>To predict whether a new customer will buy a product, we use the</span>
<span id="cb61-520"><a href="#cb61-520"></a>following equation:</span>
<span id="cb61-521"><a href="#cb61-521"></a></span>
<span id="cb61-522"><a href="#cb61-522"></a>$$</span>
<span id="cb61-523"><a href="#cb61-523"></a>Z = W_0 + W_1 \cdot \text{Time Spent} + W_2 \cdot \text{Pages Visited}</span>
<span id="cb61-524"><a href="#cb61-524"></a>$$</span>
<span id="cb61-525"><a href="#cb61-525"></a></span>
<span id="cb61-526"><a href="#cb61-526"></a>Once we calculate$Z$, we apply the **sigmoid activation function**:</span>
<span id="cb61-527"><a href="#cb61-527"></a></span>
<span id="cb61-528"><a href="#cb61-528"></a>$$</span>
<span id="cb61-529"><a href="#cb61-529"></a>\sigma(Z) = \frac{1}{1 + e^{-Z}}</span>
<span id="cb61-530"><a href="#cb61-530"></a>$$</span>
<span id="cb61-531"><a href="#cb61-531"></a></span>
<span id="cb61-532"><a href="#cb61-532"></a>where $\sigma(Z)$ represents the **probability** that the customer will</span>
<span id="cb61-533"><a href="#cb61-533"></a>buy.</span>
<span id="cb61-534"><a href="#cb61-534"></a></span>
<span id="cb61-535"><a href="#cb61-535"></a>**Example Calculation** Let&#39;s take a new customer who spends **12</span>
<span id="cb61-536"><a href="#cb61-536"></a>minutes** on the website and visits **4 pages**. We calculate $Z$ as:</span>
<span id="cb61-537"><a href="#cb61-537"></a></span>
<span id="cb61-538"><a href="#cb61-538"></a>$$</span>
<span id="cb61-539"><a href="#cb61-539"></a>Z = 0.01567795 + (0.09666904 \times 12) + (0.31485151 \times 4)</span>
<span id="cb61-540"><a href="#cb61-540"></a>$$</span>
<span id="cb61-541"><a href="#cb61-541"></a></span>
<span id="cb61-542"><a href="#cb61-542"></a>Applying the *learned weights*, the prediction follows:</span>
<span id="cb61-543"><a href="#cb61-543"></a></span>
<span id="cb61-544"><a href="#cb61-544"></a>$$</span>
<span id="cb61-545"><a href="#cb61-545"></a>Z = 2.435112</span>
<span id="cb61-546"><a href="#cb61-546"></a>$$</span>
<span id="cb61-547"><a href="#cb61-547"></a></span>
<span id="cb61-548"><a href="#cb61-548"></a>Applying the *sigmoid* function:</span>
<span id="cb61-549"><a href="#cb61-549"></a></span>
<span id="cb61-550"><a href="#cb61-550"></a>$$</span>
<span id="cb61-551"><a href="#cb61-551"></a>\sigma(2.435112) = \frac{1}{1 + e^{-2.435112}}</span>
<span id="cb61-552"><a href="#cb61-552"></a>$$</span>
<span id="cb61-553"><a href="#cb61-553"></a></span>
<span id="cb61-554"><a href="#cb61-554"></a>Approximating:</span>
<span id="cb61-555"><a href="#cb61-555"></a></span>
<span id="cb61-556"><a href="#cb61-556"></a>$$</span>
<span id="cb61-557"><a href="#cb61-557"></a>\sigma(2.435112) \approx 0.91</span>
<span id="cb61-558"><a href="#cb61-558"></a>$$</span>
<span id="cb61-559"><a href="#cb61-559"></a></span>
<span id="cb61-560"><a href="#cb61-560"></a>Thus, the model predicts **91% probability** that this customer **will</span>
<span id="cb61-561"><a href="#cb61-561"></a>buy** the product.</span>
<span id="cb61-562"><a href="#cb61-562"></a></span>
<span id="cb61-563"><a href="#cb61-563"></a>Let&#39;s see the predictions calculated in <span class="in">`r`</span> over the same data we used</span>
<span id="cb61-564"><a href="#cb61-564"></a>for training:</span>
<span id="cb61-565"><a href="#cb61-565"></a></span>
<span id="cb61-566"><a href="#cb61-566"></a><span class="in">```{r prediction-function}</span></span>
<span id="cb61-567"><a href="#cb61-567"></a><span class="co"># Function to predict probability based on learned weights</span></span>
<span id="cb61-568"><a href="#cb61-568"></a>predict_probability <span class="ot">&lt;-</span> <span class="cf">function</span>(X, W) {</span>
<span id="cb61-569"><a href="#cb61-569"></a>  <span class="fu">return</span>(<span class="fu">sigmoid</span>(X <span class="sc">%*%</span> W))</span>
<span id="cb61-570"><a href="#cb61-570"></a>}</span>
<span id="cb61-571"><a href="#cb61-571"></a><span class="in">```</span></span>
<span id="cb61-572"><a href="#cb61-572"></a></span>
<span id="cb61-573"><a href="#cb61-573"></a><span class="in">```{r chunk 6, fig.align=&#39;center&#39;, fig.width=10, fig.asp=0.318}</span></span>
<span id="cb61-574"><a href="#cb61-574"></a><span class="co"># Get predictions</span></span>
<span id="cb61-575"><a href="#cb61-575"></a>predicted_probs <span class="ot">&lt;-</span> <span class="fu">predict_probability</span>(X, W)</span>
<span id="cb61-576"><a href="#cb61-576"></a></span>
<span id="cb61-577"><a href="#cb61-577"></a><span class="co"># Create dataframe for plotting</span></span>
<span id="cb61-578"><a href="#cb61-578"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">TimeSpent =</span> X[,<span class="dv">2</span>], <span class="at">PagesVisited =</span> X[,<span class="dv">3</span>], <span class="at">Probability =</span> predicted_probs)</span>
<span id="cb61-579"><a href="#cb61-579"></a></span>
<span id="cb61-580"><a href="#cb61-580"></a><span class="co"># Plot results</span></span>
<span id="cb61-581"><a href="#cb61-581"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> TimeSpent, <span class="at">y =</span> PagesVisited, <span class="at">color =</span> Probability)) <span class="sc">+</span></span>
<span id="cb61-582"><a href="#cb61-582"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb61-583"><a href="#cb61-583"></a>  <span class="fu">scale_color_gradient</span>(<span class="at">low =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">high =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-584"><a href="#cb61-584"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Logistic Regression: Probability of Purchase&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-585"><a href="#cb61-585"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Time Spent on Website (minutes)&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-586"><a href="#cb61-586"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Number of Pages Visited&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-587"><a href="#cb61-587"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb61-588"><a href="#cb61-588"></a><span class="in">```</span></span>
<span id="cb61-589"><a href="#cb61-589"></a></span>
<span id="cb61-590"><a href="#cb61-590"></a>We apply gradient descent to learn optimal weights and predict whether</span>
<span id="cb61-591"><a href="#cb61-591"></a>future customers will buy by minimizing the loss function iteratively.</span>
<span id="cb61-592"><a href="#cb61-592"></a></span>
<span id="cb61-593"><a href="#cb61-593"></a><span class="fu"># Cat /Non-cat exercise</span></span>
<span id="cb61-594"><a href="#cb61-594"></a></span>
<span id="cb61-595"><a href="#cb61-595"></a>We are going to use the cat/non-cat dataset from <span class="in">`kaggle`</span> package to see</span>
<span id="cb61-596"><a href="#cb61-596"></a>how to use these maths to find out if a given image is a cat or not a</span>
<span id="cb61-597"><a href="#cb61-597"></a>cat. The cat dataset comes in <span class="in">`hdf5`</span>format so we will need to install a</span>
<span id="cb61-598"><a href="#cb61-598"></a>couple of libraries to load it.</span>
<span id="cb61-599"><a href="#cb61-599"></a></span>
<span id="cb61-600"><a href="#cb61-600"></a><span class="in">```{r setup-packages, echo=FALSE, warning=FALSE, message=FALSE}</span></span>
<span id="cb61-601"><a href="#cb61-601"></a><span class="co"># Install BiocManager if you don&#39;t have it (needed for rhdf5)</span></span>
<span id="cb61-602"><a href="#cb61-602"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">requireNamespace</span>(<span class="st">&quot;BiocManager&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) {</span>
<span id="cb61-603"><a href="#cb61-603"></a>    <span class="fu">install.packages</span>(<span class="st">&quot;BiocManager&quot;</span>)</span>
<span id="cb61-604"><a href="#cb61-604"></a>}</span>
<span id="cb61-605"><a href="#cb61-605"></a><span class="co"># Load BiocManager (needed for its install function)</span></span>
<span id="cb61-606"><a href="#cb61-606"></a><span class="fu">library</span>(BiocManager)</span>
<span id="cb61-607"><a href="#cb61-607"></a></span>
<span id="cb61-608"><a href="#cb61-608"></a><span class="co"># Install rhdf5 if not already installed</span></span>
<span id="cb61-609"><a href="#cb61-609"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">requireNamespace</span>(<span class="st">&quot;rhdf5&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) {</span>
<span id="cb61-610"><a href="#cb61-610"></a>    BiocManager<span class="sc">::</span><span class="fu">install</span>(<span class="st">&quot;rhdf5&quot;</span>)</span>
<span id="cb61-611"><a href="#cb61-611"></a>}</span>
<span id="cb61-612"><a href="#cb61-612"></a><span class="co"># Load rhdf5 for use in the script</span></span>
<span id="cb61-613"><a href="#cb61-613"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb61-614"><a href="#cb61-614"></a><span class="in">```</span></span>
<span id="cb61-615"><a href="#cb61-615"></a></span>
<span id="cb61-616"><a href="#cb61-616"></a>Now we load the datasets for our model. You can download the data from</span>
<span id="cb61-617"><a href="#cb61-617"></a><span class="co">[</span><span class="ot">cat/non-cat</span><span class="co">](@https://www.kaggle.com/datasets/sagar2522/cat-vs-non-cat?resource=download)</span></span>
<span id="cb61-618"><a href="#cb61-618"></a></span>
<span id="cb61-619"><a href="#cb61-619"></a>**Data Loading and Preprocessing for Image Classification**</span>
<span id="cb61-620"><a href="#cb61-620"></a></span>
<span id="cb61-621"><a href="#cb61-621"></a>Before we can feed our &quot;cat/non-cat&quot; image data into our logistic</span>
<span id="cb61-622"><a href="#cb61-622"></a>regression model, we need to load it and transform it into a format that</span>
<span id="cb61-623"><a href="#cb61-623"></a>our matrix-based gradient descent implementation can efficiently</span>
<span id="cb61-624"><a href="#cb61-624"></a>process. Image data, especially raw pixel values, requires several</span>
<span id="cb61-625"><a href="#cb61-625"></a>crucial preprocessing steps to make it suitable for machine learning</span>
<span id="cb61-626"><a href="#cb61-626"></a>algorithms.</span>
<span id="cb61-627"><a href="#cb61-627"></a></span>
<span id="cb61-628"><a href="#cb61-628"></a><span class="ss">1.  </span>**Loading the HDF5 Dataset**</span>
<span id="cb61-629"><a href="#cb61-629"></a></span>
<span id="cb61-630"><a href="#cb61-630"></a>Our image dataset is stored in HDF5 (<span class="in">`.h5`</span>) files, a format optimized</span>
<span id="cb61-631"><a href="#cb61-631"></a>for storing large arrays of numerical data. The dataset is conveniently</span>
<span id="cb61-632"><a href="#cb61-632"></a>split into two files: <span class="in">`train_catvsnoncat.h5`</span> for training and</span>
<span id="cb61-633"><a href="#cb61-633"></a><span class="in">`test_catvsnoncat.h5`</span> for evaluation. Within each file, the image pixel</span>
<span id="cb61-634"><a href="#cb61-634"></a>data is typically stored under a key like <span class="in">`train_set_x`</span> (or</span>
<span id="cb61-635"><a href="#cb61-635"></a><span class="in">`test_set_x`</span>), and their corresponding labels under <span class="in">`train_set_y`</span> (or</span>
<span id="cb61-636"><a href="#cb61-636"></a><span class="in">`test_set_y`</span>).</span>
<span id="cb61-637"><a href="#cb61-637"></a></span>
<span id="cb61-638"><a href="#cb61-638"></a>We use the <span class="in">`h5read()`</span> function from the <span class="in">`rhdf5`</span> package to extract these</span>
<span id="cb61-639"><a href="#cb61-639"></a>specific datasets from the HDF5 files.</span>
<span id="cb61-640"><a href="#cb61-640"></a></span>
<span id="cb61-641"><a href="#cb61-641"></a><span class="in">```{r dataloding-cat}</span></span>
<span id="cb61-642"><a href="#cb61-642"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb61-643"><a href="#cb61-643"></a>test_data_file_path <span class="ot">&lt;-</span> <span class="fu">file.path</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;test_catvsnoncat.h5&quot;</span>)</span>
<span id="cb61-644"><a href="#cb61-644"></a>train_data_file_path <span class="ot">&lt;-</span> <span class="fu">file.path</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;train_catvsnoncat.h5&quot;</span>)</span>
<span id="cb61-645"><a href="#cb61-645"></a></span>
<span id="cb61-646"><a href="#cb61-646"></a><span class="co"># Load training and test data</span></span>
<span id="cb61-647"><a href="#cb61-647"></a>train_dataset <span class="ot">&lt;-</span> <span class="fu">h5read</span>(train_data_file_path, <span class="st">&quot;train_set_x&quot;</span>)</span>
<span id="cb61-648"><a href="#cb61-648"></a>train_labels <span class="ot">&lt;-</span> <span class="fu">h5read</span>(train_data_file_path, <span class="st">&quot;train_set_y&quot;</span>)</span>
<span id="cb61-649"><a href="#cb61-649"></a></span>
<span id="cb61-650"><a href="#cb61-650"></a><span class="co"># Load test data</span></span>
<span id="cb61-651"><a href="#cb61-651"></a>test_dataset <span class="ot">&lt;-</span> <span class="fu">h5read</span>(test_data_file_path, <span class="st">&quot;test_set_x&quot;</span>)</span>
<span id="cb61-652"><a href="#cb61-652"></a>test_labels <span class="ot">&lt;-</span> <span class="fu">h5read</span>(test_data_file_path, <span class="st">&quot;test_set_y&quot;</span>)</span>
<span id="cb61-653"><a href="#cb61-653"></a><span class="in">```</span></span>
<span id="cb61-654"><a href="#cb61-654"></a></span>
<span id="cb61-655"><a href="#cb61-655"></a>Raw image data is inherently multi-dimensional (e.g.,</span>
<span id="cb61-656"><a href="#cb61-656"></a><span class="in">`64 pixels height x 64 pixels width x 3 color channels for RGB`</span>).</span>
<span id="cb61-657"><a href="#cb61-657"></a></span>
<span id="cb61-658"><a href="#cb61-658"></a>By looking at the dimensions of the datasets we can see that we have 209</span>
<span id="cb61-659"><a href="#cb61-659"></a>images in the train dataset and 50 in the test dataset:</span>
<span id="cb61-660"><a href="#cb61-660"></a></span>
<span id="cb61-661"><a href="#cb61-661"></a><span class="in">```{r chunk8 }</span></span>
<span id="cb61-662"><a href="#cb61-662"></a><span class="fu">dim</span>(test_dataset)</span>
<span id="cb61-663"><a href="#cb61-663"></a><span class="fu">dim</span>(train_dataset)</span>
<span id="cb61-664"><a href="#cb61-664"></a><span class="in">```</span></span>
<span id="cb61-665"><a href="#cb61-665"></a></span>
<span id="cb61-666"><a href="#cb61-666"></a>If we want to extract the data for the first cat image (the first cat</span>
<span id="cb61-667"><a href="#cb61-667"></a>image is the third image in the train dataset), for example:\</span>
<span id="cb61-668"><a href="#cb61-668"></a></span>
<span id="cb61-669"><a href="#cb61-669"></a><span class="in">```{r chunk9}</span></span>
<span id="cb61-670"><a href="#cb61-670"></a><span class="co"># Extract the third image (which is a 3D array)</span></span>
<span id="cb61-671"><a href="#cb61-671"></a>first_cat_image_raw <span class="ot">&lt;-</span> train_dataset[,,,<span class="dv">3</span>]</span>
<span id="cb61-672"><a href="#cb61-672"></a><span class="fu">dim</span>(first_cat_image_raw)</span>
<span id="cb61-673"><a href="#cb61-673"></a></span>
<span id="cb61-674"><a href="#cb61-674"></a><span class="in">```</span></span>
<span id="cb61-675"><a href="#cb61-675"></a></span>
<span id="cb61-676"><a href="#cb61-676"></a>This will give us a 3X64x64 matrix</span>
<span id="cb61-677"><a href="#cb61-677"></a></span>
<span id="cb61-678"><a href="#cb61-678"></a>To get just the first channel (Red) for the first image:</span>
<span id="cb61-679"><a href="#cb61-679"></a></span>
<span id="cb61-680"><a href="#cb61-680"></a><span class="in">```{r chunk10}</span></span>
<span id="cb61-681"><a href="#cb61-681"></a><span class="co"># Extract the Red channel (1st channel)</span></span>
<span id="cb61-682"><a href="#cb61-682"></a>first_cat_image_red_channel_raw <span class="ot">&lt;-</span> train_dataset[<span class="dv">1</span>,,,<span class="dv">3</span>]</span>
<span id="cb61-683"><a href="#cb61-683"></a><span class="fu">dim</span>(first_cat_image_red_channel_raw)</span>
<span id="cb61-684"><a href="#cb61-684"></a></span>
<span id="cb61-685"><a href="#cb61-685"></a><span class="in">```</span></span>
<span id="cb61-686"><a href="#cb61-686"></a></span>
<span id="cb61-687"><a href="#cb61-687"></a>Le&#39;t see the first 10 pixels of that channel:\</span>
<span id="cb61-688"><a href="#cb61-688"></a></span>
<span id="cb61-689"><a href="#cb61-689"></a>quarto-executable-code-5450563D</span>
<span id="cb61-690"><a href="#cb61-690"></a></span>
<span id="cb61-691"><a href="#cb61-691"></a><span class="in">```r</span></span>
<span id="cb61-692"><a href="#cb61-692"></a>first_cat_image_red_channel_raw[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb61-693"><a href="#cb61-693"></a><span class="in">```</span></span>
<span id="cb61-694"><a href="#cb61-694"></a></span>
<span id="cb61-695"><a href="#cb61-695"></a>We can see that we have hexadecimal values, so we will need to convert</span>
<span id="cb61-696"><a href="#cb61-696"></a>them to numeric:</span>
<span id="cb61-697"><a href="#cb61-697"></a></span>
<span id="cb61-698"><a href="#cb61-698"></a><span class="in">```{r chunk11}</span></span>
<span id="cb61-699"><a href="#cb61-699"></a><span class="co"># Convert character (hex) values to numeric integers ---</span></span>
<span id="cb61-700"><a href="#cb61-700"></a><span class="co"># &#39;strtoi()&#39; converts string representations of numbers in a given base </span></span>
<span id="cb61-701"><a href="#cb61-701"></a><span class="co">#(here, 16 for hexadecimal) to integers. We apply it directly to the entire array, preserving its dimensions.</span></span>
<span id="cb61-702"><a href="#cb61-702"></a>original_train_dims <span class="ot">&lt;-</span> <span class="fu">dim</span>(train_dataset)</span>
<span id="cb61-703"><a href="#cb61-703"></a>original_test_dims <span class="ot">&lt;-</span> <span class="fu">dim</span>(test_dataset)</span>
<span id="cb61-704"><a href="#cb61-704"></a></span>
<span id="cb61-705"><a href="#cb61-705"></a>train_dataset <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">strtoi</span>(<span class="fu">as.vector</span>(train_dataset), <span class="at">base =</span> <span class="dv">16</span>), <span class="at">dim =</span> original_train_dims)</span>
<span id="cb61-706"><a href="#cb61-706"></a>test_dataset <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">strtoi</span>(<span class="fu">as.vector</span>(test_dataset), <span class="at">base =</span> <span class="dv">16</span>), <span class="at">dim =</span> original_test_dims)</span>
<span id="cb61-707"><a href="#cb61-707"></a></span>
<span id="cb61-708"><a href="#cb61-708"></a><span class="in">```</span></span>
<span id="cb61-709"><a href="#cb61-709"></a></span>
<span id="cb61-710"><a href="#cb61-710"></a>Let&#39;s see the Red channel of the image again to see if its numeric now:\</span>
<span id="cb61-711"><a href="#cb61-711"></a></span>
<span id="cb61-712"><a href="#cb61-712"></a><span class="in">```{r chunk12}</span></span>
<span id="cb61-713"><a href="#cb61-713"></a><span class="fu">dim</span>(train_dataset)</span>
<span id="cb61-714"><a href="#cb61-714"></a>first_cat_image_red_channel_raw <span class="ot">&lt;-</span> train_dataset[<span class="dv">1</span>,,,<span class="dv">3</span>]</span>
<span id="cb61-715"><a href="#cb61-715"></a>first_cat_image_red_channel_raw[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb61-716"><a href="#cb61-716"></a><span class="in">```</span></span>
<span id="cb61-717"><a href="#cb61-717"></a></span>
<span id="cb61-718"><a href="#cb61-718"></a>Now we can display the Red channel as a grayscale image</span>
<span id="cb61-719"><a href="#cb61-719"></a></span>
<span id="cb61-720"><a href="#cb61-720"></a><span class="in">```{r redchannelcat, fig.align=&#39;center&#39;}</span></span>
<span id="cb61-721"><a href="#cb61-721"></a> </span>
<span id="cb61-722"><a href="#cb61-722"></a> <span class="co"># Normalize pixel values (0-255 to 0-1) for plotting</span></span>
<span id="cb61-723"><a href="#cb61-723"></a>first_cat_image_red_channel_normalized <span class="ot">&lt;-</span> first_cat_image_red_channel_raw <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb61-724"><a href="#cb61-724"></a><span class="co"># For a single channel, higher values will appear brighter.</span></span>
<span id="cb61-725"><a href="#cb61-725"></a><span class="fu">plot</span>(<span class="fu">as.raster</span>(first_cat_image_red_channel_normalized),</span>
<span id="cb61-726"><a href="#cb61-726"></a>     <span class="at">main =</span> <span class="st">&quot;First Image: Red Channel Only (Grayscale)&quot;</span>)</span>
<span id="cb61-727"><a href="#cb61-727"></a></span>
<span id="cb61-728"><a href="#cb61-728"></a><span class="in">```</span></span>
<span id="cb61-729"><a href="#cb61-729"></a></span>
<span id="cb61-730"><a href="#cb61-730"></a><span class="ss">2.  </span>**Flattening the Images**</span>
<span id="cb61-731"><a href="#cb61-731"></a></span>
<span id="cb61-732"><a href="#cb61-732"></a>As we have seen, the image data is multi-dimensional, however, our</span>
<span id="cb61-733"><a href="#cb61-733"></a>logistic regression model, which operates on linear combinations of</span>
<span id="cb61-734"><a href="#cb61-734"></a>features, expects each image to be represented as a single, flat vector</span>
<span id="cb61-735"><a href="#cb61-735"></a>of features.</span>
<span id="cb61-736"><a href="#cb61-736"></a></span>
<span id="cb61-737"><a href="#cb61-737"></a><span class="al">![](images/imagepixelvector.png)</span>{fig-align=&quot;center&quot;}</span>
<span id="cb61-738"><a href="#cb61-738"></a></span>
<span id="cb61-739"><a href="#cb61-739"></a>This step transforms each 3D image array into a 1D vector by</span>
<span id="cb61-740"><a href="#cb61-740"></a>concatenating all its pixel values. We then stack these individual</span>
<span id="cb61-741"><a href="#cb61-741"></a>vectors to form a 2D matrix, where each row corresponds to a single</span>
<span id="cb61-742"><a href="#cb61-742"></a>image (sample) and each column represents a specific pixel feature. This</span>
<span id="cb61-743"><a href="#cb61-743"></a>conversion makes the data compatible with standard matrix multiplication</span>
<span id="cb61-744"><a href="#cb61-744"></a>operations like $Z = XW$, where $X$ is a <span class="in">`(samples x features)`</span> matrix.</span>
<span id="cb61-745"><a href="#cb61-745"></a></span>
<span id="cb61-746"><a href="#cb61-746"></a><span class="in">```{r  chunk13}</span></span>
<span id="cb61-747"><a href="#cb61-747"></a><span class="co"># Flattening the images:</span></span>
<span id="cb61-748"><a href="#cb61-748"></a><span class="co"># Each image (accessed by the 4th dimension) is converted to a vector.</span></span>
<span id="cb61-749"><a href="#cb61-749"></a><span class="co"># &#39;t()&#39; transposes the result to get samples as rows, features (pixels) as columns.</span></span>
<span id="cb61-750"><a href="#cb61-750"></a>X_train <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(train_dataset, <span class="dv">4</span>, as.vector))</span>
<span id="cb61-751"><a href="#cb61-751"></a>X_test <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(test_dataset, <span class="dv">4</span>, as.vector))</span>
<span id="cb61-752"><a href="#cb61-752"></a><span class="fu">dim</span>(X_train)</span>
<span id="cb61-753"><a href="#cb61-753"></a><span class="fu">dim</span>(X_test)</span>
<span id="cb61-754"><a href="#cb61-754"></a><span class="in">```</span></span>
<span id="cb61-755"><a href="#cb61-755"></a></span>
<span id="cb61-756"><a href="#cb61-756"></a><span class="ss">3.  </span>**Converting Labels to a Matrix (Column Vector)**</span>
<span id="cb61-757"><a href="#cb61-757"></a></span>
<span id="cb61-758"><a href="#cb61-758"></a>The labels (0 for non-cat, 1 for cat) are initially loaded as simple</span>
<span id="cb61-759"><a href="#cb61-759"></a>numerical vectors. For consistency and robustness in matrix operations</span>
<span id="cb61-760"><a href="#cb61-760"></a>within our gradient descent functions (e.g., calculating <span class="in">`y_hat - y`</span>),</span>
<span id="cb61-761"><a href="#cb61-761"></a>it&#39;s good practice to explicitly convert these label vectors into</span>
<span id="cb61-762"><a href="#cb61-762"></a>single-column matrices. This ensures that matrix multiplication and</span>
<span id="cb61-763"><a href="#cb61-763"></a>subtraction behave as expected without unexpected R vector recycling</span>
<span id="cb61-764"><a href="#cb61-764"></a>rules.</span>
<span id="cb61-765"><a href="#cb61-765"></a></span>
<span id="cb61-766"><a href="#cb61-766"></a><span class="in">```{r chunk14}</span></span>
<span id="cb61-767"><a href="#cb61-767"></a><span class="co"># Converting labels to a matrix (column vector):</span></span>
<span id="cb61-768"><a href="#cb61-768"></a><span class="co"># Ensures y_train and y_test are treated as column matrices for consistent</span></span>
<span id="cb61-769"><a href="#cb61-769"></a><span class="co"># matrix operations later in the gradient descent.</span></span>
<span id="cb61-770"><a href="#cb61-770"></a>y_train <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(train_labels)</span>
<span id="cb61-771"><a href="#cb61-771"></a>y_test <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(test_labels)</span>
<span id="cb61-772"><a href="#cb61-772"></a><span class="fu">dim</span>(y_train)</span>
<span id="cb61-773"><a href="#cb61-773"></a><span class="fu">dim</span>(y_test)</span>
<span id="cb61-774"><a href="#cb61-774"></a><span class="in">```</span></span>
<span id="cb61-775"><a href="#cb61-775"></a></span>
<span id="cb61-776"><a href="#cb61-776"></a>**4. Normalizing Pixel Values**</span>
<span id="cb61-777"><a href="#cb61-777"></a></span>
<span id="cb61-778"><a href="#cb61-778"></a>Image pixel intensities range from 0 to 255. This step involves dividing</span>
<span id="cb61-779"><a href="#cb61-779"></a>all pixel values by 255 (the max value), scaling them down to a</span>
<span id="cb61-780"><a href="#cb61-780"></a>standardized range between 0 and 1.</span>
<span id="cb61-781"><a href="#cb61-781"></a></span>
<span id="cb61-782"><a href="#cb61-782"></a>This normalization is critical for several reasons:</span>
<span id="cb61-783"><a href="#cb61-783"></a></span>
<span id="cb61-784"><a href="#cb61-784"></a><span class="ss">-   </span>**Numerical Stability:** Machine learning algorithms, especially</span>
<span id="cb61-785"><a href="#cb61-785"></a>    those that rely on gradient descent, perform much better and</span>
<span id="cb61-786"><a href="#cb61-786"></a>    converge more reliably when input features are on a similar, small</span>
<span id="cb61-787"><a href="#cb61-787"></a>    scale. Large input values can lead to extremely large intermediate</span>
<span id="cb61-788"><a href="#cb61-788"></a>    calculations ($Z$ values) and gradients, potentially causing</span>
<span id="cb61-789"><a href="#cb61-789"></a>    numerical overflow or instability.</span>
<span id="cb61-790"><a href="#cb61-790"></a></span>
<span id="cb61-791"><a href="#cb61-791"></a><span class="ss">-   </span>**Faster Convergence:** Scaling features to a consistent range helps</span>
<span id="cb61-792"><a href="#cb61-792"></a>    the optimization algorithm find the optimal weights more</span>
<span id="cb61-793"><a href="#cb61-793"></a>    efficiently. The sigmoid activation function, in particular, has a</span>
<span id="cb61-794"><a href="#cb61-794"></a>    very flat gradient for very large or very small inputs; normalizing</span>
<span id="cb61-795"><a href="#cb61-795"></a>    helps keep inputs within the active range of the sigmoid, where</span>
<span id="cb61-796"><a href="#cb61-796"></a>    gradients are stronger and learning is more effective.</span>
<span id="cb61-797"><a href="#cb61-797"></a></span>
<span id="cb61-798"><a href="#cb61-798"></a><span class="in">```{r chunk15}</span></span>
<span id="cb61-799"><a href="#cb61-799"></a>X_train <span class="ot">&lt;-</span> X_train<span class="sc">/</span><span class="dv">255</span></span>
<span id="cb61-800"><a href="#cb61-800"></a>X_test <span class="ot">&lt;-</span> X_test<span class="sc">/</span><span class="dv">255</span></span>
<span id="cb61-801"><a href="#cb61-801"></a><span class="in">```</span></span>
<span id="cb61-802"><a href="#cb61-802"></a></span>
<span id="cb61-803"><a href="#cb61-803"></a>**5. Adding a Bias Term (Intercept)**</span>
<span id="cb61-804"><a href="#cb61-804"></a></span>
<span id="cb61-805"><a href="#cb61-805"></a>Finally, we add an extra column of <span class="in">`1`</span>s to the leftmost side of our</span>
<span id="cb61-806"><a href="#cb61-806"></a>feature matrices (<span class="in">`X_train`</span> and <span class="in">`X_test`</span>).</span>
<span id="cb61-807"><a href="#cb61-807"></a></span>
<span id="cb61-808"><a href="#cb61-808"></a>This column represents the **bias term** (or intercept) for our logistic</span>
<span id="cb61-809"><a href="#cb61-809"></a>regression model. In the matrix multiplication $Z = XW$, if $X$ includes</span>
<span id="cb61-810"><a href="#cb61-810"></a>this column of ones, the first element of the weight vector $W$ will</span>
<span id="cb61-811"><a href="#cb61-811"></a>correspond to $\beta_0$ (the intercept). This allows the model to learn</span>
<span id="cb61-812"><a href="#cb61-812"></a>a baseline probability (or a baseline activation) even if all other</span>
<span id="cb61-813"><a href="#cb61-813"></a>feature values are zero. It effectively shifts the decision boundary,</span>
<span id="cb61-814"><a href="#cb61-814"></a>giving the model more flexibility to fit the data.</span>
<span id="cb61-815"><a href="#cb61-815"></a></span>
<span id="cb61-816"><a href="#cb61-816"></a>quarto-executable-code-5450563D</span>
<span id="cb61-817"><a href="#cb61-817"></a></span>
<span id="cb61-818"><a href="#cb61-818"></a><span class="in">```r</span></span>
<span id="cb61-819"><a href="#cb61-819"></a><span class="co"># Prepends a column of &#39;1&#39;s to the feature matrices. This allows the model</span></span>
<span id="cb61-820"><a href="#cb61-820"></a><span class="co"># to learn an intercept, which is a baseline prediction independent of features.</span></span>
<span id="cb61-821"><a href="#cb61-821"></a>X_train <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X_train)</span>
<span id="cb61-822"><a href="#cb61-822"></a>X_test <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X_test)</span>
<span id="cb61-823"><a href="#cb61-823"></a></span>
<span id="cb61-824"><a href="#cb61-824"></a><span class="co"># --- Display Final Dimensions for Verification ---</span></span>
<span id="cb61-825"><a href="#cb61-825"></a><span class="co"># These outputs confirm the shape of your data matrices after preprocessing.</span></span>
<span id="cb61-826"><a href="#cb61-826"></a><span class="fu">cat</span>(<span class="st">&quot;--- Final Data Dimensions (after preprocessing) ---</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-827"><a href="#cb61-827"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of X_train (samples x (features + bias)):&quot;</span>, <span class="fu">dim</span>(X_train), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-828"><a href="#cb61-828"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of y_train (samples x 1):&quot;</span>, <span class="fu">dim</span>(y_train), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-829"><a href="#cb61-829"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of X_test (samples x (features + bias)):&quot;</span>, <span class="fu">dim</span>(X_test), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-830"><a href="#cb61-830"></a><span class="fu">cat</span>(<span class="st">&quot;Dimensions of y_test (samples x 1):&quot;</span>, <span class="fu">dim</span>(y_test), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-831"><a href="#cb61-831"></a><span class="in">```</span></span>
<span id="cb61-832"><a href="#cb61-832"></a></span>
<span id="cb61-833"><a href="#cb61-833"></a>**Building the parts of our algorithm**</span>
<span id="cb61-834"><a href="#cb61-834"></a></span>
<span id="cb61-835"><a href="#cb61-835"></a>The main steps for building a neural network are:</span>
<span id="cb61-836"><a href="#cb61-836"></a></span>
<span id="cb61-837"><a href="#cb61-837"></a><span class="ss">1.  </span>Define the model structure (such as number of input features)</span>
<span id="cb61-838"><a href="#cb61-838"></a></span>
<span id="cb61-839"><a href="#cb61-839"></a><span class="ss">2.  </span>Initialize the model&#39;s parameters</span>
<span id="cb61-840"><a href="#cb61-840"></a></span>
<span id="cb61-841"><a href="#cb61-841"></a><span class="ss">3.  </span>loop:</span>
<span id="cb61-842"><a href="#cb61-842"></a></span>
<span id="cb61-843"><a href="#cb61-843"></a><span class="ss">    1.  </span>Calculate current loss (forward propagation)</span>
<span id="cb61-844"><a href="#cb61-844"></a></span>
<span id="cb61-845"><a href="#cb61-845"></a><span class="ss">    2.  </span>Calculate current gradient (Backward propagation)</span>
<span id="cb61-846"><a href="#cb61-846"></a></span>
<span id="cb61-847"><a href="#cb61-847"></a><span class="ss">    3.  </span>Update parameters (gradient descent)</span>
<span id="cb61-848"><a href="#cb61-848"></a></span>
<span id="cb61-849"><a href="#cb61-849"></a><span class="ss">4.  </span>You often build 1-3 separately and integrate them into one function</span>
<span id="cb61-850"><a href="#cb61-850"></a>    we call model.</span>
<span id="cb61-851"><a href="#cb61-851"></a></span>
<span id="cb61-852"><a href="#cb61-852"></a>------------------------------------------------------------------------</span>
<span id="cb61-853"><a href="#cb61-853"></a></span>
<span id="cb61-854"><a href="#cb61-854"></a>Let&#39;s start:</span>
<span id="cb61-855"><a href="#cb61-855"></a></span>
<span id="cb61-856"><a href="#cb61-856"></a>For one sample image $x_i$ :</span>
<span id="cb61-857"><a href="#cb61-857"></a></span>
<span id="cb61-858"><a href="#cb61-858"></a>$$</span>
<span id="cb61-859"><a href="#cb61-859"></a>z_i= w^Tx_i+b</span>
<span id="cb61-860"><a href="#cb61-860"></a>$$ the probability of belonging to the cat class will be calculated as:</span>
<span id="cb61-861"><a href="#cb61-861"></a></span>
<span id="cb61-862"><a href="#cb61-862"></a>$$</span>
<span id="cb61-863"><a href="#cb61-863"></a>\hat{y_i} = a_i = sigmoid(z_i)</span>
<span id="cb61-864"><a href="#cb61-864"></a>$$</span>
<span id="cb61-865"><a href="#cb61-865"></a></span>
<span id="cb61-866"><a href="#cb61-866"></a>where the formula for the *sigmoid* function is:</span>
<span id="cb61-867"><a href="#cb61-867"></a></span>
<span id="cb61-868"><a href="#cb61-868"></a>$$</span>
<span id="cb61-869"><a href="#cb61-869"></a>\sigma(Z) = \frac{1}{1 + e^{-Z}}</span>
<span id="cb61-870"><a href="#cb61-870"></a>$$</span>
<span id="cb61-871"><a href="#cb61-871"></a></span>
<span id="cb61-872"><a href="#cb61-872"></a>and loss function:\</span>
<span id="cb61-873"><a href="#cb61-873"></a>$$</span>
<span id="cb61-874"><a href="#cb61-874"></a>L = -\frac{1}{m} \sum_{i=1}^{m} \left<span class="co">[</span><span class="ot"> y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right</span><span class="co">]</span></span>
<span id="cb61-875"><a href="#cb61-875"></a>$$</span>
<span id="cb61-876"><a href="#cb61-876"></a></span>
<span id="cb61-877"><a href="#cb61-877"></a>so for each image $x_i$:</span>
<span id="cb61-878"><a href="#cb61-878"></a></span>
<span id="cb61-879"><a href="#cb61-879"></a>$$</span>
<span id="cb61-880"><a href="#cb61-880"></a>L(a_i,y_i) = -y_i \log(a_i)- (1-y_i)log(1-a_i)</span>
<span id="cb61-881"><a href="#cb61-881"></a>$$</span>
<span id="cb61-882"><a href="#cb61-882"></a></span>
<span id="cb61-883"><a href="#cb61-883"></a>The cost is then computed by summing over all training examples:</span>
<span id="cb61-884"><a href="#cb61-884"></a></span>
<span id="cb61-885"><a href="#cb61-885"></a>$$</span>
<span id="cb61-886"><a href="#cb61-886"></a>J=\frac{1}{m} \sum^m_{i=1} L(a_1,y_i)</span>
<span id="cb61-887"><a href="#cb61-887"></a>$$</span>
<span id="cb61-888"><a href="#cb61-888"></a></span>
<span id="cb61-889"><a href="#cb61-889"></a>This will:</span>
<span id="cb61-890"><a href="#cb61-890"></a></span>
<span id="cb61-891"><a href="#cb61-891"></a><span class="ss">-   </span>initialize the parameters of the model</span>
<span id="cb61-892"><a href="#cb61-892"></a></span>
<span id="cb61-893"><a href="#cb61-893"></a><span class="ss">-   </span>learn the parameters for the model by minimizing the cost</span>
<span id="cb61-894"><a href="#cb61-894"></a></span>
<span id="cb61-895"><a href="#cb61-895"></a><span class="ss">-   </span>Use the learned parameters to make predictions (on the test set)</span>
<span id="cb61-896"><a href="#cb61-896"></a></span>
<span id="cb61-897"><a href="#cb61-897"></a><span class="ss">-   </span>Analyse the results.</span>
<span id="cb61-898"><a href="#cb61-898"></a></span>
<span id="cb61-899"><a href="#cb61-899"></a>**Create the helper functions**</span>
<span id="cb61-900"><a href="#cb61-900"></a></span>
<span id="cb61-901"><a href="#cb61-901"></a>Implement <span class="in">`sigmoid()`</span> where $z$ is a scalar or array of any size</span>
<span id="cb61-902"><a href="#cb61-902"></a></span>
<span id="cb61-903"><a href="#cb61-903"></a>quarto-executable-code-5450563D</span>
<span id="cb61-904"><a href="#cb61-904"></a></span>
<span id="cb61-905"><a href="#cb61-905"></a><span class="in">```r</span></span>
<span id="cb61-906"><a href="#cb61-906"></a><span class="co"># Sigmoid function</span></span>
<span id="cb61-907"><a href="#cb61-907"></a>sigmoid <span class="ot">&lt;-</span> <span class="cf">function</span>(z) {</span>
<span id="cb61-908"><a href="#cb61-908"></a>  <span class="fu">return</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z)))</span>
<span id="cb61-909"><a href="#cb61-909"></a>}</span>
<span id="cb61-910"><a href="#cb61-910"></a><span class="in">```</span></span>
<span id="cb61-911"><a href="#cb61-911"></a></span>
<span id="cb61-912"><a href="#cb61-912"></a>Initialize a initial weight vector with zeros:</span>
<span id="cb61-913"><a href="#cb61-913"></a></span>
<span id="cb61-914"><a href="#cb61-914"></a>quarto-executable-code-5450563D</span>
<span id="cb61-915"><a href="#cb61-915"></a></span>
<span id="cb61-916"><a href="#cb61-916"></a><span class="in">```r</span></span>
<span id="cb61-917"><a href="#cb61-917"></a>initialize_W_with_zeros<span class="ot">&lt;-</span> <span class="cf">function</span>(dim){</span>
<span id="cb61-918"><a href="#cb61-918"></a>  w <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>,dim,<span class="dv">1</span>)</span>
<span id="cb61-919"><a href="#cb61-919"></a>}</span>
<span id="cb61-920"><a href="#cb61-920"></a></span>
<span id="cb61-921"><a href="#cb61-921"></a><span class="in">```</span></span>
<span id="cb61-922"><a href="#cb61-922"></a></span>
<span id="cb61-923"><a href="#cb61-923"></a>Initialize bias as 0</span>
<span id="cb61-924"><a href="#cb61-924"></a></span>
<span id="cb61-925"><a href="#cb61-925"></a>quarto-executable-code-5450563D</span>
<span id="cb61-926"><a href="#cb61-926"></a></span>
<span id="cb61-927"><a href="#cb61-927"></a><span class="in">```r</span></span>
<span id="cb61-928"><a href="#cb61-928"></a>bias <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb61-929"><a href="#cb61-929"></a><span class="in">```</span></span>
<span id="cb61-930"><a href="#cb61-930"></a></span>
<span id="cb61-931"><a href="#cb61-931"></a>The activated input will be the *sigmoid* for all the $x_i$ values:</span>
<span id="cb61-932"><a href="#cb61-932"></a></span>
<span id="cb61-933"><a href="#cb61-933"></a>$$</span>
<span id="cb61-934"><a href="#cb61-934"></a>A= (a_1,a_2,\cdots,a_m)</span>
<span id="cb61-935"><a href="#cb61-935"></a>$$</span>
<span id="cb61-936"><a href="#cb61-936"></a></span>
<span id="cb61-937"><a href="#cb61-937"></a>and $a_1 = sigmoid (W^T x_i + b)$</span>
<span id="cb61-938"><a href="#cb61-938"></a></span>
<span id="cb61-939"><a href="#cb61-939"></a>::: {#formulaexplanation .callout-orange}</span>
<span id="cb61-940"><a href="#cb61-940"></a>When we apply the formula above in our code, we do it differently:</span>
<span id="cb61-941"><a href="#cb61-941"></a>$XW+b$ The difference between the mathematical notation $(W^T x_i + b)$</span>
<span id="cb61-942"><a href="#cb61-942"></a>and the R code <span class="in">`X %*%W +b`</span> is due to how vector and matrix dimensions</span>
<span id="cb61-943"><a href="#cb61-943"></a>are conceptualized and applied in different contexts. Both approaches</span>
<span id="cb61-944"><a href="#cb61-944"></a>are mathematically equivalent: In many mathematical texts and</span>
<span id="cb61-945"><a href="#cb61-945"></a>derivations, $W$ (the weight vector) is typically represented as a</span>
<span id="cb61-946"><a href="#cb61-946"></a>column vector of dimensions (number_of_features X 1). $x_i$ (a single</span>
<span id="cb61-947"><a href="#cb61-947"></a>input sample) is also typically represented as a column vector. To</span>
<span id="cb61-948"><a href="#cb61-948"></a>compute the doc product for a single sample, you need to multiply a row</span>
<span id="cb61-949"><a href="#cb61-949"></a>vector by a column vector, therefore, you just transpose $W$ to get</span>
<span id="cb61-950"><a href="#cb61-950"></a>$W^T$ (a $1 \times \text{number_of_features}$ row vector), which can</span>
<span id="cb61-951"><a href="#cb61-951"></a>then be multiplied by $x_i$. This results in a $1 \times 1$ scalar value</span>
<span id="cb61-952"><a href="#cb61-952"></a>for $z_i$ for that single sample.</span>
<span id="cb61-953"><a href="#cb61-953"></a></span>
<span id="cb61-954"><a href="#cb61-954"></a>In <span class="in">`R`</span> and many other programming languages and machine language</span>
<span id="cb61-955"><a href="#cb61-955"></a>libraries, the standard convention for the input matrix $X$ is *Rows</span>
<span id="cb61-956"><a href="#cb61-956"></a>represent individual samples* and *Columns represent features.*</span>
<span id="cb61-957"><a href="#cb61-957"></a></span>
<span id="cb61-958"><a href="#cb61-958"></a>So our $X$ matrix has dimensions (*number_of_samples* x</span>
<span id="cb61-959"><a href="#cb61-959"></a>*number_of_features*) (209 x 12288). Given this and our $W$ vector being</span>
<span id="cb61-960"><a href="#cb61-960"></a>a (*number_of_features* x 1) (12288 x 1), the matrix multiplication</span>
<span id="cb61-961"><a href="#cb61-961"></a>perfectly aligns. The result is a (number_of_samples x 1) matrix, where</span>
<span id="cb61-962"><a href="#cb61-962"></a>each row contains the linear combination (z) for one sample.</span>
<span id="cb61-963"><a href="#cb61-963"></a>:::</span>
<span id="cb61-964"><a href="#cb61-964"></a></span>
<span id="cb61-965"><a href="#cb61-965"></a><span class="in">```{r activate-function}</span></span>
<span id="cb61-966"><a href="#cb61-966"></a><span class="co"># Computes the activated output (predicted probabilities A) given weights W, bias b, and input X.</span></span>
<span id="cb61-967"><a href="#cb61-967"></a><span class="co"># Returns: A vector/matrix of activated outputs (probabilities), (samples x 1)</span></span>
<span id="cb61-968"><a href="#cb61-968"></a></span>
<span id="cb61-969"><a href="#cb61-969"></a>activate <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X){</span>
<span id="cb61-970"><a href="#cb61-970"></a>  Z <span class="ot">&lt;-</span> X <span class="sc">%*%</span> W <span class="sc">+</span> b</span>
<span id="cb61-971"><a href="#cb61-971"></a>  </span>
<span id="cb61-972"><a href="#cb61-972"></a>  <span class="co"># Apply the sigmoid activation function element-wise to Z</span></span>
<span id="cb61-973"><a href="#cb61-973"></a>  A <span class="ot">&lt;-</span> <span class="fu">sigmoid</span>(Z)</span>
<span id="cb61-974"><a href="#cb61-974"></a>  </span>
<span id="cb61-975"><a href="#cb61-975"></a>  <span class="fu">return</span>(A)</span>
<span id="cb61-976"><a href="#cb61-976"></a>}</span>
<span id="cb61-977"><a href="#cb61-977"></a><span class="in">```</span></span>
<span id="cb61-978"><a href="#cb61-978"></a></span>
<span id="cb61-979"><a href="#cb61-979"></a>Now we create a function to calculate the *binary cross-entropy loss*:</span>
<span id="cb61-980"><a href="#cb61-980"></a></span>
<span id="cb61-981"><a href="#cb61-981"></a>$$</span>
<span id="cb61-982"><a href="#cb61-982"></a>L = -\frac{1}{m} \sum_{i=1}^{m} \left<span class="co">[</span><span class="ot"> y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right</span><span class="co">]</span></span>
<span id="cb61-983"><a href="#cb61-983"></a>$$</span>
<span id="cb61-984"><a href="#cb61-984"></a></span>
<span id="cb61-985"><a href="#cb61-985"></a><span class="in">```{r cost-function}</span></span>
<span id="cb61-986"><a href="#cb61-986"></a><span class="co"># Returns: A single scalar value representing the total cost.</span></span>
<span id="cb61-987"><a href="#cb61-987"></a>cost_function <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X, Y) {</span>
<span id="cb61-988"><a href="#cb61-988"></a>  <span class="co"># Calculate the number of samples (m) from the dimensions of X</span></span>
<span id="cb61-989"><a href="#cb61-989"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb61-990"><a href="#cb61-990"></a>  </span>
<span id="cb61-991"><a href="#cb61-991"></a>  <span class="co"># Calculate the activated output (predicted probabilities A) using the activate function</span></span>
<span id="cb61-992"><a href="#cb61-992"></a>  A <span class="ot">&lt;-</span> <span class="fu">activate</span>(W, b, X)</span>
<span id="cb61-993"><a href="#cb61-993"></a>  </span>
<span id="cb61-994"><a href="#cb61-994"></a>  <span class="co"># Calculate the binary cross-entropy loss </span></span>
<span id="cb61-995"><a href="#cb61-995"></a>  cost <span class="ot">&lt;-</span> <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>m) <span class="sc">*</span> <span class="fu">sum</span>(Y <span class="sc">*</span> <span class="fu">log</span>(A) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> Y) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> A))</span>
<span id="cb61-996"><a href="#cb61-996"></a>  </span>
<span id="cb61-997"><a href="#cb61-997"></a>  <span class="fu">return</span>(cost)</span>
<span id="cb61-998"><a href="#cb61-998"></a>}</span>
<span id="cb61-999"><a href="#cb61-999"></a></span>
<span id="cb61-1000"><a href="#cb61-1000"></a><span class="in">```</span></span>
<span id="cb61-1001"><a href="#cb61-1001"></a></span>
<span id="cb61-1002"><a href="#cb61-1002"></a>Where:</span>
<span id="cb61-1003"><a href="#cb61-1003"></a></span>
<span id="cb61-1004"><a href="#cb61-1004"></a><span class="ss">-   </span>$w$ is the weights matrix, it will be an array of the same</span>
<span id="cb61-1005"><a href="#cb61-1005"></a>    dimensions as features in the flattered dataset</span>
<span id="cb61-1006"><a href="#cb61-1006"></a>    ($64 \times 64 \times 3, 1$)</span>
<span id="cb61-1007"><a href="#cb61-1007"></a></span>
<span id="cb61-1008"><a href="#cb61-1008"></a><span class="ss">-   </span>$b$ is a scalar representing the bias</span>
<span id="cb61-1009"><a href="#cb61-1009"></a></span>
<span id="cb61-1010"><a href="#cb61-1010"></a><span class="ss">-   </span>$X$ is the data (our flattened array)</span>
<span id="cb61-1011"><a href="#cb61-1011"></a></span>
<span id="cb61-1012"><a href="#cb61-1012"></a><span class="ss">-   </span>$Y$ is the labels matrix</span>
<span id="cb61-1013"><a href="#cb61-1013"></a></span>
<span id="cb61-1014"><a href="#cb61-1014"></a>::: callout-note</span>
<span id="cb61-1015"><a href="#cb61-1015"></a><span class="fu">## Understanding Cost</span></span>
<span id="cb61-1016"><a href="#cb61-1016"></a></span>
<span id="cb61-1017"><a href="#cb61-1017"></a>In machine learning, a **cost function** is a mathematical measure of</span>
<span id="cb61-1018"><a href="#cb61-1018"></a>how well (or how poorly) a model performs relative to its task. It</span>
<span id="cb61-1019"><a href="#cb61-1019"></a>quantifies the &quot;error&quot; or &quot;discrepancy&quot; between the model&#39;s predicted</span>
<span id="cb61-1020"><a href="#cb61-1020"></a>output and the actual true values (labels) in your training data.</span>
<span id="cb61-1021"><a href="#cb61-1021"></a></span>
<span id="cb61-1022"><a href="#cb61-1022"></a>Think of it as a scoring system:</span>
<span id="cb61-1023"><a href="#cb61-1023"></a></span>
<span id="cb61-1024"><a href="#cb61-1024"></a><span class="ss">-   </span>**Low Cost:** Indicates that your model&#39;s predictions are very close</span>
<span id="cb61-1025"><a href="#cb61-1025"></a>    to the actual values.</span>
<span id="cb61-1026"><a href="#cb61-1026"></a></span>
<span id="cb61-1027"><a href="#cb61-1027"></a><span class="ss">-   </span>**High Cost:** Means your model&#39;s predictions are significantly</span>
<span id="cb61-1028"><a href="#cb61-1028"></a>    different from the actual values, indicating poor performance.</span>
<span id="cb61-1029"><a href="#cb61-1029"></a></span>
<span id="cb61-1030"><a href="#cb61-1030"></a>The primary goal of training a machine learning model is to **minimize</span>
<span id="cb61-1031"><a href="#cb61-1031"></a>this cost function**. We do this by adjusting the model&#39;s internal</span>
<span id="cb61-1032"><a href="#cb61-1032"></a>parameters (your weights $W$ and bias $b$) during the learning process</span>
<span id="cb61-1033"><a href="#cb61-1033"></a>(*gradient descent*).</span>
<span id="cb61-1034"><a href="#cb61-1034"></a></span>
<span id="cb61-1035"><a href="#cb61-1035"></a>**The Binary Cross-Entropy Cost Function in Detail**</span>
<span id="cb61-1036"><a href="#cb61-1036"></a></span>
<span id="cb61-1037"><a href="#cb61-1037"></a>In our cat/non-cat example, we&#39;re using a specific type of cost function</span>
<span id="cb61-1038"><a href="#cb61-1038"></a>called **Binary Cross-Entropy (BCE) Loss**, also known as **Log Loss**.</span>
<span id="cb61-1039"><a href="#cb61-1039"></a>This is the standard choice for binary classification problems (where</span>
<span id="cb61-1040"><a href="#cb61-1040"></a>there are only two possible outcomes, like cat/non-cat, 0/1,</span>
<span id="cb61-1041"><a href="#cb61-1041"></a>true/false).</span>
<span id="cb61-1042"><a href="#cb61-1042"></a></span>
<span id="cb61-1043"><a href="#cb61-1043"></a>$$</span>
<span id="cb61-1044"><a href="#cb61-1044"></a>L(a_i,y_i) = -y_i \log(a_i)- (1-y_i)log(1-a_i)</span>
<span id="cb61-1045"><a href="#cb61-1045"></a>$$</span>
<span id="cb61-1046"><a href="#cb61-1046"></a></span>
<span id="cb61-1047"><a href="#cb61-1047"></a>This single-example loss function behaves differently depending on the</span>
<span id="cb61-1048"><a href="#cb61-1048"></a>true label $y_i$:</span>
<span id="cb61-1049"><a href="#cb61-1049"></a></span>
<span id="cb61-1050"><a href="#cb61-1050"></a><span class="ss">1.  </span>**When** $y_i=1$ (it&#39;s a cat): The formula simplifies to:</span>
<span id="cb61-1051"><a href="#cb61-1051"></a></span>
<span id="cb61-1052"><a href="#cb61-1052"></a>$$ </span>
<span id="cb61-1053"><a href="#cb61-1053"></a>  L(a_i, 1) = -1 \cdot \log(a_i) - (1 - 1) \cdot \log(1-a_i) </span>
<span id="cb61-1054"><a href="#cb61-1054"></a>$$ $$ </span>
<span id="cb61-1055"><a href="#cb61-1055"></a>    L(a_i, 1) = -\log(a_i) -0</span>
<span id="cb61-1056"><a href="#cb61-1056"></a>$$</span>
<span id="cb61-1057"><a href="#cb61-1057"></a></span>
<span id="cb61-1058"><a href="#cb61-1058"></a><span class="ss">-   </span>In this case, the cost is only influenced by $\log(a_i)$.</span>
<span id="cb61-1059"><a href="#cb61-1059"></a></span>
<span id="cb61-1060"><a href="#cb61-1060"></a><span class="ss">-   </span>If the model predicts a high probability for a cat ($a_i$ close to</span>
<span id="cb61-1061"><a href="#cb61-1061"></a>    1), $\log(a_i)$ will be close to 0 (since $\log(1)=0$), and thus the</span>
<span id="cb61-1062"><a href="#cb61-1062"></a>    loss $\log(a_i)$ will be close to 0. This is good!</span>
<span id="cb61-1063"><a href="#cb61-1063"></a></span>
<span id="cb61-1064"><a href="#cb61-1064"></a><span class="ss">-   </span>If the model incorrectly predicts a low probability for a cat ($a_i$</span>
<span id="cb61-1065"><a href="#cb61-1065"></a>    close to 0), $\log(a_i)$ will be a large negative number (e.g.,</span>
<span id="cb61-1066"><a href="#cb61-1066"></a>    $\log(0.01)≈−4.6$), and thus the loss $-\log(a_i)$ will be a large</span>
<span id="cb61-1067"><a href="#cb61-1067"></a>    positive number (e.g., 4.6). This penalizes the model heavily for</span>
<span id="cb61-1068"><a href="#cb61-1068"></a>    being confident but wrong.</span>
<span id="cb61-1069"><a href="#cb61-1069"></a></span>
<span id="cb61-1070"><a href="#cb61-1070"></a><span class="ss">2.  </span>**When** $y_i=0$ (it&#39;s a non-cat): The formula simplifies to: $$</span>
<span id="cb61-1071"><a href="#cb61-1071"></a>    L(a_i, 0) = -0 \cdot \log(a_i) - (1 - 0) \cdot \log(1 - a_i)</span>
<span id="cb61-1072"><a href="#cb61-1072"></a>    $$ $$ L(a_i, 0) = -\log(1 - a_i) $$ Here, the cost is only</span>
<span id="cb61-1073"><a href="#cb61-1073"></a>    influenced by $\log(1−a_i)$.</span>
<span id="cb61-1074"><a href="#cb61-1074"></a></span>
<span id="cb61-1075"><a href="#cb61-1075"></a><span class="ss">-   </span>If the model predicts a low probability for a cat ($a_i$) close to</span>
<span id="cb61-1076"><a href="#cb61-1076"></a>    0, meaning $1−a_i$ is close to 1), $\log(1−a_i)$ will be close to 0,</span>
<span id="cb61-1077"><a href="#cb61-1077"></a>    and the loss: $-\log(1−a_i)$) will be close to 0.</span>
<span id="cb61-1078"><a href="#cb61-1078"></a><span class="ss">-   </span>If the model incorrectly predicts a high probability for a cat</span>
<span id="cb61-1079"><a href="#cb61-1079"></a>    ($a_i$ close to 1, meaning $1−a_i$ is close to 0), $\log(1−a_i)$</span>
<span id="cb61-1080"><a href="#cb61-1080"></a>    will be a large negative number, and the loss $-\log(1−a_i)$) will</span>
<span id="cb61-1081"><a href="#cb61-1081"></a>    be a large positive number. Again, this penalizes confident wrong</span>
<span id="cb61-1082"><a href="#cb61-1082"></a>    predictions.</span>
<span id="cb61-1083"><a href="#cb61-1083"></a></span>
<span id="cb61-1084"><a href="#cb61-1084"></a>**The total Cost(**$J$**)**: The overall cost $J$ for the entire</span>
<span id="cb61-1085"><a href="#cb61-1085"></a>training set is the average of the individual losses over all $m$</span>
<span id="cb61-1086"><a href="#cb61-1086"></a>training examples: $$</span>
<span id="cb61-1087"><a href="#cb61-1087"></a>   J= \frac{1}{m}\sum_{i=1}^m L(a_1,y_i)</span>
<span id="cb61-1088"><a href="#cb61-1088"></a>   $$Averaging the cost is representative of the model&#39;s performance</span>
<span id="cb61-1089"><a href="#cb61-1089"></a>across the entire dataset, and it makes the cost function less sensitive</span>
<span id="cb61-1090"><a href="#cb61-1090"></a>to the size of the training set.</span>
<span id="cb61-1091"><a href="#cb61-1091"></a></span>
<span id="cb61-1092"><a href="#cb61-1092"></a>**Cost vs. Accuracy** It&#39;s important to distinguish between &quot;cost&quot; and</span>
<span id="cb61-1093"><a href="#cb61-1093"></a>&quot;accuracy&quot;:</span>
<span id="cb61-1094"><a href="#cb61-1094"></a></span>
<span id="cb61-1095"><a href="#cb61-1095"></a><span class="ss">-   </span>Cost (e.g., BCE Loss): This is a continuous value that measures the</span>
<span id="cb61-1096"><a href="#cb61-1096"></a>    overall &quot;error&quot; of the model. It&#39;s what the optimization algorithm</span>
<span id="cb61-1097"><a href="#cb61-1097"></a>    directly tries to minimize. It&#39;s often not directly interpretable as</span>
<span id="cb61-1098"><a href="#cb61-1098"></a>    &quot;percentage correct&quot; but provides a nuanced measure of certainty and</span>
<span id="cb61-1099"><a href="#cb61-1099"></a>    error.</span>
<span id="cb61-1100"><a href="#cb61-1100"></a><span class="ss">-   </span>Accuracy: This is a simple metric that measures the proportion of</span>
<span id="cb61-1101"><a href="#cb61-1101"></a>    predictions that were exactly correct (e.g., 90% of images</span>
<span id="cb61-1102"><a href="#cb61-1102"></a>    classified correctly). It&#39;s easy for humans to understand, but it&#39;s</span>
<span id="cb61-1103"><a href="#cb61-1103"></a>    a discrete measure (either right or wrong).</span>
<span id="cb61-1104"><a href="#cb61-1104"></a>:::</span>
<span id="cb61-1105"><a href="#cb61-1105"></a></span>
<span id="cb61-1106"><a href="#cb61-1106"></a>The logical next step in building a logistic regression model with</span>
<span id="cb61-1107"><a href="#cb61-1107"></a>gradient descent is to implement the &quot;backward pass&quot;, which involves</span>
<span id="cb61-1108"><a href="#cb61-1108"></a>calculating the gradients. **These gradients tell us how much each</span>
<span id="cb61-1109"><a href="#cb61-1109"></a>weight (W) and the bias (b) contribute to the total cost, and in which</span>
<span id="cb61-1110"><a href="#cb61-1110"></a>direction they need to be adjusted to minimize that cost.**</span>
<span id="cb61-1111"><a href="#cb61-1111"></a></span>
<span id="cb61-1112"><a href="#cb61-1112"></a>We will create a propagate function. It will combine the forward pass</span>
<span id="cb61-1113"><a href="#cb61-1113"></a>(computing A and cost) with the backward pass (computing the gradients</span>
<span id="cb61-1114"><a href="#cb61-1114"></a>dW and db).</span>
<span id="cb61-1115"><a href="#cb61-1115"></a></span>
<span id="cb61-1116"><a href="#cb61-1116"></a>To update our weights (W) and bias (b) during gradient descenct, we need</span>
<span id="cb61-1117"><a href="#cb61-1117"></a>to know the derivative of the *cost function* with respect to these</span>
<span id="cb61-1118"><a href="#cb61-1118"></a>parameters. These derivatives are known as gradients</span>
<span id="cb61-1119"><a href="#cb61-1119"></a></span>
<span id="cb61-1120"><a href="#cb61-1120"></a>Gradient of the cost with respect to the weights (dW): $$</span>
<span id="cb61-1121"><a href="#cb61-1121"></a>dw = \frac{1}{m} X^T (A-Y)</span>
<span id="cb61-1122"><a href="#cb61-1122"></a>$$ In <span class="in">`R`</span> this computes to <span class="in">`(1/m) * t(X) %*% (A-Y)`</span>.</span>
<span id="cb61-1123"><a href="#cb61-1123"></a></span>
<span id="cb61-1124"><a href="#cb61-1124"></a>For bias (db): $$</span>
<span id="cb61-1125"><a href="#cb61-1125"></a>db = \frac{1}{m} \sum_{i=1}^m (A_i-Y_i)</span>
<span id="cb61-1126"><a href="#cb61-1126"></a>$$</span>
<span id="cb61-1127"><a href="#cb61-1127"></a></span>
<span id="cb61-1128"><a href="#cb61-1128"></a>in <span class="in">`R`</span> this simplifies to <span class="in">`(1/m)* sum(A-Y)`</span></span>
<span id="cb61-1129"><a href="#cb61-1129"></a></span>
<span id="cb61-1130"><a href="#cb61-1130"></a><span class="in">```{r propagate-function}</span></span>
<span id="cb61-1131"><a href="#cb61-1131"></a><span class="co"># Returns: A list containing &#39;cost&#39;, &#39;dW&#39;, and &#39;db&#39;.</span></span>
<span id="cb61-1132"><a href="#cb61-1132"></a>propagate <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X, Y) {</span>
<span id="cb61-1133"><a href="#cb61-1133"></a>  <span class="co"># Get the number of samples (m)</span></span>
<span id="cb61-1134"><a href="#cb61-1134"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb61-1135"><a href="#cb61-1135"></a>  </span>
<span id="cb61-1136"><a href="#cb61-1136"></a>  <span class="co"># --- Forward Propagation ---</span></span>
<span id="cb61-1137"><a href="#cb61-1137"></a>  <span class="co"># 1. Calculate activated output (predicted probabilities A)</span></span>
<span id="cb61-1138"><a href="#cb61-1138"></a>  A <span class="ot">&lt;-</span> <span class="fu">activate</span>(W, b, X)</span>
<span id="cb61-1139"><a href="#cb61-1139"></a>  </span>
<span id="cb61-1140"><a href="#cb61-1140"></a>  <span class="co"># 2. Calculate the cost</span></span>
<span id="cb61-1141"><a href="#cb61-1141"></a>  cost <span class="ot">&lt;-</span> <span class="fu">cost_function</span>(W, b, X, Y) <span class="co"># Using the integrated cost_function</span></span>
<span id="cb61-1142"><a href="#cb61-1142"></a>  </span>
<span id="cb61-1143"><a href="#cb61-1143"></a>  <span class="co"># --- Backward Propagation (Calculate Gradients) ---</span></span>
<span id="cb61-1144"><a href="#cb61-1144"></a>  <span class="co"># 1. Calculate gradient for weights (dW)</span></span>
<span id="cb61-1145"><a href="#cb61-1145"></a> </span>
<span id="cb61-1146"><a href="#cb61-1146"></a>  dW <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>m) <span class="sc">*</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> (A <span class="sc">-</span> Y))</span>
<span id="cb61-1147"><a href="#cb61-1147"></a>  </span>
<span id="cb61-1148"><a href="#cb61-1148"></a>  <span class="co"># 2. Calculate gradient for bias (db)</span></span>
<span id="cb61-1149"><a href="#cb61-1149"></a>  <span class="co"># Sums all elements of (A - Y) and averages them.</span></span>
<span id="cb61-1150"><a href="#cb61-1150"></a>  db <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>m) <span class="sc">*</span> <span class="fu">sum</span>(A <span class="sc">-</span> Y)</span>
<span id="cb61-1151"><a href="#cb61-1151"></a>  </span>
<span id="cb61-1152"><a href="#cb61-1152"></a>  <span class="co"># Store gradients in a list</span></span>
<span id="cb61-1153"><a href="#cb61-1153"></a>  gradients <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">dW =</span> dW, <span class="at">db =</span> db)</span>
<span id="cb61-1154"><a href="#cb61-1154"></a>  </span>
<span id="cb61-1155"><a href="#cb61-1155"></a>  <span class="co"># Return results as a list</span></span>
<span id="cb61-1156"><a href="#cb61-1156"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">cost =</span> cost, <span class="at">gradients =</span> gradients))</span>
<span id="cb61-1157"><a href="#cb61-1157"></a>}</span>
<span id="cb61-1158"><a href="#cb61-1158"></a><span class="in">```</span></span>
<span id="cb61-1159"><a href="#cb61-1159"></a></span>
<span id="cb61-1160"><a href="#cb61-1160"></a>**Optimization: Learning with Gradient descent**</span>
<span id="cb61-1161"><a href="#cb61-1161"></a></span>
<span id="cb61-1162"><a href="#cb61-1162"></a>Now that we have a function (propagate) that can calculate the cost and</span>
<span id="cb61-1163"><a href="#cb61-1163"></a>the gradients, we need to use those gradients to &quot;learn&quot;. This is done</span>
<span id="cb61-1164"><a href="#cb61-1164"></a>through an optimization algorithm called *Gradient Descent*.</span>
<span id="cb61-1165"><a href="#cb61-1165"></a></span>
<span id="cb61-1166"><a href="#cb61-1166"></a>The core idea is simple: we will iteratively adjust our parameters, $W$</span>
<span id="cb61-1167"><a href="#cb61-1167"></a>and $b$, in the direction that minimally reduces the cost function $J$.</span>
<span id="cb61-1168"><a href="#cb61-1168"></a>The gradients, $dW$ and $db$, tell us the direction of the steepest</span>
<span id="cb61-1169"><a href="#cb61-1169"></a>ascent of the cost function, so to decrease the cost, we move in the</span>
<span id="cb61-1170"><a href="#cb61-1170"></a>opposite direction.</span>
<span id="cb61-1171"><a href="#cb61-1171"></a></span>
<span id="cb61-1172"><a href="#cb61-1172"></a>The update rules for the parameters are: $$</span>
<span id="cb61-1173"><a href="#cb61-1173"></a>W = W- \alpha \cdot dW</span>
<span id="cb61-1174"><a href="#cb61-1174"></a>$$</span>
<span id="cb61-1175"><a href="#cb61-1175"></a></span>
<span id="cb61-1176"><a href="#cb61-1176"></a>$$</span>
<span id="cb61-1177"><a href="#cb61-1177"></a>b = b- \alpha \cdot db</span>
<span id="cb61-1178"><a href="#cb61-1178"></a>$$ Alpha is the learning rate, a crucial hyperparameter that controls</span>
<span id="cb61-1179"><a href="#cb61-1179"></a>how large of a step we take during each update. If $\alpha$ is too</span>
<span id="cb61-1180"><a href="#cb61-1180"></a>large, we might overshoot the optimal value. If it is too small, the</span>
<span id="cb61-1181"><a href="#cb61-1181"></a>training process will be very slow. Finding a good learning rate is a</span>
<span id="cb61-1182"><a href="#cb61-1182"></a>key part of training neural networks</span>
<span id="cb61-1183"><a href="#cb61-1183"></a></span>
<span id="cb61-1184"><a href="#cb61-1184"></a>Let&#39;s create an <span class="in">`optimize()`</span> function that performs this process for a</span>
<span id="cb61-1185"><a href="#cb61-1185"></a>specified number of iterations</span>
<span id="cb61-1186"><a href="#cb61-1186"></a></span>
<span id="cb61-1187"><a href="#cb61-1187"></a>quarto-executable-code-5450563D</span>
<span id="cb61-1188"><a href="#cb61-1188"></a></span>
<span id="cb61-1189"><a href="#cb61-1189"></a><span class="in">```r</span></span>
<span id="cb61-1190"><a href="#cb61-1190"></a><span class="co"># This function optimizes W and b by running a gradient descent algorithm.</span></span>
<span id="cb61-1191"><a href="#cb61-1191"></a><span class="co">#</span></span>
<span id="cb61-1192"><a href="#cb61-1192"></a><span class="co"># Arguments:</span></span>
<span id="cb61-1193"><a href="#cb61-1193"></a><span class="co"># W              -- weights, a numerical matrix of size (num_features, 1)</span></span>
<span id="cb61-1194"><a href="#cb61-1194"></a><span class="co"># b              -- bias, a scalar</span></span>
<span id="cb61-1195"><a href="#cb61-1195"></a><span class="co"># X              -- data of size (num_samples, num_features)</span></span>
<span id="cb61-1196"><a href="#cb61-1196"></a><span class="co"># Y              -- true &quot;label&quot; vector (e.g., 0 for non-cat, 1 for cat)</span></span>
<span id="cb61-1197"><a href="#cb61-1197"></a><span class="co"># num_iterations -- number of iterations for the optimization loop</span></span>
<span id="cb61-1198"><a href="#cb61-1198"></a><span class="co"># learning_rate  -- learning rate of the gradient descent update rule</span></span>
<span id="cb61-1199"><a href="#cb61-1199"></a><span class="co"># print_cost     -- if TRUE, prints the cost every 100 iterations</span></span>
<span id="cb61-1200"><a href="#cb61-1200"></a><span class="co">#</span></span>
<span id="cb61-1201"><a href="#cb61-1201"></a><span class="co"># Returns:</span></span>
<span id="cb61-1202"><a href="#cb61-1202"></a><span class="co"># A list containing the final learned parameters (W, b) and a record of the costs.</span></span>
<span id="cb61-1203"><a href="#cb61-1203"></a></span>
<span id="cb61-1204"><a href="#cb61-1204"></a>optimize <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X, Y, num_iterations, learning_rate, <span class="at">print_cost =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb61-1205"><a href="#cb61-1205"></a>  </span>
<span id="cb61-1206"><a href="#cb61-1206"></a>  costs <span class="ot">&lt;-</span> <span class="fu">c</span>() <span class="co"># Vector to store the cost at each interval</span></span>
<span id="cb61-1207"><a href="#cb61-1207"></a>  </span>
<span id="cb61-1208"><a href="#cb61-1208"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_iterations) {</span>
<span id="cb61-1209"><a href="#cb61-1209"></a>    <span class="co"># Calculate cost and gradient for the current parameters (W, b)</span></span>
<span id="cb61-1210"><a href="#cb61-1210"></a>    results <span class="ot">&lt;-</span> <span class="fu">propagate</span>(W, b, X, Y)</span>
<span id="cb61-1211"><a href="#cb61-1211"></a>    cost <span class="ot">&lt;-</span> results<span class="sc">$</span>cost</span>
<span id="cb61-1212"><a href="#cb61-1212"></a>    gradients <span class="ot">&lt;-</span> results<span class="sc">$</span>gradients</span>
<span id="cb61-1213"><a href="#cb61-1213"></a>    dW <span class="ot">&lt;-</span> gradients<span class="sc">$</span>dW</span>
<span id="cb61-1214"><a href="#cb61-1214"></a>    db <span class="ot">&lt;-</span> gradients<span class="sc">$</span>db</span>
<span id="cb61-1215"><a href="#cb61-1215"></a>    </span>
<span id="cb61-1216"><a href="#cb61-1216"></a>    <span class="co"># Update rule for W and b</span></span>
<span id="cb61-1217"><a href="#cb61-1217"></a>    W <span class="ot">&lt;-</span> W <span class="sc">-</span> learning_rate <span class="sc">*</span> dW</span>
<span id="cb61-1218"><a href="#cb61-1218"></a>    b <span class="ot">&lt;-</span> b <span class="sc">-</span> learning_rate <span class="sc">*</span> db</span>
<span id="cb61-1219"><a href="#cb61-1219"></a>    </span>
<span id="cb61-1220"><a href="#cb61-1220"></a>    <span class="co"># Record the cost every 100 iterations</span></span>
<span id="cb61-1221"><a href="#cb61-1221"></a>    <span class="cf">if</span> (i <span class="sc">%%</span> <span class="dv">100</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb61-1222"><a href="#cb61-1222"></a>      costs <span class="ot">&lt;-</span> <span class="fu">c</span>(costs, cost)</span>
<span id="cb61-1223"><a href="#cb61-1223"></a>      <span class="cf">if</span> (print_cost) {</span>
<span id="cb61-1224"><a href="#cb61-1224"></a>        <span class="fu">cat</span>(<span class="st">&quot;Cost after iteration&quot;</span>, i, <span class="st">&quot;:&quot;</span>, cost, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-1225"><a href="#cb61-1225"></a>      }</span>
<span id="cb61-1226"><a href="#cb61-1226"></a>    }</span>
<span id="cb61-1227"><a href="#cb61-1227"></a>  }</span>
<span id="cb61-1228"><a href="#cb61-1228"></a>  </span>
<span id="cb61-1229"><a href="#cb61-1229"></a>  <span class="co"># Return the learned parameters and tracked information</span></span>
<span id="cb61-1230"><a href="#cb61-1230"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb61-1231"><a href="#cb61-1231"></a>    <span class="at">W =</span> W,</span>
<span id="cb61-1232"><a href="#cb61-1232"></a>    <span class="at">b =</span> b,</span>
<span id="cb61-1233"><a href="#cb61-1233"></a>    <span class="at">costs =</span> costs,</span>
<span id="cb61-1234"><a href="#cb61-1234"></a>    <span class="at">gradients =</span> <span class="fu">list</span>(<span class="at">dW =</span> dW, <span class="at">db =</span> db)</span>
<span id="cb61-1235"><a href="#cb61-1235"></a>  ))</span>
<span id="cb61-1236"><a href="#cb61-1236"></a>}</span>
<span id="cb61-1237"><a href="#cb61-1237"></a><span class="in">```</span></span>
<span id="cb61-1238"><a href="#cb61-1238"></a></span>
<span id="cb61-1239"><a href="#cb61-1239"></a>**Making predictions** Once the model has been trained and we have our</span>
<span id="cb61-1240"><a href="#cb61-1240"></a>optimized parameters $W$ and $b$, the final step is to make predictions</span>
<span id="cb61-1241"><a href="#cb61-1241"></a>on new data. The prediction process involves two steps:</span>
<span id="cb61-1242"><a href="#cb61-1242"></a></span>
<span id="cb61-1243"><a href="#cb61-1243"></a><span class="ss">1.  </span>Calculate the predicted probabilities \$<span class="sc">\\</span>hatY = A = <span class="sc">\\</span>sigma(XW+b)\$</span>
<span id="cb61-1244"><a href="#cb61-1244"></a>    for a given dataset \$X\$.</span>
<span id="cb61-1245"><a href="#cb61-1245"></a></span>
<span id="cb61-1246"><a href="#cb61-1246"></a><span class="ss">2.  </span>Convert these probabilities into final predictions (0 or 1). A</span>
<span id="cb61-1247"><a href="#cb61-1247"></a>    common convention is to classify an image as cat (1) if its</span>
<span id="cb61-1248"><a href="#cb61-1248"></a>    corresponding probability in $A$ is greater than 0.5, and as</span>
<span id="cb61-1249"><a href="#cb61-1249"></a>    non-cat (0) otherwise.</span>
<span id="cb61-1250"><a href="#cb61-1250"></a></span>
<span id="cb61-1251"><a href="#cb61-1251"></a>quarto-executable-code-5450563D</span>
<span id="cb61-1252"><a href="#cb61-1252"></a></span>
<span id="cb61-1253"><a href="#cb61-1253"></a><span class="in">```r</span></span>
<span id="cb61-1254"><a href="#cb61-1254"></a><span class="co"># Predicts whether the label is 0 or 1 using learned logistic regression parameters (W, b).</span></span>
<span id="cb61-1255"><a href="#cb61-1255"></a><span class="co">#</span></span>
<span id="cb61-1256"><a href="#cb61-1256"></a><span class="co"># Arguments:</span></span>
<span id="cb61-1257"><a href="#cb61-1257"></a><span class="co"># W -- weights, a numerical matrix of size (num_features, 1)</span></span>
<span id="cb61-1258"><a href="#cb61-1258"></a><span class="co"># b -- bias, a scalar</span></span>
<span id="cb61-1259"><a href="#cb61-1259"></a><span class="co"># X -- data of size (num_samples, num_features)</span></span>
<span id="cb61-1260"><a href="#cb61-1260"></a><span class="co">#</span></span>
<span id="cb61-1261"><a href="#cb61-1261"></a><span class="co"># Returns:</span></span>
<span id="cb61-1262"><a href="#cb61-1262"></a><span class="co"># Y_prediction -- a vector of size (num_samples, 1) containing all predictions (0/1) for the examples in X.</span></span>
<span id="cb61-1263"><a href="#cb61-1263"></a>predict <span class="ot">&lt;-</span> <span class="cf">function</span>(W, b, X) {</span>
<span id="cb61-1264"><a href="#cb61-1264"></a>  </span>
<span id="cb61-1265"><a href="#cb61-1265"></a>  m <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb61-1266"><a href="#cb61-1266"></a>  Y_prediction <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, m, <span class="dv">1</span>)</span>
<span id="cb61-1267"><a href="#cb61-1267"></a>  </span>
<span id="cb61-1268"><a href="#cb61-1268"></a>  <span class="co"># Compute the activation (probabilities) for the input data X</span></span>
<span id="cb61-1269"><a href="#cb61-1269"></a>  A <span class="ot">&lt;-</span> <span class="fu">activate</span>(W, b, X)</span>
<span id="cb61-1270"><a href="#cb61-1270"></a>  </span>
<span id="cb61-1271"><a href="#cb61-1271"></a>  <span class="co"># Convert probabilities to actual predictions</span></span>
<span id="cb61-1272"><a href="#cb61-1272"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(A)) {</span>
<span id="cb61-1273"><a href="#cb61-1273"></a>    <span class="cf">if</span> (A[i, <span class="dv">1</span>] <span class="sc">&gt;</span> <span class="fl">0.5</span>) {</span>
<span id="cb61-1274"><a href="#cb61-1274"></a>      Y_prediction[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb61-1275"><a href="#cb61-1275"></a>    } <span class="cf">else</span> {</span>
<span id="cb61-1276"><a href="#cb61-1276"></a>      Y_prediction[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-1277"><a href="#cb61-1277"></a>    }</span>
<span id="cb61-1278"><a href="#cb61-1278"></a>  }</span>
<span id="cb61-1279"><a href="#cb61-1279"></a>  </span>
<span id="cb61-1280"><a href="#cb61-1280"></a>  <span class="fu">return</span>(Y_prediction)</span>
<span id="cb61-1281"><a href="#cb61-1281"></a>}</span>
<span id="cb61-1282"><a href="#cb61-1282"></a><span class="in">```</span></span>
<span id="cb61-1283"><a href="#cb61-1283"></a></span>
<span id="cb61-1284"><a href="#cb61-1284"></a>**Integrating everything into a complete model**</span>
<span id="cb61-1285"><a href="#cb61-1285"></a></span>
<span id="cb61-1286"><a href="#cb61-1286"></a>We now have all the necessary components. The final step is to assemble</span>
<span id="cb61-1287"><a href="#cb61-1287"></a>them into a single, high level model function. This function will</span>
<span id="cb61-1288"><a href="#cb61-1288"></a>orchestrate the entire workflow: it will initialize the parameters, call</span>
<span id="cb61-1289"><a href="#cb61-1289"></a>the <span class="in">`optimize()`</span> function to train them, and then use the <span class="in">`predict()`</span></span>
<span id="cb61-1290"><a href="#cb61-1290"></a>function to evaluate the model&#39;s performance on both the training and</span>
<span id="cb61-1291"><a href="#cb61-1291"></a>the test data sets.</span>
<span id="cb61-1292"><a href="#cb61-1292"></a></span>
<span id="cb61-1293"><a href="#cb61-1293"></a>quarto-executable-code-5450563D</span>
<span id="cb61-1294"><a href="#cb61-1294"></a></span>
<span id="cb61-1295"><a href="#cb61-1295"></a><span class="in">```r</span></span>
<span id="cb61-1296"><a href="#cb61-1296"></a><span class="co"># Builds the logistic regression model by calling the functions we&#39;ve implemented.</span></span>
<span id="cb61-1297"><a href="#cb61-1297"></a><span class="co">#</span></span>
<span id="cb61-1298"><a href="#cb61-1298"></a><span class="co"># Arguments:</span></span>
<span id="cb61-1299"><a href="#cb61-1299"></a><span class="co"># X_train, Y_train -- training set and its labels</span></span>
<span id="cb61-1300"><a href="#cb61-1300"></a><span class="co"># X_test, Y_test   -- test set and its labels</span></span>
<span id="cb61-1301"><a href="#cb61-1301"></a><span class="co"># num_iterations   -- hyperparameter for the number of training iterations</span></span>
<span id="cb61-1302"><a href="#cb61-1302"></a><span class="co"># learning_rate    -- hyperparameter for the optimization step</span></span>
<span id="cb61-1303"><a href="#cb61-1303"></a><span class="co"># print_cost       -- if TRUE, prints the cost during training</span></span>
<span id="cb61-1304"><a href="#cb61-1304"></a><span class="co">#</span></span>
<span id="cb61-1305"><a href="#cb61-1305"></a><span class="co"># Returns:</span></span>
<span id="cb61-1306"><a href="#cb61-1306"></a><span class="co"># A list containing information about the trained model.</span></span>
<span id="cb61-1307"><a href="#cb61-1307"></a>model <span class="ot">&lt;-</span> <span class="cf">function</span>(X_train, Y_train, X_test, Y_test, <span class="at">num_iterations =</span> <span class="dv">2000</span>, <span class="at">learning_rate =</span> <span class="fl">0.5</span>, <span class="at">print_cost =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb61-1308"><a href="#cb61-1308"></a>  </span>
<span id="cb61-1309"><a href="#cb61-1309"></a>  <span class="co"># 1. Initialize parameters with zeros</span></span>
<span id="cb61-1310"><a href="#cb61-1310"></a>  <span class="co"># The number of features corresponds to the number of columns in X_train</span></span>
<span id="cb61-1311"><a href="#cb61-1311"></a>  num_features <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X_train)</span>
<span id="cb61-1312"><a href="#cb61-1312"></a>  W <span class="ot">&lt;-</span> <span class="fu">initialize_W_with_zeros</span>(num_features)</span>
<span id="cb61-1313"><a href="#cb61-1313"></a>  b <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-1314"><a href="#cb61-1314"></a>  </span>
<span id="cb61-1315"><a href="#cb61-1315"></a>  <span class="co"># 2. Gradient Descent: Learn parameters by calling optimize()</span></span>
<span id="cb61-1316"><a href="#cb61-1316"></a>  optimization_results <span class="ot">&lt;-</span> <span class="fu">optimize</span>(W, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span>
<span id="cb61-1317"><a href="#cb61-1317"></a>  </span>
<span id="cb61-1318"><a href="#cb61-1318"></a>  W_final <span class="ot">&lt;-</span> optimization_results<span class="sc">$</span>W</span>
<span id="cb61-1319"><a href="#cb61-1319"></a>  b_final <span class="ot">&lt;-</span> optimization_results<span class="sc">$</span>b</span>
<span id="cb61-1320"><a href="#cb61-1320"></a>  costs <span class="ot">&lt;-</span> optimization_results<span class="sc">$</span>costs</span>
<span id="cb61-1321"><a href="#cb61-1321"></a>  </span>
<span id="cb61-1322"><a href="#cb61-1322"></a>  <span class="co"># 3. Predict on the training and test sets</span></span>
<span id="cb61-1323"><a href="#cb61-1323"></a>  Y_prediction_test <span class="ot">&lt;-</span> <span class="fu">predict</span>(W_final, b_final, X_test)</span>
<span id="cb61-1324"><a href="#cb61-1324"></a>  Y_prediction_train <span class="ot">&lt;-</span> <span class="fu">predict</span>(W_final, b_final, X_train)</span>
<span id="cb61-1325"><a href="#cb61-1325"></a>  </span>
<span id="cb61-1326"><a href="#cb61-1326"></a>  <span class="co"># 4. Calculate and print accuracies</span></span>
<span id="cb61-1327"><a href="#cb61-1327"></a>  train_accuracy <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(Y_prediction_train <span class="sc">-</span> Y_train)) <span class="sc">*</span> <span class="dv">100</span></span>
<span id="cb61-1328"><a href="#cb61-1328"></a>  test_accuracy <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(Y_prediction_test <span class="sc">-</span> Y_test)) <span class="sc">*</span> <span class="dv">100</span></span>
<span id="cb61-1329"><a href="#cb61-1329"></a>  </span>
<span id="cb61-1330"><a href="#cb61-1330"></a>  <span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">-------------------------------------------</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-1331"><a href="#cb61-1331"></a>  <span class="fu">cat</span>(<span class="st">&quot;Train Accuracy:&quot;</span>, train_accuracy, <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-1332"><a href="#cb61-1332"></a>  <span class="fu">cat</span>(<span class="st">&quot;Test Accuracy:&quot;</span>, test_accuracy, <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-1333"><a href="#cb61-1333"></a>  <span class="fu">cat</span>(<span class="st">&quot;-------------------------------------------</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-1334"><a href="#cb61-1334"></a>  </span>
<span id="cb61-1335"><a href="#cb61-1335"></a>  <span class="co"># Return a comprehensive list of model results</span></span>
<span id="cb61-1336"><a href="#cb61-1336"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb61-1337"><a href="#cb61-1337"></a>    <span class="at">costs =</span> costs,</span>
<span id="cb61-1338"><a href="#cb61-1338"></a>    <span class="at">Y_prediction_test =</span> Y_prediction_test,</span>
<span id="cb61-1339"><a href="#cb61-1339"></a>    <span class="at">Y_prediction_train =</span> Y_prediction_train,</span>
<span id="cb61-1340"><a href="#cb61-1340"></a>    <span class="at">W =</span> W_final,</span>
<span id="cb61-1341"><a href="#cb61-1341"></a>    <span class="at">b =</span> b_final,</span>
<span id="cb61-1342"><a href="#cb61-1342"></a>    <span class="at">learning_rate =</span> learning_rate,</span>
<span id="cb61-1343"><a href="#cb61-1343"></a>    <span class="at">num_iterations =</span> num_iterations</span>
<span id="cb61-1344"><a href="#cb61-1344"></a>  ))</span>
<span id="cb61-1345"><a href="#cb61-1345"></a>}</span>
<span id="cb61-1346"><a href="#cb61-1346"></a><span class="in">```</span></span>
<span id="cb61-1347"><a href="#cb61-1347"></a></span>
<span id="cb61-1348"><a href="#cb61-1348"></a>**Training the model and analyzing the results**</span>
<span id="cb61-1349"><a href="#cb61-1349"></a></span>
<span id="cb61-1350"><a href="#cb61-1350"></a>Let&#39;s call our <span class="in">`model()`</span> function with our preprocessed data and see how</span>
<span id="cb61-1351"><a href="#cb61-1351"></a>well it performs. We will set the learning rate to $0.005$ and run for</span>
<span id="cb61-1352"><a href="#cb61-1352"></a>2000 iterations.</span>
<span id="cb61-1353"><a href="#cb61-1353"></a></span>
<span id="cb61-1354"><a href="#cb61-1354"></a>quarto-executable-code-5450563D</span>
<span id="cb61-1355"><a href="#cb61-1355"></a></span>
<span id="cb61-1356"><a href="#cb61-1356"></a><span class="in">```r</span></span>
<span id="cb61-1357"><a href="#cb61-1357"></a><span class="co"># Set hyperparameters</span></span>
<span id="cb61-1358"><a href="#cb61-1358"></a>learning_rate_val <span class="ot">&lt;-</span> <span class="fl">0.005</span></span>
<span id="cb61-1359"><a href="#cb61-1359"></a>num_iterations_val <span class="ot">&lt;-</span> <span class="dv">2000</span></span>
<span id="cb61-1360"><a href="#cb61-1360"></a></span>
<span id="cb61-1361"><a href="#cb61-1361"></a><span class="co"># Run the model</span></span>
<span id="cb61-1362"><a href="#cb61-1362"></a>model_results <span class="ot">&lt;-</span> <span class="fu">model</span>(X_train, y_train, X_test, y_test, </span>
<span id="cb61-1363"><a href="#cb61-1363"></a>                       <span class="at">num_iterations =</span> num_iterations_val, </span>
<span id="cb61-1364"><a href="#cb61-1364"></a>                       <span class="at">learning_rate =</span> learning_rate_val, </span>
<span id="cb61-1365"><a href="#cb61-1365"></a>                       <span class="at">print_cost =</span> <span class="cn">TRUE</span>)</span>
<span id="cb61-1366"><a href="#cb61-1366"></a><span class="in">```</span></span>
<span id="cb61-1367"><a href="#cb61-1367"></a></span>
<span id="cb61-1368"><a href="#cb61-1368"></a>**Analyzing the learning curve**</span>
<span id="cb61-1369"><a href="#cb61-1369"></a></span>
<span id="cb61-1370"><a href="#cb61-1370"></a>A great way to check if our gradient descent is working correctly is to</span>
<span id="cb61-1371"><a href="#cb61-1371"></a>plot the cost against the number of iterations. If the model is</span>
<span id="cb61-1372"><a href="#cb61-1372"></a>learning, we should see the cost steadily decrease over time.</span>
<span id="cb61-1373"><a href="#cb61-1373"></a></span>
<span id="cb61-1374"><a href="#cb61-1374"></a>quarto-executable-code-5450563D</span>
<span id="cb61-1375"><a href="#cb61-1375"></a></span>
<span id="cb61-1376"><a href="#cb61-1376"></a><span class="in">```r</span></span>
<span id="cb61-1377"><a href="#cb61-1377"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb61-1378"><a href="#cb61-1378"></a></span>
<span id="cb61-1379"><a href="#cb61-1379"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb61-1380"><a href="#cb61-1380"></a>cost_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb61-1381"><a href="#cb61-1381"></a>  <span class="at">iterations =</span> <span class="fu">seq</span>(<span class="dv">100</span>, num_iterations_val, <span class="at">by =</span> <span class="dv">100</span>),</span>
<span id="cb61-1382"><a href="#cb61-1382"></a>  <span class="at">cost =</span> model_results<span class="sc">$</span>costs</span>
<span id="cb61-1383"><a href="#cb61-1383"></a>)</span>
<span id="cb61-1384"><a href="#cb61-1384"></a></span>
<span id="cb61-1385"><a href="#cb61-1385"></a><span class="co"># Plot the cost</span></span>
<span id="cb61-1386"><a href="#cb61-1386"></a><span class="fu">ggplot</span>(cost_data, <span class="fu">aes</span>(<span class="at">x =</span> iterations, <span class="at">y =</span> cost)) <span class="sc">+</span></span>
<span id="cb61-1387"><a href="#cb61-1387"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-1388"><a href="#cb61-1388"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb61-1389"><a href="#cb61-1389"></a>  <span class="fu">labs</span>(</span>
<span id="cb61-1390"><a href="#cb61-1390"></a>    <span class="at">title =</span> <span class="st">&quot;Cost Function Decrease Over Iterations&quot;</span>,</span>
<span id="cb61-1391"><a href="#cb61-1391"></a>    <span class="at">x =</span> <span class="st">&quot;Number of Iterations&quot;</span>,</span>
<span id="cb61-1392"><a href="#cb61-1392"></a>    <span class="at">y =</span> <span class="st">&quot;Cost&quot;</span></span>
<span id="cb61-1393"><a href="#cb61-1393"></a>  ) <span class="sc">+</span></span>
<span id="cb61-1394"><a href="#cb61-1394"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb61-1395"><a href="#cb61-1395"></a><span class="in">```</span></span></code></pre></div>
</div>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->

</body>

</html>
