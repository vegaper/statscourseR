---
title: "Statistical Relationship and Analysis"
format: html
editor: visual
---

```{r}
#| echo: false
library(dplyr)
library(tidyverse)
#library(here)
library(readxl)
library(easystats)
library(infer)
library(kableExtra)
#library(plotly)
library(ggplot2)
library(patchwork)
library(BSDA) 
library(MASS)
library(rafalib)
library(UsingR) #datasets
library(ISLR2) #datasets
#library(scatterplot3d)
#library(gridExtra)
#library(caret) #confusionMatrix
#library(pROC)
#library(class)
#library(boot) #crossvalidation
#library(leaps) #best subset selection
#library(glmnet) #ridge regression and lasso
#library(survival) #survival 
#library(survminer) #survival ggplots
#library(splines) #splines 
theme_set(theme_minimal())
options(scipen= 999)
```

# Introduction to Regression Models

The methods described above relate to **univariate** variables. In sciences it is common to be interested in the relationship between two or more variables.

Suppose that we observe a quantitative response Y and p different predictors or inputs (X1,X2, . . . ,Xp). We assume that there is some relationship between Y and X = (X1,X2, . . . ,Xp), which can be written in the very general form: $$
Y = f(X)+\epsilon
$$

$f$ represents the systematic information that X provides about Y and $\epsilon$ is a random error term, which is independent from X. In essence, statistical learning and regression techniques refers to a set of approaches for estimating $f$

There are two main reasons that we may wish to estimate $f$: prediction and inference.

## Prediction

In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y: $$
\hat{Y}= \hat{f}(X)
$$ where $\hat{f}$ represents our estimate for $f$ and $\hat{Y}$ represents the resulting prediction for Y. In this setting $\hat{f}$ is often treated as a black box and we are not tipically concerned with the extact function, provided that it yiedls accurate predictions of Y.

## Inference

We are often interested in understanding the association between Y and X1, . . . ,Xp. In this situation we wish to estimate f, but our goal is not necessarily to make predictions for Y . Now ˆ f cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions: • Which predictors are associated with the response? It is often the case that only a small fraction of the available predictors are substantially associated with Y . Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application. • What is the relationship between the response and each predictor? Some predictors may have a positive relationship with Y , in the sense that larger values of the predictor are associated with larger values of Y . Other predictors may have the opposite relationship. Depending on the complexity of f, the relationship between the response and a given predictor may also depend on the values of the other predictors. • Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Historically, most methods for estimating f have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.

Relationships between variables are usually better visualized using scatterplots:

```{r, fig.align='center'}

data(father.son,package="UsingR") 
x <- father.son$fheight
y <- father.son$sheight
plot(x,y,xlab="Father's height in inches",ylab="Son's height in inches",main=paste("correlation =",signif(cor(x,y),2)))
```

The scatter plot shows a general trend: the taller the father the taller to son. A summary of this trend is the correlation coefficient which in this case is 0.5. We motivate this statistic by trying to predict son's height using the father's.

Quick facts:

::: {#CorrelationFunFacts style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
-   Correlation's curveball: Correlation doesn't imply causation. Two variables can move together without one causing the other, often due to lurking third variables.

-   Residual Revelations: Residuals plots, which showcase deviations from a model, can often tell more about data relationships than the fit itself, hinting at non-linearity, heteroscedasticity, or other intricacies.

-   Multicollinearity Maze: In multiple regression, if predictors are too related, it can muddy interpretations.
:::

# Linear Models

Linear models are the simplest structural models. It assumes that the dependence of Y on the predictors $X_1,X_2,\dots X_p$ is linear. We represent our function like this: $$
f_L(X)=\beta_0+\beta_1X_1+\beta_2X_2+\dots\beta_pX_p
$$ it has $p$ features and $p+1$ parameters $\beta$. We can estimate the parameters of the model by fitting the model to training data.

we assume a model:

$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+\dots\beta_pX_p+\epsilon
$$

Where $\epsilon$ is some noise or errors in the function, $\beta_0$ and $\beta_p$ are unknown constants that represent the intercept and the slope, also known as coefficients or parameters.

Given some estimates $\hat{\beta_0}$ and $\hat{\beta_p}$ for the model coefficients we predict the values of Y using the model: $\hat{y} = \hat{\beta_0}+\hat{\beta_1}x+\dots\hat{\beta_p}x$ the hat symbol over the variables indicate that they are an estimated value.

A linear model is almost never correct, but it is usually a good approximation to the real function of the data. There are other models that may fit the data better, but there is a trade off in interpretability. Linear model have less prediction accuracy but are easy to interpret.

## Correlation coefficient (Pearson) {#correlation-coefficient}

If we standardize the values of our data, the line that we use to predict one value from the other follows a slope. That slope is the correlation.

In our substance_abuse dataset we have two variables for each observation DLA1 and DLA2 that we can plot in a scatter plot to see their relationship:

```{r, fig.align="center"}
file <- here::here("data", "substance_abuse.xlsx") 
substance_abuse <- read_excel(file) 
substance_abuse$DLA_improvement <- substance_abuse$DLA2 - substance_abuse$DLA1 
treatment <- dplyr::filter(substance_abuse,
                    Program == "Intervention")

ggplot(treatment, aes(x = DLA1, y = DLA2)) + 
  geom_point() # Linear

```

It shows a linear relationship between the two variables: one one increase the other tend to increase as well.

To show other possible relationships between data we will generate a dataset that shows a non-linear relationship

```{r, fig.align='center'}
set.seed(7)
x <- abs(rchisq(100, 10) - 2.7)
y <- 1/x + rnorm(100, 1, .1) - .9
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()
```

The **correlation coefficient formula**:

$$
r = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s_x} \right) \times \left( \frac{y_i - \bar{y}}{s_y} \right) = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2}\sqrt{\sum(y_i - \bar{y})^2}}
$$ {#eq-pearsoncorrelation}

This formula standardizes the values of ($x$) and ($y$) by subtracting their means $\bar{x}$ and $\bar{y}$ and dividing by their standard deviations $s_x$ and $s_y$, then calculates the average product of these standardized values. It's essentially the covariance of the variables divided by the product of their standard deviations.

**Sample correlation** measures the strength of an observed **linear** relationship between two variables in a data set. Fundamentally, it measures the spread (or variability) of sample data along a line of best fit

As a convention, the variable on the horizontal axis is called **explanatory variable or predictor** and the one in the vertical axis is called **response variable.**

```{r, fig.align='center'}
 ggplot(treatment, aes(x = DLA1, y = DLA2)) + 
  geom_point() +
  geom_smooth(method = "lm",
                           se = FALSE,
                           color = "grey")
```

To calculate the correlation we just have to use the command `cor()` and it's value will go from -1 to 1. The sign of $r$ gives the direction of the association and its absolute value gives the strength. The default type of correlation for `cor()` is the Pearson correlation, so it is the same as writing `cor(x, y, method = "pearson")`

::: callout-note
You can use `cor()` with other types of correlation specifying them, for example:

\`cor(x, y, method = "spearman")
:::

Since both $x$ and $y$ were standardized when computing $r$, $r$ has no units ans it is not affected by changing the center or the scale of either variable.

```{r}
cor(treatment$DLA1,
    treatment$DLA2)
```

Be careful to **refer to correlation only when a pair of variables has a linear relationship**. For instance, the following variables have a clear association, but their coefficient of correlation is close to 0:

```{r fig.align='center'}
set.seed(2)
x <- rnorm(100, 0, .5)
y <-  (x^2-1) + rnorm(1.5, -2.5, .8)
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()

cor(x,y)

```

We need to be aware that when we are working with samples, we may find correlation between two variables just by random chance. For example, if we plot the variable DLA_improvement against Age we can see that there is no correlation between the two:

```{r, fig.align='center'}
ggplot(treatment, aes(Age, y = DLA_improvement)) + 
  geom_point() # No association

cor(substance_abuse$Age, substance_abuse$DLA_improvement)
```

but if we get a random sample of those variables, we may find some correlation:

```{r, fig.align='center'}
set.seed(0)

sample <- slice_sample(treatment, n = 25)
cor(sample$Age, sample$DLA_improvement)
ggplot(sample, aes(x = Age, y = DLA_improvement)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "grey")
```

In the next chapters we will learn how to find out if the correlation found in our sample shows a true relationship in the population or if it is due to random chance.

Remember that we already saw that the correlation coefficient is very sensitive to outliers. (See [spearman correlation](#spearman-correlation))

## Covariance

Covariance measures the extent to which two variables change together. If the variables tend to increase together, the covariance is positive. If one tends to increase while the other decreases, the covariance is negative. For variables $X$ and $Y$: $$ \text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})$$ {#eq-covariance}

Covariance and correlation are related, but they're not the same thing:

Covariance:

-   Measures: The degree to which two variables change together.

-   Scale: It's dependent on the units of the variables, making it difficult to interpret the strength of the relationship.

Correlation:

-   Measures: The strength and direction of the linear relationship between two variables.

-   Scale: Standardizes the measure of covariance, ranging between -1 and 1.

Summary:

Covariance tells you if two variables tend to increase or decrease together, but not how strong that relationship is.

Correlation standardizes this measure, making it easier to interpret the strength and direction of the relationship on a consistent scale.

So, while covariance gives you a raw measure of how two variables vary together, correlation refines it to a standardized measure of that relationship.

Calculating the *covariance matrix* in r is easy using the `cov()` function:

```{r}
# Sample data frame
data <- data.frame(X = c(4, 6, 8, 10, 12), Y = c(2, 4, 6, 8, 10), Z = c(1, 2, 3, 4, 5))

# Calculate covariance matrix
cov_matrix <- cov(data)
print(cov_matrix)
```

How to Interpret: Diagonal Elements: These are the variances of the individual variables.

-   Var(X): 10.00

-   Var(Y): 10.00

-   Var(Z): 2.50

Off-Diagonal Elements: These are the covariances between pairs of variables.

-   Cov(X, Y): 10.00

-   Cov(X, Z): 5.00

-   Cov(Y, Z): 5.00

Example Interpretation: Variance: The variance of X is 10, meaning X values are spread out with a variance of 10 around their mean.

Covariance (X and Y): The positive covariance of 10 between X and Y suggests that when X increases, Y tends to increase as well.

Covariance (X and Z): The positive covariance of 5 between X and Z indicates a positive relationship, though not as strong as between X and Y.

Covariance (Y and Z): The positive covariance of 5 between Y and Z also indicates a positive relationship.

In essence, the covariance matrix provides insights into how pairs of variables vary together, helping you understand relationships and dependencies in your data.

## Regression Line and the Method of Least Squares (RSS) {#Residualsumofsquares}

If the scatterplot shows a linear association, then this relationship between our data points can be summarized by a line. The equation for a line is $\hat{y}*i=a+bx_*i$. The idea is to choose the line that minimizes the sum of the squared distances between the observed $y_i$ and the predicted $\hat{y_i}$

The method of least squares is the method we use to minimize this **Residual sum of squares (RSS)**

$$
RSS=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 
$$ {#eq-RSSsimpleRegression}

$$
RSS=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left( y_i - (\hat{\beta_o} + \hat{\beta_1} x_i) \right)^2
$$

$$
RSS = (y_1 - \beta_0 - \beta_1 x_{1})^2 + (y_2 - \beta_0 - \beta_1 x_{2})^2 + \cdots + (y_n - \beta_0 - \beta_1 x_{n})^2
$$

The intercept $\beta_0$ is the expected value of Y when X=0, in other words, the value at which the regression line crosses the Y axis.

$$
\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$ $$
\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}
$$ where $\bar{x}$ and $\bar{y}$ are the sample means

It turns out that $\beta_1=r\frac{s_y}{s_x}$ where $r$ is the correlation coefficient and $s_y$ and $s_x$ are the two standard deviations and $\hat{\beta_0} = \bar{y} - \hat{\beta_i}\bar{x}$.

The main use of regression is to predict y from x. If we are given x, then we need $r, \bar{x},\bar{y},s_x, s_y$ to calculate the regression line. In code, we can compute that line using 'lm' in r language

::: exercise-box
Exercise: midterm vs final exam student's grades (cont)

*We know that the average score for the midterm exams was 49.5 and the average score for the final exam is 69.1, and the standard deviation for the midterm exams was 10.2 while the standard deviation for the final exams was 11.8. We also know that the correlation coefficient is 0.67.*

*Predict the final exam score of a student who scored 41 on the midterm.*

Sol: 41 is 8,5 below the midterm average. We want to standardize this value. We already know how to calculate this: $$z = \frac{x_1-\bar{x}}{sd} = \frac{41-49.5}{10.2}=-0.83$$ so now looking at the formula for the regression line we expect $y$ to be r times 0.83 times the standard deviation of the final exam below the average final score. $$\bar{y}\pm r*sd_{final}*0.83 =69.1 -0.67*11.8*0.83=62.5$$
:::

### Predicting x from y:

When predicting x from y it is a mistake to use the regression line that we calculated for regressing y on x and solve for x. This is because regressing x on y will result in a different regression line.

## Normal approximation in regression

Linear regression requires that the data in the scatter plot is more or less following an elypse shape. Once we have the scatter plot with the regression line, we know that given a value of $x$, the value of $y$ will be close to the $y$ point of the regression line for the $x$ value. In the image below, given $x$, we trace a vertical line to the regression line and then we trace a perpendicular line to the $y$ axis (green line) and that is approximately the expected value of $y$.

```{r, fig.align='center', echo=FALSE}

set.seed(412)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Create a data frame
data <- data.frame(x, y)

# Fit a linear model
model <- lm(y ~ x, data = data)

# Choose a random value in x
random_x <- sample(x, 1)

# Predict the y value from the linear model
predicted_y <- predict(model, newdata = data.frame(x = random_x))

# Create the scatter plot with regression line and vertical/horizontal lines
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_vline(xintercept = random_x, linetype = "dashed", color = "red") +
  geom_hline(yintercept = predicted_y, linetype = "dashed", color = "green") 

```

but we also know that $y$ will not be necessarily exactly at that point, but at certain distance of it, and that approximation follows the normal curve, so we can then use normal approximation to calculate the value of $y$: subtract off the predicted value $\hat{y}$, then divide by $\sqrt{1-r^2} * s_y$.

$$ 
z = \frac{y_1-\bar{y}}{\sqrt{1-r^2}\cdot s_y} 
$$

::: exercise-box
Exercise:

*Among the students who scored around 41 on the midterm, what percentage scored above 60 on the final?*

We already predicted that the expected value for the score on the final exam for a student that scored 41 in midterm is 62.5. That means that the normal curve is centered at 62.5 so the percentage of students that will score above 60 is the area bellow that normal curve to the right of 60 (red line).

```{r, fig.align='center'}
#| echo =FALSE
# Create a sequence of x values
x <- seq.int(from =1,to=  125, by=1)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x, mean = 62.5, sd = 14, log=F)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue")
# Add a vertical line at x = 0.71
abline(v = 60, col = "red", lwd = 2, lty = 2)
abline(v = 62.5, col = "purple", lwd = 2, lty = 2)
```

For that we standardize 60 like this: $z = \frac{y_1 - \bar{y}}{\sqrt{1 - r^2} \cdot s_y} = \frac{60 - 62.5}{\sqrt{1 - 0.67^2} \cdot 11.8} = -0.29$ so the standardized curve would look like this:

```{r, fig.align='center'}
#| echo =FALSE
# Create a sequence of x values
x <- seq(from = -3, to = 3, by = 0.01)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x, mean = 0, sd = 1)

# Plot the bell curve
plot(x, y, type = "l", lwd = 2, col = "blue",
     main = "Standard Normal Distribution",
     xlab = "X-axis",
     ylab = "Density")

# Add a vertical line at x = 0.71
abline(v = -0.20, col = "red", lwd = 2, lty = 2)

```

now we just have to use a table or software to calculate that value

```{r, fig.align='center'}
zscore <- -0.29

# Calculate the area to the left of the z-score
area_left <- pnorm(zscore)

# Calculate the area to the right of the z-score
area_right <- 1 - area_left

# Print the result
area_right
```

in our case we are interested to the area to the right, so our answer is 61.41%
:::

::: exercise-box
Exercise: midterm vs final exam student's grades

*In a biology class, both the midterm scores and the final exam scores have an average of 50 and a standard deviation of 10. The scatterplot looks football-shaped and the correlation coefficient is 0.6.*

*Emily got exactly the mean score of 50 on the midterm. What is your prediction for Emily's score on the final?*

sol:

Since the distance in standard deviations of Emily's midterm score from the average midterm score is 0, the corresponding distance of Emily's predicted final score from the average final score is r\*0 = 0, so our prediction is the average =50

*What is the "give or take" number for your prediction?*

sol:

$$
10\sqrt{1-0.6^2} = 8
$$
:::

## Residuals {#residualplots}

As mentioned before, the observed values of $Y$ will not fall directly over the regression line. At each value of $X=x$ there is typically a distribution of possible $Y$ values.

For each observation we will have the predicted value $\hat{y}$ and the observed value $y$ . That difference is called the residual. The **residual plot** is a scatter plot of the residuals against the $x$ values. It should show an unstructured horizontal band and it is used to check if the regression you are using is appropriate.

For example if we have data following a linear correlation:

```{r, fig.align='center'}
#|echo = FALSE
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Create the scatter plot
plot(x, y, main = "Scatter Plot with Linear Regression Line", 
     xlab = "X-axis", ylab = "Y-axis", pch = 19, col = "blue")

# Add the linear regression line
abline(lm(y ~ x), col = "red", lwd = 2)
```

the residual scatter plot will look like this:

```{r, fig.align='center'}
model <- lm(y ~ x)

# Get the residuals and fitted values
residuals <- resid(model)
fitted_values <- fitted(model)

# Create the residual scatter plot
plot(fitted_values, residuals, 
     main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     pch = 19, 
     col = "blue")

# Add a horizontal line at 0
abline(h = 0, col = "red", lwd = 2)
```

now, if we have data that does not follow a linear correlation:

```{r, fig.align='center'}
#|echo = FALSE
set.seed(7)
x <- abs(rchisq(100, 10) - 2.7)
y <- 1/x + rnorm(100, 1, .1) - .9
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()
```

the residual will look different:

```{r, fig.align='center'}
#|echo = FALSE
model <- lm(y ~ x)

# Get the residuals and fitted values
residuals <- resid(model)
fitted_values <- fitted(model)

# Create the residual scatter plot
plot(fitted_values, residuals, 
     main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     pch = 19, 
     col = "blue")

# Add a horizontal line at 0
abline(h = 0, col = "red", lwd = 2)
```

that is an indication that the relation is not linear, and we should not use regression for this data.

### Heteroscedascidity {#heteroscedasticity}

Another variation is when the scatter shows in a fan way, this means that the variability increases with the value of $X$, this is called heteroscedastic.

```{r, fig.align='center'}
#|echo = FALSE

# Generate some example data with heteroscedasticity
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100, sd = 0.9 * x)

plot(x, y, 
     main = "(Heteroscedasticity)", 
     pch = 19, 
     xlim = c(0, max(x)),
     col = "blue")

```

Sometimes we can fix a non linear relationship by modifying y values, x values or both, for example applying log transformation.

### Outliers, leverage and influential points

Points with very large residuals are called outliers and they should be examined to see if they represent an interesting phenomena or an error in the data. {#highLeverage} When the value that deviates is the $x$ value and not the $y$ we say that it is a high leverage point, and it has the potential to cause a big change in the regression line.

```{r, fig.align='center'}
#|echo = FALSE
# Generate some example data
set.seed(123)
x <- rnorm(20)
y <- 2 * x + rnorm(20)

# Add a high leverage point
x <- c(x, 10)
y <- c(y,4)

# Fit linear models
model_with_point <- lm(y ~ x)
model_without_point <- lm(y ~ x, subset = -21)

# Create the scatter plot
plot(x, y, 
     main = "High Leverage Point Influence", 
     xlab = "X", 
     ylab = "Y", 
     pch = 19, 
     col = ifelse(1:21 == 21, "red", "blue"))

# Add regression lines
abline(model_with_point, col = "red", lwd = 2)
abline(model_without_point, col = "green", lwd = 2, lty = 2)

# Add a legend
legend("bottomright", legend = c("With High Leverage Point", "Without High Leverage Point"), 
       col = c("red", "green"), lty = c(1, 2), lwd = 2)
```

In order to quantify and observation's leverage we can compute the *leverage statistic*. A large value of this statistic indicates an observation with high leverage. The formula for simple linear regression is: $$
h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum^n_{i'=1}(x_{i'}-\bar{x})^2}
$${#leverageStatistic} There is another formula for multiple predictors, but we won't see the formula here. The leverage statistic is always between $1/n$ and 1 and the average leverage for all the observations is always equal to $(p+1)/n$. So if a given observation has a leverage statistic that greatly exceeds $(p+1)/n$ then we may suspect that the corresponding point has leverage.

::: {#summarieswarn .callout-orange}
Beware of data that are summaries (e.g. averages of data). Those are less variable than individual observations and correlations between averages tend to overstate the strength of the relationship
:::

## Assessing the Accuracy of the Coefficient Estimates and Confidence Intervals {#assessing-the-accuracy-of-the-coefficient-estimates-and-confidence-intervals}

The *standard error* of an estimator reflects how it varies under repeated sampling.

$$
SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^ n(x_i-\bar{x})^2}
$$ {#eq-standardErrorSlope} What the denominator in this formula is telling us is that the more spread out our values are along the x axis, the better we will be able to predict the correct slope.

$$
SE(\hat{\beta_0})^2 = \sigma^2 \left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^ n(x_i-\bar{x})^2}\right]
$$ {#eq-standardErrorIntercept} where $\sigma^2$ is the variance of the error $\sigma^2=Var(\epsilon)$

These standard errors can be used to compute *confidence intervals* For example for 95 confidence:

$$
\hat{\beta_1} \mp 2 * SE(\hat{\beta_1}) = \left[\hat{\beta_1}-2 * SE(\hat{\beta_1}),\hat{\beta_1}+2 * SE(\hat{\beta_1})\right]
$$

## Hypothesis testing and significance of correlation

The most important thing to remember about correlation testing is that **it only applies to quantitative variables that have a generally linear relationship**.

A correlation test checks the **null hypothesis that the population correlation** $\rho$ **is equal to 0**. This is, there is no relationship between X and Y. $H_0:\beta_1=0$

To test the null hypothesis, we compute a *t-statistic* $$
t= \frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}
$$ This will have a t-distribution with n-2 degrees of freedom. Using statistical software, it is easy to compute the probability of observing any value equal to *t* or larger. We call this probability the $p$-value. The confidence intervals can also tell us if we should reject the null hypothesis. If 0 falls in between the range of the confidence interval, that means that we cannot exclude the possibility that the slope is 0, meaning that there is no relation between the parameters. The confidence interval is also going to tell you how big the effect is, so it is always a good practice to compute confidence intervals as well as doing a hypothesis testing.

The test **assumes that the data isn't too skewed in any other direction and that there aren't extreme outliers**. As usual, a larger sample provides a certain degree of protection.

**If the sample is very small, the test will be unlikely to rule out the possibility that** $\rho = 0$ even if that is the case. That is, the test will be underpowered.

Similarly, **if the sample is very large, the test will be likely to conclude that** $\rho \neq 0$ \*\*. This gets to an important idea that we've touched on before which is that there is a difference between a sample statistic being statistically significant and it actually being important or meaningful. For instance in a very large sample you may get a sample correlation of 0.001 but it may come back as statistically significant. In the real world, you should always take into consideration not just statistical significance but also effect size when you make real world decisions.

```{r}
testResult<- cor.test(treatment$DLA2,
         treatment$DLA1)
report(testResult)
```

In this type of test our null hypothesis is that the true correlation between those two variables is 0. In our example, the test is giving us a sample correlation of `r testResult$estimate` and a $p$-value `r testResult$p.value` which in simple terms mean that the probability of getting this correlation results in our sample if the true correlation in the population was actually 0 would be less than 0.001, so highly unlikely. The test also gives us a confidence interval, in our case `r testResult$conf.int` and its interpretation is exactly the same as it always is for our confidence interval: if we were to go out and get many many samples from the same population and compute correlations of them, 95% of the time that confidence interval would capture the true parameter, in our case the true population correlation.

With this new knowledge we are going to test the sample we created before showing a relationship between age and DLA_improvement and compare it with the correlation for the population (all records in our file)

```{r, fig.align='center'}
#population correlation between age and dla improvement

ggplot(treatment, aes(x = Age, y = DLA_improvement)) + 
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "grey")

report(cor.test(treatment$Age,
         treatment$DLA_improvement))

#small sample correlation:
set.seed(0)
sample <- slice_sample(treatment, n = 25)
report(cor.test(sample$Age,
         sample$DLA_improvement))

```

::: {.callout-orange appearance="simple" icon="false"}
A significant correlation means that the likelihood of observing such a correlation (or stronger) by random chance is low, given the null hypothesis of no correlation.
:::

## Interpretation of Linear Regression results

Any statistical package will have a function to calculate the line that fits the linear relationship in our data. In r it is lm command. We put the response variable first, and then the explanatory variable, finally the dataset.

```{r}
lm(DLA2 ~ DLA1, data = treatment)
```

It is read DLA2 is explained by DLA1 and we get two coefficients called the slope and the intercept. The interpretation of these numbers is very important. When you have a straight line, the slope shows the increase in $y$ when $x$ increases by 1. The intercept means where is the $y$ value when $x=0$

The most important use of a regression line is that it allows you to make predictions.

In our example, if we get a new patient with DLA1 of 3.6 we can use these values to calculate what it is the expected DLA2 value:

```{r}
DLA1 <- 3.6
DLA2 <- .07243 + .9660 *DLA1
DLA2
```

A very important thing to bear in mind is that **a regression line should only be used to make predictions on individuals whose explanatory variable falls in the range of the values used to calculate the linear regression model**, for example in our case our model was trained with DLA1 between 2.5 and 5 so we should not use this model to make predictions on individuals whose DLA1 is 6, for example.

We can get more information about our model if we ask for their summary:

```{r}
summary(lm(DLA2 ~ DLA1, data = treatment))
```

The $p$-value that corresponds with the intercept is a the $p$-value of a test done against the null hypothesis that the intercept is actually 0.

The $p$-value that corresponds with the dependent variable is the result of a test done against the null hypothesis that the slope is actually 0.

We can retrieve those $p$-values using the coefficients table from our lm result:

```{r}
testResult<- summary(lm(DLA2 ~ DLA1, data = treatment))
testResult$coefficients[,4]
```

When looking at coefficients one must be aware that the units we used in our data will affect them. The z-statistic is not affected by units.

We can use the `names()` function in order to find out what other pieces of information are stored. Although we can extract these quantities by name---e.g. `testResults$coefficients`---it is safer to use the extractor functions like `coef()` to access them.

```{r }
lm.fit<- lm(DLA2 ~ DLA1, data = treatment)
names(lm.fit)
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command.

```{r }
confint(lm.fit)
```

The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of Y for a given value of X.

```{r chunk8}
# Choose a random value in x
random_x <- sample(x, 1)

predict(lm.fit, newdata = data.frame(x = random_x),
    interval = "confidence")
predict(lm.fit, newdata = data.frame(x = random_x),
    interval = "prediction")
```

R is also retrieving a p value for the overall model that's coming from an Anova F-Test. that corresponds with the coefficient for the slope.

our diagnostic plots are automatically produced by applying the `plot()` function directly to the output from `lm()`. In general, this command will produce one plot at a time, and hitting *Enter* will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the `par()` and `mfrow()` functions, which tell `R` to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, `par(mfrow = c(2, 2))` divides the plotting region into a $2 \times 2$ grid of panels.

```{r, fig.align='center', fig.width=7}
par(mfrow = c(2, 2))
plot(lm.fit)
```

::: exercise-box
Exercises:

*Using the mpg dataset:*

-   *Is there a linear relationship between city mileage and highway mileage in this set?*

    ```{r, fig.align='center'}
    file2 <- here::here("data", "mpg_2008.xlsx")
    mpg_2008 <- read_excel(file2)

    ggplot(mpg_2008, aes(x = cty, y = hwy))+
      geom_point()+
      geom_smooth(method ='lm',
                  se = FALSE,
                  color = 'purple' )
    ```

-   *What is the sample correlation?*

    ```{r}
    cor(mpg_2008$cty, mpg_2008$hwy)
    ```

-   *Does the sample correlation provide evidence that the population correlation is different from zero?*

    ```{r}
    testResult<- cor.test(mpg_2008$cty, mpg_2008$hwy)
    testResult
    report(testResult)
    ```

    Yes.

-   *Find the equation of the regression line*

```{r}
lm (cty ~hwy, data= mpg_2008)
lm (hwy ~ cty, data= mpg_2008)
```

the equation will be $cty = 0.6687 * hwy + 1.017$ or $hwy = 1.39 *cty + .2388$

*Estimate the highway mileage of an unknown car in this population with a city mileage of 24 miles per gallon.*

```{r}
1.29*24+.2388
```
:::

## Simulating Random Numbers from a linear model in r

Suppose we want to simulate data from the linear model:

$$
y = \beta_0+\beta_1 x+ \epsilon
$$

where epsilon has a normal distribution with sd =2, the intercept $\beta_0$ = 0.5 and the slope $\beta_1$ = 2

```{r}
set.seed(20)
x<- rnorm(100)
e<- rnorm(100,0,2)
y= 0.5 +2*x+e
summary(y)
plot(x,y)
```

if $x$ is binomial:

```{r}
set.seed(10)
set.seed(20)
x<- rbinom(100,1,0.5)
e<- rnorm(100,0,2)
y= 0.5 +2*x+e
summary(y)
plot(x,y)
```

::: {.callout-orange appearance="simple" icon="false"}
Linear regression models the relationship between a dependent variable and one or more independent variables. Remember a well-fitting model doesn't always imply causality.
:::

# Extending the linear models

## Categorical predictors in regression models. {#linearmodelscategorical}

Sometimes we may want to include in the model a variable that is not numeric, for example gender. To do that we create what we call a dummy variable, for example $x_i = 1$ if the person is female and $x_i = 0$ if it is a male. The resulting model will be: $$
y_i=\beta_o+\beta_1x_i+\epsilon_i
$$ which will result in this if the person is female: $$
y_i=\beta_o+\beta_1+\epsilon_i
$$ and if the person is male: $$
y_i=\beta_o+\epsilon_i
$$ so what this is telling us is that $\beta_1$ is the effect of being female vs the baseline (in this case male)

If we have more than two levels what we do is create more dummy variables. For example we look at three different etnicities, Asian, Caucasian and African American, we create:

$x_{i1} = 0$ if person is Asian and $x_{i1} = 1$ if the person is not Asian $x_{i2} = 0$ if person is Caucasian and $x_{i2} = 1$ if the person is not. We don't need a level for African American because that will be deducted when $x_{i1} = 0$ and $x_{i2} = 0$. So for categorical variables we create $k-1$ dummy variables. The level with no dummy variable is known as *baseline*. The choice of a baseline will not affect the fit of the model, the residual sum of squares would be the same, but the coefficient and the $p$-values will change because each other category will be compared with the baseline.

The equation will now look like this: $$
y_1 = \beta_o+\beta_1x_{i1}+\beta_2x_{i2}+\epsilon_i
\begin{cases} 
\beta_0 + \beta_1 + e_1 & \text{if condition 1} \\
\beta_0 + \beta_2 + e_1 & \text{if condition 2} \\
\beta_0 + e_1 & \text{if condition 3}
\end{cases}
$$

::: exercise-box
Example

*The Carseats data from the library ISLR2 includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location---that is, the space within a store in which the car seat is displayed---at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically.*

```{r}
lm.fit <- lm(Sales ~ ShelveLoc, 
    data = Carseats)
summary(lm.fit)
contrasts(Carseats$ShelveLoc)
```

The contrasts() function returns the coding that R uses for the dummy variables.

R has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.
:::

## Interactions

In our previous examples, we have assume independence from one parameter to the others, but that is not always the case, sometimes the change in one predictor affects the results in another.

::: {exercise-box}
Example Imagine we run a campaign to sell a product, and we advertise in three mediums, newspapers, radio and tv. It could be that the effect of radio increases the effectiveness of the adds in tv, this in marketing is called synergy effect, and in statistics we refer to it as interaction effect. If we detect this interaction, spending part of our budget in radio and part on tv could be more effective than spending all in only the media with the most effect.

```{r}
Advertising <- readr::read_csv("data/Advertising.csv")
lm.fit<- lm(sales ~ TV + radio, data = Advertising)
summary(lm.fit)
```

to include interactions in our model we create a new variable with the product of the two predictors. If we ignore newspaper our equation would be:

$$
sales = \beta_0+\beta_1\times TV+\beta_2\times Radio + \beta_3 \times(radio\times TV)+ \epsilon
$$

$$
sales = \beta_0+(\beta_1 + \beta_3\times Radio) \times TV+\beta_2\times Radio + \epsilon
$$ and if we get a summary of the linear model we will see if the interaction between tv and radio is indeed significant or not:

```{r}
lm.fit_interaction <- lm(sales ~ TV * radio , data = Advertising)
summary(lm.fit_interaction)
# or
lm.fit_interaction <- lm(sales ~ TV + radio + TV:radio , data = Advertising)
summary(lm.fit_interaction)

```

the $p$-value of the interaction indicates that in our example this interaction is significant, we can also see how our R-squared is higher now (0.9678) than when we did not include the interaction in our model (0.8972). This means that (96.8-89.7)/(100-89.7)=69% of the variability in sales that remains after fitting the initial model has been explained by the interaction term.

The coefficients estimates in the table suggest that an increase in radio advertising of \$1000 is associated with increased sales of $(\hat{\beta_2}+\hat{\beta_3}\times TV)\times 1000 = 29+1.1\times TV units$ An increase in TV advertising of \$1000 is associated with an increase of sales of $(\hat{\beta_1}+\hat{\beta_3}\times radio)\times 1000 = 19+1.1\times radio units$
:::

Sometimes it is the case that an interaction term has a very small $p$-value, but the associated main effects do not. The *hierarchy principle* states that if we include an interaction in a model, we should also include the main effects, even if the $p$-values associated with their coefficients are not significant.

## Non-linear transformations of the predictors.

If a linear model does not quite fit our data:

```{r, fig.align='center'}

lm.fit<- lm(mpg~ hp, data= mtcars)
plot(mtcars$hp, mtcars$mpg)
abline(lm.fit)
summary(lm.fit)
```

we can transform this into a polynomial regression by making extra variables to accommodate polynomials, for example we add another variable horsepower squared:

```{r}
lm.fit <- lm(mpg ~ hp + I(hp^2), data = mtcars)

plot(mtcars$hp, mtcars$mpg, xlab = "Horsepower", ylab = "Miles per Gallon")

points(mtcars$hp, fitted(lm.fit), col = "pink", lwd = 2)

#or if we want to show a line:
# Create a sequence of hp values for prediction
hp_seq <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 100)

# Predict mpg for each hp value in the sequence
predicted_mpg <- predict(lm.fit, newdata = data.frame(hp = hp_seq))

lines(hp_seq, predicted_mpg, col = "blue", lwd = 2)

summary(lm.fit)

```

but there is a better way of fitting polynomials with r using the `poly()` function:

```{r, fig.align='center'}
lm.fit <- lm(mpg ~ poly(hp,4), data = mtcars) 
plot(mtcars$mpg ~ mtcars$hp)
points(mtcars$hp, fitted(lm.fit), col = "pink", lwd = 2)

```

## Potential problems in linear models

We already talked about most of them, but let's do a summary:

::: callout-orange
1.  **Non linearity**: The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. We saw how we can make use of the residual plot to see if the relation is liner or not when we talked about [residuals](%7B#residualplots%7D)

2.  **Correlation of error terms**: An important assumption of the linear regression model is that the error terms are uncorrelated.If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors and this will affect the $p$-values, confidence intervals etc. Why might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. time series. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no discernible pattern. On the other hand, if the error terms are positively correlated, then we may see tracking in the residuals.Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals' heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family, eat the same diet,or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.

3.  **Non-constant variance of error terms [(heteroscedasticity)](#heteroscedasticity)** : error terms have a constant variance, Var(ϵi) = σ2. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. Unfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response

4.  **Outliers**: An outlier is a point for which $y$ is far from the value predicted by the model. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance it affects the RSE. Since the RSE is used to compute all confidence intervals and $p$-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Residual plots can be used to identify outliers.

5.  **High-leverage points**: Observations with [high leverage](#highLeverage) have an unusual value for $x_i$. Removing a high leverage point has much more impact than removing an outlier. In a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor's values, but that is unusual in terms of the full set of predictors.

6.  **Collinearity**: we already dedicated [a section](#collinearity) to these problem.
:::

# Introduction to ANOVA (Analysis of Variance) {#sec-anova}

Analysis of variance, or ANOVA, is used to test **the relationship between a categorical and a quantitative variable**. Specifically, it test the null hypothesis that the population mean of the quantitative variable is the same in each of the groups. A low $p$-value indicates that the data would be unlikely if in fact those population means where equal.

ANOVA requires that the data be relatively symmetric in each group and not include extreme outliers. Additionally, the spread of the data within the groups should be similar. Always plot your data before running ANOVA to check these assumptions.

ANOVA is an omnibus test: on it's own, it does not say anything about which group or groups might be interesting.

We are going back to our attrition dataset and analyse if the variable of work life balance gives us any information about the money they make (MonthlyRate). A first intuitive approach is to calculate the average MontlyRate per group:

```{r}
file <- here::here("data", "attrition1.xlsx")
attrition1 <- read_excel(file)
attrition1 %>% group_by(WorkLifeBalance) %>% 
  summarise (mean(MonthlyRate)) 
```

this shows that there is in fact a difference but, is this difference important or significant?

One first approach to answer this question is observing the data in a boxplot:

```{r, fig.align='center', echo=FALSE}
ggplot (data= attrition1, aes(x = WorkLifeBalance, y= MonthlyRate))+
  geom_boxplot()
```

This already shows us that the spread of the data in each of the groups is big compared to the the spread of the data between the groups, but one way we can test this is doing an ANOVA test that is going to compare the variances within the groups to the variance between the groups.

```{r}
testResult<- (aov(MonthlyRate ~ WorkLifeBalance, data = attrition1))
sumRes<- summary(testResult)
sumRes
```

if we look at our $p$-value that is testing if in fact all of these groups have the same mean it is `` r sumRes[[1]]$`Pr(>F)` `` which means that in almost 65% of the cases we would see these kind of data even if the population means for these groups were the same.

Our null hypothesis is that all group means are equal. If we had only two groups, we would use a t test

$$
t=\frac{difference\ in \ sample\ means}{SE\ of\ difference}
$$

now we generalize this idea to the instance when we have several groups. If the differences between the samples means are large relative to the variability within the groups, this suggest that our null hypothesis is not true. In contrast, if the differences in the means are quite small compared with the variability within each groups, that suggest that the differences in the means are due to variability in the sample.

```{r, fig.height=6, fig.width=12, fig.align='center'}
#|echo: FALSE
# Set seed for reproducibility
set.seed(123)

# Generate data for large differences between group means
group_A1 <- rnorm(100, mean = 10, sd = 2)
group_B1 <- rnorm(100, mean = 20, sd = 2)
group_C1 <- rnorm(100, mean = 30, sd = 2)

# Generate data for small differences between group means
group_A2 <- rnorm(100, mean = 15, sd = 10)
group_B2 <- rnorm(100, mean = 17, sd = 10)
group_C2 <- rnorm(100, mean = 19, sd = 10)

# Create data frames
data_large_diff <- data.frame(value = c(group_A1, group_B1, group_C1),
                              group = rep(c("A", "B", "C"), each = 100))

data_small_diff <- data.frame(value = c(group_A2, group_B2, group_C2),
                              group = rep(c("A", "B", "C"), each = 100))

# Create boxplots
p1 <- ggplot(data_large_diff, aes(x = group, y = value)) + 
  geom_boxplot() + 
  ggtitle("Large Differences Between Group Means") +
  theme_minimal()

p2 <- ggplot(data_small_diff, aes(x = group, y = value)) + 
  geom_boxplot() + 
  ggtitle("Small Differences Between Group Means") +
  theme_minimal()

# Combine plots side by side
combined_plot <- p1 + p2

# Print the combined plot
print(combined_plot)

```

But unfortunately things are not as easy as looking at the boxplots, the reason is that according to the square root law, the chance variability in the sample mean is smaller than the chance variability in the data, so the evidence against $H_0$ is not obvious from the boxplots. Computation is necessary.

We have k groups with n observations:

| observation | group 1  | group 2  | group k  |
|-------------|----------|----------|----------|
| 1           | $y_{11}$ | $y_{12}$ | $y_{1k}$ |
| 2           | $y_{21}$ | $y_{22}$ | $y_{2k}$ |
| ...         | ...      | ...      | ...      |
| n           | $y_{n1}$ | $y_{n2}$ | $y_{nk}$ |

in total there are $N=n_1+n_2+\cdots+n_k$ observations. The sample mean of one group is $\bar{y_j}=\frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}$ and the overall sample mean or grand mean is $\bar{\bar{y}}= \frac{1}{N}\sum_{j=1}^{k}\sum_{i=1}^{n_j}y_{ij}$

The analysis of variance compute two important variables, one is the **treatment sum of squares (SST)** that look at the difference between the group means to the overall mean

$$
SST=\sum_j\sum_i(\bar{y_j}-\bar{\bar{y}})^2 
$$ {#eq-SSTTreatmentSumSquares}

and has k-1 degrees of freedom.

If we divide this by its degrees of freedom we get what is called the **treatment mean square (MST)** and measures the variability of the treatment means $\bar{y_j}$

$$
MST= \frac{SST}{k-1}
$$ {#eq-MSTTreatmentMeanSquare}

The other quantity we are interested in is the **error sum of squares (SSE)** where we look at the difference between each observation and the group mean.

$$
SSE= \sum_j\sum_i({y_{ij}}-\bar{y_j})^2
$$ {#eq-SSEErrorSumSquares}

and has N-k degrees of freedom.

Dividing the error sum of square by its degrees of freedom we get what is called the **error mean square or Mean Square for Error (MSE)** and measures the variability within the groups. Not to be confused with the the [MSE (Mean Squared Error)](#MeanSquaredError) that we saw in linear model. In both cases, MSE is a measure of variability of the errors, but In ANOVA, MSE (Mean Square for Error) assesses within-group variance. In linear models, MSE (Mean Squared Error) assesses the goodness of fit of the model by looking at prediction errors.

$$
MSE= \frac{SSE}{N-k}
$$ {#eq-MSEErrorMeanSquare}

Since we want to compare the variation between the groups to the variation within the groups we look at the ratio

$$
F= \frac{MST}{MSE}
$$ {#eq-Fstatistic}

Under the null hypothesis of equal group means this ratio should be about 1. It will not be exactly one due to sampling variability. It follows a F-distribution with k-1 and N-k degrees of freedom. Large values of F suggest that the variation between groups is unusually large. We reject the null hypothesis if F is the right 5% of the tail, i.e. when the $p$-value is smaller than 5%.

The result of the test is summarized in the ANOVA table:

| Source    | df  | Sum of Squares | Mean Square | F       | p-value |
|-----------|-----|----------------|-------------|---------|---------|
| Treatment | k-1 | SST            | MST         | MST/MSE |         |
| Error     | N-k | SSE            | MSE         |         |         |
| Total     | N-1 | TSS            |             |         |         |

Where **TSS is the Total Sum of Squares** and measures the total variability in the data (the overall variation of the observed data points around their mean):

$$
TSS=\sum_j\sum_i({y_j}-\bar{\bar{y}})^2 
$$ {#eq-TSS1}

### The one-way ANOVA model

The idea behind the anova table is that each observation is generated as the sum of a treatment $\mu_j$ plus and error term $\epsilon_{ij}$ (measurment of error) that follow the normal curve with mean 0 and common variance $\sigma^ 2$

$$
y_{ij}=\mu_j+\epsilon_{ij}
$$

our null hypothesis is that all treatment means are the same $\mu_1=\mu_2=\cdots=\mu_k$ Instead of looking at the group means it is helpful to look at deviations from an overall mean $\mu$ that deviation is represented by the greek letter 'tau' and called the **treatment effect** $\tau_j=\mu_j-\mu$ so the model is the overall mean plus the deviation plus an error term $$
y_{ij}=\mu+\tau_j+\epsilon_{ij}
$$

We estimate the overall mean $\mu$ by the grand mean $\bar{\bar{y}}$. then the estimate $\tau_j=\bar{y_j}-\bar{\bar{y}}$ and the estimate of the error is the residual $\epsilon_{ij}=y_{ij}-\bar{y_j}$. so we can write the previous equation like this:\
$$
y_{ij}=\mu+\tau_j+\epsilon_{ij}=\bar{\bar{y}}+(\bar{y_j}-\bar{\bar{y}})+(y_{ij}-\bar{y_j})
$$

now if we move $\bar{\bar{y}}$ to the left of the equation:

$$
y_{ij}-\bar{\bar{y}}=(\bar{y_j}-\bar{\bar{y}})+(y_{ij}-\bar{y_j})
$$

and we sum and square the terms:

$$
\sum_{j} \sum_{i} (y_{ij} - \bar{y}_{i})^2 = \sum_{j} \sum_{i} (y_{ij} - \bar{y}_{j})^2 + \sum_{j} \sum_{i} (\bar{y}_{j} - \bar{y}_{i})^2
$$

which is equal to say:

$$
TSS=SST+SSE
$$ {#eq-TSS2} Which means that total variation can be split into two sources, the treatment sum of squares and the error sum of squares. This is the decomposition that is behind the ANOVA table.

::: {#anovaassump style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;" icon="false"}
ANOVA Assumptions

-   Data are normally distributed

-   The F-test assumes that all the groups have the same variance. This can be roughly checked with side by side plots, but there are formal test we can perform as well.

-   The data are independent within and across groups. This would be the case if the subjects were assigned treatment at random. On the other hand, if the data was acquired from an observational study, we need to be very careful because this assumption would not be met and there could be cofounders so we will not be able to claim that there is causation with the treatment and the results of the tests.
:::

## Advanced ANOVA Techniques

Now we are going to use the mpg_2008 data set again to see if there is a difference in highway millage with respect to the 'Drive' categorical variable (front wheel, rear wheel and 4 wheel drive). Let's do the same we did above and calculate the average per group first

```{r}
mpg_2008 |> 
  group_by(drv) |> 
  dplyr::summarize(mean(hwy))
```

and we plot the data:

```{r, fig.align='center', echo=FALSE}
ggplot(mpg_2008, aes(x = drv, y = hwy)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter()
```

The boxplot allows us to see that the spread of the data in these groups is similar (the width of these boxes is about the same) so we can run an anova test on these data.

if we calculate the anova for this we find that the evidence support the hypothesis that there is in fact a difference in hwy between these groups:

```{r}
model <- aov(hwy ~ drv, data = mpg_2008)
summary(model)
```

As we mentioned, the ANOVA test does not tell us anything about each group in particular, for this we can run another test, one recommended is the **Tukey Honest Significant Difference** test and this will give us a $p$-value and a confidence interval for each pair of categories in our data.

```{r}
TukeyHSD(model)
```

The Tukey Honest Significant Difference test is better than running a Welch Two sample test by selecting just two variables in our dataset and testing one against the other. If we just run three different t-test comparing the three groups, we're potentially going to have an increased probability of a false positive, so the Tukey HSD test is specifically controlling for that multiple comparison problem.

Let's run a Welch Two sample test over one of the pairs in this dataset and see how it varies from the results from the TukeyHSD test:

```{r}
mpg_r4 <- filter(mpg_2008, drv != "f")
t.test(mpg_r4$hwy ~ mpg_r4$drv)
```

we can see that the $p$-value here is smaller than the one calculated for that same pair in the Tukey Test.

::: {.callout-orange appearance="simple" icon="false"}
ANOVA tests if there are statistically significant differences between the means of three or more groups. It essentially extends the t-test to multiple groups.
:::

Recap:

::: {#correlationkeypoints style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
1.  Sample correlation measures the strength and direction of the linear relationship between two variables, providing insights into their association.

2.  Hypothesis testing and significance of correlation allow you to determine whether the observed correlation is statistically significant, indicating a relationship beyond random chance.

3.  Linear regression enables you to model relationships between variables, predicting outcomes and understanding the impact of independent variables on the dependent variable.

4.  ANOVA is a statistical technique used to compare means between multiple groups, assessing whether there are significant differences among the groups.

5.  Independence testing of categorical variables examines whether there is a relationship between two categorical variables, determining if they are independent or associated.

6.  Known the assumptions: whether it's correlation, regression or ANOVA, always ensure that assumptions (like normality or homoscedasticity) are met before drawing any conclusions.
:::

# Generalized Linear Models (GLM)

Generalized Linear Models (GLMs) are a flexible extension of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. GLMs are used to model a wide range of data types and relationships.

**Common types of GLMs**

-   Linear Regression: when the response variable is continuous and normally distributed.

-   Logistic Regression: When the response is binary (e.g. success/failure)

-   Poisson Regression: For count data (e.g. number of events)

## Poisson Distribution

The Poisson distribution is a probability distribution that expresses the probability of a given number of events occurring within a fixed interval of time or space, provided that these events occur with a known constant mean rate and independently of the time since the last event.

### Characteristics

-   **Discrete distribution**: The Poisson distribution applies to events that can be counted in whole numbers.

-   **Parameter**: The Poisson distribution is characterized by a single parameter, (\lambda) (lambda), which represents the average rate (mean) of occurrences within a given interval.

-   **Probability Mass Function (PMF)**: The probability of observing (k) events (where (k) is a non-negative integer) is given by the formula:

$$
  P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$ where (e) is the base of the natural logarithm (approximately equal to 2.71828), and (k!) is the factorial of (k).

### Usage

-   **Modeling Rare Events**: The Poisson distribution is often used to model rare events, such as the number of phone calls received by a call center per hour or the number of accidents occurring at a busy intersection per day.

-   **Independent Events**: It assumes that the number of events occurring in disjoint intervals are independent.

### Example

Suppose a bookstore observes that, on average, 3 customers enter the store per hour. To calculate the probability that exactly 5 customers will enter the store in the next hour, we can use the Poisson distribution with (\lambda = 3) and (k = 5):

```{r}
# Calculate the probability of exactly 5 customers entering the bookstore
lambda <- 3
k <- 5
probability <- (lambda^k * exp(-lambda)) / factorial(k)
probability
```

```{r}
# Compute the probability using dpois
dpois(5, lambda = 3)
```

## Poison regression model

We will use the dataset `bikershare` from ISLR2 package. The response is *bikers* that are the number of hourly users in `bikeshare` program in Washington DC.

First we are just going to plot the data in the dataset, without any fitting for the number of riders per hour of the day:

```{r bikeshareScatter, echo=FALSE, fig.align='center', fig.width=6}

plot(jitter(as.numeric(Bikeshare$hr)), jitter(Bikeshare$bikers), 
     xlab = "Hour", 
     ylab = "Number of Bikers", 
     main = "Number of Bikers per Hour",
     pch = 19, # Point character
     col = 'lightblue',
     cex = 0.5
  )
smooth.spline.fit <- smooth.spline(as.numeric(Bikeshare$hr), Bikeshare$bikers, spar=0.5)
lines(smooth.spline.fit, col = "darkblue", lwd = 2)
```

Now if we use boxplots we can see how as the number of bikers increase, so does the standard deviation:

```{r, echo=FALSE, fig.align='center', fig.width=6}
plot(Bikeshare$hr, Bikeshare$bikers, col= 'lightblue',
     xlab = "Hour", 
     ylab = "Number of Bikers", 
     main = "Number of Bikers per Hour")
```

If we use linear model to make predictions and we plot the results as we did before in a scatter plot:

To perform this analysis, first we fit a linear regression model. We are going to show two different ways of doing it:

```{r lmoverbikesharedata1}
lm.fit <- lm(
bikers ~ mnth + hr + workingday + temp + weathersit ,
data = Bikeshare)
summary(lm.fit)
```

In this model the first level of hr (0) and mnth (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines.

We created another second model with numbered values for `mnth` and `hr`.

```{r lmoverbikesharedata2}
contrasts (Bikeshare$hr) = contr.sum (24)
contrasts (Bikeshare$mnth) = contr.sum (12)
lm.fit <- lm(
  bikers ~  mnth + hr + workingday + temp + weathersit ,
data = Bikeshare)
summary(lm.fit)
```

Notice that we used contrast for month and hour. `Bikeshare$hr` and `Bikeshare$mnth` are categorical variables representing hour and month, respectively. Sum-to-Zero contrasts ensure that the coefficients of the categories sum to zero. This makes the interpretation of the regression coefficients easier.

For instance, if you have 24 hours in a day and set up sum-to-zero contrasts (`contr.sum(24)`), the sum of all hour coefficients will be zero.

This helps in comparing each category to the overall mean effect, rather than to a reference category.

By default, categorical variables are typically converted to dummy variables (0/1), which can lead to difficulties in interpreting the results, especially with a large number of categories.

In this second model the coefficient estimate for the last level of `mnth` is not zero: instead, it equals the negative of the sum of the coefficient estimates for all of the other levels. Similarly, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all of the other levels. This indicates that the difference between the mean level and the coefficients of `hr` and `mnth` in the second model will always total to zero.

We use the linear model to predict the values:

```{r bikersharepredlm, echo=FALSE, fig.align='center', fig.width=6}
lm.pred <- predict(lm.fit)
point_colors <- ifelse(lm.pred < 0, "red", "lightblue")
plot(jitter(as.numeric(Bikeshare$hr)), jitter(lm.pred), 
     xlab = "Hour", 
     ylab = "Number of Bikers", 
     main = "Number of Bikers per Hour",
     pch = 19, # Point character
     col = point_colors,
     cex = 0.5
  )
smooth.spline.fit <- smooth.spline(as.numeric(Bikeshare$hr), lm.pred, spar=0.5)
lines(smooth.spline.fit, col = "darkblue", lwd = 2)
```

One of first things that we notice is that there are negative values (red points) predicted, and this is because the linear model does not have any type of constraint for that, so **linear models are not the best choice for counts**, we have a better alternative and this is the *Poisson Regression Model*.

The Probability mass function (PMF) is this for a single variance: $$
P(Y = k) = \frac{e^{-\lambda} \lambda^k}{k!}
$$ {#eq-poissonpmf} where:

-   $P(Y = k)$: Probability of observing ( k ) events.
-   $\lambda$: Average rate (mean number of events) in a given interval.
-   $e$: Euler's number (approximately 2.71828).
-   $k!$: Factorial of $k$ (number of events).

Note that for the Poisson distribution, the mean and the variance are directly related, meaning that when the mean is higher, the variance is also higher. In fact we assume that the variance equals the mean.

This formula calculates the probability of exactly $k$ events occurring in a fixed interval when events happen at a constant mean rate and independently of the time since the last event. The Poisson distribution is useful for modeling the number of events in a specific time.

When we have multiple parameters, the model changes to include the covariates:

$$
\lambda(X_1,\dots,X_p)=e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}
$$ {#eq-poissonpmfcovariates}

Now we are going to use `glm()` function with family Poisson in r to fit this model:

```{r bikershareglm}
glm.fit <- glm(bikers ~  mnth + hr + workingday + temp + weathersit ,
data = Bikeshare, family=poisson)
summary(glm.fit)
```

and the predicted values plot show that we don't have negative counts anymore. We must use the argument type= "response" which tells R to output probabilities of the form $P(Y=1|X)$ as opposed to other information such as the logit.

```{r bikersharepredglm, echo=FALSE, fig.align='center', fig.width=6}

glm.pred <- predict(glm.fit, type = "response")
point_colors <- ifelse(glm.pred < 0, "red", "lightblue")
plot(jitter(as.numeric(Bikeshare$hr)), jitter(glm.pred),
     xlab = "Hour",
     ylab = "Number of Bikers",
     main = "Number of Bikers per Hour",
     pch = 19, # Point character
     col = point_colors,
     cex = 0.5
  )
smooth.spline.fit <- smooth.spline(as.numeric(Bikeshare$hr), glm.pred, spar=0.5)
lines(smooth.spline.fit, col = "darkblue", lwd = 2)
```

In order to visualize the outputs of the models, we plot the response estimates against the coefficients for both the linear model and the Poisson model. But first, it is important to obtain the coefficient estimates associated with the last month and hour. The coefficients for January through November can be obtained directly from the lm.fit object. The coefficient for December must be explicitly computed as the negative sum of all the other months. The linear model results:

```{r mgralph, echo=FALSE, fig.align='center', fig.width=6}
coef.months <- c( coef (lm.fit)[2:12],-sum ( coef (lm.fit)[2:12]))

plot (coef.months , xlab = " Month ", ylab = "Coefficient",
xaxt = "n", col = " blue ", pch = 19, type = "o")

axis (side = 1, at = 1:12, labels = c("J", "F", "M", "A",
"M", "J", "J", "A", "S", "O", "N", "D"))
```

and the glm results:

```{r glmcoeffgraphmonth , echo=FALSE, fig.align='center', fig.width=6}
coef.mnth <- c(coef (glm.fit)[2:12],
-sum ( coef (glm.fit)[2:12]))
plot(coef.mnth , xlab = " Month ", ylab = " Coefficient ",
xaxt = "n", col = " blue ", pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A", "M",
"J", "J", "A", "S", "O", "N", "D"))
```

And for the hours using the linear model:

```{r plothourscoefficients, echo=FALSE, fig.align='center', fig.width=6}
coef.hours <- c(coef(lm.fit)[13:35],
-sum (coef(lm.fit)[13:35]))
plot(coef.hours , xlab = " Hour ", ylab = " Coefficient ",
col = " blue ", pch = 19, type = "o")
```

And the glm poisson model:

```{r hoursplotglm, echo=FALSE, fig.align='center', fig.width=6}
coef.hours <- c(coef(glm.fit)[13:35],-sum (coef(glm.fit)[13:35]))
plot(coef.hours , xlab = " Hour ", ylab = " Coefficient ",
col = " blue ", pch = 19, type = "o")
```

The predictions from the Poisson regression model are correlated with those from the linear model; however, our Poisson model produces non-negative outputs. For Poisson regression the responses at each level of X become more variable with increasing means, where variance=mean.In addition, the mean values of Y at each level of X fall on a curve, not a line. AS a result, at either very low or very high levels of Y, the Poisson regression predictions tend to be larger than those from the linear model.

## Simulating data from a Generalized Linear Model

Suppose we want to simulate from a Poisson model where Y\~poisson($\mu$) \$ log \mu = \beta\_0\beta\_1x\$ and $\beta_0 = 0.5$ and $\beta_1 = 0.3$

```{r}
set.seed(1)
x<- rnorm(100)
log.mu<- 0.5+0.3 *x
y <- rpois(100, exp(log.mu))
summary(y)
plot(x,y)

```

# Summary

::: orange-box
**Linear Regression:** Use When: You want to predict a continuous outcome based on one or more predictor numerical variables. Example: Predicting house prices based on size, location, and age.

**Logistic Regression:** Use When: You want to predict a binary outcome (0/1) based on one or more predictor variables. Example: Predicting whether a customer will buy a product (Yes/No) based on their browsing history.

**Discriminant Analysis (LDA/QDA):** Use When: You want to classify observations into predefined categories based on predictor variables. LDA is used when the assumptions of equal covariance matrices are valid (same variance in each class), while QDA is used when they are not.

Example: Classifying iris species based on sepal and petal measurements.

**Naive Bayes** is useful when the number of parameters is large

**Generalized Linear Models (GLMs)**: Use When: You need more flexibility in modeling different types of outcomes (e.g., counts, binary, proportions) and you might have predictors that are not normally distributed.

Example: Using Poisson regression (a type of GLM) to model the number of calls received by a call center per hour.
:::
