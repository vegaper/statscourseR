---
title: "Exploratory Analysis"
format: html
editor: visual
---

```{r}
#| echo: false
library(dplyr)
library(tidyverse)
library(here)
library(readxl)
#library(infer)
library(ggplot2)
library(UsingR)
```

# Exploratory Analysis (EDA)

## Mean

The mean (or arithmetic mean) is a measure of central tendency that represents the average value of a set of numbers. It is calculated by summing all the values in the dataset and then dividing by the number of values. $$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$ {#eq-mean} When we talk about the population mean we use the greek letter $\mu$ and when we talk about the sample mean we use our variable with a bar on top $\bar{x}$ .

## Variance

Variance measures the average squared deviations from the mean. To calculate it:

Find the mean of the data set. Subtract the mean from each data point and square the result. Average these squared differences.

Mathematically, for a population, the variance $\sigma^2$ is: $$
\sigma^2 = \frac{1}{N}\sum^N_{i=1}(x_i-\mu)^ 2
$$ {#eq-variance}

where (N) is the number of data points, $x_i$ are the data points, and $\mu$ is the mean.

## Standard deviation

Standard deviation is the square root of the variance. It provides a measure of spread in the same units as the data, making it more interpretable. It quantifies how much the values in a dataset deviate from the mean (average) of the dataset. A low standard deviation indicates that the values are close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.

Population standard deviation:

$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2}
$$ {#eq-standardDeviation}

where:

-   ($\sigma$) is the population standard deviation

-   \(N\) is the population size

-   ($X_i$) is the (i)-th observation in the population

-   ($\mu$) is the population mean

We can calculate the population standard deviation like this:

```{r}
heights=father.son$fheight
popsd <- function(x) sqrt(mean((x-mean(x))^2))
popsd(heights)
```

Note the `sd` function in R gives us a sample estimate of the σ as opposed to the population σ

```{r}
standardDeviation <- sd(heights)
standardDeviation
```

The sample standard deviation formula is very similar:

$$
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2}
$$ {#eq-sampleStandarDeviation}

### The empirical rule:

If the data follows the normal curve then

-   about 2/3 (68%) of the data fall within one standard deviation of the mean.

-   about 95% fall within 2 standard deviations of the mean

-   99.7% fall within 3 standard deviations of the mean

```{r, fig.align='center', echo=FALSE}
#| warning= FALSE
data <- data.frame (values = rnorm(1000, mean = 10, sd = 3))
# Calculate mean, median, and standard deviation
mean_val <- mean(data$values)
median_val <- median(data$values)

sd_val <- sd(data$values)
gg<- ggplot(data, aes(x = values))+
  geom_histogram()

# Add vertical lines for mean, median, and standard deviation
gg <- gg +
  geom_vline(xintercept = mean_val, color = "red", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = median_val, color = "green", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val + sd_val, color = "blue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - sd_val, color = "blue", linetype = "dashed", linewidth = 1)+
  geom_text(aes(x = mean_val, y = 10, label = "Mean"), color = "red", vjust = -0.5, angle=90) +
  geom_text(aes(x = median_val, y = 10, label = "Median"), color = "green", vjust = -0.5,hjust = -2, angle=90) +
  geom_text(aes(x = mean_val + sd_val, y = 10, label = "SD1"), color = "blue", vjust = -0.5, angle=90) +
  geom_text(aes(x = mean_val - sd_val, y = 10, label = "SD1"), color = "blue", vjust = -0.5, angle=90)
gg

```

Biases, systematic errors and unexpected variability are common in data. Failure to discover these problems often leads to flawed analyses and false discoveries. As an example, consider that experiments sometimes fail and not all data processing pipelines, such as the \`t.test\` function in R, are designed to detect these. Yet, these pipelines still give you an answer. Furthermore, it may be hard or impossible to notice an error was made just from the reported results.

Graphing data is a powerful approach to detecting these problems. We refer to this as *exploratory data analysis* (EDA). Many important methodological contributions to existing techniques in data analysis were initiated by discoveries made via EDA.

## Median, Median Absolute Deviation (MAD) and outliers

The **median** is a measure of central tendency that represents the middle value in a sorted list of numbers. If the list has an odd number of observations, the median is the middle number. If the list has an even number of observations, the median is the average of the two middle numbers. The median is less affected by outliers and skewed data compared to the mean.

The *Median Absolute Deviation (MAD)* is a robust measure of statistical dispersion. It is defined as the median of the absolute deviations from the data's median. MAD is less sensitive to outliers compared to the standard deviation. $$
MAD= median(|X_i-median(X)|)
$$ {#eq-mad} For example, I create a vector with 100 values following a normal distribution with average 0 and standard deviation 1, but then I change one of the points to a value of 100.

```{r boxplot_showing_outlier, fig.cap="Normally distributed data with one point that is very large due to a mistake.", fig.align='center',echo=FALSE}
set.seed(1)
x=c(rnorm(100,0,1)) ##real distribution
x[23] <- 100 ##mistake made in 23th measurement
boxplot(x)
```

now we are going to see how this datapoint is affecting our statistics:

```{r,echo=FALSE}
cat("The average is",mean(x))
cat("The SD is",sd(x))
cat("The median is ", median(x))
cat("The MAD is ", mad(x))
```

## Spearman correlation {#spearman-correlation}

The correlation (it will be presented later in [another chapter](#confidenceIntvsSignifTest)) is also very sensitive to outliers. In this example we create two datasets with no correlation whatsoever:

```{r, fig.align='center',echo=FALSE}
set.seed(1)
x=rnorm(100,0,1) 
y=rnorm(100,0,1) 

plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),
     pch=21,bg=1)
abline(0,1)
```

now we introduce as we did before one datapoint of a much higher value for x and y. This is affecting our correlation coefficient:

```{r, scatter_plot_showing_outlier,fig.cap="Scatterplot showing bivariate normal data with one signal outlier resulting in large values in both dimensions.", fig.align='center',echo=FALSE}
set.seed(1)
x[23] <- 100 ##mistake made in 23th measurement
y[23] <- 84 ##similar mistake made in 23th measurement

plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),
     pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
abline(0,1)
```

The Spearman correlation follows the general idea of median and MAD, that of using quantiles. The idea is simple: we convert each dataset to ranks and then compute correlation:

```{r spearman_corr_illustration, fig.cap="Scatterplot of original data (left) and ranks (right). Spearman correlation reduces the influence of outliers by considering the ranks instead of original data.",fig.width=10.5,fig.height=5.25, fig.align='center',echo=FALSE}
par(1,2)
plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
plot(rank(x),rank(y), main=paste0("correlation=",round(cor(x,y,method="spearman"),3)),
     pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
abline(0,1)
```

In general, if we know there are outliers, then median and MAD are recommended over the mean and standard deviation counterparts.

## Empirical Cumulative Density Function (CDF)

What exactly is a distribution? The simplest way to think of a *distribution* is as a compact description of many numbers.

Although not as popular as the histogram for EDA, the empirical cumulative density function (CDF) (or cumulative distribution function) shows us the same information and does not require us to define bins. For any number a the empirical CDF reports the proportion of numbers in our list smaller or equal to a.

$$
F(a) \equiv Pr(x \leq a)
$$ {#eq-cdf}

This is called the cumulative distribution function (CDF). When the CDF is derived from data, as opposed to theoretically, we also call it the empirical CDF (ECDF).

R provides a function that has as out the empirical CDF function.

```{r}
heights=father.son$fheight 
round(sample(heights,20),1) 
myCDF <- ecdf(heights) 
```

The `ecdf` function is a function that returns a function, which is not typical behavior of R functions. We create a function called myCDF based on our data *heights* that can then be used to generate a plot:

```{r ecdffunction, fig.align='center',echo=FALSE}
##We will evaluate the function at these values:
xs<-seq(floor(min(heights)),ceiling(max(heights)),0.1) 
### and then plot them:
plot(xs,myCDF(xs),type="l",xlab="x=Height",ylab="F(x)")
```

## Boxplot

When the data is not normally distributed the mean and the standard deviation are not enough to summarise the data. A better approach would be to present the data in a boxplot.

```{r, fig.align='center',echo=FALSE}
# Create a not normally distributed sample (exponential distribution)
set.seed(123)  # For reproducibility
data <- rexp(100, rate = 0.2)

# Create a boxplot with ggplot2
p <- ggplot(data.frame(value = data), aes(x = "", y = value)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 3, color = "blue") +
  labs(title = "Boxplot of Exponentially Distributed Data",
       y = "Values") +
  theme_minimal()

# Add labels for the important parts of the boxplot
p + annotate("text", x = 1.2, y = quantile(data, 0.75)+1.5, label = "Q3 (75th percentile)", hjust = 0) +
  annotate("text", x = 1.2, y = quantile(data, 0.25)-1.5, label = "Q1 (25th percentile)", hjust = 0) +
  annotate("text", x = 1.2, y = median(data)+1, label = "Median", hjust = 0) +
  annotate("text", x = 1.2, y = mean(data)+0.5, label = "Mean", hjust = 4.5, color = "blue") +
  annotate("text", x = 1.2, y = max(data[data <= quantile(data, 0.75) + 1.5 * IQR(data)]), label = "Max (whisker)", hjust = 2) +
  annotate("text", x = 1.2, y = min(data[data >= quantile(data, 0.25) - 1.5 * IQR(data)]), label = "Min (whisker)", hjust = 2)


```

-   Q1 (25th percentile): The lower quartile.

-   Median: The middle value of the data.

-   Mean: The average value of the data (marked in blue).

-   Q3 (75th percentile): The upper quartile.

-   Box (between the 25th percentile and the 75th percentile) is showing the middle half of the data.

-   Max (whisker): The maximum value within 1.5 times the IQR above Q3.

-   Min (whisker): The minimum value within 1.5 times the IQR below Q1.

-   Outliers: Points beyond the whiskers, marked in red.

## Histogram {#histogram}

We can think of any given dataset as a list of numbers. Suppose you have measured the heights of all men in a population. Imagine you need to describe these numbers to someone that has no idea what these heights are, for example an alien that has never visited earth. One approach is to simply list out all the numbers for the alien to see. Here are 20 randomly selected heights of 1,078

From scanning through these numbers we start getting a rough idea of what the entire list looks like but it is certainly inefficient. We can quickly improve on this approach by creating bins, say by rounding each value to the nearest inch, and reporting the number of individuals in each bin. A plot of these heights is called a histogram

We can specify the bins and add better labels in the following way:

```{r histogramofnormaldist,fig.align='center',echo=FALSE}
bins <- seq(floor(min(heights)),ceiling(max(heights)))  
hist(heights,breaks=bins,xlab="Height",main="Adult men heights") 
```

Showing this plot is much more informative than showing the numbers. Note that with this simple plot we can approximate the number of individuals in any given interval. For example, there are about 70 individuals over six feet (72 inches) tall.

This denotes the probability that the random variable ( x ) is between ( a ) and ( b )

$$P(a \leq x \leq b) = F(b) - F(a)$$ {#eq-probabilityHistogram}

## Probability Distribution

Summarizing lists of numbers is one powerful use of distribution. An even more important use is describing the possible outcomes of a random variable. Unlike a fixed list of numbers, we don't actually observe all possible outcomes of random variables, so instead of describing proportions, we describe probabilities. For instance, if we pick a random height from our list, then the probability of it falling between $a$ and $b$ is denoted with: $P(a \leq X \leq b) = F(b) - F(a)$

Note that the $X$ is now capitalized to distinguish it as a random variable and that the equation above defines the probability distribution of the random variable.

### The p-value

Knowing this distribution is incredibly useful in science. If we know the distribution of our data when the null hypothesis is true, referred to as the *null distribution*, we can compute the probability of observing a value as large as we did in our experiment, referred to as a $p$-value.

::: exercise-block
Example.

We have data from 24 mice fed two different diets. We want to know if the mice fed with high fat diet increased their body weight more than the mice fed with the control diet.

```{r}
femaleMiceWeights <- read.csv("data/femaleMiceWeights.csv")

head(femaleMiceWeights)
control<- femaleMiceWeights %>% filter (Diet=='chow')
treatment <-  femaleMiceWeights %>% filter (Diet=='hf')
tm<- mean(treatment$Bodyweight)
cm<- mean(control$Bodyweight)
obsdiff <- tm - cm
print(paste("treatment mean:", tm))
print(paste("control mean:", cm))
print(paste("difference =", obsdiff))

```

We can see that there is a difference between the two means but if we choose random samples of 12 individuals from the mice population, each of the samples would have different means just by chance. So how can we be sure if that difference is due to the change in diet or to the random sampling?

We are going to recreate that by choosing 24 mice and assigning them randomly to either treatment or control. We will repeat this 10000 times:

```{r}
n <- 10000
null <- vector("numeric",n)
for (i in 1:n) {
  control <- sample(femaleMiceWeights$Bodyweight,12)
  treatment <- sample(femaleMiceWeights$Bodyweight,12)
  null[i] <- mean(treatment) - mean(control)
}
head(null)
```

The values in `null` form what we call the *null distribution*.

What percentage of our 10000 experiments have a difference weight between control and treatment higher or equal to the one we observed for the different diets?

```{r}
mean(null >= obsdiff)
```

We see a difference as big as the one we observed only a small percentage of the time. This is what is known as a $p$-value. We can plot the null histogram and mark the line with our observed weight difference for the high fat diet.

```{r null_and_obs,fig.cap="Null distribution with observed difference marked with vertical red line.", fig.align='center', echo=FALSE}
hist(null, freq=TRUE)
abline(v=obsdiff, col="red", lwd=2)
```
:::

An important point to keep in mind here is that while we defined $\mbox{Pr}(a)$ by counting cases, we will learn that, in some circumstances, mathematics gives us formulas for $\mbox{Pr}(a)$ that save us the trouble of computing them as we did here. One example of this powerful approach uses the normal distribution approximation.

We will see later on [how to calculate the $p$-values](#pvalues)

# Normal distribution

A **normal distribution**, also known as a Gaussian distribution, is a continuous probability distribution that is symmetrical around its mean. This means that most of the observations cluster around the central peak, and the probabilities for values further away from the mean taper off equally in both directions. When plotted on a graph, it forms a bell-shaped curve, often referred to as a "bell curve".

Key properties of a normal distribution include

-   **Symmetry**: The left and right sides of the distribution are mirror images.

-   **Mean, Median, and Mode**: All three measures of central tendency are equal and located at the center of the distribution.

```{r, fig.align='center',echo=FALSE}
data <- data.frame (values = rnorm(1000, mean = 10, sd = 3))

gg<- ggplot(data, aes(x = values))+
  geom_histogram()
gg
```

probability density function (PDF) of a normal distribution

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$ {#eq-PDF}

we can use a mathematical formula to approximate the proportion of values or outcomes in any given interval:

$$
\mbox{Pr}(a < x < b) = \int_a^b \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left( \frac{-(x-\mu)^2}{2 \sigma^2} \right)} \, dx
$$ {#eq-pnorm}

While the formula may look intimidating, don't worry, you will never actually have to type it out, as it is stored in a more convenient form (as `pnorm` in R which sets *a* to $-\infty$, and takes *b* as an argument). We can compute the proportion of values below a value `x` with `pnorm(x,mu,sigma)`. A very useful characteristic of this approximation is that one only needs to know $\mu$ and $\sigma$ to describe the entire distribution. From this, we can compute the proportion of values in any interval.

## Standardizing data

A normal curve is determined by the mean $\bar{x}$ and the standard deviation $s$ . If the data follow the normal curve, then knowing those two values mean we know the whole histogram. To compute areas under the normal curve, we first standardize the data by subtracting $\bar{x}$ and dividing by $s$. The resulting value is called the standardized value or $z$-score $z$

$$
z = \frac{x_1 - \bar{x}}{s}
$$ {#eq-zscore}

After standardizing all points in our data we will get a distribution with mean = 0 and standard deviation =1

```{r,fig.align='center',echo=FALSE}
standardizedValues <- data.frame(values = scale(data$values))
mean_val <- mean(standardizedValues$values)
median_val <- median(standardizedValues$values)
sd_val <- sd(standardizedValues$values)

# Add vertical lines for mean, median, and standard deviation
gg <- ggplot(standardizedValues, aes(x = values))+
  geom_histogram()+
  geom_vline(xintercept = mean_val, color = "darkred", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = median_val, color = "green", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val + sd_val, color = "lightblue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - sd_val, color = "lightblue", linetype = "dashed", linewidth = 1)+
  geom_vline(xintercept = mean_val + 2*sd_val, color = "lightblue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - 2*sd_val, color = "lightblue", linetype = "dashed", linewidth = 1)+
  geom_text(aes(x = mean_val, y = 10, label = "Mean"), color = "darkred", vjust = -0.5, angle=90) +
  geom_text(aes(x = median_val, y = 10, label = "Median"), color = "green", vjust = -0.5,hjust = -2, angle=90) +
  geom_text(aes(x = mean_val + sd_val, y = 10, label = "SD1"), color = "lightblue", vjust = -0.5, angle=90)  +
  geom_text(aes(x = mean_val - sd_val, y = 10, label = "SD1"), color = "lightblue", vjust = -0.5, angle=90)  +
  geom_text(aes(x = mean_val + 2*sd_val, y = 10, label = "SD2"), color = "lightblue", vjust = -0.5, angle=90) +
  geom_text(aes(x = mean_val - 2*sd_val, y = 10, label = "SD2"), color = "lightblue", vjust = -0.5, angle=90)
gg

```

So now, the value of $z$ for a specific value $x_1$ is indicating how many standard deviations our value is away from the mean, and just by simple approximation using the empirical rule, if for example you standardize your height and you get a value of 1, you already know without having to look into more detail that you are approximately higher than 84% of the population.

```{r empiricalRule, fig.height=4,fig.width=6, fig.align='center',echo=FALSE}

# Plot the cumulative percentages
# Define the range for the x-axis
x <- seq(-4, 4, length = 1000)

# Calculate the density of the normal distribution
y <- dnorm(x)

# Define the z-values
z_values <- c(-3, -2, -1, 0, 1, 2, 3)

# Calculate the cumulative percentages
cumulative_percentages <- pnorm(z_values) * 100

# Plot the normal distribution curve
plot(x, y, type = "l", lwd = 2, col = "lightblue", ylab = "Density", xlab = "Z-score",
     main = "Standard Normal Distribution with Cumulative Percentages", ylim = c(0, 0.7))

# Add vertical lines at the z-values
abline(v = z_values, col = "darkred", lty = 2)

# Annotate the cumulative percentages
text(z_values, dnorm(z_values), labels = paste0(round(cumulative_percentages, 2), "%"), pos = 3, col = "darkred")
grid()

```

## Normal Distribution functions in R

### rnorm()

The `rnorm()` function generates random numbers from a normal distribution. 
`rnorm(n, mean = 0, sd = 1)` 

- n: Number of observations. 

- mean: Mean of the distribution (default is 0). 

- sd: Standard deviation of the distribution (default is 1). 

```{r}
# Generate 5 random numbers from a normal distribution with mean 10 and standard deviation 2
rnorm(5, mean = 10, sd = 2)

```

### dnorm()
The `dnorm()` function computes the density (height of the probability density function) of the normal distribution.
`dnorm(x, mean = 0, sd = 1, log = FALSE)` 

- x: A vector of quantiles.

- mean: Mean of the distribution (default is 0).

- sd: Standard deviation of the distribution (default is 1).

- log: If TRUE, returns the logarithm of the density (default is FALSE).
```{r}
# Compute the density of the normal distribution at x = 2
dnorm(2, mean = 0, sd = 1)
```


### pnorm()
The `pnorm()` function computes the cumulative distribution function (CDF) of the normal distribution.
`pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)` 
- q: A vector of quantiles.

- mean: Mean of the distribution (default is 0).

- sd: Standard deviation of the distribution (default is 1).

- lower.tail: If TRUE, returns the cumulative probability up to q (default is TRUE).

- log.p: If TRUE, returns the logarithm of the cumulative probability (default is FALSE).

```{r}
# Compute the cumulative probability for x = 1.5
pnorm(1.5, mean = 0, sd = 1)
```

### qnorm()
The `qnorm()` function computes the quantile (inverse of the CDF) of the normal distribution.
`qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)`
- p: A vector of probabilities.

- mean: Mean of the distribution (default is 0).

- sd: Standard deviation of the distribution (default is 1).

- lower.tail: If TRUE, returns the quantile corresponding to the cumulative probability p (default is TRUE).

- log.p: If TRUE, p is given as log(p) (default is FALSE).

```{r}
# Compute the quantile for cumulative probability 0.975
qnorm(0.975, mean = 0, sd = 1)

```


## Normal approximation

When the histogram of a list of numbers approximates the normal distribution we can use a convenient mathematical formula to approximate the proportion of individuals in any given interval. We already saw this equation (@eq-pnorm)

$$
P(a < x < b) = \int_{a}^{b} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(\frac{-(x - \mu)^2}{2\sigma^2}\right) \, dx
$$ {#eq-pnorm1}

If this approximation holds for our data then the population mean and variance of our data can be used in the formula above.

::: exercise-box
Example

*Let's calculate the number of individuals that are taller than 72 inches.* First we are going to calculate the number manually:

```{r}
n_individuals = length(heights)
taller = length(which(heights>72))
propTaller = taller/n_individuals
propTaller
```

so we have roughly 6.4% of the individuals taller than 72 inches. Now wee can calculate it using normal approximation and it will gives us an similar number:

```{r}
1-pnorm(72,mean(heights),rafalib::popsd(heights)) 
```
:::

if we do this check for different values and continue to see that the two values are similar we can assume a normal distribution, but a better way to do it is using a QQ plot:

## QQ plot

To corroborate that the normal distribution is in fact a good approximation we can use **quantile-quantile plots (QQ-plots)**. Quantiles are best understood by considering the special case of percentiles. The p-th percentile of a list of a distribution is defined as the number $q$ that is bigger than p% of numbers. For example, the median 50-th percentile is the median. We can compute the percentiles for our list and for the normal distribution.

```{r, fig.align='center'}
#generate the percentiles
ps <- seq(0.01,0.99,0.01)
# The quantile function returns the values below which a given percentage of data falls. 
qs <- quantile(heights,ps)
#calculates the theoretical quantiles from a normal distribution with the same mean and standard deviation as the heights data. The qnorm function returns the quantiles of the normal distribution for the given probabilities (ps), mean, and standard deviation
normalqs <- qnorm(ps,mean(heights),rafalib::popsd(heights))
plot(normalqs,qs,xlab="Normal percentiles",ylab="Height percentiles")
abline(0,1) ##identity line
```

This line adds an identity line (a line with slope 1 and intercept 0) to the plot. The `abline(0, 1)` function draws a 45-degree line through the origin. This line helps to visually assess how closely the data follows a normal distribution. If the points lie close to this line, it suggests that the data is approximately normally distributed. This code is creating a Normal Quantile-Quantile (QQ) Plot. A QQ plot is used to compare the distribution of a dataset to a theoretical distribution---in this case, the normal distribution. If the heights data is normally distributed, the points in the plot will lie approximately along the identity line.

We can generate the same plot with a simplified code:

```{r, fig.align='center'}
qqnorm(heights)
qqline(heights) 
```

Data is not always normally distributed. Income is widely cited example. In these cases the average and standard deviation are not necessarily informative since one can't infer the distribution from just these two numbers. The properties described above are specific to the normal. For example, the normal distribution does not seem to be a good approximation for the direct compensation for 199 United States CEOs in the year 2000

```{r, fig.align='center', echo=FALSE}
hist(exec.pay)
```

the QQ plot would look like this:

```{r, fig.align='center', echo=FALSE}
qqnorm(exec.pay)
qqline(exec.pay)
```

## Use of standardadization in binomial probability

We know that the probability in any giving birth to have a girl is 0.49%. What is the probability of having 2 girls out of 3 births?

In binomial probability we call the outcome we are interested in a 'success' and the other outcome 'failure' in our example, having a girl is success, having a boy is failure. When the number of repetitions in our experiment is small, we can just draw the possible outcomes.

Given that each birth is independent from the others, we have these possibilities for getting the outcome we are interested:

$GGB | GBG | BGG (G=Girl, B=Boy)$

The probability would be $P= P(G)P(G)P(B)+P(G)P(B)P(G)+P(B)P(G)P(G)$

That is the same as $P= 3(PG)P(G)P(B) = 3*0.49*0.49*0.51$

```{r}
pG = 0.49
pB =1-pG
p = 3*pG*pG*pB
p
```

But when the number of experiments grows it gets complicated to just find out all the combinations by hand, so we can use the binomial coefficient to know the number of ways one can arrange k success in n experiments:

$$
\binom{n}{k} = \frac{n!}{k!(n-k)!}
$$ {#eq-binomialCoefficient}

and to calculate the probability we use the coefficient with the *binomial probability* formula. It's used to find the probability of getting exactly k successes in n independent Bernoulli trials, where each trial has a success probability p.

$$
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
$$ {#eq-binomialProbabilityFormula}

if we calculate this manually:

```{r}
# Parameters
n <- 3
k <- 2
p <- 0.49

# Calculate binomial coefficient
binom_coeff <- factorial(n) / (factorial(k) * factorial(n - k))

# Calculate probability
probability <- binom_coeff * (p^k) * ((1 - p)^(n - k))

# Print the result
print(probability)

```

but in r we would do it like this

```{r #binomialprobability}

# Calculate binomial probability
probability <- dbinom(k, n, p)
print(probability)

```

If we have more than two outcomes but we are interested in one of them only we can still use the binomial formula.

**Example** *We are playing an online game, the probability of winning a big prize is 10%, the probability of winning a small prize is 20% and the probability of not winning anything is 70%. We want to know, in 10 repetitions, what is the probability that we win two small prizes:*

-   success = win small prize

-   failure = anything else.

```{r}
# Parameters
n <- 10
k <- 2
p <- 0.20

# Calculate binomial coefficient
binom_coeff <- factorial(n) / (factorial(k) * factorial(n - k))

# Calculate probability
probability <- binom_coeff * (p^k) * ((1 - p)^(n - k))

# Print the result
print(probability)
```

*but now, what is the probability of winning **at most**, 12 small prizes in 50 repetitions?* we would have to calculate the probability of winning 12 small prizes, 11 small prizes etc. but we can use standardization instead to achieve the same goal:

To standardize data for a binomial experiment, you typically use the $z$-score formula. This formula converts the binomial distribution to a standard normal distribution. The $z$-score (@eq-zscore) formula is: $$
z=\frac{X-\mu}{\sigma}
$$ {#eq-zscore2}

-   ( $X$ ) is the number of successes.

-   ( $\mu$ ) is the mean of the binomial distribution, calculated as ( $\mu = np$ ). where $n$ is the number of repetitions and $p$ is the the probability of success.

-   ( $\sigma$ ) is the **standard deviation of the binomial distribution**, calculated as $$\sigma = \sqrt{np(1-p)}$$ {#eq-standardDeviationBinomialDistribution}

So, the complete formula for the $z$-score in a binomial experiment is:

$$
z = \frac{X - np}{\sqrt{np(1-p)}}
$$ {#eq-zscoreBinomial}

This formula allows you to standardize the number of successes ( X ) by subtracting the mean and dividing by the standard deviation, converting it to a $z$-score.

```{r}
# Parameters
n <- 50
k <- 12
p <- 0.20

sigma <- sqrt(n*p*(1-p))
mu <- n*p
zScore <- (k-mu)/sigma
print(zScore)

```

## Calculating the p-values {#pvalues}

once we have calculated the $z$-score we can calculate the area on the left of that number under the normal distribution curve, and that will be the probability we are looking for.

```{r, fig.align='center', echo=FALSE}

# Create a sequence of x values
x <- seq(-4, 4, length=100)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue", xlab="Z", ylab="Density", main="Standard Normal Distribution")
# Add a vertical line at x = 0.71
abline(v = 0.71, col = "red", lwd = 2, lty = 2)
```

We can now can find manually the probability using $z$-score tables. In R, we may use the `pnorm()` function to find the $p$-value associated with a $z$-score, which has the following syntax.

`pnorm(q, mean = 0, sd = 1, lower.tail = TRUE)`

Where\
- `q = z-Score value`. - lower.tail: If TRUE, the probability in the normal distribution to the left of $z$-score is returned. The probability to the right is returned if FALSE. TRUE is the default value.

To find the $p$-value for the two-tailed test:

`2*pnorm(q=0.71, lower.tail=FALSE)`

```{r}
# Calculate the area to the left of the z-score
(area_left <- pnorm(zScore))

# Calculate the area to the right of the z-score
(area_right <- 1 - area_left)
#or
(area_rigt<- pnorm(zScore,lower.tail=FALSE))

# Calculate the cumulative probability using the z-score, in our case we use the left area:
probability <- pnorm(zScore)
print(probability)
```

Instead of plugging in the values and calculate the $z$-score first, we can simplify our operations in r like this:

```{r}
# Parameters
n <- 50
k <- 12
p <- 0.20
# Calculate the cumulative probability using an expression
probability <- pnorm((k - n * p) / sqrt(n * p * (1 - p)))
print(probability)
```
