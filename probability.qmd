---
title: "Probability"
format: html
---

```{r}
library(ggplot2)
library(tinytex)
```

# Rules of Probability

Probabilities are defined for events. An event is some outcome that we could potentially or hypothetically observe or experience, such as the result of rolling a fair six-sided die. In mathematical notation, we often write an event as a capital letter, for example A. If A is the event of rolling a die and getting a "4", this event has probability 1/6 and we write that as $P(A) = \frac{1}{6}$ We might want to represent the numerical result of the die roll as the random variable $X$ and then we write $P(X=4)=\frac{1}{6}$

Probabilities must be between zero and one for any event A because if we add up the probabilities of all possible events, those probabilities must add to one. For the die rolling example: $$
\sum^6_{i=1}P(X=i)=1
$$ The complement of an event $A^C$ means that the event does not happen. Since probabilities must add to one, $P(A^C)=1-P(A)$

If $A$ and $B$ are two events, the probability that $A$ or $B$ happens (this is an inclusive 'or', meaning that either $A$ or $B$ or both happen) is the probability of the union of the events: $$
P(A\cup B) = P(A)+P(B)-P(A \cap B)
$$ Where $\cup$ represents union "or" and $\cap$ represents intersection "and" . If a set of events $A_i$ for $i=1,\dots,m$ are mutually exclusive (only one can happen), then$$
P(\bigcup^m_{i=m}A_i)=\sum^m_{i=1}P(A_i)
$$ Odds Probabilities can be re-expressed in terms of odds. Suppose again that we denote rolling a "4" on a fair six-sided die as the event A. Then P(A) = 1/6 The odds for event A, denoted $\mathcal{O}(A)$ is defined as $$
\mathcal{O}(A) = \frac{P(A)} {P (A ^ c)} = \frac{P(A)}{(1 - P(A))}
$$ Hence, in this example: $$
\mathcal{O}(A) = \frac{1/6} {5/6} = \frac{1}{5}
$$ This can also be expressed as 1:5 or as 5:1 "odds against". Thus, an event with probability 3/10 has 3/7 odds (7:3 "odds against") and an event with probability 4/5 has 4:1 odds.

We can also calculate probabilities from odds. If an event B has a:b odds, (with a\>0 and b\>0) then $$
\frac{P(B)}{(1-P(B))} = \frac{a}{b} \rightarrow P(B)\times b = a-P(B)\times a \rightarrow P(B)= \frac{a}{a+b}
$$ Thus, an event with 2:5 odds has probability 2/7

::: callout-orange
# Fundamental Rules of Probability

## Probability of a Specific Value

For a continuous random variable, the probability of a specific value is always 0. This is because the probability is spread over an interval, and the exact point has no width.

$$
P(X = x) = 0
$$

## Probability of an Interval

The probability that a continuous random variable falls within an interval is given by the integral of the probability density function over that interval.

$$
P(a \leq X \leq b) = \int_{a}^{b} f(x) \, dx
$$

## Probability over the Entire Range

The probability that a continuous random variable falls within its entire range is always 1. This means that the total area under the probability density function is 1.

$$
P(-\infty < X < \infty) = 1
$$

## Complement Rule

The probability of an event not occurring is 1 minus the probability of the event occurring.

$$
P(A^c) = 1 - P(A)
$$

## Addition Rule

For any two events A and B, the probability of A or B occurring is given by:

$$
P(A \cup B) = P(A) + P(B) - P(A \cap B)
$$

## Multiplication Rule

For any two independent events A and B, the probability of both A and B occurring is given by:

$$
P(A \cap B) = P(A) \times P(B)
$$

These fundamental rules provide a basis for understanding and calculating probabilities in various contexts.
:::

## Expectation

The expected value of a random variable ($X$) is described as a weighted average of the values that ($X$) can take, with weights given by the probabilities of those values.

The formula for the expected value is given as:

$$
E(X) = \sum_{i=1}^{n} x_i \cdot P(X = x_i)
$$

An example is provided for the expected value of a fair six-sided die:

$$
E(X) = \sum_{i=1}^{6} i \cdot P(X = i) = \sum_{i=1}^{6} i \cdot \frac{1}{6} = \frac{1}{6} (1 + 2 + 3 + 4 + 5 + 6) = 3.5
$$ Note that the die cannot achieve this value, but if you were to roll the die many times and average the values, the result would likely be close to 3.5

While the die cannot achieve this value in a single roll, rolling the die many times and averaging the values would likely result in a value close to 3.5.

# Bayesian vs frequentist probability

In statistics, *frequentist probability* interprets probability as the long-run frequency of an event in repeated trials, while *Bayesian probability* interprets probability as a degree of belief about an event, incorporating prior knowledge and updating that belief as new data is collected; essentially, frequentist probability focuses on the data itself, while Bayesian probability focuses on the probability of a hypothesis given the data, allowing for the inclusion of prior information.

*Frequentist*: Probability is the limit of the relative frequency of an event in an infinite number of repetitions of an experiment.Â 

*Bayesian*: Actively incorporates prior beliefs about parameters through a "prior distribution" which is updated with new data to form a "posterior distribution".

**Hypothesis testing**

*Frequentist*: Uses $p$-values to test hypotheses based on the likelihood of observing data as extreme or more extreme than what was observed, assuming the null hypothesis is true.

*Bayesian*: Calculates the probability of a hypothesis given the observed data, directly assigning probabilities to hypotheses.

# Framework of Bayesian probability

We can quantify probabilities by thinking about what is a fair bet. So for example, we want to ask *what's the probability it rains tomorrow?* Then we can ask about a bet that you might be willing to take if you think it's fair. Suppose you'd be willing to take the bet that if it rains tomorrow, you win 4\$ and if it doesn't, you lose 1\$, you can think of this as odds 4:1. This is equivalent to the other direction of the bet: if it rains you lose 4\$and if it doesn't, you win 1\$. We can see this is fair by looking at the expected return. You are defining a probability of rain $P(rain)=\frac 1 5$

We can see that these two bets are equivalent looking at your expected return:

$P(rain)=\frac 1 5$

$P(no\ rain)=\frac 4 5$

We can check if the numbers in this bet are fair by checking that the expected return in case of the two events occurring is $0$:

Bet 1: return if it rains: $4 \frac 1 5 = 0.8$ Return if it doesn't rain$1 \frac 4 5 = 0.8$

Bet 2: return if it rains: $1 \frac 4 5 = 0.8$ Return if it doesn't rain$4 \frac 1 5 = 0.8$

::: exercise-box
Consider this game:

you flip a fair coin. If the result is 'heads' you win 3\$, if the result is 'tails' you lose 4\$. What is your expected return?

3(1/2)-4(1/2) = -0.5
:::

::: exercise-box
Chess bet

Your friend offers a bet that she can beat you in a game of chess. If you win, she owes you \$5, but if she wins, you owe her \$3

Odds: 5:3 $P(win)= \frac{5}{8}$

She is 100% confident that she will beat you. What is her expected return for this game? $P(win) = 3 *\frac{1}{1} - 5\cdot 0 = 3$

Suppose she is only 50% confident that she will beat you (her personal probability of winning is $p=0.5$. What is her expected return now? $P(win) = 3 \cdot \frac{1}{2} - 5 \cdot \frac{1}{2} = -1$

Now assuming your friend will only agree to fair bets (expected return of \$0), find her personal probability that she will win. Report your answer as a simplified fraction.

To find your friend's personal probability of winning the game of chess under the condition of a fair bet (expected return of \$0), we set up the equation using the concept of expected value.

Let's denote: / - $p$ as the probability that your friend will win the game. - $1 - p$ as the probability that you will win the game.

The expected return for a fair bet should be $0$. Therefore, we set up the expected value equation as follows: $5 \times (1 - p) - 3 \times p = 0$ Now, let's solve for (p):

$5 - 5p - 3p = 0$\

$5 - 8p = 0$\

$-8p = -5$\

$p = \frac{5}{8}$\

Therefore, the probability that your friend will win is $\frac{5}{8}$.

This means your friend believes she has a $\frac{5}{8}$ chance of winning the game to agree to this fair bet.

Her expected return: 3(5/8)-5(3/8)
:::

::: exercise-box
Weather bet

Suppose your friend offers a pair of bets:

-   if it rains or is overcast tomorrow, you pay him 4\$, otherwise he pays you 6\$- if it sunny you pay him 5\$, otherwise he pays you 5\$

Suppose rain, overcast and sunny are the only events in consideration. If you make both bets simultaneously, this is called a *Dutch book* as you are guaranteed to win money. How much do you win regardless of the outcome?

-   if it rains or overcast: -4+5 =1
-   if it is sunny: 6-5 =1

Apparently your friend doesn't understand the laws of probability. Let's examine the bets he offered.

For bet (i) to be fair, his probability that it rains or is overcast must be .6 (you can verify this by calculating his expected return and setting it equal to \$0):

$4p-6(1-p) = 0$ $4p-6+6p =0$ $10p -6 =0$ $10p= 6$ $p = 6/10 =0.6$

For bet (ii) to be fair, his probability that it will be sunny must be .5:

$5p-5(1-p) = 0$ $5p-5+5p =0$ $10p =5$ $p = 5/10 =0.5$

This results in a "Dutch book" because your friend's probabilities are not coherent. They do not add up to 1. What do they add up to? $.6+0.5=1.1$
:::

## Priors

A prior in Bayesian statistics represents your initial beliefs or assumptions about a parameter before seeing any data. It incorporates any prior knowledge or expertise you might have. Priors can be expressed in various forms, such as:

Non-informative (or flat) priors: These are used when you have no strong prior beliefs. They spread the probability evenly across all possible values of the parameter.

Informative priors: These incorporate specific prior knowledge or beliefs. For example, if you have historical data or expert opinions, you can use these to inform your prior.

[^1]: These are chosen because they simplify calculations. When combined with the likelihood, the result is a posterior distribution that belongs to the same family as the prior.

[^1]: InÂ [Bayesian probability](https://en.wikipedia.org/wiki/Bayesian_probability "Bayesian probability")Â theory, if, given aÂ [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function "Likelihood function")Â p(xâˆ£Î¸), theÂ [posterior distribution](https://en.wikipedia.org/wiki/Posterior_probability "Posterior probability")Â p(Î¸âˆ£x)Â is in the sameÂ [probability distribution family](https://en.wikipedia.org/wiki/List_of_probability_distributions "List of probability distributions")Â as theÂ [prior probability distribution](https://en.wikipedia.org/wiki/Prior_probability_distribution "Prior probability distribution")Â p(Î¸), the prior and posterior are then calledÂ **conjugate distributions**Â with respect to that likelihood function and the prior is called aÂ **conjugate prior**Â for the likelihood functionÂ p(xâˆ£Î¸).

Bayesian probability adjust the probabilities based on updated information, let's see this with an example:

::: exercise-box
Estimating the Size of Atacama

The country of Chile is divided administratively into 15 regions. The size of the country is 756,096 square kilometers. How big do you think the region of Atacama is? Let's define the following events:

-   (A_1): The event that Atacama is less than 10,000 square kilometers.

-   (A_2): The event that Atacama is between 10,000 and 50,000 square kilometers.

-   (A_3): The event that Atacama is between 50,000 and 100,000 quare kilometers.

-   (A_4): The event that Atacama is more than 100,000 square kilometers.

We initially assign probabilities to these events based on the average size of Chile's regions (756096/15 = 50406):

-   (A_1): P((A_1)) = 5%

-   (A_2): P((A_2)) = 45%

-   (A_3): P((A_3)) = 45%

-   (A_4): P((A_4)) = 5%

Given that Atacama is the fourth largest region:

-   (A_1): P((A_1)) = 1%

-   (A_2): P((A_2)) = 14%

-   (A_3): P((A_3)) = 60%

-   (A_4): P((A_4)) = 25%

Given that the smallest region (Santiago Metropolitan) has an area of 15,403 square kilometers:

-   (A_1): P((A_1)) = 0%

-   (A_2): P((A_2)) = 15%

-   (A_3): P((A_3)) = 60%

-   (A_4): P((A_4)) = 25%

Given that the third largest region, AysÃ©n del General Carlos IbÃ¡Ã±ez del Campo, has an area of 108,494 square kilometers:

-   (A_1): P((A_1)) = 0%

-   (A_2): P((A_2)) = 10%

-   (A_3): P((A_3)) = 65%

-   (A_4): P((A_4)) = 25%
:::

# Conditional probability

In conditional probability we ask what is the probability of event $A$ happening, given that event $B$ happened. $P(A|B)$ (read *Probability of A given B*)

$P(A|B)$ is the probability of event $A$ occurring, given that event $B$ has already occurred. This means you are only considering the subset of cases where $B$ happens and then looking how often $A$ also happens in those cases.

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$ This, we will see later, is $f(y\mid \theta)=\frac{f(y,\theta)}{f(\theta)}$

**Join Probability** $P(A \cap B)$ is the probability that both events $A$ and $B$ occur simultaneously. This is considering the likelihood of both events happening at the same time without any additional conditions $$
P(A \cap B) = P(B) \times P(A \mid B)
$$

Let's get a better understanding of the concepts with an example

::: exercise-box
Understanding Conditional and Joint Probability

Let's consider a deck of 52 cards to understand the difference between conditional probability and joint probability.

*Definitions*

-   $P(A \mid B)$: The probability of event$A$occurring given that event$B$has already occurred.\
-   $P(A \cap B)$: The probability that both events $A$ and $B$ occur simultaneously.

We want to find the probability of drawing a red card (event $A$) given that the card is a heart (event $B$).

*Event A*: **Probability of Drawing a Red Card** ($P(A)$): $$
   P(A) = \frac{\text{Number of red cards}}{\text{Total number of cards}} = \frac{26}{52} = \frac{1}{2}
$$

*Event B*: **Probability of Drawing a Heart** ($P(B)$): $$
   P(B) = \frac{\text{Number of hearts}}{\text{Total number of cards}} = \frac{13}{52} = \frac{1}{4}
$$

*Join Probability*: **Probability of Drawing a Red Card and a Heart** ($P(A \cap B)$): Since all hearts are red, the probability of drawing a red card and a heart is the same as the probability of drawing a heart: $$
   P(A \cap B) = P(B) = \frac{13}{52} = \frac{1}{4}
$$

*Conditional Probability* **Probability of Drawing a Red Card Given that the Card is a Heart**($P(A \mid B)$): Using Bayes' Theorem: $$
   P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{13}{52}}{\frac{13}{52}} = 1
$$

*Conclusion*:

-   $P(A \mid B)$: The probability of drawing a red card given that the card is a heart is 1.\
-   $P(A \cap B)$: The probability of drawing a red card and a heart is$\frac{1}{4}$.

This example demonstrates how conditional probability considers the subset of cases where event$B$has occurred, while joint probability considers the likelihood of both events happening together within the entire sample space.
:::

## Indepence of the events

If the two events $A$ and $B$ are independent from each other, then the probability of $A$ given $B$ is just the probability of $A$. In that case the probability of the two together is $P(A\cap B) = P(A)P(B)$

Taking this to the example we just see, we can see that the probability of being a female given that they are a computer science student is not equal to the probability of being a female, which means that these two events are not independent.

# Bayes Theorem for discrete probabilities:

The general formula for conditional probability is: $$
P(A \mid B) = \frac{P(A \cap B)}{P(B)} \\
\text{ and  } \\
P(B \mid A) = \frac{P(B \cap A)}{P(A)}
$$ {#eq-ConditionalProbability}

This formula tells us how to find the probability of event $A$ occurring given that event $B$ has already occurred by dividing the joint probability $P(A \cap B)$ by the probability of $B$.

Bayes' Theorem is a specific application of conditional probability and is stated as:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B \mid A) P(A) + P(B \mid A^C) P(A^C)}=  \frac{P(B \mid A) \cdot P(A)}{P(B)} = \frac{P(A\cap B)}{P(B)}
$$ {#eq-BayesTheorem}

we can expand it to:

$$
P(A \mid B) = \frac{P(B|A)\cdot P(A)}{P(B|A)\cdot P(A) + P(B|A^c)\cdot P(A^C)}
$$ {#eq-BayesTheorem2}

In this case,$A$ and $B$ are specific events, and we are using the conditional probability of $B$ given $A$ to find the conditional probability of $A$ given $B$.

We are now just going to see how each term relates to each other to get to the equation above:

We can use the conditional probability formula to find $P(B \mid A)$ and apply it in the Bayes probability theorem.

$$
P(A \mid B) = \frac{\frac{P(B \cap A)}{P(A)} \cdot P(A)}{P(B)} \rightarrow \frac{{P(B \cap A)} }{P(B)}
$$

and $$
P(A \cap B) =  P(A \mid B) \cdot P(B) \\ \text{ and  } \\ 
P(B \cap A) =  P(B \mid A) \cdot P(A)
$$

Using Bayes' Theorem again, we have: $$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

let's practice this:

::: exercise-box
Bayesian Probability Exercise

We have a total of 30 students:

-   9 are females.
-   12 are computer science majors.
-   4 of the computer science students are females.

We want to determine the probability that a student is female given that they are a computer science major.

Let:

-   $F$ be the event that a student is female.
-   $CS$ be the event that a student is a computer science major.

We are looking for $P(F \mid CS)$, the probability that a student is female given that they are a computer science major.

*Applying Bayes' Theorem:*

Bayes' Theorem states: $$
P(F \mid CS) = \frac{P(CS \mid F) \cdot P(F)}{P(CS)}
$$

1.  **Calculate** $P(F)$:

$$
P(F) = \frac{\text{Number of females}}{\text{Total number of students}} = \frac{9}{30}
$$

2.  **Calculate** $P(CS)$:

$$
P(CS) = \frac{\text{Number of computer science majors}}{\text{Total number of students}} = \frac{12}{30}
$$

3.  **Calculate** $P(CS \mid F)$:

$$
P(CS \mid F) = \frac{\text{Number of female computer science students}}{\text{Total number of females}} = \frac{4}{9}
$$

4.  **Apply Bayes' Theorem:**

$$
P(F \mid CS) = \frac{P(CS \mid F) \cdot P(F)}{P(CS)} = \frac{\left(\frac{4}{9}\right) \cdot \left(\frac{9}{30}\right)}{\left(\frac{12}{30}\right)}
$$

$$
P(F \mid CS) = \frac{\frac{4}{30}}{\frac{12}{30}} = \frac{4}{12} = \frac{1}{3}
$$

So, the probability that a student is female given that they are a computer science major is $\frac{1}{3}$.

*Calculate the probability that a Student is a Female Given that She is Not a Computer Science Major (*$CS^C$):

1.  **Calculate** $P(CS^C)$:

$$
P(CS^C) = \frac{\text{Number of non-Computer Science majors}}{\text{Total number of students}} = \frac{18}{30}
$$

2.  **Calculate** $P(CS^C \cap F)$:

$$
P(CS^C \cap F) = \frac{\text{Number of females who are not Computer Science majors}}{\text{Total number of students}} = \frac{5}{30}
$$

3.  **Calculate** $P(F \mid CS^C)$:

$$
P(F \mid CS^C) = \frac{P(CS^C \cap F)}{P(CS^C)} = \frac{\frac{5}{30}}{\frac{18}{30}} = \frac{5}{18}
$$

So, the probability that a student is female given that they are not a computer science major is $\frac{5}{18}$.

*Conclusion*

The probability that a student is female given that they are a computer science major is $\frac{1}{3}$. The probability that a student is female given that they are not a computer science major is $\frac{5}{18}$.

Notice that the probability of being a female student and studying computer science is not the same as the probability of studying computer science and being a female:

$P(CS \mid F) = 4/9 \neq P(F \mid CS)= 1/3$
:::

Let's see this in another example

::: exercise-box
Lab Test

The probability of a test being positive given that the person has HIV is .977 The probability of a test being negative given that they don't have the virus is .926

We calculated that the probability of a USA citizen having HIV is .0026

So if we randomly select someone from this population and tested them with this test, what is the probability that they actually have HIV given that they tested positive?

-   $P(T^+ \mid H) = 0.977$
-   $P(T^- \mid H^C) = 0.926$
-   $P(H) = 0.0026$

Bayes' Theorem: $$
P(H \mid T^+) = \frac{P(T^+ \mid H) \cdot P(H)}{P(T^+)}
$$

**Probability of testing positive** The probability of testing positive is the probability of testing positive given that they have HIV plus the probability of testing positive given that they don't have HIV

$$
P(T^+) = P(T^+ \mid H) \cdot P(H) + P(T^+ \mid H^C) \cdot P(H^C)
$$

a.  **Calculate the probability of not having HIV:**$$
    P(H^C) = 1 - P(H) = 1 - 0.0026 = 0.9974
    $$
b.  **Calculate the probability of testing positive given not having HIV:**$$
    P(T^+ \mid H^C) = 1 - P(T^- \mid H^C) = 1 - 0.926 = 0.074
    $$

$$
P(T^+) = (0.977 \cdot 0.0026) + (0.074 \cdot 0.9974) \rightarrow 0.0025402 + 0.0738076 = 0.0763478
$$

**Substitute the values in Bayes Theorem:** $$
P(H \mid T^+) = \frac{0.977 \cdot 0.0026}{0.0763478} \rightarrow \frac{0.0025402}{0.0763478} \approx 0.0333
$$ Therefore, the probability that a person has HIV given that they tested positive is approximately 0.0333.
:::

# Bayes' Theorem with Multiple Outcomes

For two outcomes we have seen: $$
P(A \mid B) = \frac{P(B|A)\cdot P(A)}{P(B|A)\cdot P(A) + P(B|A^c)\cdot P(A^C)}
$$

When there are more than two possible outcomes, Bayes' Theorem expands to handle the multiple outcomes. For example, when there are three possible outcomes $A_1$, $A_2$, and $A_3$, such that exactly one of these must happen, Bayes' Theorem is as follows:

$$
P(A_1 \mid B) = \frac{P(B \mid A_1) \cdot P(A_1)}{P(B \mid A_1) \cdot P(A_1) + P(B \mid A_2) \cdot P(A_2^C) + P(B \mid A_3) \cdot P(A_3^C)}
$$

## Partition of the Space

If the events $A_1, A_2, \ldots, A_m$ form a partition of the space (exactly one of the $A_i$'s must occur, i.e., the $A_i$'s are mutually exclusive and: $$
\sum_{i=1}^{m} P(A_i) = 1
$$

Then, Bayes' Theorem can be written as: $$
P(A_1 \mid B) = \frac{P(B \mid A_1) \cdot P(A_1)}{\sum_{i=1}^{m} P(B \mid A_i) \cdot P(A_i)}
$$

For continuous distributions, the sum in Bayes' Theorem is replaced with an integral. We will explore this in the next lesson.

::: exercise-box
Practice exercise: Titanic

Refer to the following table regarding passengers of the famous Titanic, which tragically sank on its maiden voyage in 1912. The table organizes passenger/crew counts according to the class of their ticket and whether they survived.

```{r}
titanic_data <- data.frame(
  Class = c("First", "Second", "Third", "Crew"),
  Survived = c(203, 118, 178, 212),
  Not_Survived = c(122, 167, 528, 673)
)

print(titanic_data)

```

If we randomly select a person's name from the complete list of passengers and crew, what is the probability that this person traveled in 1st class?

```{r}
passengers <- sum(titanic_data$Survived)+sum(titanic_data$Not_Survived)
firstClass <- sum(titanic_data$Survived[titanic_data$Class=='First' ])+sum(titanic_data$Not_Survived[titanic_data$Class=='First' ])

round(firstClass/passengers,2)

```

What is the probability that a (randomly selected) person survived?

```{r}
survived<- sum(titanic_data$Survived)
round(survived/passengers,2)
```

What is the probability that a (randomly selected) person survived, given that they were in 1st class? Round your answer to two decimal places.

```{r}
round(titanic_data$Survived[titanic_data$Class =='First']/firstClass,2)
```
:::

::: exercise-box
Practice exercise: Marvels

You have three bags, labeled A, B, and C. - Bag A contains two red marbles and three blue marbles. - Bag B contains five red marbles and one blue marble. - Bag C contains three red marbles only.

```{r}
# Creating a dataframe for the marbles in the three bags
marbles_data <- data.frame(
  Bag = c("A", "B", "C"),
  Red_Marbles = c(2, 5, 3),
  Blue_Marbles = c(3, 1, 0)
)

# Display the dataframe
print(marbles_data)

```

If you select from bag B, what is the probability that you will draw a red marble? Express the exact answer as a simplified fraction.

```{r}
round(5/6,2)
```

If you randomly select one of the three bags with equal probability (so that P(A)=P(B)=P(C)=1/3) and then randomly draw a marble from that bag, what is the probability that the marble will be blue? Round your answer to two decimal places.

This is the marginal probability P(blue). You can obtain this using the law of total probability (which appears in the denominator in Bayes' theorem) $$
P(\text{blue}) = P(\text{blue} \cap A) + P(\text{blue} \cap B) + P(\text{blue} \cap C)
$$ Using conditional probabilities, we can write this as:

$$
= P(\text{blue} \mid A) \cdot P(A) + P(\text{blue} \mid B) \cdot P(B) + P(\text{blue} \mid C) \cdot P(C)
$$

```{r}
pbag = 1/3
PblueA <- 3/5
PblueB <- 1/6
pblueC <- 0

pblue <- PblueA * pbag + PblueB *pbag +pblueC *pbag
round(pblue,2)
```

Suppose a bag is randomly selected (again, with equal probability), but you do not know which it is. You randomly draw a marble and observe that it is blue. What is the probability that the bag you selected this marble from is A? That is, find P(Aâˆ£blue) $$
P(A|blue) = \frac{P(Blue|A)\cdot P(A)}{P(blue)}
$$

```{r}
pABlue <- (PblueA *1/3)/pblue
round(pABlue,2)
```

Suppose a bag is randomly selected (again, with equal probability), but you do not know which it is. You randomly draw a marble and observe that it is blue. What is the probability that the bag you selected from is C? That is, find P(Câˆ£blue).

0

Suppose a bag is randomly selected (again, with equal probability), but you do not know which it is. You randomly draw a marble and observe that it is red. What is the probability that the bag you selected from is C? That is, find P(Câˆ£red).

$$
P(c|red) = \frac{P(red|c)\cdot P(c)}{P(red)}
$$

```{r}
PredC = 1
Pred = 1-pblue

PcRed = (1*(1/3))/Pred

round(PcRed,2)
```
:::

# Bernoulli distribution

Bernoulli distribution is used when we have two possible outcomes, such as flipping a coin. We denote this like $X \sim B(p)$ as $X$ follows a Bernoulli distribution with probability: $P(X=1)=p$ $P(x=0) = 1-p$

We can write this as a function $f(X=x\mid p) \rightarrow f(x \mid p)$ where the capital letters refer to the variable, while the lowercase letters refer to the outcome.

For Bernoulli this is: $$
f(x \mid p)= p^x(1-p)^{1-x}
$$

# Binomial distribution

Binomial Distribution extends the Bernoulli distribution to multiple trials. It represents the number of successes in $n$ independent Bernoulli trials, each with the same probability of success $p$. The probability of observing exactly $k$ successes in $n$ trials is given by the binomial probability formula:

The probability formula for a binomial distribution is given by:

$$
P(X = k \mid n, p) = \binom{n}{k} p^k (1-p)^{n-k}
$$

where: - $P(X = k \mid n, p)$ is the probability of observing exactly $k$ successes in $n$ trials, - $\binom{n}{k}$ is the binomial coefficient, representing the number of ways to choose $k$ successes out of $n$ trials, - $p$ is the probability of success in a single trial, - $(1-p)$ is the probability of failure in a single trial.

::: callout-orange
Indicator function

The concept of an indicator function is a really useful one. This is a function that takes the value one if its argument is true, and the value zero if its argument is false. I write an indicator function as $I\{A\}(x)$, although sometimes they are written $1_{\{A\}}(x)$. If the context is obvious, we can also simply write $I\{A\}$.

The indicator function $I_A(x)$ of a subset $A$ of a set $X$i s defined as:

$$
I_A(x) = \begin{cases} 
1 & \text{if } x \in A \\
0 & \text{if } x \notin A 
\end{cases}
$$

For example, given$X = \{1, 2, 3, 4, 5\}$ and $A = \{2, 4\}$, the indicator function$I_A$is:

$$
I_A(1) = 0, \quad I_A(2) = 1, \quad I_A(3) = 0, \quad I_A(4) = 1, \quad I_A(5) = 0
$$Indicator functions are useful for making sure that you don't take the log of a negative number, and things like that. Indicator functions are always first in the order of operations if the indicator function is zero, you don't try to evaluate the rest of the expression. When taking derivatives they just go along for the ride. When taking integrals, they may affect the range over which the integral is evaluated.
:::

The indicator function for our Bernoulli distribution can be expressed as: $I_{\{x \in (0,1) \}^x}$

## Expected Value

The Expected Value is the theoretical average of the outcomes. For our binary outcome with possible outcomes 0 and 1 it will be:

$$
E[X] = \sum_x x \times P(X=x) \rightarrow (1)p+(0)p = p
$$ {#eq-BernoulliExpectedValue}

## Variance

Variance is the square of the standard deviation. For Bernoulli:$$
\sigma^2 = p \times (1-p)
$$ {#eq-BernoulliVariance}

## Repeated experiments in binomial distribution

The generalization of the Bernoulli when we have $N$ repeated trials follows a binomial distribution with parameters $n$ and $p$. In this case, the probability function is

$$
f(x\mid p) = \binom{n}{x} p^x (1-p)^{n-x}
$$

$$
\binom{n}{x}= \frac{n!}{x!(n-x)!}
$$

$binom{4}{3}$ for example reads "four choose three" and this counts all possible Bernoulli sequences that result in three successes out of four trials

```{r}
n <- 4
x <- 3

sol <- factorial(n)/(factorial(x) * factorial(n-x))
sol
```

in `R` we can simplify this calculus using a handy formula:

```{r}
n <- 4
x <- 3

sol <- choose(n, x)
print(sol)

```

The expected value is:$$
E[X] = n\times p
$$

And the Variance:$$
\sigma^2 = n\times p(1-p)
$$

::: exercise-box
Expected value

Calculate the expected value of the following random variable: X takes on values {0,1,2,3} with corresponding probabilities {0.5,0.2,0.2,0.1}.

```{r}
# Values and corresponding probabilities
values <- c(0, 1, 2, 3)
probabilities <- c(0.5, 0.2, 0.2, 0.1)

# Calculate the expected value
E <- sum(values * probabilities)
print(round(E, 1))  

```
:::

::: exercise-box
Exercise 2

Imagine you have a coin that lands on heads (success) with a probability of 0.2. You flip the coin 3 times, and you want to find the probability of getting 0 heads (0 successes) in those 3 coin flips.

Given a random variable $X \sim \binom {3}{ 0.2}$, we want to calculate$P(X = 0)$.

The probability mass function for a binomial random variable $X$ with parameters $n$ and $p$ is given by: $P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}$

$P(X = 0) = \binom{3}{0} (0.2)^0 (1 - 0.2)^{3 - 0}$

$P(X = 0) = \binom{3}{0} \cdot 1 \cdot (0.8)^3$

$P(X = 0) = 1 \cdot (0.8)^3$

$P(X = 0) = (0.8)^3$

$P(X = 0) = 0.512$

```{r}
# Parameters
n <- 3
p <- 0.2
x <- 0

# Calculate the binomial probability
prob <- dbinom(x, n, p)
round(prob, 2)
```

Now calculate the probability that the number of successes is less or equal than 2.

$$
P(X \le 2)
$$This is 1 minus the probability of getting 3 successes.

```{r}
# Parameters
n <- 3
p <- 0.2
x <- 3

# Calculate the binomial probability
prob <- 1-dbinom(x, n, p)
round(prob, 2)
```

or you can also calculate it as P(X=0)+P(X=1)+P(X=2)
:::

# Continuous Distributions

## Probability Density Function (PDF)

The **Probability Density Function (PDF)** is a function that describes the likelihood of a continuous random variable taking on a particular value. The PDF, denoted as \$ f(x) \$, represents the density of the probability distribution at each point \$ x \$. For a random variable \$ X \$ with a PDF \$ f(x) \$, the probability that \$ X \$ falls within a specific interval $[a, b]$ is given by the integral of \$ f(x) \$ over that interval:

$$P(a \leq X \leq b) = \int_{a}^{b} f(x) \, dx$$ A few important properties of the PDF:

1.  $f(x) \geq 0$ for all $x$.
2.  The area under the entire curve of the PDF is equal to 1:

$$\int_{-\infty}^{\infty} f(x) \, dx = 1$$

The PDF provides a way to understand the distribution and density of probabilities across different values of a continuous random variable.

## Uniform distribution.

The uniform distribution is used for random variables whose possible values are equally likely over an interval.

We can define a random continue variable $X$ with its *Probability Density Function* (PDF). If you integrate the PDF over an interval, it will give you the probability that the random variable will be in that interval.

If the interval is $(a,b)$, then the uniform probability density function (PDF) f(x) is flat for all values in the interval and 0 everywhere else. $$
f(X\mid a,b)= \frac{1}{b-a} I_{\{a\leq x \leq b\}}(x)
$$

We have a uniform distribution that can take any value between 0 and 1 $X \sim U[0,1]$ or using the indicator function\>$$
f(x) = \begin{cases} 
1 & \text{if } x \in [0,1] \\
0 & \text{alternative } 
\end{cases}
$$

```{r, fig.align='center'}
# Define the uniform distribution
x <- seq(0, 1, by = 0.01)
y <- dunif(x, min = 0, max = 1)

# Create the PDF plot
pdf_plot <- data.frame(x, y)

# Plot using ggplot2
ggplot(pdf_plot, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  ggtitle("PDF of Uniform Distribution (0, 1)") +
  xlab("x") +
  ylab("Density") +
  theme_minimal()
```

What's the probability that $x$ will be between 0 and 0.5?

The probability that $x$ will be between 0 and 0.5 for a uniform distribution $U(0,1)$ is given by:

$$
P(0 \leq x \leq 0.5) = \int_{0}^{0.5} f(x) \, dx =\int_{0}^{0.5} 1 \, dx = [x]_{0}^{0.5} = 0.5 - 0 = 0.5
$$

Therefore, the probability is 0.5. Let's calculate it using R:

```{r}
# Define the probability density function for the uniform distribution
pdf_uniform <- function(x) {
  ifelse(x >= 0 & x <= 1, 1, 0)
}

# Integrate the PDF from 0 to 0.5
prob <- integrate(pdf_uniform, lower = 0, upper = 0.5)$value
print(prob)

```

```{r}
# Define the probability density function for the uniform distribution
pdf_uniform <- function(x) {
  ifelse(x >= 2 & x <= 6, 1, 0)
}

# Integrate the PDF from 0 to 0.5
prob <- integrate(pdf_uniform, lower = 2, upper = 3)$value
print(prob)

```

or in `r` we can simply:

```{r}
# Calculate the probability that x is between 0 and 0.5
prob <- punif(0.5, min = 0, max = 1) - punif(0, min = 0, max = 1)
print(prob)
```

What is the probability that $x$ is exactly 0.5, and mathematically, by integrating from 0.5 to 0.5 we will get 0

### Expected value

The formula for the expected value (mean) of a continuous random variable is given by:

$$
E(X) = \int_{-\infty}^{\infty} x \cdot f(x) \, dx
$$ {#eq-ExpectedVAlueContinousRandomVariable}

In this formula: - $E(X)$ represents the expected value of the continuous random variable $X$. - $f(x)$ is the probability density function (PDF) of $X$. - The integral is taken over the entire range of $X$, from $-\infty$ to $\infty$.

For a continuous random variable $X$ uniformly distributed over the interval $[a, b]$, the probability density function (PDF) is given by:

$$
f(x) =
\begin{cases} 
\frac{1}{b-a} & \text{for } a \le x \le b \\
0 & \text{otherwise}
\end{cases}
$$

The general formula for the expected value of a continuous random variable is:

$$
E(X) = \int_{-\infty}^{\infty} x \cdot f(x) \, dx
$$

Given $f(x)$ for the uniform distribution, we apply it to the general expected value formula:

$$
E(X) = \int_{-\infty}^{\infty} x \cdot f(x) \, dx = \int_{a}^{b} x \cdot \frac{1}{b-a} \, dx
$$

Since $f(x) = \frac{1}{b-a}$ only over the interval $[a, b]$, the integration limits change accordingly. Evaluating this integral:

$$
E(X) = \frac{1}{b-a} \int_{a}^{b} x \, dx
$$

The integral of $x$ from $a$ to $b$ is:

$$
\int_{a}^{b} x \, dx = \left. \frac{x^2}{2} \right|_{a}^{b} = \frac{b^2}{2} - \frac{a^2}{2} = \frac{b^2 - a^2}{2}
$$

Substituting back, we get:

$$
E(X) = \frac{1}{b-a} \cdot \frac{b^2 - a^2}{2}
$$

This simplifies to

::: important-formula
$$
E(X) = \frac{b + a}{2}
$$ {#eq-ExpectedValueUniformDistribution}
:::

Hence, the formula $E(X) = \frac{a+b}{2}$ is derived from the integral expression for the expected value of a uniform distribution. Both expressions are equivalent; the integral formulation is the general definition, while the latter is the specific result for the uniform distribution.

### General Uniform Distribution.

The above formulas work for a uniform distribution on the interval \[0,1\] When we generalize the uniform distribution to any interval

Uniform : $X \sim [\theta_1, \theta_2]$ the PDF for this distribution changes accordingly to ensure that the total probability across this interval is 1: $$
f(x\mid \theta_1,\theta_2) = \frac{1}{\theta_2-\theta_1}I_{\{ \theta_1 \leq x \leq \theta_2\}}
$$ {#eq-uniformProbabilityDensityFunction}

$$
f(x) = \begin{cases} 
\frac{1}{\theta_2-\theta_1} & \text{if } x \in [\theta_1,\theta_2] \\
0 & \text{otherwise } 
\end{cases}
$$ this formula generalize the concept of the uniform distribution to any interval.

Example: Consider a uniform distribution on the interval \[2,5\]

$$
f(x\mid 2,5 ) = \begin{cases} 
\frac{1}{5-2} = \frac{1}{3}& \text{if } x \in [2,5] \\
0 & \text{otherwise } 
\end{cases}
$$

```{r}

# Function to plot uniform distribution
plot_uniform_distribution <- function(theta1, theta2) {
  # Create a sequence of x values
  x <- seq(theta1 - 1, theta2 + 1, by = 0.01)
  
  # Calculate the PDF values
  y <- ifelse(x >= theta1 & x <= theta2, 1 / (theta2 - theta1), 0)
  
  # Create a data frame for plotting
  data <- data.frame(x, y)
  
  # Plot the uniform distribution
  ggplot(data, aes(x = x, y = y)) +
    geom_line() +
    geom_area(fill = "lightblue", alpha = 0.5) +
    labs(title = paste("Uniform Distribution U[", theta1, ",", theta2, "]", sep = ""),
         x = "x", y = "Density") +
    theme_minimal()
}

# Plot the uniform distribution U[2, 5]
plot_uniform_distribution(2, 5)
```

then to calculate in `R` the probability of $X$ being between two values:

```{r}
# Define the limits of the uniform distribution
theta1 <- 2
theta2 <- 5

# Define the PDF for the uniform distribution U[2,6]
pdf_uniform <- function(x) {
  ifelse(x >= theta1 & x <= theta2, 1 / (theta2 - theta1), 0)
}

# Integrate the PDF from 2 to 3
prob <- integrate(pdf_uniform, lower = 2, upper = 3)$value
print(prob)

```

### Variance of uniform Distribution

Variance $\text{Var}(X)$ The variance is calculated as:

$$
\text{Var}(X) = E(X^2) - [E(X)]^2 = \frac{b^2 + ab + a^2}{3} - \left(\frac{a + b}{2}\right)^2
$$

Simplifying this further, we arrive at:

::: important-formula
$$
\sigma^2= \frac{(b-a)^2}{12}
$$ {#eq-VarianceUniformDistribution}
:::

## Exponential Distribution

The exponential distribution is often used to model the waiting time between random events. Indeed, if the waiting times between successive events are independent from an $Exp(\lambda)$ distribution, then for any fixed time window of length $t$, the number of events occurring in that window will follow a *Poisson distribution* with mean $t\lambda$. Similar to the Poisson distribution, the parameter $\lambda$ is interpreted as the rate at which the events occur.

If $X$ follows an exponential distribution with a rate parameter: $X \sim Exp(\lambda)$

The **probability density function** (PDF) for the exponential distribution can be written as

$$
f(x\mid \lambda) = \lambda e^{-\lambda x} I_{\{x \geq 0\}}(x)
$$ {#eq-CDFExponential}

$$
f(x \mid \lambda )= \lambda e^{-\lambda x}
$$or:

$$ f(x \mid \lambda) = 
\begin{cases} 
\lambda e^{-\lambda x} & \text{for } x \ge 0 \\
0 & \text{for } x < 0 
\end{cases}
$$

::: {callout-grey}
We can integrate the density and show that it is indeed a proper density and integrates to one:

$$int_{0}^{\infty} \lambda e^{-\lambda x} I\{x \ge 0\}(x) dx = \int_{0}^{\infty} \lambda e^{-\lambda x} dx = -e^{-\lambda x} \bigg|_0^{\infty} = 0 - (-1) = 1$$

The derivative of the density function would be:

$$frac{d}{dx} \left( \lambda e^{-\lambda x} I\{x \geq 0\}(x) \right) = -\lambda^2 e^{-\lambda x} I\{x \geq 0\}(x)$$
:::

And the **Expected value** is:$$
E[X]= \frac{1}{\lambda}
$$

**The variance**:$$
\sigma^2 = \frac{1}{\lambda^2}
$$

and the **standard deviation**: Standard Deviation\*\*: $\sigma = \frac{1}{\lambda}$

## Normal distribution

If variable $X$ follows a normal distribution defined by its mean $\mu$ and its variance $\sigma^2$: $X \sim N(\mu, \sigma^2)$

then the *probability density function* (PDF) comes defined as:

$$
f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$ {#eq-normalDistributionProbabilityDensityFunction}

The expected value will be $\mu$

### Examples

1.  **Lifespan of a Light Bulb**: If the lifespan of a light bulb follows an exponential distribution with a rate parameter $\lambda = 0.1$ (i.e., the average lifespan is 10 hours), then the density function is $f(x) = 0.1 e^{-0.1x}$.

2.  **Customer Arrival Times**: If customers arrive at a bank at an average rate of 3 customers per hour (i.e., $\lambda = 3$), then the time between arrivals follows an exponential distribution with a density function $f(x) = 3 e^{-3x}$.

Here's an example of how you can generate and visualize an exponential distribution in R:

```{r, fig.align='center'}
# Set the rate parameter
lambda <- 2

# Generate a sample of 1000 random values from the exponential distribution
set.seed(123) 
sample <- rexp(1000, rate = lambda)

# Plot the histogram of the sample
hist(sample, breaks = 30, probability = TRUE,
     main = "Histogram of Exponential Distribution",
     xlab = "Value", ylab = "Density")

# Add the theoretical density curve
curve(lambda * exp(-lambda * x), col = "red", add = TRUE)

# Mean and variance of the sample
sample_mean <- mean(sample)
sample_variance <- var(sample)

cat("Sample Mean:", sample_mean, "\n")
cat("Sample Variance:", sample_variance, "\n")
```

## Beta Distribution

The Beta distribution is a continuous probability distribution defined on the interval \[0, 1\]. It's characterized by two shape parameters, $\alpha$ and $\beta$, which determine the shape of the distribution. The probability density function (PDF) of the Beta distribution is given by:

$$
\text{Beta}(\theta; \alpha, \beta) = \frac{\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}{B(\alpha, \beta)}
$$

Where $B(\alpha, \beta)$ is a normalization constant, specifically the Beta function.

### Real-Life Examples

1.  **Proportion of Successes:**

    Suppose you are interested in modeling the probability of success for a new drug. Before conducting trials, you have some prior belief about the drug's success rate. As you collect data from trials, you update this belief. The Beta distribution is a natural choice for representing your belief about the success rate because it's defined on the interval \[0, 1\] and can be updated with new data easily.

2.  **Quality Control:**

    In manufacturing, you might use the Beta distribution to model the proportion of defective items in a batch. Based on prior knowledge and observed data (e.g., inspections), you can use the Beta distribution to update your estimate of the defect rate.

3.  **User Behavior:**

    In web analytics, the Beta distribution can model the probability of a user clicking on an ad. Based on past click data, you can update your belief about the click-through rate using the Beta distribution.

### Plotting Beta Distribution in R

Here's some R code that demonstrates how to plot the Beta distribution for different values of $\alpha$ and $\beta$:

```{r, fig.align='center', fig.width=5}

# Define a function to plot the Beta distribution
plot_beta <- function(alpha, beta) {
  # Create a sequence of theta values
  theta <- seq(0, 1, length.out = 100)
  
  # Calculate the Beta density values
  density <- dbeta(theta, alpha, beta)
  
  # Create a data frame for plotting
  df <- data.frame(theta, density)
  
  # Plot the Beta distribution using ggplot2
  ggplot(df, aes(x = theta, y = density)) +
    geom_line(color = "blue") +
    labs(title = paste("Beta Distribution (Î± =", alpha, ", Î² =", beta, ")"),
         x = "Î¸",
         y = "Density")
}

# Example: Plotting Beta distribution for different values of alpha and beta
par(mfrow = c(2, 2))
plot_beta(2, 2)  # Symmetric distribution
plot_beta(5, 2)  # Skewed distribution
plot_beta(2, 5)  # Another skewed distribution
plot_beta(1, 1)  # Uniform distribution

```

In Bayesian inference, the beta distribution is the *conjugate prior probability distribution* for the Bernoulli, binomial, negative binomial, and geometric distributions.

# Likelihood function

The likelihood function is a fundamental concept in statistics, particularly in the context of parameter estimation. It represents the probability of observing the given data as a function of the parameters of a statistical model.

While both the likelihood function and probability deal with the concept of uncertainty, they serve different purposes and have distinct interpretations.

*Probability* is used to describe how likely an event is to occur, given a fixed set of parameters or conditions. For example, if we toss a fair coin, the probability of getting heads is 0.5. Here, the probability is calculated based on the known parameters (the fairness of the coin).

*Likelihood function*, on the other hand, is used primarily in statistical inference to estimate parameters of a statistical model. It measures how well a given set of parameters explains the observed data. In other words, instead of calculating the probability of an event based on known parameters, the likelihood function evaluates how likely the observed data is, given different possible values of the parameters. For example, if we observe a series of coin tosses, the likelihood function helps us estimate the fairness of the coin based on the observed data.

To sum it up:

-   Probability: Fixed parameters, calculates the likelihood of an event.

-   Likelihood function: Fixed observed data, evaluates how well different parameter values explain the data.

More formally, if we have a set of observed data points $x_1, x_2, \ldots, x_n$ and a statistical model parameterized by the $\theta$. The likelihood function $L(\theta)$ is defined as:

$$
L(\theta) = P(X=x_1, X= x_2, \cdots, X=x_n \mid \theta)
$$ {#eq-likelihodFunction} where $X$ represents the random variable, and $P$ denotes the probability.

In the context of the likelihood function, ðœƒ represents the parameter or set of parameters that we want to estimate based on the observed data. The likelihood function ð¿(ðœƒ) quantifies how likely it is to observe the given data for different values of ðœƒ.

The likelihood function does have a general formula, but its specific form depends on the statistical model and the data. In general, the likelihood function is given by:

$$
L(\theta) = P(Data|\theta)
$$

where: - $L(\theta)$ is the likelihood function. - $\theta$ represents the parameters of the model. - $Data$ represents the observed data. - $P(Data|\theta)$ is the probability of observing the data given the parameters $\theta$.

For example, if you have a set of independent and identically distributed (i.i.d.) observations $x_1, x_2, ..., x_n$ and a probability distribution with parameters $\theta$, the likelihood function is often expressed as the product of the individual probabilities:

$$
L(\theta) = \prod_{i=1}^{n} P(x_i|\theta)
$$

In the case of a normal distribution with mean $\mu$ and standard deviation $\sigma$, the likelihood function for a set of observations $x_1, x_2, ..., x_n$ would be:

$$
L(\mu, \sigma) = \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left( -\frac{(x_i - \mu)^2}{2\sigma^2} \right)
$$

So while the general concept of the likelihood function is consistent, its specific formula does depend on the data and the statistical model you're using.

## Maximum Likelihood Estimation

The likelihood function is used to estimate the parameters $\theta$ that are most likely to have generated the observed data. This is done through the method of *Maximum Likelihood Estimation* (MLE), which finds the parameter values that maximize the likelihood function.

Theta ($\theta$) is the value of the probability that has the highest likelihood for the data we have observed.

::: exercise-box
Likelihood calculation (binomial distribution)

Let's consider an example where we are trying to infer the death rate of patients admitted to a specific hospital for heart attacks. Suppose we have the following data:

-   Number of patients admitted for heart attacks: 400

-   Number of patients who died: 72

We want to infer the death rate $p$ for the whole region based on this data.

In this example, we are dealing with a binomial distribution, where each patient admitted for a heart attack can either survive or die. The probability of a patient dying is denoted by $p$, and the probability of a patient surviving is $1-p$.

In a binomial distribution, the probability of observing exactly $k$ successes (deaths, in this case) out of $n$ trials (patients) is given by the binomial probability formula: $$P(X = k \mid n, p) = \binom{n}{k} p^k (1-p)^{n-k}$$

In this case, the likelihood function can be expressed as:

$$
L(p) = P(X=72 \mid n=400, p) = \binom {400}{72} p^{72}(1-p)^{328}
$$ The binomial coefficient $\binom{400}{72}$ represents the number of ways to choose 72 out of 100 patients.

To find the *Maximum Likelihood Estimate* (MLE) of $p$, we need to maximize the likelihood function $L(p)$. This is typically done by taking the derivative of the log-likelihood function with respect to $p$, setting it to zero, and solving for $p$.

```{r, fig.align='center'}
#| layout-ncol: 2
# Data
n <- 400
x <- 72

# Define the likelihood function
likelihood <- function(p) {
  choose(n, x) * p^x * (1 - p)^(n - x)
}

# Generate a sequence of p values for plotting
p <- seq(0, 1, by = 0.01)
likelihood_values <- sapply(p, likelihood)

# Find the MLE by maximizing the likelihood function
mle <- p[which.max(likelihood_values)]

# Plot the likelihood function
data <- data.frame(
  p = p,
  Likelihood = likelihood_values
)

ggplot(data, aes(x = p, y = Likelihood)) +
  geom_line() +
  geom_vline(xintercept = mle, linetype = "dashed", color = "red") +
  labs(title = "Likelihood Function",
       x = "Survival Rate (p)",
       y = "Likelihood") +
  annotate("text", x = mle, y = max(likelihood_values), label = paste("MLE: p =", round(mle, 2)), vjust = 2) +
  theme_minimal()


#define the logLikelihood function
loglike = function(p){
  x*log(p)+(n-x)*log(1-p)
}

plot(p,loglike(p), main = "log-likelihood plot")
abline(v= mle)
```

Here's what the MLE tells us in this case:

*Observed Data*: We have data from a specific hospital where 400 patients were admitted for heart attacks, and 72 of them survived.

*Parameter of Interest*: The survival rate ð‘ for the whole region.

*Likelihood Function*: The likelihood function represents how likely it is to observe 72 survivors out of 400 patients given different values of ð‘.

MLE: The value of ð‘ that maximizes the likelihood function is the MLE. In this case, the MLE of ð‘ is 0.18, which means that the estimated survival rate for the whole region based on this data is 18%.

So, the MLE provides the most likely value of the survival rate given the observed data.
:::

::: exercise-box
Is it a fair coin?

*We know our brother has a loaded coin, that falls heads 75% of the times. He makes a bet with us that the coin is going to fall heads and tells us that he is not using the loaded coin. We don't trust him so we agree that he will flip the coin five times to prove that is not loaded. It falls 2 heads and 3 tails. With this information you need to decide if it is the loaded coin or a fair coin and how confident you are so you can decide how much money you want to bet.*

We start by defining our parameter $\theta$ to decide if it is a fair coin$$
\theta = \{ \text fair, loaded\}
$$Our data is a binomial of 5 trials $X \sim \text binomial (5,p)$. So we can write our likelihood:

$$
f(x \mid \theta) = \begin{cases}  \binom{5}{x} (\frac{1}{2})^5 \text { if fair} \\
\binom{5}{x} (0.7)^x (0.3)^{5-x} \text  { if loaded}
\end{cases}
$$ we can write the same using indicator functions: $$
f(x \mid \theta) = \binom{5}{x} \left[ \left(\frac{1}{2}\right)^5 I_{\{\theta = \text{fair}\}} + (0.7)^x (0.3)^{5-x} I_{\{\theta = \text{loaded}\}} \right]
$$ In our trials we have observed $x=2$ what is our likelihood?

For $\theta = \text{fair}$:

$$
f(x=2 \mid \text{fair}) = \binom{5}{2} \left(\frac{1}{2}\right)^5 = 10 \cdot \left(\frac{1}{32}\right) = \frac{10}{32} = \frac{5}{16} = 0.3125
$$

For $\theta = \text{loaded}$:

$$
f(x=2 \mid \text{loaded}) = \binom{5}{2} (0.7)^2 (0.3)^3 = 10 \cdot 0.49 \cdot 0.027 = 10 \cdot 0.01323 = 0.1323
$$

```{r}
# Define the binomial coefficient function
choose <- function(n, x) {
  factorial(n) / (factorial(x) * factorial(n - x))
}

# Values for x and n
x <- 2
n <- 5

# Case 1: Fair
prob_fair <- choose(n, x) * (1/2)^5
cat("f(2 | fair) =", prob_fair, "\n")

# Case 2: Loaded
prob_loaded <- choose(n, x) * (0.7)^x * (0.3)^(n-x)
cat("f(2 | loaded) =", prob_loaded, "\n")

```

giving this data, it is most likely that the coin is fair.

To know how sure are we of this result, we need to do more calculations.

You need to ask yourself, what is the probability that the coin is fair, given that we have observed two heads $P(\theta = fair \mid x=2)$

**Using the frequentist approach**

-   Null hypothesis is that the coin is fair.\
-   Alternative hypothesis is that the coin is loaded.

We can use a binomial test, which is suitable for small sample sizes.

The number of heads observed (ð‘¥=2) out of 5 flips (ð‘›=5) can be tested against the expected number of heads for both hypotheses. Under the null hypothesis (ð»0), the probability of getting 2 heads in 5 flips with a fair coin is 0.31 as we saw already:

$$
f(x=2 \mid \text{fair}) = \binom{5}{2} \left(\frac{1}{2}\right)^5 = 10 \cdot \left(\frac{1}{32}\right) = \frac{10}{32} = \frac{5}{16} = 0.3125
$$ We then compare this to the observed frequency of 2 heads. Typically, we use a significance level (ð›¼), such as 0.05. If the $p$-value is less than or equal to ð›¼, we reject the null hypothesis in favor of the alternative hypothesis.

```{r}
# Perform a binomial test
result <- binom.test(2, 5, p = 0.5, alternative = "two.sided")

# Print the result
print(result)

```

*p-value*: The $p$-value of 1 suggests that there is no evidence to reject the null hypothesis. This means the observed result (2 heads out of 5) is very consistent with what we would expect if the coin were fair (with a probability of 0.5 for heads).

*Confidence Interval*: The 95% confidence interval for the true probability of success (getting heads) ranges from approximately 0.053 to 0.853. **This wide interval indicates a high level of uncertainty due to the small sample size (only 5 flips).**

*Sample Estimate*: The estimated probability of success (getting heads) from the observed data is 0.4. This is simply the proportion of heads observed (2 heads out of 5 flips).

This does not really gives us a lot of useful information.

**Now we can use Bayes to calculate posterior probability.**: *Calculate the posterior probability that the coin is fair given the observed data.*

*Assume we don't have prior information and both coins have equal prior probability, ð‘ƒ(fair)=ð‘ƒ(loaded)=0.5.*

Using Bayes' theorem: $$ 
P(\text{fair} \mid X = 2) = \frac{P(X = 2 \mid \text{fair}) \cdot P(\text{fair})}{P(X = 2 \mid \text{fair}) \cdot P(\text{fair}) + P(X = 2 \mid \text{loaded}) \cdot P(\text{loaded})} 
$$

Plugging in the values: $$ P(\text{fair} \mid X = 2) = \frac{0.3125 \cdot 0.5}{0.3125 \cdot 0.5 + 0.1323 \cdot 0.5} = \frac{0.15625}{0.15625 + 0.06615} = \frac{0.15625}{0.2224} \approx 0.703 
$$

So, with the observed data, we are approximately 70.3% confident that the coin is fair.

*Now let's assume that our brother has played this trick with us many times and we calculate that based on our experience, before tossing the coin, the probability that he is using a loaded coin is 0.6*

If we know that the prior probability for the loaded coin is ð‘ƒ(loaded)=0.6 then the prior probability for the fair coin is ð‘ƒ(fair)=0.4. .

Using Bayes' theorem:$$ P(\text{fair} \mid X = 2) = \frac{P(X = 2 \mid \text{fair}) \cdot P(\text{fair})}{P(X = 2 \mid \text{fair}) \cdot P(\text{fair}) + P(X = 2 \mid \text{loaded}) \cdot P(\text{loaded})} 
$$

Plugging in the values:$$ 
P(X = 2 \mid \text{fair}) = 0.3125 \quad \text{(calculated earlier)} $$$$ P(X = 2 \mid \text{loaded}) = 0.1323 \quad \text{(calculated earlier)} 
$$

Now substituting these into Bayes' theorem:$$
P(\text{fair} \mid X = 2) = \frac{0.3125 \cdot 0.4}{0.3125 \cdot 0.4 + 0.1323 \cdot 0.6} = \frac{0.125}{0.125 + 0.07938} = \frac{0.125}{0.20438} \approx 0.6114 
$$

So, with the updated prior probability, we are approximately 61.14% confident that the coin is fair, or $1-0.6114=0.3886$ probability that the coin is loaded.

*How many flip would we need to be sure that coin is fair with 90% confident?*

To determine the number of initial flips needed to be sure with 90% certainty, we need to calculate the posterior probability for various numbers of flips until we achieve a posterior probability of 90%.

This involves calculating probabilities for multiple scenarios. Let's use a script to find the minimum number of flips required:

```{r}
# Define the binomial coefficient function
choose <- function(n, x) {
  factorial(n) / (factorial(x) * factorial(n - x))
}

# Probability functions
prob_fair <- function(x, n) {
  choose(n, x) * (1/2)^n
}

prob_loaded <- function(x, n) {
  choose(n, x) * (0.7)^x * (0.3)^(n-x)
}

# Function to calculate posterior probability for 'fair' coin
posterior_prob_fair <- function(x, n) {
  p_fair = prob_fair(x, n)
  p_loaded = prob_loaded(x, n)
  p_fair / (p_fair + p_loaded)
}

# Find the minimum number of flips for 90% certainty
n <- 5
while(TRUE) {
  x <- 0:n
  probs <- sapply(x, function(k) posterior_prob_fair(k, n))
  if (max(probs) >= 0.9) break
  n <- n + 1
}

cat("Minimum number of flips for 90% certainty:", n, "\n")

```

Surprisingly, this is the same number of tosses we had before. In the previous calculation, we assumed both coins (fair and loaded) have equal prior probabilities, meaning ð‘ƒ(fair)=ð‘ƒ(loaded)=0.5. The posterior probability of 70.3% was calculated for this specific observed outcome (2 heads out of 5) based on the prior probabilities and likelihoods.

However, the R script's loop is designed to find the minimum number of flips needed to achieve 90% posterior probability for any outcome. This means we're looking for the number of flips that would allow us to be 90% certain in at least one scenario, not specifically for the observed 2 heads in 5 flips.

Thus, it is possible for the same number of flips (5) to yield different posterior probabilities depending on the observed outcomes. The loop in the R script checks various outcomes until it finds one that achieves the 90% certainty threshold.

The calculated 70.3% certainty is specific to the observed outcome of 2 heads in 5 flips.

The R script's result of 5 flips indicates that there exists an outcome within 5 flips that would give us 90% certainty.

*Suppose now that your brother has a third coin which comes up tails 70% of the time. Again, you don't know which coin your brother has brought to you, so you are going to test it by flipping it 4 times, were X counts the number of heads.*

Let $\theta$ identify the coin so that there are three possibilities: - $\theta$ = fair - $\theta$ = loaded Heads - $\theta$ = loaded Tails

$x= heads$

$$
f(x \mid \theta) = \begin{cases}  \binom{5}{x} (\frac{1}{2})^5 \text { if fair} \\
\binom{5}{x} (0.7)^x (0.3)^{5-x} \text  { if loaded heads}\\
\binom{5}{x} (0.3)^x (0.7)^{5-x} \text  { if loaded tails}
\end{cases}
$$

Your prior is: - $P(\theta =fair) = 0.4$ - $P(\theta =loadedH) = 0.3$ - $P(\theta =loadedT) = 0.3$

Our prior probability that the coin is loaded is still 0.6, but we do not know which loaded coin it is, so we split the probability evenly between the two options

*What is the form of the likelihood now that we have three options?* Likelihood:

$$
L(\theta) = P(X=x_1, X= x_2, \cdots, X=x_n \mid \theta)
$$the probability function for a binomial distribution:$$
f(x\mid p) = \binom{n}{x}p^x(1-p)^{n-x}
$$

$n=4$ substituting:

$$f(x \mid \theta) = \binom{4}{x} 0.5^x \times 0.5^{4-x} I_{fair}  + \binom{4}{x} 0.7^x \times 0.3^{4-x} I_{fair.heads}  +\binom{4}{x} 0.3^x \times 0.7^{4-x} I_{fair.tails}$$

$$f(x \mid \theta) = \binom{4}{x} 0.5^x \times 0.5^{4-x} \times 0.4 + \binom{4}{x} 0.7^x \times 0.3^{4-x} \times 0.3 +\binom{4}{x} 0.3^x \times 0.7^{4-x}  \times 0.3$$

*Suppose you flip the coin four times and it comes up heads twice. What is the posterior probability that this is the fair coin*

To determine the posterior probability that the coin is fair, we can use Bayes' theorem.

$$
P(\theta=fair \mid x=2)= \frac{P(X=2\mid \theta = fair)P(\theta=fair)}{P(x=2)}
$$

Where $P(x=2)$ is

$$P(x=2) =
P(X=2\mid \theta = fair)P(\theta=fair) + \\
P(X=2\mid \theta = loaded.heads)P(\theta=loaded.heads) + \\
P(X=2\mid \theta = loaded.tails)P(\theta=loaded.tails) 
$$ Given the following parameters: - $P(\theta = \text{fair}) = 0.4$ - $P(\theta = \text{loaded heads}) = 0.3$ - $P(\theta = \text{loaded tails}) = 0.3$

First, we calculate $P(X = 2 \mid \theta = \text{fair})$:

$$P(X = 2 \mid \theta = \text{fair}) = \binom{4}{2} (0.5)^2 (0.5)^{2} = \frac{4!}{2!2!} (0.5)^4 = 6 \times 0.0625 = 0.375$$

Next, we calculate $P(X = 2 \mid \theta = \text{loaded heads})$:

$$P(X = 2 \mid \theta = \text{loaded heads}) = \binom{4}{2} (0.7)^2 (0.3)^2 = \frac{4!}{2!2!} (0.7)^2 (0.3)^2 = 6 \times 0.49 \times 0.09 = 6 \times 0.0441 = 0.2646$$And $P(X = 2 \mid \theta = \text{loaded tails})$:

$$P(X = 2 \mid \theta = \text{loaded tails}) = \binom{4}{2} (0.3)^2 (0.7)^2 = \frac{4!}{2!2!} (0.3)^2 (0.7)^2 = 6 \times 0.09 \times 0.49 = 6 \times 0.0441 = 0.2646$$

Now, we substitute:

$$
P(\theta=fair \mid x=2)= \frac{0.375\times 0.4} {0.375\times 0.4+0.2646 \times 0.3+0.2646\times 0.3} =0.486
$$In code:

```{r}
# Given parameters
P_fair <- 0.4
P_loaded_heads <- 0.3
P_loaded_tails <- 0.3

# Likelihoods
P_X_2_given_fair <- choose(4, 2) * (0.5^2) * (0.5^2)
P_X_2_given_loaded_heads <- choose(4, 2) * (0.7^2) * (0.3^2)
P_X_2_given_loaded_tails <- choose(4, 2) * (0.3^2) * (0.7^2)

# Marginal likelihood
P_X_2 <- (P_X_2_given_fair * P_fair) + 
         (P_X_2_given_loaded_heads * P_loaded_heads) + 
         (P_X_2_given_loaded_tails * P_loaded_tails)

# Posterior probability
P_fair_given_X_2 <- (P_X_2_given_fair * P_fair) / P_X_2

# Display the result
P_fair_given_X_2
```
:::

## MLE for Uniform Distribution

Let's consider a uniform distribution defined over the interval $[a, b]$. The probability density function (pdf) of the uniform distribution is given by:

$$
f(x \mid a, b) = \frac{1}{b - a} \quad \text{for} \quad a \leq x \leq b
$$

Suppose we have a sample of observed data points $x_1, x_2, \ldots, x_n$ from the uniform distribution. The likelihood function \$ L(a, b) \$ is given by:

$$
L(a, b) = \prod_{i=1}^{n} f(x_i \mid a, b) = \prod_{i=1}^{n} \frac{1}{b - a} = \left( \frac{1}{b - a} \right)^n
$$Every value between a and b are equally likely To find the MLE, we need to maximize the likelihood function. In the case of the uniform distribution, we need to find the values of $a$ and $b$ that maximize the likelihood function. The MLE for $a$ and $b$ are:

$$
\hat{a} = \min(x_1, x_2, \ldots, x_n)
$$$$
\hat{b} = \max(x_1, x_2, \ldots, x_n)
$$

```{r}
# Sample data
x <- c(2, 3, 5, 6, 7)

# MLE for the uniform distribution
a_hat <- min(x)
b_hat <- max(x)

# Print the estimates
cat("MLE for a:", a_hat, "\n")
cat("MLE for b:", b_hat, "\n")

```

## MLE for Exponential Distribution

The exponential distribution is defined by its rate parameter \$ \lambda \$. The probability density function (pdf) of the exponential distribution is given by:

$$
f(x \mid \lambda) = \lambda e^{-\lambda x} \quad \text{for} \quad x \geq 0
$$

Suppose we have a sample of observed data points \$ x_1, x_2, \ldots, x_n \$ from the exponential distribution. The likelihood function \$ L(\lambda) \$ is given by:

$$
L(\lambda) = \prod_{i=1}^{n} f(x_i \mid \lambda) = \prod_{i=1}^{n} \lambda e^{-\lambda x_i} = \lambda^n e^{-\lambda \sum_{i=1}^{n} x_i}
$$

To find the MLE, we take the natural logarithm of the likelihood function to obtain the log-likelihood function:

$$
\log L(\lambda) = n \log \lambda - \lambda \sum_{i=1}^{n} x_i
$$

Next, we take the derivative of the log-likelihood function with respect to \$ \lambda \$ and set it to zero to find the maximum:

$$
\frac{d}{d\lambda} (\log L(\lambda)) = \frac{n}{\lambda} - \sum_{i=1}^{n} x_i = 0
$$

Solving for \$ \lambda \$, we get:

$$
\hat{\lambda} = \frac{n}{\sum_{i=1}^{n} x_i}
$$

```{r}
# Sample data
x <- c(1.2, 0.5, 2.8, 3.3, 1.1)

# MLE for the exponential distribution
n <- length(x)
lambda_hat <- n / sum(x)

# Print the estimate
cat("MLE for lambda:", lambda_hat, "\n")

```

::: exercise-box
Practice: political view

You are trying to ascertain your American colleague's political preferences. To do so, you design a questionnaire with five yes/no questions relating to current issues. The questions are all worded so that a "yes" response indicates a conservative viewpoint.

Let Î¸ be the unknown political viewpoint of your colleague, which we will assume can only take values Î¸=conservative or Î¸=liberal. You have no reason to believe that your colleague leans one way or the other, so you assign the prior P(Î¸=conservative)=0.5.

Assume the five questions are independent and let Y count the number of "yes" responses. If your colleague is conservative, then the probability of a "yes" response on any given question is 0.8. If your colleague is liberal, the probability of a "no" response on any given question is 0.7.

What is an appropriate likelihood for this scenario?

The likelihood formula:

$$
L(\theta) = P(X=x_1, X= x_2, \cdots, X=x_n \mid \theta)
$$ the probability function for a binomial distribution:

$$
f(x\mid p) = \binom{n}{x}p^x(1-p)^{n-x}
$$

$P(x = yes \mid \theta=conservative) = \binom{n}{x} 0.8^x \times (1-0.8)^{n-x}$

$P(x = no \mid \theta=liberal) = \binom{n}{x} 0.7^x \times (1-0.7)^{n-x}$

$P(x = yes \mid \theta=liberal) = \binom{n}{x} 0.3^x \times (1-0.3)^{n-x}$ $n=5$

$$f(x\mid \theta) = \binom{5}{x} 0.8^x \times 0.2^{5-x} +\binom{5}{x} 0.3^x \times (0.7)^{5-x}$$

*Suppose you ask your colleague the five questions and he answers "no" to all of them. What is the MLE for* $\theta$

```{r}
n= 5
x= 0
pyes_con = 0.8
pno_con = 0.2

pyes_lib = 0.3
pno_lib =0.7

# Define the binomial coefficient function
choose <- function(n, x) {
  factorial(n) / (factorial(x) * factorial(n - x))
}

# Probability functions
prob_conservative <- function(x, n) {
  choose(n, x) * pyes_con^x *pno_con^(n-x)
}

prob_liberal <- function(x, n) {
  choose(n, x) * pyes_lib^x *pno_lib^(n-x)
}

# Function to calculate posterior probability for 'fair' coin
posterior_prob_conv <- function(x, n) {
  p_cons = prob_conservative(x, n)
  p_lib = prob_liberal(x, n)
  p_cons / (p_cons + p_lib)
}
print(paste0("conservative =", round(posterior_prob_conv(x,n),3)))

posterior_prob_lib <- function(x, n) {
  p_cons = prob_conservative(x, n)
  p_lib = prob_liberal(x, n)
  p_lib / (p_cons + p_lib)
}
print(paste0("liberal =", round(posterior_prob_lib(x,n),3)))

```

he is most likely liberal according to these questions.
:::

# Continuous version of Bayes' therorem

The versions of Bayes' theorem seen before this lesson where for discrete probabilities with *probability mass functions*. In this lesson, $\theta$ is a continuous quantity which can take on infinitely many values, so the prior (and consequently the posterior) for $\theta$ is a probability density function (PDF).\

$$
f(\theta \mid x) = \frac{f(x\mid \theta) \times f(\theta)}{f(x)} = \frac{f(x\mid \theta) \times f(\theta)}{\int f(x\mid \theta) \times f(\theta) \, d\theta} = \frac{\text{likelihood} \times \text{prior}}{\text{normalizing constant}} \propto \text{likelihood} \times \text{prior}
$$Â 

Where $\propto$ means proportional to.

In reality the normalizing constant can be difficult to calculate and we can ignore it because the posterior is a PDF of $\theta$, but $\theta$ does not appear in $f(x)$ so the absence of $f(x)$ does not change the form of the posterior. The normalizing constant is just that, some constant number not involving $\theta$. We can work with the numerator alone because the posterior distribution of $\theta$ is still unique up to a constant.

::: exercise-box
Coin toss: Posterior probability

We toss a coin and it has unknown probability $\theta$ of coming up heads. We express ignorance about the value of $\theta$ by assigning it a uniform distribution. So we can write the $f(\theta)$ is just the indicator function that $\theta$ is between zero and one.

$$
I_{\{ 0 \leq \theta \leq 1\}} = \begin{cases} 
1  \ if\  0 \leq \theta \leq 1 \\
0 \text { otherwise}
\end{cases}
$$

Now we flip the coin and get one head. We want to say, having observed one flip of the coin, what's our posterior probability distribution of $\theta$.

*Bayes' theorem*:

$$
P(\theta \mid x=1) = \frac{p(x=1\mid \theta)\cdot p(\theta)}{p(x=1)}
$$

-   $p(\theta \mid x=1)$ is the posterior distribution of $\theta$ given that we observed one head.

-   $p(x=1\mid \theta)$ is the likelihood, or the probability of getting a head given the value of $\theta$.

The *likelihood function for the binomial distribution* is: $$
f(x\mid p) = \binom{n}{x}p^x (1-p)^{n-x}
$$

In this case, we have ð‘›=1 (one coin toss) and ð‘¥=1 (we observed one head), so the likelihood function simplifies to: $f(x=1\mid \theta) = \theta$

-   $p(\theta)$ is the prior distribution of $\theta$, which is uniform so $p(\theta) = 1 \quad \text{for} \quad 0 \leq \theta \leq 1$ ($I_{\{ 0 \leq \theta \leq 1\}} =1$)

-   $p(x=1)$ is the marginal probability of observing one head. This can be computed by integrating the likelihood times the prior over all possible values of $\theta$

Substituting the prior and likelihood in the numerator Bayes' formula: $$
 p(\theta \mid X=1) \propto \theta \cdot 1 = \theta
$$

To get the actual posterior distribution, we need to normalize it by dividing by the marginal probability of observing ð‘‹=1:\

$$
p(\theta \mid X=1) =\frac{\theta^1(1-\theta)^0 \cdot I_{\{ 0 \leq \theta \leq 1\}}} {\int_{-\infty}^\infty \theta^1 \cdot (1-\theta)^0 \cdot I_{\{ 0 \leq \theta \leq 1\} }\cdot d\theta} = \frac{\theta}{\int_0^1 \theta \, d\theta} 
$$

-   $\int_{-\infty}^\infty \theta^1 \cdot (1-\theta)^0 \cdot I_{\{ 0 \leq \theta \leq 1\}}$ This is the integral over the range where $\theta$ is between 0 and 1. The indicator function ensures that the integration is only performed within this range.\

Compute the integral (marginal probability):\
$$int_0^1 \theta \, d\theta = \left[ \frac{\theta^2}{2} \right]_0^1 = \frac{1}{2}$$

Therefore, the posterior distribution is:\
$$p(\theta \mid X=1) = \frac{\theta}{\frac{1}{2}} = 2\theta$$

So, the posterior distribution of $\theta$ given that we observed one head is:

$$
p(\theta | X = 1) = 2\theta \quad \text{for} \quad 0 \leq \theta \leq 1
$$

This distribution is a linear function of $\theta$ that starts at 0 and increases to 2 at $\theta = 1$. This makes sense because observing one head suggests that higher values of $\theta$ (closer to 1) are more likely than lower values (closer to 0).

*After the coin-flipping example that resulted in heads, the posterior for* $\theta$ became $f(\theta \mid x_1 =1)= 2\theta I_{\{0 \leq \theta \leq 1\}}$ *Now use this posterior as your prior for* $\theta$ before the next second flip. Which is the formula that represents the posterior PDF for $\theta$ after the second flip (also resulting in heads)

$$
f(\theta \mid x) = \frac{\text{likelihood} \times \text{prior}}{\text{normalizing constant}} \propto \text{likelihood} \times \text{prior} = \frac{f(x\mid \theta) \times f(\theta)}{f(x)} = \frac{f(x\mid \theta) \times f(\theta)}{\int f(x\mid \theta) \times f(\theta) \, d\theta} 
$$Â 

prior = $2\theta I_{\{0 \leq \theta \leq 1\}}$ likelihood: $P(x_2=1 \mid \theta) = \theta$ as seen in the previous part.

posterior:$$
f(\theta \mid x) = \frac{\theta \cdot 2\theta I_{\{0 \leq \theta \leq 1\}}}{\text {normalizing constant}}
$$

normalizing constant: $\int_0^1 f(x_2 \mid \theta) \cdot f(\theta) d\theta$

$\int_0^1 \theta \cdot 2\theta d\theta$

$$
f(\theta \mid x_2) = \frac{\theta \cdot 2\theta I_{\{0 \leq \theta \leq 1\}}}{\int_0^1 \theta \cdot 2\theta d\theta}= 3\theta^2 I_{\{0 \leq \theta \leq 1\}
$$

*Now we flip the coin and get 0 heads. We want to say, having observed one flip of the coin, what's our posterior probability distribution of* $\theta$.

*Bayes' theorem:*

$$
P(\theta \mid x=0) = \frac{p(x=0\mid \theta)\cdot p(\theta)}{p(x=0)}
$$

-   $p(\theta \mid x=0)$ is the posterior distribution of $\theta$ given that we observed zero heads.
-   $p(x=0\mid \theta)$ is the likelihood, or the probability of getting zero heads given the value of $\theta$.

The *likelihood function for the binomial distribution* is: $$
f(x\mid p) = \binom{n}{x}p^x (1-p)^{n-x}
$$

In this case, we have $n=1$ (one coin toss) and $x=0$ (we observed zero heads), so the likelihood function simplifies to: $$
f(x=0\mid \theta) = (1-\theta)
$$

-   $p(\theta)$ is the prior distribution of $\theta$, which is uniform so $p(\theta) = 1 \quad \text{for} \quad 0 \leq \theta \leq 1$ ($I_{\{ 0 \leq \theta \leq 1\}} =1$).
-   $p(x=0)$ is the marginal probability of observing zero heads. This can be computed by integrating the likelihood times the prior over all possible values of $\theta$.

Substituting the prior and likelihood in the numerator of Bayes' formula:$$
p(\theta \mid X=0) \propto (1-\theta) \cdot 1 = (1-\theta)
$$

To get the actual posterior distribution, we need to normalize it by dividing by the marginal probability of observing $X=0$: $$
p(\theta \mid X=0) = \frac{(1-\theta)^1 \cdot I_{\{ 0 \leq \theta \leq 1\}}} {\int_{-\infty}^\infty (1-\theta)^1 \cdot I_{\{ 0 \leq \theta \leq 1\}} \, d\theta} = \frac{(1-\theta)}{\int_0^1 (1-\theta) \, d\theta}
$$

-   $\int_{-\infty}^\infty (1-\theta)^1 \cdot I_{\{ 0 \leq \theta \leq 1\}}$: This is the integral over the range where $\theta$ is between 0 and 1. The indicator function ensures that the integration is only performed within this range.

Compute the integral (marginal probability):$$
\int_0^1 (1-\theta) \, d\theta = \left[ \theta - \frac{\theta^2}{2} \right]_0^1 = \frac{1}{2}
$$

Therefore, the posterior distribution is: $$
p(\theta \mid X=0) = \frac{(1-\theta)}{\frac{1}{2}} = 2(1-\theta)
$$

So, the posterior distribution of $\theta$ given that we observed zero heads is: $$
p(\theta \mid X = 0) = 2(1-\theta) \quad \text{for} \quad 0 \leq \theta \leq 1
$$

This distribution is a linear function of $\theta$ that starts at 2 when $\theta = 0$ and decreases to 0 at $\theta = 1$. This makes sense because observing zero heads suggests that lower values of $\theta$ (closer to 0) are more likely than higher values (closer to 1).
:::

# Prior interval estimates

For the exercise above of a uniform distribution between 0 and 1, before observing any coin toss:

we have a uniform distribution with values 1 for $x$ between 0 and 1

```{r}
# Function to plot uniform distribution

plot_uniform_distribution <- function(theta1, theta2) {
  # Create a sequence of x values
  x <- seq(theta1 - 1, theta2 + 1, by = 0.01)
  
  # Calculate the PDF values
  y <- ifelse(x >= theta1 & x <= theta2, 1 / (theta2 - theta1), 0)
  
  # Create a data frame for plotting
  data <- data.frame(x, y)
  
  # Plot the uniform distribution
  ggplot(data, aes(x = x, y = y)) +
    geom_line() +
    geom_area(fill = "lightblue", alpha = 0.5) +
    labs(title = paste("Uniform Distribution U[", theta1, ",", theta2, "]", sep = ""),
         x = "x", y = "Density") +
    theme_minimal()
}

# Plot the uniform distribution U[0, 1]
plot_uniform_distribution(0, 1)
```

the probability of observing a value between 0.025 and 0.975 is .95

we use the quantile function in r to calculate the lower and upper limits.

```{r}
# Define the prior distribution function (which is uniform)
prior <- function(theta) {
  return(rep(1, length(theta)))
}

# Define the lower and upper bounds of the interval
lower_bound <- qunif(0.025, min = 0, max = 1) 
upper_bound <- qunif(0.975, min = 0, max = 1) 

# Compute the integral using the integrate() function
result <- integrate(prior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P(0.025 <= Î¸ <= 0.975) =", probability, "\n")

```

and what is the prior probability of observing a value higher than .05?

```{r}
# Define the prior distribution function (which is uniform)
prior <- function(theta) {
  return(rep(1, length(theta)))
}

# Define the lower and upper bounds of the interval
lower_bound <- qunif(.05, min = 0, max = 1) 
upper_bound <- qunif(1, min = 0, max = 1) 


# Compute the integral using the integrate() function
result <- integrate(prior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P(.05 <= Î¸ <= 1) =", probability, "\n")

```

If we were to compute the probability for a normal distribution instead of a uniform distribution we do the same, but we will have to change the formula to calculate the prior for a `dnorm` and the quantiles for `qnorm`

```{r}
# Define the mean and standard deviation
mu <- 0
sigma <- 1

# Define the prior distribution function (which is normal)
prior <- function(theta) {
  return(dnorm(theta, mean = mu, sd = sigma))
}

# Define the lower and upper bounds based on the percentiles
lower_bound <- qnorm(0.025, mean = mu, sd = sigma)
upper_bound <- qnorm(0.975, mean = mu, sd = sigma)

# Compute the integral using the integrate() function
result <- integrate(prior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P(0.025 <= Î¸ <= 0.975) =", probability, "\n")


```

::: exercise-box
Prior example

Suppose you are trying to calibrate a thermometer by testing the temperature it reads when water begins to boil. Because of natural variation, you take several measurements to estimate $\theta$, the mean temperature reading for this thermometer at the boiling point. You know thast at sea level, water should boil at 100 degrees celcius, so you use a precise prior with $P(\theta = 100) =1$ You then observe the following measurements: 94.6, 95.4, 96.2, 94.9, 95.9

What would the posterior for $\theta$ look like? a) Most posterior probability will be concentrated near the sample mean of 95.4 degrees Celsius. b) Most posterior probability will be spread between the sample mean of 95.4 degrees Celsius and the prior mean of 100 degrees. c) The posterior will be $\theta =100$ with probability 1, regardless of the data. d) None of the above.

-   Prior: $\theta =100$
-   Likelihood. Assuming the temperature readings are normally distributed around $\theta$ with some unknown variance, the likelihood for a single observations $y_1$ is:

$$
 y_1 \sim N(\theta, \sigma^2)
 $$

$$P(y_i | Î¸, Ïƒ^2) = \frac{1}{\sqrt{2Ï€Ïƒ^2}} \exp \left(-\frac{(y_i - Î¸)2}{2Ïƒ2}\right)$$

For multiple observations, the joint likelihood is the product of individual likelihoods: $$
P(y\mid \theta, \sigma^2) = \prod_{i=1}^n(P(y_1\mid \theta, \sigma^2)
$$

-   Posterior distribution: Bayes' theorem combines the prior and likelihood to give the posterior: $$
    P(\theta\mid y)= \frac{P(y\mid \theta)P(\theta)}{P(y)}
    $$

since the prior $P(\theta =100) =1$, the posterior is proportional to the likelihood. $$
P(\theta\mid y) \prop P(y\mid \theta)
$$ since the prior is a delta function at 100, the posterior would simply update the variance but not the mean. The posterior will still be centered around $\theta =100$. Given this unusual precise prior, the posterior mean remains fixed at $\theta =100$, because no evidence can shift it from this exact value, but the variance would be adjusted according to the observed data.

Calculation with the data:

The observed data are: 94.6, 95.4, 96.2, 94.9, 95.9.

Sample Mean ($\bar{y}$) and Variance ($s^2$)

1.  Calculate the sample mean ($\bar{y}$):

```{r}
data <- c(94.6, 95.4, 96.2, 94.9, 95.9)
n <- length(data)
sample_mean <- mean(data)
sample_mean
```

2.  Calculate the sample variance ($s^2$)

$$
s^2 = \frac{1}{n-1} \sum_{i=1}^5(y_1-\bar y)^2 = 0.512
$$

```{r}
sample_variance <- var(data)
sample_variance

```

Posterior variance. The posterior variance is adjusted based on the observed data's variance. $$
\sigma^2_{posterior}= \frac{\simga^2}{n}
$$

Summary: Clearly this was a poor choice of prior, especially in light of the data we collected.
:::

# Prior Predictive Distributions

The prior predictive distribution is the distribution of potential observed data, considering only the priors. It's derived by combining the prior distribution with the likelihood of the data. This can help evaluate whether the chosen priors make sense given the potential data outcomes. It can also help in model checking and understanding the implications of your prior choices.

Using priors allows you to incorporate existing knowledge, which can be particularly valuable when data is scarce. However, it's essential to choose priors carefully, as they can significantly influence the results. The prior predictive distribution helps ensure that your priors lead to reasonable predictions.

Example: Predicting the return on an investment portfolio.

Prior: Based on historical performance, you believe the annual return is normally distributed with a mean of 5% and a standard deviation of 2%.

Likelihood: Current market data affecting the portfolio's performance.

Prior Predictive Distribution: This would combine the prior belief with current market conditions to predict the range of possible returns before actually observing the annual return.

Steps to Calculate Prior Predictive Distributions 1.- Specify the Prior Distribution:

Define your prior beliefs about the parameter of interest. This could be in the form of a probability distribution (e.g., normal, beta, etc.).

2.  Define the Likelihood Function:

Specify the likelihood of the observed data given the parameter. This is often based on a probability distribution that describes the data generation process (e.g., binomial, normal, Poisson, etc.).

3.  Integrate the Prior and Likelihood:

The prior predictive distribution is obtained by integrating (or summing) the product of the prior distribution and the likelihood function over all possible values of the parameter.

Mathematically, if ðœƒ is the parameter, ð‘(ðœƒ) is the prior distribution, and ð‘(ð‘¥âˆ£ðœƒ) is the likelihood, the prior predictive distribution ð‘(ð‘¥) is given by: ð‘(ð‘¥)=âˆ«ð‘(ð‘¥âˆ£ðœƒ)ð‘(ðœƒ)ð‘‘ðœƒ

$$
f(x)=\int f(x,\theta)d\theta
$$ Supoose we want to toss a coing 10 times and we want to predict the number of heads we are going to see. This will of course depend on our prior about the fairness of the coin.

Imagine that we think that all possible coins are equally likely (all possible probabilities from all heads to all tails), then we can put a prior for $\theta$ that's flat over the interval from 0 to 1.

if X is each heads $$
X = \sum_{i=1}^{10}Y_i
$$ where $Y$ is each individual coin toss.

If we believe that heads and tails are equally likely, then our prior for $\theta$ will be equally distributed,so we choose a uniform prior for theta. $$
f(\theta)=I_{\{0\leq \theta \leq 1 \}}
$$ the predictive distribution will be: $$
f(x) = \int \text{likelihood} \times \text{prior}
$$

$$
f(x) = \int f(x \mid \theta) \times f(\theta)d(\theta)
$$ since we have a coin toss, this is a binomial likelihood for X:

given that $f(\theta)=1$ for this case:

$$
f(x) = \int_0^1 \frac {10!}{x! \times (10-x)!}\theta^x\times(1-\theta)^{10-x}\times 1 \times d(\theta)
$$ this resolves at the end in $\frac{1}{11}$ for $X \in {0,1,2,\cdots,10}$

Which just means that any possible number of heads is equally likely, because we started with a uniform prior.

::: exercise-box
Example: Normal--Normal Model

Imagine you want to model a measurement ð‘¥ that comes with some measurement error. You have a belief (or prior) about the true underlying value ðœƒ, and your observed data ð‘¥ is a noisy measurement of ðœƒ.

Model Setup Prior for ðœƒ: Suppose you believe that the true value ðœƒ is normally distributed with a mean of $\mu_0$ and variance $\pi^2$. For example, you may be confident that the true average measurement is around 50 with a standard deviation of 10. That is,$\theta \sim N (\mu_0, \pi^2)$ with $\mu_0 =50$ and $\pi^2=10$

Likelihood for ð‘¥ given ðœƒ: When you measure ð‘¥, you know that the measurement has error. Suppose the measurement error is normally distributed with a standard deviation $\sigma = 5$. Then, $$
x \mid \theta \sim N(\theta,\sigma^2)
$$

In density form:

$$
p(x \mid \theta) = \frac{1}{\sqrt{2\pi\,\sigma2}},\exp!\left(-\frac{(x-\theta)2}{2\sigma^2}\right)$$

Prior Predictive Distribution:

The prior predictive distribution tells you what values of ð‘¥you would expect before observing any new data, solely based on your prior belief about ðœƒ and the measurement process. It is given by $$
p(x)= \int_{-\infty}^\infty p(x \mid \theta) p(\theta) d\theta
$$

Calculating the Prior Predictive Distribution For our normal--normal model, plug in the formulas for the prior and the likelihood:

Prior density:

$$
p(\theta) = \frac{1}{\sqrt{2\pi\,\tau2}},\exp!\left(-\frac{(\theta-\mu_0)2}{2\tau^2}\right)
$$

Likelihood density:

$$
p(x \mid \theta) = \frac{1}{\sqrt{2\pi\,\sigma2}},\exp!\left(-\frac{(x-\theta)2}{2\sigma^2}\right)$$

Thus,

$$p(x) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi\,\sigma2}},\exp!\left(-\frac{(x-\theta)2}{2\sigma^2}\right) \cdot \frac{1}{\sqrt{2\pi\,\tau2}},\exp!\left(-\frac{(\theta-\mu_0)2}{2\tau^2}\right)d\theta$$

Because the convolution of two normal distributions is again normal, this integral simplifies (without getting lost in the algebra) to another normal density: $$
x \sim N(\mu_{0}, \pi^2 +\sigma^2)
$$ plugging in our numbers: $$
x \sim N(50,10^2+5^2) \rightarrow x \sim N(50,100+25) \rightarrow x \sim N(50,125)
$$ so the prior predictive distribution for x is: $$
p(x)= \frac{1}{\sqrt{2\pi \times 125}}\exp \left( -\frac{(x-50^2)}{2\times 125} \right)
$$ This distribution represents what values ð‘¥might take before you actually collect any data, based solely on your prior expectation about ðœƒ and the noise in your measurements.

Breaking Down the Integration (Intuition) When you perform the integration: $$
p(x)= \int_{-\infty}^\infty p(x \mid \theta) p(\theta) d\theta
$$

you are essentially "averaging" all the likelihood functions ð‘(ð‘¥âˆ£ðœƒ) weighted by the probability of each ðœƒ given byð‘(ðœƒ). Since bothð‘(ð‘¥âˆ£ðœƒ) andð‘(ðœƒ) are Gaussians, the result turns out to be a Gaussian with its variance increased by the variability in the prior $\pi^2$ and the measurement error variance $\sigma^2$
:::

::: exercise-box
Prior predictive distribution formula

The prior predictive distribution for X when $\theta$ is continuous is given by $$
\int f(x\mid \theta) \cdot f(\theta)d\theta
$$

The analogous expression when $\theta$ is discrete is

$$
\sum_\theta f(x\mid \theta) \cdot f(\theta)
$$

adding all possible values of theta.

Let's return to the example of your brother's loaded coin from Lesson 5. Recall that he has a fair coin where heads comes up on average 50% fo the time (p=0.5) and a loaded coin (p=0.7) If we flip the coin five times, the likelihood is binomial $$
f(x\mid p) = \binom{5}{x} p^x (1-p)^{5-x}
$$ where X counts the number of heads.

Suppose you are confident, but not sure that he has brought you the loaded coin, so that your prior is $f(p)=0.9 I_{\{p=0.7 \}} + 0.1 I_{\{p=0.5 \}}$

What is the expression of the prior predictive distribution of X?

sol: $$
f(X) = \binom{5}{x} 0.7^x \cdot 0.3^{5-x} \cdot 0.9 + \binom{5}{x} 0.5^x \cdot 0.5^{5-x} \cdot 0.1
$$
:::

# Posterior interval estimates

Let's draw the posterior probability functions:

```{r, fig.align='center', fig.width=10}
#| layout-ncol: 2
# Load necessary libraries
library(ggplot2)

# Define the posterior distributions
posterior_x1 <- function(theta) {
  return(2 * theta)
}

posterior_x0 <- function(theta) {
  return(2 * (1 - theta))
}

# Create a data frame for plotting
theta_values <- seq(0, 1, by = 0.01)
data <- data.frame(
  theta = theta_values,
  posterior_x1 = posterior_x1(theta_values),
  posterior_x0 = posterior_x0(theta_values)
)

# Plot the posterior distributions
ggplot(data) +
  geom_line(aes(x = theta, y = posterior_x1), color = "blue", size = 1, linetype = "solid") +
  labs(title = "Posterior Distributions for Î¸",
       x = "Î¸",
       y = "Posterior Probability Density heads= 1") +
  scale_y_continuous(limits = c(0, 2.1)) +
  theme_minimal() +
  annotate("text", x = 0.25, y = 1.7, label = "X = 1 (2Î¸)", color = "blue") 
  


# Create a data frame for plotting
theta_values <- seq(0, 1, by = 0.01)
data <- data.frame(
  theta = theta_values,
  posterior_x1 = posterior_x1(theta_values),
  posterior_x0 = posterior_x0(theta_values)
)

# Plot the posterior distributions
ggplot(data) +
  geom_line(aes(x = theta, y = posterior_x0), color = "red", size = 1, linetype = "dashed") +
  labs(title = "Posterior Distributions for Î¸ heads = 0",
       x = "Î¸",
       y = "Posterior Probability Density") +
  scale_y_continuous(limits = c(0, 2.1)) +
  theme_minimal() +
  annotate("text", x = 0.25, y = 1.7, label = "X = 0 (2(1-Î¸))", color = "red") 
  
```

**The posterior interval estimates** Under the posterior density we can ask again what is the probability that $\theta$ is between 0.025 and 0.95 given that we have observed 1 head.

$$
P(.025 \leq 0 \leq .95) = \int_{0.25}^{0.975} 2\theta d\theta = .975^2 -.025^2 = .95
$$

```{r}
# Define the posterior distribution function
posterior <- function(theta) {
  return(2 * theta)
}

# Define the lower and upper bounds of the interval
lower_bound <- 0.025
upper_bound <- 0.975

# Compute the integral using the integrate() function
result <- integrate(posterior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P(0.025 <= Î¸ <= 0.975) =", probability, "\n")

```

```{r}
# Define the posterior distribution function
posterior <- function(theta) {
  return(2 * (1- theta))
}

# Define the lower and upper bounds of the interval
lower_bound <- 0.025
upper_bound <- 0.975

# Compute the integral using the integrate() function
result <- integrate(posterior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P(0.025 <= Î¸ <= 0.975) =", probability, "\n")

```

Now what's the probability that $\theat$ is bigger than 0.05 given that heads =1?

$$
P(.05 \leq 0 \leq 1) = \int_{0.5}^{1} 2\theta d\theta = 1-.05^2 = .9975
$$

```{r}
# Define the posterior distribution function
posterior <- function(theta) {
  return(2 * theta)
}

# Define the lower and upper bounds of the interval
lower_bound <- 0.05
upper_bound <- 1

# Compute the integral using the integrate() function
result <- integrate(posterior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P(0.05 <= Î¸ <= 0.05) =", probability, "\n")

```

And if we calculate for having observed heads = 0:

```{r}
# Define the posterior distribution function
posterior_x0 <- function(theta) {
  return(2 * (1 - theta))
}

# Define the lower and upper bounds of the interval
lower_bound <- 0.05
upper_bound <- 1

# Compute the integral using the integrate() function
result <- integrate(posterior_x0, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P(0.5 <= Î¸ <= 1 | X=0) =", probability, "\n")

```

## Equal-Tailed Posterior Intervals

In Bayesian statistics, equal-tailed posterior intervals (also known as credible intervals) are used to indicate the range within which a parameter is likely to lie, given the observed data and prior information. These intervals provide a measure of uncertainty for the parameter estimates.

An equal-tailed posterior interval for a parameter $\theta$ is an interval $[a, b]$ such that the probability of $\theta$ lying below the interval is equal to the probability of $\theta$ lying above the interval. In other words:

$$
P(\theta < a \mid \text{data}) = P(\theta > b \mid \text{data}) = \frac{\alpha}{2}
$$

where $\alpha$ is the significance level (e.g., $\alpha = 0.05$ for a 95% credible interval).

To calculate an equal-tailed posterior interval:

1.  **Determine the Posterior Distribution**: Obtain the posterior distribution $p(\theta \mid \text{data})$ based on the observed data and prior information.
2.  **Find the Interval Limits**: Identify the values $a$ and $b$ such that the cumulative probability at $a$ is $\frac{\alpha}{2}$ and the cumulative probability at $b$ is $1 - \frac{\alpha}{2}$.

Suppose we have a posterior distribution for $\theta$ given by $p(\theta \mid \text{data})$, and we want to calculate a 95% *equal-tailed posterior interval*. We need to find values $a$ and $b$ such that:

$$
P(\theta < a \mid \text{data}) = 0.025 \quad \text{and} \quad P(\theta > b \mid \text{data}) = 0.025
$$

The interval $[a, b]$ will then contain 95% of the posterior probability. If we want an interval that covers 95% of the probability, the tails will be $\frac{1-0.95}{2}=0.025$ so we calculate the probability of $theta$ being less than that number.

::: exercise-box
Equal tailed posterior interval.

We can think again about flipping a coin. He flipped a coin once and it came up head. We don't know if the coin is fair or not.

We already calculated the posterior distribution for ðœƒ givenð‘‹=1 (one head observed) is:

$$
P(\theta \mid x=1)= 2\theta
$$

We want to find the 95% equal-tailed posterior interval for $\theta$.

*Lower Limit* We need to find $a$ such that $P(\theta \leq a \mid x=1) =0.025$ The cumulative distribution function (CDF) for the posterior distribution is:

$$
P(\theta\leq a \mid x=1)=\int_0^a 2\theta d\theta = a^2 
$$We want $q$ such that $q^2=0.025$:$$
q = \sqrt{0.025} \approx 0.158
$$So, the lower limit $a$ is approximately 0.158.

*Upper limit* We need to find $b$ such that $P(\theta \geq b \mid x=1) =0.025$ This is equivalent to finding $b$ such thatð‘ƒ(ðœƒâ‰¤ð‘âˆ£ð‘‹=1)=0.975 :
$$
\int_0^b 2\theta \, d\theta = b^2 = 0.975 
$$
$$b = \sqrt{0.975} \approx 0.987$$Therefore, the 95% equal-tailed posterior interval for ðœƒ is\[0.158,0.987\].

To verify, we can check that the probability of ðœƒ lying within this interval is indeed 0.95:

$$
P(\sqrt{0.025}\leq 0\leq \sqrt{0.925}) = P(.158 \leq 0 \leq .987)= .95
$$
Using the CDF:
$$
P(\theta \leq 0.987 \mid X=1) = (0.987)^2 \approx 0.975 
$$

```{r}
# Define the posterior distribution function
posterior <- function(theta) {
  return(2 * theta)
}

# Define the lower and upper bounds of the interval
lower_bound <- 0
upper_bound <- 0.987

# Compute the integral using the integrate() function
result <- integrate(posterior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P( Î¸ <= 0.987) =", probability, "\n")
```

$$
P(\theta \leq 0.158 \mid X=1) = (0.158)^2 \approx 0.025 
$$

```{r}
# Define the posterior distribution function
posterior <- function(theta) {
  return(2 * theta)
}

# Define the lower and upper bounds of the interval
lower_bound <- 0
upper_bound <- 0.158

# Compute the integral using the integrate() function
result <- integrate(posterior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
probability <- result$value

# Print the result
cat("P( Î¸ <= 0.158) =", probability, "\n")

```

So: $P(0.158 \leq \theta \leq 0.987) = 0.975 - 0.025 = 0.95$
:::

An equal-tailed posterior interval provides a range of values for the parameter $\theta$ that is consistent with the observed data and prior information. Unlike frequentist confidence intervals, credible intervals directly represent the probability of the parameter lying within the interval, given the data and prior information.

Equal-tailed posterior intervals are a useful tool in Bayesian analysis for quantifying uncertainty and providing plausible ranges for parameter estimates. These intervals are derived from the posterior distribution and give a direct probability interpretation, making them intuitive and informative.

# Posterior predictive distribution

What is our posterior predictive distribution after we have observed some data?

::: exercise-box
Coin toss: posterior predictive distribution.

We can think again about flipping a coin, we don't know if it is a fair coin, so we don't know what the probability comes up head is. We observe, after one flip, a head. We want to ask, *what's our predicted distribution for the second flip, given that we saw a head on the first flip?*

$$
f(x_2 \mid x_1) = \int f(x_2\mid \theta, x_1) \times \text{posterior distribution for }\theta \times d\theta
$$

$$
f(x_2 \mid x_1) = \int f(x_2\mid \theta, x_1) \times \ f(\theta \mid x_1) \times d\theta
$$

We assume that the second toss is independent from the first toss, so $f(x_2\mid \theta, x_1) = f(x_2 \mid \theta)$

Substituting:

$$
f(x_2 \mid x_1) = \int f(x_2 \mid \theta) \times \ f(\theta \mid x_1) \times d\theta
$$

Now we cannot keep on assuming that $\theta$ follows an uniform distribution where all the probabilities of getting heads are equal, because we have already observed one head, so it is more likely now to think that the coin has 0.5 probability or more of falling heads.

$$
f(x_2 \mid x_1) = \int_0^1 \theta^{x_2} (1-\theta)^{1-x_2} \times f(\theta \mid x_1) \times d\theta
$$

We know, because we calculated above that the posterior is $2\theta$

$$
f(x_2 \mid x_1) = \int_0^1 \theta^{x_2} (1-\theta)^{1-x_2} \times 2\theta \times d\theta = \int_0^1 2\theta ^{x_2+1}(1-\theta)^{1-x_2} \times d\theta 
$$ we can solve that equation or, given that x can only take two values, 1 and 0, we can resolve for each of the values:

$$
f(x_2=1 \mid x_1=1)= \int 2\theta ^2 d\theta = \frac{2}{3}
$$ $$
f(x_2=0 \mid x_1=1)= 1 - \frac{2}{3} = \frac{1}{3}
$$
:::

::: exercise-box
posterior of second toss given first toss is tails

Consider the coin-flippin example. The likelihood for each coin flip is a Bernoulli with probability of head $\theta$, that is $f(x\theta)=\theta^x(1-\theta)^1-x$ for y=0 or y=1, and we used a uniform prior on $\theta$ Recall that if we had observed $Y_1=0$ instead of $X_1=1$, the posterior distribution for $\theta$ woudl have been $f(\theta \mid X_1=0 = 2(1 -\theta)I_{\{0 \leq \theta \leq 1 \}}$ Which is the correct expression for the posterior predictive distribution for the next flip $X_2 \mid X_1 =0$

Sol: $$
f(x_2 \mid X_1=0) = \int_0^1 \theta^{x_2}(1-\theta)^{1-x_2} \times 2(1-\theta)d\theta \text {for } x_2=0 \text{ or } x_2=1
$$
:::

# Highest Posterior Density (HPD) Interval

In Bayesian statistics, the Highest Posterior Density (HPD) interval is a credible interval that provides a range of values for a parameter $\theta$ that is most probable given the observed data. Unlike equal-tailed intervals, HPD intervals are not necessarily symmetric but have the property that the posterior density within the interval is higher than outside the interval.

The HPD interval for a parameter $\theta$ is the interval $[a, b]$ such that:

1.  The posterior density at any point within the interval is higher than at any point outside the interval:$$
    p(\theta \mid \text{data}) \geq p(\theta' \mid \text{data}) \quad \text{for all} \quad \theta \in [a, b] \quad \text{and} \quad \theta' \notin [a, b]
       $$
2.  The interval $[a, b]$ contains a specified proportion of the posterior probability, typically 95%:$$
    \int_a^b p(\theta \mid \text{data}) \, d\theta = 0.95
       $$

To calculate an HPD interval:

1.  **Determine the Posterior Distribution**: Obtain the posterior distribution $p(\theta \mid \text{data})$ based on the observed data and prior information.
2.  **Find the Interval Limits**: Identify the values $a$ and $b$ such that the interval $[a, b]$ contains the highest posterior density and covers the specified proportion of the posterior probability.

Example

Suppose we have a posterior distribution for $\theta$ given by $p(\theta \mid \text{data})$, and we want to calculate a 95% HPD interval. The steps are as follows:

1.  Calculate the posterior density for $\theta$ at different values.
2.  Identify the interval $[a, b]$ where the posterior density is highest and covers 95% of the posterior probability.
3.  Ensure that the posterior density within the interval is higher than outside the interval.

An HPD interval provides a range of values for the parameter $\theta$ that are most credible given the observed data and prior information. It is often preferred over equal-tailed intervals because it reflects the highest density region of the posterior distribution, making it a more accurate representation of uncertainty in parameter estimates.

Highest Posterior Density (HPD) intervals are a valuable tool in Bayesian analysis for identifying the range of parameter values that are most probable given the data. They are derived from the posterior distribution and provide a direct probability interpretation, offering a clear and intuitive way to understand the uncertainty in parameter estimates.

# Confidence intervals vs credible intervals.

Frequentist confidence intervals have the interpretation that "If you were to repeat many times the process of collecting data and computing a 95% confidence interval, then on average about 95% of those intervals would contain the true parameter value; however, once you observe data and compute an interval the true value is either in the interval or it is not, but you can't tell which." Bayesian credible intervals have the interpretation that "Your posterior probability that the parameter is in a 95% credible interval is 95%." Under what circumstances would you prefer a frequentist confidence interval, and when would you prefer a Bayesian credible interval?

**Frequentist Confidence Intervals**:

*Repetition and Long-term Frequency*: If your focus is on long-term performance and repeated sampling, frequentist confidence intervals are useful. For example, if you're running quality control tests on a production line and need to ensure that your intervals consistently capture the true parameter over many trials, frequentist methods are appropriate.

*No Prior Information*: When you lack prior information about the parameter and want to make minimal assumptions, frequentist methods provide a way to construct intervals based solely on the data at hand.

*Regulatory and Standard Practices*: In fields like clinical trials, regulatory agencies often have established guidelines favoring frequentist methods due to their long-standing use and interpretability.

**Bayesian Credible Intervals**:

*Incorporating Prior Information*: If you have prior knowledge or beliefs about the parameter, Bayesian methods allow you to incorporate this information through a prior distribution. This can be particularly useful in scientific research where previous studies inform current analyses.

*Direct Probabilistic Interpretation*: Bayesian credible intervals provide a direct probabilistic interpretation, making them more intuitive for some users. For instance, if you're presenting results to a non-statistical audience, saying that there's a 95% probability that the true parameter lies within the interval can be more understandable.

*Flexible Modeling*: Bayesian methods offer flexibility in modeling complex data structures and incorporating uncertainty in a more holistic way. This can be advantageous in fields like machine learning and decision analysis.

Summary:

Use Frequentist Confidence Intervals when: You're focused on long-term performance, lack prior information, or are working within regulatory frameworks that favor frequentist methods.

Use Bayesian Credible Intervals when: You have prior knowledge, need a direct probabilistic interpretation, or require flexibility in modeling complex data.

Ultimately, the choice between frequentist and Bayesian methods depends on the specific context and your objectives. Both approaches have their strengths and limitations, and understanding these can help you make informed decisions in your analysis.

# Normalizing Constants and Proportionality

The full expression for a posterior distribution of some parameter $\theta$ is given by

$$
f(\theta|x) = \frac{f(x|\theta)f(\theta)}{\int_{-\infty}^\infty f(x|\theta)f(\theta)d\theta}.
$$

As we will see in coming lessons, it is often more convenient to work with the numerator only: $f(\theta|x) \propto f(x|\theta)f(\theta)$, which is the likelihood times the prior. The symbol $\propto$ stands for "is proportional to." We can multiply a function of $\theta$ by any constant and maintain proportionality. For example, if $f(\theta) = 5\theta$, then $f(\theta) \propto \theta$. However, $f(\theta)$ is not proportional to $\theta + 1$. We maintain proportionality only by modifying constants which are multiplied by the entire function $f(\theta)$. Hence $5(\theta + 1) \propto \theta + 1$.

The reason we can write $f(\theta|x) \propto f(x|\theta)f(\theta)$ is because the denominator $\int f(x|\theta)f(\theta)d\theta$ is free of $\theta$. It is just a normalizing constant.

This is an integral over all possible values of ðœƒ. When we integrate over ðœƒ, we are essentially summing up the values of the function ð‘“(ð‘¥âˆ£ðœƒ)ð‘“(ðœƒ) for all possible values of ðœƒ. The result of this integral is a single number (a constant) that does not depend on the specific value of ðœƒ we are considering in the numerator.

Here is a simple analogy: Imagine you have a jar of jellybeans, and you want to know the total number of jellybeans. If you count all the jellybeans (integrate over all ðœƒ), you get a total number that doesn't depend on any specific jellybean (specific ðœƒ). This total count is like our denominator---it's a fixed number once you've counted all the jellybeans.

Since the denominator is just a normalizing constant that ensures the entire posterior distribution integrates to 1, it doesn't change withðœƒ. Therefore, we can focus on the numerator ð‘“(ð‘¥âˆ£ðœƒ)ð‘“(ðœƒ), which does depend on ðœƒ,

In other words, the shape of the posterior distribution is determined by the productð‘“(ð‘¥âˆ£ðœƒ)ð‘“(ðœƒ), but the exact scale (or normalization) is handled by the denominator. When we say ð‘“(ðœƒâˆ£ð‘¥)âˆð‘“(ð‘¥âˆ£ðœƒ)ð‘“(ðœƒ), we're focusing on the relative values, knowing that the denominator will take care of the normalization.

Therefore, we can ignore any multiplicative terms not involving $\theta$. For example, if $\theta \sim N(\mu, \sigma^2)$, then

$$
f(\theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(\theta - \mu)^2}{2\sigma^2} \right),
$$

Clearly, the expression in (1) does not integrate to 1 (it integrates to $\sqrt{2\pi\sigma^2}$). Although it is not a PDF, it is proportional to the $N(\mu, \sigma^2)$ PDF and can be normalized to represent the $N(\mu, \sigma^2)$ distribution only. Likewise, the posterior $f(\theta|x)$ maintains its uniqueness as long as we specify it up to a proportionality constant.

To evaluate posterior quantities such as posterior probabilities, we will eventually need to find the normalizing constant. If the integral required is not tractable, we can often still simulate draws from the posterior and approximate posterior quantities. In some cases, we can identify $f(x|\theta)f(\theta)$ as being proportional to the PDF of some known distribution. This will be a major topic of Lesson 6.

Remember also that in the posterior distribution of $\theta$, we are treating $x$ as a known constant.

::: exercise-box
Normalizing constants

Find the value of the normalizing constant $c$, which will cause the following integral to evaluate to 1. $$
\int _0^1 c \cdot z^3 (1-z)^1 dz
$$ Hint, notice that this is proportional to a beta density. We only need to find the values of the parameter $\alpha$ and $\beta$ and plug those into the usual normalizing constant for a beta density.

```{r}
posterior <- function(theta) {
  return(theta^3 *(1-theta))
}

lower_bound <- 0
upper_bound <- 1

result <- integrate(posterior, lower = lower_bound, upper = upper_bound)

# Extract the value of the integral
(probability <- result$value)

(c <- probability *20)
```
:::

# Bernoulli Likelihood uniform prior

We will explore the concept of using a uniform prior for a Bernoulli likelihood and how it leads to a Beta posterior. We'll break down the concepts and provide some intuitive explanations.

## Beta Distribution

The Beta distribution is defined as:

$$
\text{Beta}(\theta; \alpha, \beta) = \frac{\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}{B(\alpha, \beta)}
$$

Where $\alpha$ and $\beta$ are parameters that shape the distribution and $B(\alpha, \beta)$ is a normalization constant.

## Bernoulli Likelihood

A Bernoulli trial is a simple experiment where there are only two possible outcomes, often labeled as 1 (success) and 0 (failure). An example of this is flipping a coin, where you might label heads as 1 and tails as 0.

The probability of success (1) is denoted by $\theta$. Thus, the likelihood function, which tells us how probable a given outcome is, looks like this:

$$
P(y \mid \theta) = \theta^y (1 - \theta)^{1 - y}
$$
for n independent trials:
$$
P(y_1,y_2, \cdots, y_n \mid \theta) = \theta \sum_{y_i}(1-\theta)^{n-\sum_{y_i}}
$$
## Uniform Prior

A prior distribution represents our beliefs about $\theta$ before we observe any data. A uniform prior is the simplest form and indicates that we have no preference for any particular value of $\theta$ between 0 and 1. Mathematically, it's written as:

$$
P(\theta) = 1 \quad \text{for } 0 \leq \theta \leq 1
$$
In fact, the uniform distribution, is a beta one one. And any beta distribution, is conjugate for the Bernoulli distribution. Any beta prior, will give a beta posterior.

Assuming uniform prior over $\theta$, which is a special case of Beta distribution $\theta \sim Beta(1,1)$

## Posterior Distribution

Bayes' theorem combines our prior beliefs with the observed data to update our belief about $\theta$:

$$
P(\theta \mid \mathbf{y}) = \frac{P(\mathbf{y} \mid \theta) P(\theta)}{P(\mathbf{y})}
$$

Where $\mathbf{y}$ represents the observed data.

Given a uniform prior and Bernoulli likelihood, the posterior distribution turns out to be a Beta distribution. The Beta distribution is particularly convenient because it's a conjugate prior for the Bernoulli likelihood, meaning the posterior distribution is also a Beta distribution.

The posterior distribution follows from the conjugacy of the Beta distribution:
$$
P(\theta \mid data) \sim Beta(\alpha + \sum y_i, \beta+ n-\sum y_i)
$$
where $\alpha$ and $\beta$ are the parameters of the prior. Since the prior is Beta(1,1), we set $\alpha=1$ and $\beta=1$ so





When we start with a uniform prior (which is $\text{Beta}(1, 1)$) and observe $n$ Bernoulli trials with $y$ successes, the posterior becomes:

$$
Beta(1 + \sum y_i, 1+n -\sum Y_i) 
$$

::: exerxise-box
In practice:

Return to the example of flipping a coin with unknown probability of heads $\theta$. If we use a Bernoulli likehood for each coin flip, ie.  
$$
P(y \mid \theta) = \theta^y (1 - \theta)^{1 - y} I_{\{ 0 \leq \theta \leq 1\}}
$$
and a uniform prior for $\theta$, what is the posterior distribution for $\theta$ if we observe the following sequence (Heads, Heads, Tails)

Sol:
$$
Beta(1 + \sum y_i, 1+n -\sum Y_i) = Beta(1 +2, 1+ 3-2) = Beta(3,2)
$$

*Suppose we perform 10 Bernoulli trials and observe 7 successes:*

-   Prior: $\text{Beta}(1, 1)$
-   Posterior: $\text{Beta}(1 + 7, 1 + (10 - 7)) = \text{Beta}(8, 4)$

This posterior distribution now reflects our updated belief about $\theta$ after observing the data.

*We repeat the example of flipping a coin with unkown probability of heads theta, but with a different prior.*
If we use a Bernoulli likelihood for each coin flip$
P(y_i \mid \theta) = \theta^y_i (1 - \theta)^{1 - y} I_{\{0 \leq \theta \leq 1 \}}$ and Beta(5,5) prior. What is the posterior distribution for $\theta$ if we observe the following sequence: (H,H,T)

$$
P(\theta \mid data) \sim Beta(\alpha + \sum y_i, \beta+ n-\sum y_i)
$$
$$
P(\theta \mid data) \sim Beta(5 + 2, 5+ 3-2)= Beta (7,6)
$$
:::

# The beta distribution as a conjugate prior

The Uniform(0,1) distribution is a special case of the Beta distribution, specifically: Beta(1,1)

More generally, any Beta distribution can be used as a prior for the Bernoulli likelihood.

The posterior distribution remains in the Beta family, making Bayesian inference much easier.

2. Deriving the Posterior Distribution

The posterior for $\theta$, given observed data $Y$, is proportional to:

$$P(\theta | Y) \propto \text{Likelihood} \times \text{Prior}$$

The likelihood function for $n$ Bernoulli trials is:

$$P(Y | \theta) = \theta^{\sum y_i} (1 - \theta)^{n - \sum y_i}$$

The Beta prior is:

$$
f(\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}  I_{\{ 0 \leq \theta \leq 1\}}
$$

After multiplying, the posterior remains a Beta distribution:

$\theta | Y  \sim Beta(\alpha + \sum y_i, \beta + n - \sum y_i)$

If we use a Uniform(0,1) prior (Beta(1,1)), the posterior simplifies to:

$Beta(1 + \sum y_i, 1 + n - \sum y_i)$

3. Conjugate Families and Their Benefits

A conjugate family means that if we choose a prior from a certain family, the posterior remains in that same family.

The Beta distribution is conjugate to both the Bernoulli and Binomial distributions, making it easy to update beliefs when new data arrives.

This avoids complex integrals that arise in Bayesian inference when using non-conjugate priors.

4. Hierarchical Bayesian Models

In a hierarchical Bayesian model, we introduce hyperparameters:

$\alpha$ and $\beta$ (parameters of the Beta prior)

We can assign priors to hyperparameters for more flexibility.

However, for simple problems, adding more hierarchy increases complexity without much benefit.

5. Conclusion

The Beta distribution is the natural prior for the Bernoulli and Binomial distributions.

Posterior updates follow the rule:

$Beta(\alpha + \sum y_i, \beta + n - \sum y_i)$

Conjugate priors make Bayesian inference easier by keeping calculations in a known form.

Hierarchical models add flexibility but can also make computations more complex.


The Beta distribution is a natural choice for a prior in Bayesian inference when working with Bernoulli or Binomial likelihoods.

The posterior distribution remains in the Beta family, which simplifies computations.

The prior Beta distribution is given by:

$$P(\theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}$$

If we start with a Uniform(0,1) prior, which is equivalent to Beta(1,1), the posterior remains a Beta distribution.

2. Posterior Distribution with Beta Prior

Given a Bernoulli likelihood with $n$ observations and a Beta prior $Beta(\alpha, \beta)$, the posterior follows:

$$\theta | Y \sim Beta(\alpha + \sum y_i, \beta + n - \sum y_i)$$

This shows how both the prior and observed data contribute to the posterior.

3. Posterior Mean and Effective Sample Size

The expectation (mean) of a Beta distribution is:

$$E[\theta] = \frac{\alpha}{\alpha + \beta}$$

For the posterior, the mean is given by:

$$E[\theta | Y] = \frac{\alpha + \sum y_i}{\alpha + \beta + n}$$

Rewriting this:
$$
E[\theta | Y] = \left(\frac{\alpha + \beta}{\alpha + \beta + n} \right) \cdot \left( \frac{\alpha}{\alpha + \beta} \right) + \left( \frac{n}{\alpha + \beta + n} \right) \cdot \left( \frac{\sum y_i}{n} \right)$$

This shows that the posterior mean is a weighted average of the prior mean and the sample mean.

The prior contributes $\alpha + \beta$ effective observations.

The data contributes $n$ actual observations.

If $\alpha + \beta$ is small compared to $n$, the posterior is mostly driven by the data.

4. Bayesian Credible Intervals

A frequentist 95% confidence interval for $\theta$ is:
$$
\hat{\theta} \pm 1.96 \sqrt{\frac{\hat{\theta} (1 - \hat{\theta})}{n}}
$$
In Bayesian statistics, a 95% credible interval is obtained directly from the posterior distribution. Using computational tools such as R, we can find the interval that contains 95% of the posterior probability mass, giving a true probability statement about $\theta$.

5. Sequential Updating in Bayesian Inference

One advantage of Bayesian inference is the ability to update beliefs sequentially:

Start with a prior $P(\theta)$.

Observe $n$ data points and compute the posterior $P(\theta | Y_1, \dots, Y_n)$.

When additional data arrives, treat the posterior as the new prior and update again.

Mathematically, updating in steps or all at once yields the same final posterior:
$$
P(\theta | Y_1, \dots, Y_n, Y_{n+1}, \dots, Y_{n+m}) = P(\theta | Y_1, \dots, Y_{n+m})
$$
This contrasts with frequentist methods, where analysis must be predefined and cannot be updated without altering results.

6. Application in Medical Device Testing

One of the first government-approved uses of Bayesian statistics was in medical device testing. Bayesian methods are ideal in this field because:

Sample sizes are small.

New device modifications require only small updates to existing trials.

Sequential analysis allows real-time adjustments without restarting the entire study.



