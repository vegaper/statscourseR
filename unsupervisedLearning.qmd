---
title: "Unsupervised Learning"
format: html
---

```{r}
library(dplyr)
library(ISLR2)
```
This chapter will focus on unsupervised learning, a set of statistical
tools intended for the setting in which we have only a set of features
X1,X2, . . . ,Xp measured on n observations. We are not interested
in prediction, because we do not have an associated response variable Y .
Rather, the goal is to discover interesting things about the measurements
on X1,X2, . . . ,Xp. Is there an informative way to visualize the data? Can
we discover subgroups among the variables or among the observations?
Unsupervised learning refers to a diverse set of techniques for answering
questions such as these. In this chapter, we will focus on two particular
types of unsupervised learning: principal components analysis, a tool
used for data visualization or data pre-processing before supervised techniques
are applied, and clustering, a broad class of methods for discovering
unknown subgroups in data.
Unsupervised learning is often
performed as part of an exploratory data analysis.
Examples of goals of unsupervised learning can be:
An online shopping site might try
to identify groups of shoppers with similar browsing and purchase histories,
as well as items that are of particular interest to the shoppers within
each group. Then an individual shopper can be preferentially shown the
items in which he or she is particularly likely to be interested, based on
the purchase histories of similar shoppers.
A search engine might choose
which search results to display to a particular individual based on the click
histories of other individuals with similar search patterns. These statistical
learning tasks, and many more, can be performed via unsupervised learning
techniques.

# Principal Components Analysis (PCA)
We briefly discussed [Principal Component](modelAccuracy.html#dimension-reduction-methods) in the context of regression

Suppose that we wish to visualize n observations with measurements on a set of p features, $X_1,X_2,\dots,X_p$, as part of an exploratory data analysis.
We could do this by examining two-dimensional scatterplots of the data, each of which contains the n observationsâ€™ measurements on two of the features. However, there are many possible combinations of those features. 
We would like to find a low-dimensional representation of the data that captures as much of the information as possible. If we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this low-dimensional space.

PCA produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.
Frist we normalize our features.

$$
z = \frac{x-\bar x}{sd(x)}
$$


our starting point is a matrix $n \times p$ where $p$ is the number of features and $n$ is the number of observations. 
$$

X_{n,p} = \begin{pmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p}
\end{pmatrix}

$$
We want to visualize our features on a reduced dimensional space while still representative of the level of variance of the data. In order to do this, we reduce $p$ to a lower value $M$
so we convert the linear combination of the original features:
$$
\beta_{11} x_{i1} + \beta_{21} x_{i2} + \cdots + \beta_{p1} x_{ip}
$$

into:
$$
Z_m = \sum_{j=1}^{p} \phi_{jm} X_j = \phi_{1m} X_1 + \phi_{2m} X_2 + \cdots + \phi_{pm} X_p
$$


The first principal components of a set of features $X_1,X_2,\dots,X_p$ is the normalized linear combination of the features that has the largest variance. By normalized, we mean that the sum of the squares of phi is 1
$$
Z_1 = \phi_{11}X_1+\phi_{21}X_2+\dots+\phi_{p1}X_p
$$
where $\sum_{j=1}^p \phi^2_{j1}=1$. We constrain this way otherwise setting these elements to be arbitrary large in absolute value could result in an arbitrary large variance.

We refer to the $\phi$ elements as the loadings of the first principal component, together, the loadings make up the principal component loading vector $\phi_1=(\phi_{11} \phi_{21}\dots \phi_{p1})^T$

Suppose we have a $n \times p$ data set $X$. Since we are only interested in variance, we assume that each of the variables in $X$ has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form:
$$
z_1 = \phi_{11}x_1+\phi_{21}x_2+\dots+\phi_{p1}x_p
$$
for $i=1,\dots,n$ that has the largest sample variance, subject to the constraint $\sum_{j=1}^p \phi^2_{j1}=1$.
Since each of the $x_{ij}$ has mean zero, then so does $z_i1$ so the sample variance of the $z_i1$ can be written as $\frac{1}{n}\sum^n_{i=1}z^2_{i1}$ so now we need to maximize that variance:
$$
\begin{equation}
\begin{aligned}
& \text{maximize}_{\phi_{11}, \ldots, \phi_{p1}} \left\{ \frac{1}{n} \sum_{i=1}^{n} \left( \sum_{j=1}^{p} \phi_{j1} x_{ij} \right)^2 \right\} \\
& \text{subject to} \quad \sum_{j=1}^{p} \phi_{j1}^2 = 1.
\end{aligned}
\end{equation}

$$

This problem can be solved via singular-value decomposition of the matrix X, a standard technique in linear algebra.


The second principal component is the linear combination of $X_1,X_2,\dots,X_p$ that has maximal variance among all linear combinations that are *uncorrelated* with $Z_1$
$$
z_2 = \phi_{12}x_1+\phi_{22}x_2+\dots+\phi_{p2}x_p
$$
There are at most min(n-1,p) principal components, where n is the number of data points and p the number of features.

## Proportion Variance Explained
To understand the strength of each component, we are interested in knowing the proportion of variance explained (PVE) by each one. 
The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as 
$$
\sum^p_{j=1}Var(X_j) = \sum^p_{j=1}\frac{1}{n}\sum^n_{i=1}x^2_{ij}
$$
and the variance explained by the $m$th principal component is :
$$
Var(Z_m) = \frac{1}{n}\sum^n_{i=1}z^2_{im}
$$
and 
$$
\sum^p_{j=1}Var(X_j) = \sum ^M_{m=1} Var(Z_m)
$$
therefore, the PVE of the $m$th principal component is given by the positive quantity between 0 and 1, because we can talk about the proportion of variance
explained by looking at the variance of an individual z relative to the sum of the variances of all of the components. And that gives you an idea of the importance of each of the components.

:::{.exercise-box}
Principal Components

We are going to take a look at the `USArrests` data set, which is part of the base `R` package as an example for this lecture. For each of the fifty states in the United States, the data set constrains the number of arrest per 100k residents for each of three crimes: Assault, Murder and Rape. We also record the percentage of the population in each state living in urban areas (UrbanPop).
The principal component score vectors have length $n=50$ and the principal component loading vectors have length $p=4$.
We standardize each variable to have mean 0 and standard deviation 1

The columns of the data set contain the four variables.

```{r }
names(USArrests)
```
We first briefly examine the data. We notice that the variables have vastly different means.

```{r }
apply(USArrests, 2, mean)
```

Note that the `apply()` function allows us to apply a function---in this case, the `mean()` function---to each row or column of the data set. The second input here denotes whether we wish to compute the mean of the rows, $1$, or the columns, $2$. We see that there are on average three times as many rapes as murders, and more than eight times as many assaults as rapes.
We can also examine the variances of the four variables using the `apply()` function.

```{r }
apply(USArrests, 2, var)
apply(USArrests, 2, sd)
```

Not surprisingly, the variables also have vastly different variances:
The `UrbanPop` variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes
in each state per 100,000 individuals.
If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the `Assault` variable, since it has by far the largest mean and variance.
Thus, it is important to standardize the variables to have mean zero and standard deviation one before performing PCA.

We now perform principal components analysis using the `prcomp()` function, which is one of several functions in `R` that perform PCA.

```{r }
pr.out <- prcomp(USArrests, scale = TRUE)
```

By default, the `prcomp()` function centers the variables to have mean zero. By using the option `scale = TRUE`, we scale the variables to have standard deviation one. The output from
`prcomp()` contains a number of useful quantities.

```{r }
names(pr.out)
```

The `center` and `scale` components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.

```{r }
pr.out$center
pr.out$scale
```

The `rotation` matrix provides the principal component *loadings*;
each column of `pr.out$rotation` contains the corresponding principal component loading vector.


```{r }
pr.out$rotation
```
We see that there are four distinct principal components. This is to be expected because there are in general $\min(n-1,p)$ informative principal components in a data set with $n$ observations and $p$ variables.

The sign of the principal components is irrelevant, so being negative does not mean anything because we are measuring variance. 

We see that the first principal component is mainly loaded with the crime stats, while the second one is more loaded by the urban population. 

Using the `prcomp()` function, we do not need to explicitly multiply the data by the principal component loading vectors  in order to obtain the principal component score vectors. Rather the $50 \times 4$ matrix `x` has as its columns the principal component score vectors. That is, the $k$th column is the $k$th principal component score vector.

```{r }
dim(pr.out$x)
```

We can plot the first two principal components as follows:

```{r fig.align='center', fig.width=10, fig.height=10}
biplot(pr.out, scale = 0)
```

The `scale = 0` argument to `biplot()` ensures that the arrows are scaled to represent the loadings; other values for `scale` give slightly different biplots with different interpretations.

In this case the component was negative, it had negative loading. Ans so negative scores in that component means negative times negative is positive so the state of Florida is high crime while being in the end of the negative x axis. 

The Y axis is showing the second component so New Yersey has a high urban population while Arkansas is on the lower side. 

We can alter that to see a more human readable representation:

```{r fig.align='center', fig.width=7, fig.height=7}
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)
```

The `prcomp()` function also outputs the standard deviation of each principal component. For instance, on the `USArrests` data set, we can access these standard deviations as follows:

```{r }
pr.out$sdev
```
The variance explained by each principal component is obtained by squaring these:

```{r }
pr.var <- pr.out$sdev^2
pr.var
```

To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components:

```{r}
pve <- pr.var / sum(pr.var)
pve
```

We see that the first principal component explains $62.0 \%$ of the variance in the data, the next principal component explains $24.7 \%$ of the variance, and so forth.
 We can plot the PVE explained by each component, as well as the cumulative PVE, as follows:

```{r , fig.align='center', fig.width=7}
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
    ylab = "Proportion of Variance Explained", ylim = c(0, 1),
    type = "b")
plot(cumsum(pve), xlab = "Principal Component",
    ylab = "Cumulative Proportion of Variance Explained",
    ylim = c(0, 1), type = "b")
```

 The result is shown in Figure 12.3.
Note that the function `cumsum()` computes the cumulative sum of the elements of  a numeric vector.
:::

# Missing Values and Matrix completion

It is often the case that data matrices have missing entries that are often represented as NAs. These are nuisance, since many of our modeling procedures such as linear regression, GLMs and principal components require complete data. 
One simple approach is *mean imputation* where you replace the missing values for a variable by the mean of the non-missing entries. This ignores the correlations among variables, and we should be able to exploit these correlations among variables when imputing missing variables. This also assumes that the missing values are missing at random.

The idea is that we are going to use the Principal Components model to do the imputation. 
We have a matrix A $n \times m$ $m$ being the number of rows and we have a matrix B which is $p \times m$ $p$ being the number of columns or variables. Because principal components uses the correlation between the variables, when finding missing values we can impute the value of those like doing a regression over the known correlated variables. 
[I skipped the mathematical explanation for this because it was too complicated, if you are interested, check in the ISLR2 book]

**Steps**:

Standardize the Values:

Ensure that each feature has a mean of zero and a standard deviation of one to standardize the data.

Initialize Missing Values:

Create a complete data matrix 
ð‘‹
Ë‰
 by filling in the missing values using mean imputation (the mean of the respective column).

Iterate PCA and Reconstruction Steps:

Repeat steps ð‘Ž toð‘until the objective in ð‘ fails to decrease:

a) Apply PCA:

Solve the matrix approximation by computing the principal components of 
$\bar X$
Ë‰
$$

\begin{equation}
\text{minimize} \quad \left\{ \sum_{j=1}^{p} \sum_{i=1}^{n} \left( \tilde{x}_{ij} - \sum_{m=1}^{M} a_{im} b_{jm} \right)^2 \right\}
\end{equation}
$$
$$
A \in \mathbb{R}^{n \times M}, \quad B \in \mathbb{R}^{p \times M}
$$



b) Reconstruct the Data:

For each missing entry:

Use the principal components to reconstruct the dataset. The principal components are linear combinations of the original variables that capture the maximum variance.

Replace the missing values with the corresponding values from the reconstructed dataset.

$$ (i, j) \notin \mathcal{O}, \text{ set } \tilde{x}{ij} \leftarrow \sum{m=1}^{M} \hat{a}{im} \hat{b}{jm}. $$

c) Compute the Objective:

$$ \sum_{(i,j) \in \mathcal{O}} \left( x_{ij} - \sum_{m=1}^{M} \hat{a}{im} \hat{b}{jm} \right)^2 $$
This represents the sum of squared differences between observed values $x_{ij}$ and their estimates based on matrix factors $\hat a_{im}$ and $\hat b_{jm}$ within the set $\mathcal{O}$

3. Finalizing Imputed Data:
Once the iterative process converges and the objective no longer decreases, the final reconstructed dataset with imputed values is obtained.

Example in r
```{r}
#load the softImpute package

library(softImpute)

# Example data with missing values
data <- matrix(c(1, 2, NA, 4, 5, 6, 7, NA, 9), nrow = 3, byrow = TRUE)

# Apply softImpute to the data
fit <- softImpute(data, rank.max = 2, lambda = 1)

# Impute the missing values
completed_data <- complete(data, fit)

# View the imputed data
completed_data

```
:::{.exercise-box}
Missing values and Matrix Completion

We now re-create the analysis carried out on the `USArrests` data using matrices.

The USArrest data has only four variables, which is on the low end for this method to work well. For this reason, for this demonstration we randomly set at most one variable per state to be missing, and only used M=1 principal component.
In general, in order to apply this algorithm, we must select M, the number of principal components to use for the imputation. 
One approach is to randomly set to NA some elements that were actually observed, and select M based on how well those known values are recovered. This is closely related to the cross validation approach. 

 We turn the data frame into a matrix, after centering and scaling each column to have mean zero and variance one.

```{r }
X <- data.matrix(scale(USArrests))
pcob <- prcomp(X)
summary(pcob)
```

We see that the first principal component explains $62\%$ of the variance. 
Solving the optimization on a centered data matrix $\bf X$ is equivalent to computing the first $M$ principal
components of the data. The *singular value decomposition* (SVD)  is a general algorithm for this.

```{r chunk18}
sX <- svd(X)
names(sX)
round(sX$v, 3)
```

The `svd()` function returns three components, `u`, `d`, and `v`. The matrix `v` is equivalent to the
loading matrix from principal components (up to an unimportant sign flip).

```{r chunk19}
pcob$rotation
```

The matrix `u` is equivalent to the matrix of *standardized* scores, and the standard deviations are in the vector `d`. We can recover the score vectors using the output of `svd()`.
They are identical to the score vectors output by `prcomp()`.
Here we print the first 10 rows 

```{r chunk20}
t(sX$d * t(sX$u))[1:10,]
pcob$x[1:10,]
```

While it would be possible to carry out this lab using the `prcomp()` function, here we use the `svd()` function in order to illustrate its use.


We now omit 20 entries in the $50\times 4$ data matrix at random. We do so
by first selecting 20 rows (states) at random, and then selecting one
of the four entries in each row at random. This ensures that every row has
at least three observed values.


```{r chunk21}
nomit <- 20
set.seed(15)
ina <- sample(seq(50), nomit)
inb <- sample(1:4, nomit, replace = TRUE)
Xna <- X
index.na <- cbind(ina, inb)
Xna[index.na] <- NA
```

Here, `ina` contains 20 integers from 1 to 50; this represents the states that are selected to contain missing values. And `inb` contains 20 integers from 1 to 4, representing the features that contain the missing values for each of the selected states. To perform the final indexing, we create `index.na`, a two-column matrix whose columns are `ina` and `inb`. We have indexed a matrix with a matrix of indices!

We now write a  function that takes in a matrix, and returns an approximation to the matrix using the `svd()` function.
This will be needed in Step 2.  As mentioned earlier, we could do this using the `prcomp()` function, but instead we use the `svd()` function for illustration.

```{r chunk22}
fit.svd <- function(X, M = 1) {
   svdob <- svd(X)
   with(svdob,
       u[, 1:M, drop = FALSE] %*%
       (d[1:M] * t(v[, 1:M, drop = FALSE]))
     )
}
```

Here, we did not bother to explicitly call the `return()` function to return a value from `fit.svd()`; however,
the computed quantity is automatically returned by `R`.  We use the `with()` function to
make it a little easier to index the elements of `svdob`. 

To conduct Step 1 of the algorithm, we initialize `Xhat` -- this is $\tilde{\bf X}$ -- by replacing
the missing values with the column means of the non-missing entries.

```{r chunk24}
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]
```


Before we begin Step 2, we set ourselves up to measure the progress of our iterations:

```{r chunk25}
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)
```

Here  `ismiss` is a new logical matrix with the same dimensions as `Xna`; a given element equals `TRUE` if the corresponding matrix element is missing. This is useful because it allows us to access both the missing and non-missing entries. We store the mean of the squared non-missing elements in `mss0`.
We store the mean squared error  of the non-missing elements  of the old version of `Xhat` in `mssold`. We plan to store the mean squared error of the non-missing elements of the current version of `Xhat` in `mss`, and will then  iterate Step 2 of Algorithm until the *relative error*, defined as `(mssold - mss) / mss0`, falls below `thresh = 1e-7`. (until is no longer decreasing. Determining whether is decreasing requires us only to keep track of `mssold - mss`. However, in practice, we keep track of `(mssold - mss) / mss0` instead: this makes it so that the number of iterations required for Algorithm to converge does not depend on whether we multiplied the raw data $\bf X$ by a constant factor. )


In Step 2(a) of Algorithm, we  approximate `Xhat` using `fit.svd()`; we call this `Xapp`.   In Step 2(b), we  use `Xapp`  to update the estimates for elements in `Xhat` that are missing in `Xna`. Finally, in Step 2(c), we compute the relative error. These three steps are contained in this `while()` loop:

```{r chunk26}
while(rel_err > thresh) {
    iter <- iter + 1
    # Step 2(a)
    Xapp <- fit.svd(Xhat, M = 1)
    # Step 2(b)
    Xhat[ismiss] <- Xapp[ismiss]
    # Step 2(c)
    mss <- mean(((Xna - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss) / mss0
    mssold <- mss
    cat("Iter:", iter, "MSS:", mss,
      "Rel. Err:", rel_err, "\n")
    }
```

We see that after eight iterations, the relative error has fallen below `thresh = 1e-7`, and so the algorithm terminates. When this happens, the mean squared error of the non-missing elements equals $0.369$.

Finally, we compute the correlation between the 20 imputed values
and the actual values:

```{r }
cor(Xapp[ismiss], X[ismiss])
```
In this lab, we implemented Algorithm ourselves for didactic purposes. However, a reader who wishes to apply matrix completion to their data should use the `softImpute` package on `CRAN`, which provides a very efficient implementation of a generalization of this algorithm.
:::

# Clustering
Clustering refers to a very broad set of techniques for finding subgroups or clusters in a data set.
We seek a partition of the data into distinct groups so that the observation within each group are quite similar to each other. To make this concrete, we must define what it means for two or more observations to be similar or different. This is often a domain-specific consideration that must be made based on knowledge of the data being studied.

Suppose we have access to a laerge number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people.
Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or morle likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.
One popular way of dong this is *$k$-means clustering*. in which we seek to partition the observations into a pre-specified number of clusters.
Another popular way is *hierarchical clustering* where we do not know in advance how many clusters we want, we end up with a tree-like visual representation of the observations, called a *dendogram*, that allows us to view at once the clustering obtained for each possible number of clusters from 1 to $n$

## k-means clustering

Let's define $C_1,\dots,C_k$ clusters that holds the indices of each of the observations pertaining to that cluster. There is no overlap between the clusters, so one observations can only pertain to one clusters, and all the observations are registered in one of the clusters. 
The idea behind $K$-meand clustering is that a *good clustering* is one for which the within-cluster variation is as small as possible. The within-cluster variation (WCV) for cluster $C_k$ is a measure WCV($C_k$) of the amount by which the observations within a cluster differ from each other. 
WE want to minimize: 

$$
\begin{equation}
\begin{aligned}
& \text{minimize}_{C_1, \ldots, C_k} \left\{ \sum_{k=1}^{K} WCV(C_k) \right\} 
\end{aligned}
\end{equation}
$$
Most of the time to use euclidean distance for this: we define WCV to be the pair wise squared distance between each pair of observations in the cluster.

First we randomly assign a number from 1 to $k$ to each of the observations. These serve as initial cluster assignments for the observations. 
Next we iterate until the cluster assignments stop changing:
 - For each of the $K$ clusters, compute the cluster *centroid*. The $k$th cluster centroid is the vector of the $p$ feature means for the observations in the $k$th cluster. 
 - Assign each observation to the cluster whose centroid is closest (euclidean distance)

This method is guaranteed to decrease the value of the objective at each step, however, it is not guaranteed to give the global minimum. 
 
```{r, echo=FALSE, fig.align='center'}
# Load necessary library
library(ggplot2)

# Define a function with both a local minimum and a global minimum
f <- function(x) {
  sin(x) + 0.05 * (x^2 - 10 * x)
}

# Generate a sequence of x values
x_values <- seq(-10, 10, by = 0.1)

# Compute the corresponding y values
y_values <- f(x_values)

# Create a data frame for plotting
df <- data.frame(x = x_values, y = y_values)

# Find the global minimum
global_min <- df[df$y == min(df$y), ]

# Find a local minimum in a specific range (e.g., between 0 and 5)
local_range <- df[df$x > -3 & df$x < 1, ]
local_min <- local_range[local_range$y == min(local_range$y), ]

# Plot the function
ggplot(df, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  geom_point(data = global_min, aes(x = x, y = y), color = "red", size = 3) + # Global minimum
  geom_point(data = local_min, aes(x = x, y = y), color = "green", size = 3) + # Local minimum
  labs(title = "Function with Local and Global Minimum",
       x = "x",
       y = "f(x)") +
  theme_minimal()

```
When the function is not convex and has more than one minimum, it is possible to get different solutions for the algorithm when we run it because the starting points are random, so they may pick up different valleys. 

:::{.exercise-box}
$K$-Means Clustering


The function `kmeans()` performs $K$-means clustering in
`R`.  We begin with a simple simulated example in which there
truly are two clusters in the data: the first 25 observations have a
mean shift relative to the next 25 observations.

```{r , fig.align='center'}
set.seed(2)
x <- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
head(x)
plot(x)
```

We now perform $K$-means clustering with $K=2$.

```{r chunk29}
km.out <- kmeans(x, 2, nstart = 20)
```

The cluster assignments of the 50 observations are contained in  `km.out$cluster`.

```{r chunk30}
km.out$cluster
```

The $K$-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to `kmeans()`. We can plot the data, with each observation colored according to its cluster assignment.

```{r fig.align='center'}
plot(x, col = (km.out$cluster + 1),
    main = "K-Means Clustering Results with K = 2",
    xlab = "", ylab = "", pch = 20, cex = 2)
```

Here the observations can be easily plotted because they are two-dimensional. If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors.

In this example, we knew that there really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters. We could instead have performed $K$-means clustering on this example with $K=3$.

```{r fig.align='center'}
set.seed(4)
km.out <- kmeans(x, 3, nstart = 20)
km.out
plot(x, col = (km.out$cluster + 1),
    main = "K-Means Clustering Results with K = 3",
    xlab = "", ylab = "", pch = 20, cex = 2)
```

When $K=3$, $K$-means clustering  splits up the two clusters.

To run the `kmeans()` function in `R` with multiple initial cluster assignments, we use the `nstart` argument. If a value of `nstart` greater than one is used, then $K$-means clustering will be performed using multiple random assignments in Step~1 of Algorithm, and the `kmeans()` function will report only the best results. Here we compare using `nstart = 1` to `nstart = 20`.

```{r chunk33}
set.seed(4)
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(x, 3, nstart = 20)
km.out$tot.withinss
```

Note that `km.out$tot.withinss` is the total within-cluster sum of squares, which  we seek to minimize by performing $K$-means clustering. The individual within-cluster sum-of-squares are contained in the vector `km.out$withinss`.

We *strongly* recommend always running $K$-means clustering with a large value of `nstart`, such as 20 or 50, since otherwise an undesirable local optimum may be obtained.

When performing $K$-means clustering, in addition to using multiple initial cluster assignments, it is
also  important to set a random seed using the `set.seed()` function. This way, the initial cluster assignments in Step~1 can be replicated, and the $K$-means output will be fully reproducible.
:::

## Hierarchical Clustering

$k$-means clustering requires us to pre-specify the number of clusters $K$, this can be a disadvantage.
Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of $K$.
In this section we describe *bottom-up* or *agglomerative* clustering. This is the most common type of hierarchical clustering, and refers to the fact that a dendogram is built starting from the leaves and combining clusters up to the trunk. 

It first find the closest pair of observations and then it looks for the next closest pair, and it continues like this except that it does not necessarily join only pairs of observations, it may also join an observation with an existing cluster or two clusters together. 
To join clusters together it can use four techniques, *complete linkage* would be using the distance from the points in each cluster that are the furthest apart, while *single linkage* does the opposite and measures the distance from the closest points in each cluster. *Average linkage* will use the mean between each pair of points in each clusters and *centroid linkage* calculates the centroid of each cluster and measure the distance between the centroids of the clusters. The most used methods are complete and average

When we view the dendogram, the height of the lines is proportional to the distances between the points, so the smallest the distance, the smallest the lines in the dendogram.

**Choice of Dissimilarity Measure** 

So far we have talked about euclidean distance. An alternative is *correlation-based distance* which considers two observations to be similar if their features are highly correlated. This is an unusual use of correlation, which is normally computed between variables, here it is computed between observations profiles for each pair of observations. This is often used when features are actually measurements at different times, so you can think of each measurement for an individual as a series of points over time in a time series.

**Practical considerations**
As in principal components and $K$-means, scaling variables matters. Should the observations of features be standardized in some way? For instance, maybe the variables should be centered to have mean zero and scaled to have standard deviation one. If the variables are not in the same units, you should standardize them. If they are in the same unit, it is useful to do both, a trial standardizing them and another one leaving them as they are. 

Even though in hierarchical clustering you don't choose the number of clusters to begin with, at some point you will need to look at the dendogram and choose the division that makes sense to you. There is no mathematical process that can do that choice for you.  



:::{.exercise-box}
Hierarchical clustering

The `hclust()` function implements  hierarchical clustering in `R`. In the following example we use the data from the previous lab to plot the hierarchical clustering dendrogram using `complete`, `single`, and `average` linkage clustering, with Euclidean distance as the dissimilarity measure.
We begin by clustering observations using complete linkage. The `dist()` function is used to compute the $50 \times 50$ inter-observation Euclidean distance matrix.

```{r, fig.align='center', fig.height=6, fig.width=7}
hc.complete <- hclust(dist(x), method = "complete")
plot(hc.complete)
```

We could just as easily perform hierarchical clustering with average or single linkage instead:

```{r, fig.align='center', fig.height=6, fig.width=10}
hc.average <- hclust(dist(x), method = "average")


hc.single <- hclust(dist(x), method = "single")

```

The numbers at the bottom of the plot identify each observation.

```{r, fig.align='center', fig.height=6, fig.width=14}
par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage",
    xlab = "", sub = "", cex = .9)
plot(hc.average, main = "Average Linkage",
    xlab = "", sub = "", cex = .9)
plot(hc.single, main = "Single Linkage",
    xlab = "", sub = "", cex = .9)
```


To determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the `cutree()` function:
The second argument to `cutree()` is the number of clusters we wish to obtain.

```{r chunk37}
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
```
For this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage identifies one point as belonging to its own cluster. A more sensible answer is obtained when four clusters are selected, although there are still two singletons.

```{r chunk38}
cutree(hc.single, 4)
```
To scale the variables before performing hierarchical clustering of the observations, we use the `scale()` function:

```{r , fig.align='center', fig.height=6, fig.width=7}
xsc <- scale(x)
plot(hclust(dist(xsc), method = "complete"),
    main = "Hierarchical Clustering with Scaled Features")
```

Correlation-based distance can be computed using the `as.dist()` function, which converts an arbitrary square symmetric matrix into a form that the `hclust()` function recognizes as a distance matrix. However, this only makes sense for data with at least three features since the absolute correlation between any two observations
with measurements on two features is always 1. Hence, we will cluster a three-dimensional data set. This data set does not contain any true clusters.

```{r , fig.align='center', fig.height=6, fig.width=7}
x <- matrix(rnorm(30 * 3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))

plot(hclust(dd, method = "complete"),
    main = "Complete Linkage with Correlation-Based Distance",
    xlab = "", sub = "")
```

:::



# NCI60 Data Example


Unsupervised techniques are often used in the analysis of genomic data. In particular, PCA and hierarchical clustering are popular tools.
 We  illustrate these techniques on the `NCI` cancer cell line microarray data, which consists of $6{,}830$ gene expression measurements on $64$ cancer cell lines.

```{r}

nci.labs <- NCI60$labs
nci.data <- NCI60$data
```

Each cell line is labeled with a cancer type, given in `nci.labs`. We do not make use of the cancer types in performing PCA and clustering, as these are unsupervised techniques. But after performing PCA and clustering, we will check to see the extent to which these cancer types agree with the results of these unsupervised techniques.

The data has $64$ rows and $6{,}830$ columns.

```{r chunk42}
dim(nci.data)
nci.data[1:10,1:10]
```
The values in the NCI-60 Human Tumor Cell Lines dataset represent gene expression measurements. Each float number corresponds to the expression level of a specific gene in a particular cancer cell line. These measurements indicate how active or inactive a gene is in each cell line. In simpler terms, the numbers tell you how much of each gene is being expressed in the different cancer cell lines, which can help researchers identify patterns and differences in gene expression across various types of cancer.

We begin by examining the cancer types for the cell lines. `nci.labs` have the cancer type and `nci.data` the cells in the sample

```{r chunk43}
nci.labs[1:4] #first 4 types
table(nci.labs) 

```
## PCA on the NCI60 Data

We first perform PCA on the data after scaling the variables (genes) to have standard deviation one, although one could reasonably argue that it is better not to scale the genes.

```{r chunk44}
pr.out <- prcomp(nci.data, scale = TRUE)
```

We now  plot the first few principal component score vectors, in order to visualize the data. The observations (cell lines) corresponding to a given cancer type will be plotted in the same color, so that we can see to what extent the observations within a cancer type are similar to each other. We first create a simple function that assigns a distinct color to each element of a numeric vector.
The function will be used to assign a  color to each of the $64$ cell lines, based on the cancer type to which it corresponds.

```{r chunk45}
Cols <- function(vec) {
   cols <- rainbow(length(unique(vec)))
   return(cols[as.numeric(as.factor(vec))])
 }
```

Note that the `rainbow()` function takes as its argument a positive integer, and returns a vector containing that number of distinct colors.  We now can plot the principal component score vectors.

Let's take a look first at the first 10 rows and columns of the principal components data:

```{r }
pr.out$x[1:10,1:10]
```

the first and second principal components
```{r }
pr.out$x[1:10, 1:2]
```

the first and third principal components
```{r }
pr.out$x[1:10, c(1, 3)]
```

```{r , fig.align='center', fig.width=14, fig.height=6}
par(mfrow = c(1, 2))
plot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19,
    xlab = "Z1", ylab = "Z2")

plot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19,
    xlab = "Z1", ylab = "Z3")
legend("topright", legend = unique(nci.labs), col = unique(Cols(nci.labs)), pch = 19)

```

 On the whole, cell lines corresponding to a single cancer type do tend to have similar values on the first few principal component score vectors. This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels.

We can obtain a summary of the proportion of variance explained (PVE) of the first few principal components using the `summary()` method for a `prcomp` object (we have truncated the printout):

```{r chunk47}
summary(pr.out)
```

Using the `plot()` function, we can also plot the variance explained by the first few principal components.

```{r fig.align='center', fig.width=7}
plot(pr.out)
```

Note that the height of each bar in the bar plot is given by squaring the corresponding element of `pr.out$sdev`.
However, it is more informative to plot the PVE of each principal component (i.e. a scree plot) and the cumulative PVE of each principal component. This can be done with just a little work.

```{r, fig.align='center', fig.width=7}
pve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev^2)
par(mfrow = c(1, 2))
plot(pve,  type = "o", ylab = "PVE",
    xlab = "Principal Component", col = "blue")
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE",
    xlab = "Principal Component", col = "brown3")
```

(Note that the elements of `pve` can also be computed directly from the summary, `summary(pr.out)$importance[2, ]`, and the elements of `cumsum(pve)` are given by `summary(pr.out)$importance[3, ]`.)
We see that together, the first seven principal components explain around $40 \%$ of the variance in the data. This is not a huge amount of the variance. However, looking at the scree plot, we see that while each of the first seven principal components explain a substantial amount of  variance, there
is a marked decrease in the variance explained by further principal components. That is, there is an *elbow* in the plot after approximately the seventh principal component.
This suggests that there may be little benefit to examining more than seven or so principal components (though even examining seven principal components may be difficult).

To see the loadings (here we only plot the first 10 rows of the first 10 PC:

```{r}
# Loadings
abs(pr.out$rotation[1:10,1:10])
```
Interpreting Loadings: The loadings tell you which variables (genes) are most influential in each principal component. High absolute values of loadings indicate variables that strongly influence that principal component.
By examining the loadings and the variance explained by each principal component, you can understand which variables (genes) are driving the differences in your dataset.


## Clustering the Observations of the NCI60 Data

We now proceed to hierarchically cluster the cell lines in the `NCI` data, with the goal of finding out whether or not the observations cluster into distinct types of cancer. To begin, we standardize the variables to have mean zero and standard deviation one. As mentioned earlier, this step is optional and should be performed only if we want each gene to be on the same *scale*.

```{r chunk50}
sd.data <- scale(nci.data)
```

We now perform hierarchical clustering of the observations using complete, single, and average linkage. Euclidean distance is used as the dissimilarity measure.

```{r, fig.align='center', fig.width=10, fig.height=7}
#par(mfrow = c(1, 3))
data.dist <- dist(sd.data)
plot(hclust(data.dist), xlab = "", sub = "", ylab = "",
    labels = nci.labs, main = "Complete Linkage")
plot(hclust(data.dist, method = "average"),
    labels = nci.labs, main = "Average Linkage",
    xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"),
    labels = nci.labs,  main = "Single Linkage",
    xlab = "", sub = "", ylab = "")
```

We see that the choice of linkage certainly  does affect the results obtained. Typically, single linkage will tend to yield *trailing* clusters: very large clusters onto which individual observations attach  one-by-one. On the other hand, complete and average linkage tend to yield more balanced, attractive clusters. For this reason, complete and average linkage are generally preferred to single linkage.
Clearly cell lines within a single cancer type do tend to cluster together, although the clustering is not perfect. We will use complete linkage hierarchical clustering for the analysis that follows.


We can cut the dendrogram at the height that will yield a particular number of clusters, say four:

```{r chunk52}
hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out, 4)
table(hc.clusters, nci.labs)
```

There are some clear patterns. All the leukemia cell lines fall in cluster $3$, while the breast cancer cell lines are spread out over three different clusters.  We can plot the cut on the dendrogram that produces these four clusters:

```{r fig.align='center', fig.width=10, fig.height=6}
par(mfrow = c(1, 1))
plot(hc.out, labels = nci.labs)
abline(h = 139, col = "red")
```

The `abline()` function draws a straight line on top of any existing plot in~`R`. The argument `h = 139` plots a horizontal line at height $139$ on the
dendrogram; this is the height that results in four distinct clusters. It is easy to verify that the resulting clusters are the same as the ones we obtained using `cutree(hc.out, 4)`.

 
Printing the output of `hclust` gives a useful brief summary of the object:

```{r chunk54}
hc.out
```
We claimed earlie that $K$-means clustering and hierarchical clustering with the dendrogram cut to obtain the same number of clusters can yield very different results.

How do these `NCI` hierarchical clustering results compare to what we  get if we perform $K$-means clustering with $K=4$?

```{r chunk55}
set.seed(2)
km.out <- kmeans(sd.data, 4, nstart = 20)
km.clusters <- km.out$cluster
table(km.clusters, hc.clusters)
```

We see that the four clusters obtained using hierarchical clustering and $K$-means clustering  are somewhat different. Cluster $4$ in $K$-means clustering is identical to cluster $3$ in hierarchical clustering. However, the other clusters differ: for instance, cluster $2$ in $K$-means clustering contains a portion of the observations assigned to cluster 1 by hierarchical clustering, as well as all of the observations assigned to cluster $2$ by hierarchical clustering.

Rather than performing hierarchical clustering on the entire data matrix, we can simply perform hierarchical clustering on the first few principal component score vectors, as follows:

```{r, fig.align='center', fig.width=10, fig.height=6}
hc.out <- hclust(dist(pr.out$x[, 1:5]))
plot(hc.out, labels = nci.labs,
    main = "Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out, 4), nci.labs)
```

 Not surprisingly, these results are different from the ones that we
 obtained when we performed hierarchical clustering on the full data
 set. Sometimes performing clustering on the first few principal
 component score vectors can give better results than performing
 clustering on the full data.  In this situation, we might view the principal
component step as one of denoising the data.
We could also perform $K$-means clustering on the first few principal component score vectors rather
 than the full data set.
:::
