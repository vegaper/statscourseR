---
title: "Tree Based Methods"
format: html
editor: visual
---

```{r}
#| echo: false
library(dplyr)
library(tidyverse)
library(readxl)
library(ggplot2)
library(ISLR2) #datasets
library(boot) #crossvalidation
library(tree)
library(randomForest)
theme_set(theme_minimal())
options(scipen= 999)
```

# Tree-based methods

In this chapter, we describe tree-based methods for regression and classification. These involve stratifying or segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode response value for the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods. Tree-based methods are simple and useful for interpretation. However, they typically are not competitive with the best supervised learning approaches, such as those seen already, in terms of prediction accuracy. Hence in this chapter we also introduce bagging, random forests, boosting, and Bayesian additive regression trees. Each of these approaches involves producing multiple trees which are then combined to yield a single consensus prediction. We will see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation.

Let's look at the Baseball salary data. We are focusing in two variables to predict the Salary: Hits and Years. How could we stratify it?

```{r, fig.align='center', fig.width=6}
ggplot(Hitters, aes(x = Years, y = Hits, color = Salary)) + geom_point() + scale_color_gradient(low = "blue", high = "red") + labs(title = "Scatter Plot of Hits vs Years", x = "Years", y = "Hits", color = "Salary") + theme_minimal()

```

Visually looking at this we can say that overall, players with less than 5 years are in the lower range of salary and after 5 years it depends based on the Hits. A decision tree is going to try to do that split for us.

The algorithm divides the predictor space (the set of possible values for $X_1,X_2\dots,X_p$) into $J$ distinct and non-overlapping regions $R_1,R_2,\dots,R_j$ For every observation that falls into the region, we make the same prediction, which is simply the mean of the response value for the training observations in $R_j$. To decide on the splits we could think about trying to write the feature space into boxes where the edges of the regions are parallel to the axes.

```{r, fig.align='center', fig.width=6, echo=FALSE}

Hitters<- na.omit(Hitters)
tree.Hitters <- tree(Salary ~ Hits + Years, Hitters)
tree.Hitters <- prune.tree(tree.Hitters, best = 3)
# Extract split points and variables
splits <- tree.Hitters$frame$var[tree.Hitters$frame$var != "<leaf>"]
split_points <- tree.Hitters$frame$splits[tree.Hitters$frame$var != "<leaf>"]

# Create a data frame to hold split information
split_data <- data.frame(variable = splits, split = split_points)

# Extract numerical part from split points
split_data$numeric_split <- as.numeric(gsub("[^0-9.]", "", split_data$split))

# Function to add split lines correctly
add_split_lines <- function(p, split_data) {
  for (i in 1:nrow(split_data)) {
    if (split_data$variable[i] == "Years") {
      # Vertical line at Years split
      p <- p + geom_vline(xintercept = split_data$numeric_split[i], linetype = "dashed", color = "black")
    } else if (split_data$variable[i] == "Hits") {
      # Horizontal line at Hits split within the region defined by Years
      if (i == 1) {
        p <- p + geom_segment(aes(x = -Inf, xend = split_data$numeric_split[i-1], y = split_data$numeric_split[i], yend = split_data$numeric_split[i]), linetype = "dashed", color = "black")
      } else {
        p <- p + geom_segment(aes(x = split_data$numeric_split[i-1], xend = Inf, y = split_data$numeric_split[i], yend = split_data$numeric_split[i]), linetype = "dashed", color = "black")
      }
    }
  }
  return(p)
}

# Initial scatter plot
p <- ggplot(Hitters, aes(x = Years, y = Hits, color = Salary)) +
  geom_point() +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "Scatter Plot of Hits vs Years",
       x = "Years",
       y = "Hits",
       color = "Salary") +
  theme_minimal()

# Add split lines to the plot
p <- add_split_lines(p, split_data)

# Print the plot
print(p)


```

In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles for simplicity and for ease of interpretation of the resulting predictive model. The goal is to find boxes to minimize the RSS:

$$
\sum^J_{j=1}\sum_{i\in R_j}(y_i-\hat y_{Rj})^2
$$ where $\hat y_{Rj}$ is the mean response for the training observations within the $jth$ box.

Unfortunately, it is computationally unfeasible to consider every possible partition of the feature space into J boxes, for this reason, we take a *top-down* approach that is known as recursive binary splitting. The approach is top-down because it begins at the top of the tree and then successively splits the predictor space, each split is indicated via two new branches further down on the three. **At each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step**. We repeat the process, in each region, looking a the best predictor and the best cut-points in order to split the data further so as to minimize the RSS within each of the resulting regions. The process continues until a stopping criterion is reached, for instance, we may continue until no region contains more than 5 observations.

```{r, fig.align='center', fig.width=7}

Hitters<- na.omit(Hitters)
tree.Hitters <- tree(Salary ~ Hits + Years, Hitters)
tree.Hitters <- prune.tree(tree.Hitters, best = 3)
summary(tree.Hitters)
plot(tree.Hitters, pretty=0)
# Add text labels to the plot 
text(tree.Hitters, pretty = 0)
```

The values at the end of each node show the mean salary for each bin. The salary is expressed in the dataset in annual salary in thousands of dollars. We can calculate those means manually to confirm:

```{r}
# Fit the decision tree
tree.Hitters <- tree(Salary ~ Hits + Years, data = Hitters)
tree.Hitters <- prune.tree(tree.Hitters, best = 3)
# Function to extract node means
node_means <- function(tree_model, data) {
  node_indices <- tree_model$where
  node_list <- split(data$Salary, node_indices)
  sapply(node_list, mean)
}

# Calculate the mean salaries for each terminal node
mean_salaries <- node_means(tree.Hitters, Hitters)
mean_salaries
```

**Terminology for Trees** In keeping with the tree analogy, the regions at the end are called *terminal nodes* Decision trees are typically drawn upside down in the sense that the leaves are at the bottom of the tree. The points along the tree where the predictor space is split are referred as *internal nodes* In the hitters tree shown above, the two internal nodes are indicated by the ext `years<4.5` and `Hits < 117.5`

**Interpretation of the results** The first division is done over years of experience, this means that the most important factor in determining Salary is years. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect the Salary, with players with more Hits having higher salaries. (Note that we have actually limited the number of nodes with the function `prune.tree()`, otherwise it would have shown further divisions). The longer the vertical sections of the diagram the more RSS reduction that split created.

## Predictions

We predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.

## Pruning

If we leave the tree to have as many nodes as possible, it will produce a tree that may produce good predictions on the training set, but it is likely to overfit the data, leading to poor test set performance. A smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of a little bias.

One possible alternative to the process described above is to grow the tree only so long as the decrease in the RSS due to each split exceeds some threshold. This strategy will result in smaller trees, but it is too short sighted: a seemingly worthless split early on in the tree might be followed by a very good split (that is, a split that leads to a large reduction in RSS later on)

A better strategy is to grow a very large tree $T_0$ and then prune it back in order to obtain a subtree. A strategy called *cost complexity pruning* or *weakest link pruning* is used to do this.

We consider a sequence of trees indexed by a non negative tuning parameter $\alpha$. For each value of alpha there corresponds a subtree $T \subset T_0$ such that the expression below is as small as possible:

$$
\sum^{|T|}_{m=1}\sum_{i:x_i\epsilon R_m}(y_i-\hat y_{Rm})^2+\alpha|T|
$$

where $|T|$ indicates the number of terminal nodes of the tree. $R_m$ is the rectangle or region (the subset of predictor space) corresponding to the mth terminal node, and $\hat y_{Rm}$ is the mean of the training observations in the region. So we want the sum of squares in the region to be as small as possible, but we put a penalty on the number of terminals by adding the $\alpha |T|$. The concept is similar to the *lasso*. The tuning parameter alpha controls a trade-off between the subtree's complexity and its fit to the training data. We select an optimal value for alpha using cross validation. We then return to the full dataset and obtain the subtree corresponding to that value of alpha.

We are going to do this manually here to show how the model overfits as more splits are added: the training MSE decreases, but the test MSE increases. The cross validation will help us choose the correct number of nodes to use:

Randomly divided the data set in half and built large regression tree on training data and varied α to create subtrees with different numbers of terminal nodes Finally, performed 6-fold cross-validation to estimate the cross-validated MSE of the trees as a function of α

```{r, fig.align='center', fig.width=6}
set.seed(123) 
trainIndex <- sample(c(TRUE,FALSE), nrow(Hitters), TRUE)

train <- Hitters[trainIndex,] 
test <- Hitters[!trainIndex,]

tree_model <- tree(Salary ~ Hits + Years, data = train)

# Perform cross-validation 
cv_tree <- cv.tree(tree_model, FUN = prune.tree, K = 6)

# Extract the tree size and corresponding deviance
(tree_sizes <- cv_tree$size)
tree_deviances <- cv_tree$dev

# Calculate training and test MSE for each tree size
train_mse_list <- c()
test_mse_list <- c()
for (size in tree_sizes) { 
  pruned_tree <- prune.tree(tree_model, best = size) 
  # Skip tree sizes that result in a single node 
if (nrow(pruned_tree$frame) > 1) { 
  #calculate MSE
  train_pred <- predict(pruned_tree, train) 
  train_mse <- mean((train_pred - train$Salary)^2) 
  test_pred <- predict(pruned_tree, test) 
  test_mse <- mean((test_pred - test$Salary)^2) 
  } 
else { 
  train_mse <- NA 
  test_mse <- NA } 
train_mse_list <- c(train_mse_list, train_mse) 
test_mse_list <- c(test_mse_list, test_mse)
}

# Calculate cross-validated MSE (average deviance from cross-validation)
cv_mse <- tree_deviances / length(train$Salary)

results <- data.frame(
  TreeSize = rep(tree_sizes, 3),
  MSE = c(train_mse_list, test_mse_list, cv_mse),
  Data = rep(c("Training", "Test", "Cross-Validation"), each = length(tree_sizes))
)

ggplot(results, aes(x = TreeSize, y = MSE, color = Data)) +
  geom_line() +
  geom_point() +
  labs(title = "MSE vs Tree Size", x = "Tree Size", y = "MSE") +
  theme_minimal()

```

Let's see how the tree on the hits dataset looks without manual pruning. We are letting the software choose the right number of nodes:

```{r, fig.align='center', fig.width=7}

tree.Hitters <- tree(Salary ~ Hits + Years, Hitters)

plot(tree.Hitters, pretty=0)
# Add text labels to the plot 
text(tree.Hitters, pretty = 0)
```

The tree function grows a large, unpruned tree. This tree is typically overgrown and will have more terminal nodes than necessary to capture the underlying structure of the data. By default, the tree function will grow the tree until the decrease in deviance (a measure of model fit) is not statistically significant at each split.

Cross-Validation and Pruning: The process we followed to determine the optimal number of terminal nodes involves:

Growing a large tree: This is similar to what the tree function does by default.

Pruning the tree: We use cross-validation to determine the optimal size of the tree that minimizes the mean squared error (MSE). This involves pruning the large tree to a smaller, more optimal size.

```{r}
cv_tree <- cv.tree(tree_model, FUN = prune.tree, K = 6)

# Determine the tree size with the minimum deviance 
optimal_size <- tree_sizes[which.min(tree_deviances)] 

# Prune the tree to the optimal size 
pruned_tree <- prune.tree(tree_model, best = optimal_size)
summary(pruned_tree)

```

::: exercise-box
Fitting Regression trees

Here we fit a regression tree to the `Boston` data set. First, we create a training set, and fit the tree to the training data.

```{r chunk14}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

Notice that the output of `summary()` indicates that only four of the variables have been used in constructing the tree. In the context of a regression tree, the **deviance** is simply the sum of squared errors for the tree. We now plot the tree.

```{r ,fig.align='center', fig.height=6, fig.width=9}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

The variable `lstat` measures the percentage of individuals with {lower socioeconomic status}, while the variable `rm` corresponds to the average number of rooms. The tree indicates that larger values of `rm`, or lower values of `lstat`, correspond to more expensive houses. For example, the tree predicts a median house price of $45{,}400$ for homes in census tracts in which `rm >= 7.553`.

It is worth noting that we could have fit a much bigger tree, by passing `control = tree.control(nobs = length(train), mindev = 0)` into the `tree()` function.

Now we use the `cv.tree()` function to see whether pruning the tree will improve performance.

```{r ,fig.align='center'}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

In this case, the most complex tree under consideration is selected by cross-validation. However, if we wish to prune the tree, we could do so as follows, using the `prune.tree()` function:

```{r, fig.align='center', fig.width=10, fig.height=4}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.

```{r, fig.align='center'}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
```

In other words, the test set MSE associated with the regression tree is $35.29$. The square root of the MSE is therefore around $5.941$, indicating that this model leads to test predictions that are (on average) within approximately $5{,}941$ of the true median home value for the census tract.
:::

# Classification Trees

Very similar to regression tree, except that it is used to predict a qualitative response rather than a quantitative one, but the mechanism is very similar. For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region where it belongs.

Just as in the regression setting, we use recursive binary splitting to grow a classification tree. In the classification setting, RSS cannot be used as a criterion for making binary splits. A natural alternative to RSS is the *classification error rate* this is simply the fraction of the training observations in that region that do not belong to the most common class. However, classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.

## Gini index and Deviance

In decision tree algorithms, the *Gini Index* is used as a criterion to measure the purity or impurity of a split in the dataset. The goal is to create nodes that contain observations of mostly one class, leading to better classification accuracy. The Gini index is defined by

$$
G = \sum^K_{k=1} \hat p_{mk}(1-\hat p_{mk})
$$ {#eq-giniIndex}

K is the number of classes

the binomial variance formula is: $$
var(X)=n \times p \times (1-p)
$$ {#eq-VarianceBinomialDistribution}

so this is a measure of the variability of that region.

If the Gini index is really small, that means that one class is favored and all the rest are really small. If the region is pure, the Gini will be 0.

An alternative is the *deviance* or *cross-entropy* and this is based on the binomial log likelihook. It behaves similarly to the Gini index and both give similar results. $$
D = -\sum^K_{k=1} \hat p_{mk}log \ \hat p_{mk}
$${eq-crossEntropy}

Let's look at an example with the Heart data. These data have a binary response called HD. Yes indicates presence of heart disease and No means no heart disease. There are 13 predictors.

The unpruned tree looks like this:

```{r, fig.align='center', fig.width=10, fig.height=6}
heart<- kmed::heart
heart$hd <- case_when(heart$class == 0 ~"No", 
                      TRUE ~ "Yes")
heart_tree <- heart %>% 
  mutate_if(is.character, as.factor) 

set.seed(417)
index <- sample(nrow(heart), size = nrow(heart)*0.80)
heart_train_tree <- heart_tree[index,] #take 80%
heart_test_tree <- heart_tree[-index,] #take 20%

tree.heart <- tree(hd ~ . -class, data=heart_train_tree)
summary(tree.heart)
plot(tree.heart)
text(tree.heart, pretty=0)
```

Let's do cross-validation now:

```{r, fig.alt='center', fig.width=12}
set.seed(7)
cv.heart <- cv.tree(tree.heart, FUN = prune.misclass)

par(mfrow = c(1, 2))
plot(cv.heart$size, cv.heart$dev, type = "b")
plot(cv.heart$k, cv.heart$dev, type = "b")
```

cross validation indicates that the best number of nodes is 6

```{r, fig.align='center', fig.width=10}
prune.heart <- prune.misclass(tree.heart, best = 6)
plot(prune.heart)
text(prune.heart, pretty = 0)
```

::: {.callout-orange appearance="simple" icon="false"}
Advantages and disadvantages of Trees

Pros: - Trees are very easy to explain to people. - Some people believe that decision trees more closely mirror human decision-making than do regression and classification approaches. - Trees can be displayed graphically. - Trees can easily handle qualitative predictors without the need to create dummy variables.

Cons: - Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen already.
:::

::: exercise-box
Fitting classification trees

We will have a look at the `Carseats` data using the `tree` package

We want a response variable that is qualitative so we are going to turn sales into a binary response. Let's see what value can be appropriate:

```{r, fig.align='center'}
hist(Carseats$Sales)
```

We create a binary response variable 'high' for high sales:`ifelse()` function creates a variable, called `High`, which takes on a value of `Yes` if the `Sales` variable exceeds $8$,and takes on a value of `No` otherwise.

```{r}
Carseats$High <- factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
```

We now use the `tree()` function to fit a classification tree in order to predict `High` using all variables but `Sales`. The syntax of the `tree()` function is quite similar to that of the `lm()` function.

```{r chunk4}
tree.carseats <- tree(High ~ . - Sales, Carseats)
```

The `summary()` function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.

```{r chunk5}
summary(tree.carseats)
```

We see that the training error rate is $9\%$. For classification trees, the deviance reported in the output of `summary()` is given by

$$
-2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk},
$$

where $n_{mk}$ is the number of observations in the $m$th terminal node that belong to the $k$th class. This is closely related to the entropy. A small deviance indicates a tree that provides a good fit to the (training) data. The *residual mean deviance* reported is simply the deviance divided by $n-|{T}_0|$, which in this case is $400-27=373$.

One of the most attractive properties of trees is that they can be graphically displayed. We use the `plot()` function to display the tree structure, and the `text()` function to display the node labels. The argument `pretty = 0` instructs `R` to include the category names for any qualitative predictors, rather than simply displaying a letter for each category.

```{r fig.align='center', fig.width = 10, fig.height=8}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

The most important indicator of `Sales` appears to be shelving location, since the first branch differentiates `Good` locations from `Bad` and `Medium` locations.

If we just type the name of the tree object, `R` prints output corresponding to each branch of the tree. `R` displays the split criterion (e.g. `Price < 92.5`), the number of observations in that branch, the deviance, the overall prediction for the branch (`Yes` or `No`), and the fraction of observations in that branch that take on values of `Yes` and `No`. Branches that lead to terminal nodes are indicated using asterisks.

```{r chunk7}
tree.carseats
```

In order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. The `predict()` function can be used for this purpose. In the case of a classification tree, the argument `type = "class"` instructs `R` to return the actual class prediction. This approach leads to correct predictions for around $77 \%$ of the locations in the test data set.

```{r chunk8}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]

tree.carseats <- tree(High ~ . - Sales, Carseats,
    subset = train)

tree.pred <- predict(tree.carseats, Carseats.test,
    type = "class")
table(tree.pred, Carseats.test$High)
(104 + 50) / 200
```

(If you re-run the `predict()` function then you might get slightly different results, due to "ties": for instance, this can happen when the training observations corresponding to a terminal node are evenly split between `Yes` and `No` response values.)

Next, we consider whether pruning the tree might lead to improved results. The function `cv.tree()` performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration.

We use the argument `FUN = prune.misclass` in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the `cv.tree()` function, which is deviance.

The `cv.tree()` function reports the number of terminal nodes of each tree considered (`size`) as well as the corresponding *error rate* and the value of the cost-complexity parameter used (`k`, which corresponds to $\alpha$).

```{r, fig.align='center'}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
plot(cv.carseats)
```

Despite its name, `dev` corresponds to the number of cross-validation errors. The tree with 9 terminal nodes results in only 74 cross-validation errors. We plot the error rate as a function of both `size` and `k`.

```{r, fig.align='center', fig.width=10}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

We now apply the `prune.misclass()` function in order to prune the tree to obtain the nine-node tree.

```{r, fig.align='center', fig.width=10, fig.height=7}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

How well does this pruned tree perform on the test data set? Once again, we apply the `predict()` function.

```{r chunk12}
tree.pred <- predict(prune.carseats, Carseats.test,
    type = "class")
table(tree.pred, Carseats.test$High)
(97 + 58) / 200
```

Now $77.5 \%$ of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also slightly improved the classification accuracy.

If we increase the value of `best`, we obtain a larger pruned tree with lower classification accuracy:

```{r , fig.align='center', fig.width=10}
prune.carseats <- prune.misclass(tree.carseats, best = 14)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test,
    type = "class")

table(tree.pred, Carseats.test$High)
(102 + 52) / 200
```
:::

# Bagging

*Bootstrap aggregation* or *bagging* is a general-purpose procedure for reducing the variance of a statistical learning method, it is particularly useful and frequently used in the context of decision trees. 
Recall that given a set of n independent observations $Z_1,Z_2,\dots,Z_n$ , each with variance $\sigma^2$, the variance of the mean $\bar Z$ of the observations is given by $\sigma^2/n$. In other words, averaging a set of observations reduces variance. Of course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the single training set. 
In this approach, we generate B different bootstrapped training datasets. We then train our method on the bth bootstrapped training set in order to get the prediction at point x. We then average all the predictions. 
For classification it works similarly. We can have 200 trees, we have two classes, we make a prediction of each x, and for example 150 trees says class 1 and 50 says class 2, then our prediction will be class 1, so we adjust by majority vote.

**Out-of-bag** Error estimation.
It turns out that there is a very straightforward way to estimate the **test error** of a bagged model. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred as to the *out-of-bag (OOB)* observations. We can predict the response for the $i$th observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the $i$th observation, which we average. We do that for each of the observations. 
This estimate is essentially the Leave One Out (LOO) cross-validation error for bagging, if B is large.

Bagging results in improved accuracy over prediction using single tree but it can be difficult to interpret the resulting model. We can't represent the statistical learning procedure using a single tree. It's not clear which variables are most important to the procedure (we have many trees each of which may give a differing view on the importance of a given predictor). So which predictors are important? An overall summary of the importance of each predictor can be achieved by recording how much the average RSS or Gini index improves or decreases when each tree is split over a given predictor (averaged over all B trees)

# Random Forest

A problem with bagging is that bagged trees may be highly similar to each other. For example if there is a strong predictor in the data, most of the bagged trees will use this predictor in the top split so that the predictions of the bagged trees will be highly correlated. Averaging highly correlated quantities does not lead to as large a reduction in variance as averaging many uncorrelated quantities.

*Random forests* provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. This reduces the variance when we average the trees. 
As in bagging, we build a number of decision trees on bootstrapped training samples, but when building these trees, each time a split in a tree is considered, a random selection of $m$ predictors is chosen as a split candidates from the full set of $p$ predictors. The split is allowed to use only one of the $m$ predictors. A fresh selection of $m$ predictors is taken at each split, and typically, we choose $m \sim \sqrt p$ that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors. The effect of this is that it forces the tree to choose different predictors at different times, and since we are going to build lots of trees, all predictors will be considered at each split among the different trees.

::: exercise-box
Bagging and Random forest

Here we apply bagging and random forests to the `Boston` data, using the `randomForest` package in `R`. The exact results obtained in this section may depend on the version of `R` and the version of the `randomForest` package installed on your computer.
Recall that bagging is simply a special case of a random forest with
$m=p$. Therefore, the `randomForest()` function can be used to perform both random forests and bagging. 

We perform bagging as follows:

```{r, warning=FALSE}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)

bag.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 12, importance = TRUE)
bag.boston
```
The argument `mtry = 12` indicates that all $12$ predictors should be considered for each split of the tree -in other words, that bagging should be done.
How well does this bagged model perform on the test set?

```{r, fig.align='center'}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```

The test set MSE associated with the bagged regression tree is $23.42$, about two-thirds of that obtained using an optimally-pruned single tree.

We could change the number of trees grown by `randomForest()` using the `ntree` argument:

```{r chunk21}
bag.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```
Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the `mtry` argument. By default, `randomForest()` uses $p/3$ variables when building a random forest of regression trees, and $\sqrt{p}$ variables when building a random forest of classification trees. Here we use `mtry = 6`.

```{r chunk22}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
    subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

The test set MSE is $20.07$; this indicates that random forests yielded an improvement over bagging in this case.

Using the `importance()` function, we can view the importance of each variable.

```{r chunk23}
importance(rf.boston)
```

Two measures of variable importance are reported. The first is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is permuted. The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees. 
In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can be produced using the `varImpPlot()` function.

```{r , fig.align='center', fig.width=10}
varImpPlot(rf.boston)
```

The results indicate that across all of the trees considered in the random forest, the wealth of the community (`lstat`) and the house size (`rm`) are by far the two most important variables.
:::

# Boosting

Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification, but we will only discuss boosting for decision trees. Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Notably, each tree is built on a bootstrap data set, independent of the other trees. *Boosting* works in a similar way except that the trees are grown sequentially, each tree is grown using information from previously grown trees. (More specific instructions can be found in the book.) Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach learns slowly. Given the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter in the algorithm. By fitting small trees to the residuals, we slowly improve the function in areas where it was not performing well. The shrinkage parameter lambda slows the process down even further, allowing more an different shaped trees to attack the residuals. The package `gbm` (gradient boosted models) handles a variety of regression and classification problems.

## Tuning parameters for boosting

**The number of trees B.** Unlike bagging and random forest, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross validation to select B.

**The shrinkage parameter lambda**, a small, positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small lambda can require using a very large value of B in order to achieve performance.

**The number of splits** d in each tree, which controls the complexity of the boosted ensemble. Often d=1 works well, in which each tree is a *stump*, consisting of a single split and resulting in an additive model. More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables. So with d=1 no interaction between parameters is allowed, and d=2 is a pairwise interaction.

# Bayesian Additive Regression Trees (BART)

Bart is related to both random forest and boosting: each tree is constructed in a random manner as in bagging and random forest, and each tree tries to capture signal not yet accounted for by the current model, as in boosting. The main novelty in BART is the way in which new trees are generated. BART can be applied to regression, classification and other problems.

All the different trees are built in parallel, and constructed like in boosting, where each step in each tree takes care of the residuals of the previous step, the difference here is that we apply a perturbance to each tree, so in step 2, each tree will have to deal with different parameters for example, or we grow a branch in one, but remove an existing branch in another tree.

K denote the number of regression trees and B the number of iterations or steps for which the BART algorithm will be run. The prediction at $x$ for the $kth$ regression tree used in the $bth$ iteration $\hat f^b_k(x)$. At the end of each iteration the K trees from that iteration will be summed i.e

$$
\hat f^b_k(x) = \sum^K_{k=1}\hat f^b_k(x) \ for \ b=1,\dots,B
$$ In the first iteration, all trees are initialized to have a single root node and the predictions will be just the average of the observations in all trees:

$$
\hat f^b_k(x) = \sum^K_{k=1}\hat f^1_k(x) = \frac{1}{n}\sum^n_{i=1} y_1
$$ In subsequent iterations, BART updates each of the K trees, one at a time. In the bth iteration, to update the kth tree, we substract from each response value the predictions from all but the kth tree, in order to obtain a *partial residual* $$
r_i = y_i -\sum_{k'<k}\hat f^{b-1}_k'(x_i) - \sum _{k'<k}\hat f^{b-1}_{k'}(x_i)
$$ and then we create a tree over the partial residual but using a perturbation from a set of possible perturbations. The output of BART is a collection of prediction models. To obtain a single prediction, we simply take the average after some L *burn-in iterations* (we discard some of the initial iterations and calculate only based on later trees).

When we apply BART we must select the number of trees K, the number of iterations (B) and the number of burn-in iterations. We typically choose large values for B and K and a moderate value for L.

The perturbation-style avoids overfitting so we can choose a big number of iterations without risk of overfitting to the training data.
