---
title: "Mastering Statistics: Fundamentals of Data Analysis."
format: 
  html: 
    toc: true
    toc-depth: 3
    toc-title: Contents
    number-sections: true
    number-depth: 3
    embed-resources: true
    fig-align: 'center'
    fig-cap-location: margin
    fig-width: 4
    fig-height: 4
    css: custom-style.css
    page-layout: full
    crossrefs-hover: true
    footnotes-hover: true
    citations-hover: true
    grid: 
      sidebar-width: 10px
      body-width: 1300px
      gutter-width: 0.5rem
    margin-left: 50px
    margin-right: 10px
    anchor-sections: true
  pdf: 
    toc: true
    number-sections: true
    colorlinks: true
editor: visual
html-math-method: mathjax
execute: 
  engine: knitr
  warning: false
  echo: true
knitr:
  opts_chunk:
    label: true
    number: true
    echo: true
---

```{r}
#| echo: false
library(tidyverse)
library(here)
library(readxl)
library(easystats)
library(infer)
library(kableExtra)
library(plotly)
library(ggplot2)
library(patchwork)
library(BSDA)
library(rafalib)
library(UsingR)
library(dplyr)
library(ISLR2)
library(scatterplot3d)
theme_set(theme_minimal())
options(scipen= 999)
```

This document is a summary of different stats courses:

-   Mastering Statistics: Fundamentals of Data Analysis (Udemy)

-   Introduction to Statistics (Standford University via Coursera)

-   Statistics and R (HarvardX PH525.1x via Edx)

-   Statistical Learning (Stanford Online STATSX0001 via Edx) [additional resources in https://www.statlearning.com/resources-second-edition]

# Exploratory Analysis (EDA)

## Mean

The mean (or arithmetic mean) is a measure of central tendency that represents the average value of a set of numbers. It is calculated by summing all the values in the dataset and then dividing by the number of values. 
$$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$ {#eq-mean}
When we talk about the population mean we use the greek letter $\mu$ and when we talk about the sample mean we use our variable with a bar on top $\bar{x}$ .

## Variance

Variance measures the average squared deviations from the mean. To calculate it:

Find the mean of the data set. Subtract the mean from each data point and square the result. Average these squared differences.

Mathematically, for a population, the variance $\sigma^2$ is: 
$$
\sigma^2 = \frac{1}{N}\sum^N_{i=1}(x_i-\mu)^ 2
$$ {#eq-variance}

where (N) is the number of data points, $x_i$ are the data points, and $\mu$ is the mean.

## Standard deviation

Standard deviation is the square root of the variance. It provides a measure of spread in the same units as the data, making it more interpretable. It quantifies how much the values in a dataset deviate from the mean (average) of the dataset. A low standard deviation indicates that the values are close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.

Population standard deviation:

$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2}
$$ {#eq-standardDeviation}

where:

-   ($\sigma$) is the population standard deviation

-   \(N\) is the population size

-   ($X_i$) is the (i)-th observation in the population

-   ($\mu$) is the population mean

We can calculate the population standard deviation like this:

```{r}
heights=father.son$fheight
popsd <- function(x) sqrt(mean((x-mean(x))^2))
popsd(heights)
```

Note the `sd` function in R gives us a sample estimate of the σ as opposed to the population σ

```{r}
standardDeviation <- sd(heights)
standardDeviation
```

The sample standard deviation formula is very similar:

$$
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2}
$$ {#eq-sampleStandarDeviation}

### The empirical rule:

If the data follows the normal curve then

-   about 2/3 (68%) of the data fall within one standard deviation of the mean.

-   about 95% fall within 2 standard deviations of the mean

-   99.7% fall within 3 standard deviations of the mean

```{r, fig.align='center', echo=FALSE}
#| warning= FALSE
data <- data.frame (values = rnorm(1000, mean = 10, sd = 3))
# Calculate mean, median, and standard deviation
mean_val <- mean(data$values)
median_val <- median(data$values)

sd_val <- sd(data$values)
gg<- ggplot(data, aes(x = values))+
  geom_histogram()

# Add vertical lines for mean, median, and standard deviation
gg <- gg +
  geom_vline(xintercept = mean_val, color = "red", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = median_val, color = "green", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val + sd_val, color = "blue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - sd_val, color = "blue", linetype = "dashed", linewidth = 1)+
  geom_text(aes(x = mean_val, y = 10, label = "Mean"), color = "red", vjust = -0.5, angle=90) +
  geom_text(aes(x = median_val, y = 10, label = "Median"), color = "green", vjust = -0.5,hjust = -2, angle=90) +
  geom_text(aes(x = mean_val + sd_val, y = 10, label = "SD1"), color = "blue", vjust = -0.5, angle=90) +
  geom_text(aes(x = mean_val - sd_val, y = 10, label = "SD1"), color = "blue", vjust = -0.5, angle=90)
gg

```

Biases, systematic errors and unexpected variability are common in data. Failure to discover these problems often leads to flawed analyses and false discoveries. As an example, consider that experiments sometimes fail and not all data processing pipelines, such as the \`t.test\` function in R, are designed to detect these. Yet, these pipelines still give you an answer. Furthermore, it may be hard or impossible to notice an error was made just from the reported results.

Graphing data is a powerful approach to detecting these problems. We refer to this as *exploratory data analysis* (EDA). Many important methodological contributions to existing techniques in data analysis were initiated by discoveries made via EDA.

## Median, Median Absolute Deviation (MAD) and outliers

The **median** is a measure of central tendency that represents the middle value in a sorted list of numbers. If the list has an odd number of observations, the median is the middle number. If the list has an even number of observations, the median is the average of the two middle numbers. The median is less affected by outliers and skewed data compared to the mean.

The *Median Absolute Deviation (MAD)* is a robust measure of statistical dispersion. It is defined as the median of the absolute deviations from the data's median. MAD is less sensitive to outliers compared to the standard deviation. 
$$
MAD= median(|X_i-median(X)|)
$$ {#eq-mad}
For example, I create a vector with 100 values following a normal distribution with average 0 and standard deviation 1, but then I change one of the points to a value of 100.

```{r boxplot_showing_outlier, fig.cap="Normally distributed data with one point that is very large due to a mistake.", fig.align='center',echo=FALSE}
set.seed(1)
x=c(rnorm(100,0,1)) ##real distribution
x[23] <- 100 ##mistake made in 23th measurement
boxplot(x)
```

now we are going to see how this datapoint is affecting our statistics:

```{r,echo=FALSE}
cat("The average is",mean(x))
cat("The SD is",sd(x))
cat("The median is ", median(x))
cat("The MAD is ", mad(x))
```

## Spearman correlation {#spearman-correlation}

The correlation (it will be presented later in [another chapter](#confidenceIntvsSignifTest)) is also very sensitive to outliers. In this example we create two datasets with no correlation whatsoever:

```{r, fig.align='center',echo=FALSE}
set.seed(1)
x=rnorm(100,0,1) 
y=rnorm(100,0,1) 

plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),
     pch=21,bg=1)
abline(0,1)
```

now we introduce as we did before one datapoint of a much higher value for x and y. This is affecting our correlation coefficient:

```{r, scatter_plot_showing_outlier,fig.cap="Scatterplot showing bivariate normal data with one signal outlier resulting in large values in both dimensions.", fig.align='center',echo=FALSE}
set.seed(1)
x[23] <- 100 ##mistake made in 23th measurement
y[23] <- 84 ##similar mistake made in 23th measurement

plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),
     pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
abline(0,1)
```

The Spearman correlation follows the general idea of median and MAD, that of using quantiles. The idea is simple: we convert each dataset to ranks and then compute correlation:

```{r spearman_corr_illustration, fig.cap="Scatterplot of original data (left) and ranks (right). Spearman correlation reduces the influence of outliers by considering the ranks instead of original data.",fig.width=10.5,fig.height=5.25, fig.align='center',echo=FALSE}
mypar(1,2)
plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
plot(rank(x),rank(y), main=paste0("correlation=",round(cor(x,y,method="spearman"),3)),
     pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
abline(0,1)
```

In general, if we know there are outliers, then median and MAD are recommended over the mean and standard deviation counterparts.

## Empirical Cumulative Density Function (CDF)

What exactly is a distribution? The simplest way to think of a *distribution* is as a compact description of many numbers.

Although not as popular as the histogram for EDA, the empirical cumulative density function (CDF) (or cumulative distribution function) shows us the same information and does not require us to define bins. For any number a the empirical CDF reports the proportion of numbers in our list smaller or equal to a.

$$
F(a) \equiv Pr(x \leq a)
$$ {#eq-cdf}

This is called the cumulative distribution function (CDF). When the CDF is derived from data, as opposed to theoretically, we also call it the empirical CDF (ECDF).

R provides a function that has as out the empirical CDF function.

```{r}
heights=father.son$fheight 
round(sample(heights,20),1) 
myCDF <- ecdf(heights) 
```

The `ecdf` function is a function that returns a function, which is not typical behavior of R functions. We create a function called myCDF based on our data *heights* that can then be used to generate a plot:

```{r ecdffunction, fig.align='center',echo=FALSE}
##We will evaluate the function at these values:
xs<-seq(floor(min(heights)),ceiling(max(heights)),0.1) 
### and then plot them:
plot(xs,myCDF(xs),type="l",xlab="x=Height",ylab="F(x)")
```

## Boxplot

When the data is not normally distributed the mean and the standard deviation are not enough to summarise the data. A better approach would be to present the data in a boxplot.

```{r, fig.align='center',echo=FALSE}
# Create a not normally distributed sample (exponential distribution)
set.seed(123)  # For reproducibility
data <- rexp(100, rate = 0.2)

# Create a boxplot with ggplot2
p <- ggplot(data.frame(value = data), aes(x = "", y = value)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 3, color = "blue") +
  labs(title = "Boxplot of Exponentially Distributed Data",
       y = "Values") +
  theme_minimal()

# Add labels for the important parts of the boxplot
p + annotate("text", x = 1.2, y = quantile(data, 0.75)+1.5, label = "Q3 (75th percentile)", hjust = 0) +
  annotate("text", x = 1.2, y = quantile(data, 0.25)-1.5, label = "Q1 (25th percentile)", hjust = 0) +
  annotate("text", x = 1.2, y = median(data)+1, label = "Median", hjust = 0) +
  annotate("text", x = 1.2, y = mean(data)+0.5, label = "Mean", hjust = 4.5, color = "blue") +
  annotate("text", x = 1.2, y = max(data[data <= quantile(data, 0.75) + 1.5 * IQR(data)]), label = "Max (whisker)", hjust = 2) +
  annotate("text", x = 1.2, y = min(data[data >= quantile(data, 0.25) - 1.5 * IQR(data)]), label = "Min (whisker)", hjust = 2)


```

-   Q1 (25th percentile): The lower quartile.

-   Median: The middle value of the data.

-   Mean: The average value of the data (marked in blue).

-   Q3 (75th percentile): The upper quartile.

-   Box (between the 25th percentile and the 75th percentile) is showing the middle half of the data.

-   Max (whisker): The maximum value within 1.5 times the IQR above Q3.

-   Min (whisker): The minimum value within 1.5 times the IQR below Q1.

-   Outliers: Points beyond the whiskers, marked in red.

## Histogram {#histogram}

We can think of any given dataset as a list of numbers. Suppose you have measured the heights of all men in a population. Imagine you need to describe these numbers to someone that has no idea what these heights are, for example an alien that has never visited earth. One approach is to simply list out all the numbers for the alien to see. Here are 20 randomly selected heights of 1,078

From scanning through these numbers we start getting a rough idea of what the entire list looks like but it is certainly inefficient. We can quickly improve on this approach by creating bins, say by rounding each value to the nearest inch, and reporting the number of individuals in each bin. A plot of these heights is called a histogram

We can specify the bins and add better labels in the following way:

```{r histogramofnormaldist,fig.align='center',echo=FALSE}
bins <- seq(floor(min(heights)),ceiling(max(heights)))  
hist(heights,breaks=bins,xlab="Height",main="Adult men heights") 
```

Showing this plot is much more informative than showing the numbers. Note that with this simple plot we can approximate the number of individuals in any given interval. For example, there are about 70 individuals over six feet (72 inches) tall.

This denotes the probability that the random variable ( x ) is between ( a ) and ( b )

$$P(a \leq x \leq b) = F(b) - F(a)$$ {#eq-probabilityHistogram}

## Probability Distribution

Summarizing lists of numbers is one powerful use of distribution. An even more important use is describing the possible outcomes of a random variable. Unlike a fixed list of numbers, we don't actually observe all possible outcomes of random variables, so instead of describing proportions, we describe probabilities. For instance, if we pick a random height from our list, then the probability of it falling between $a$ and $b$ is denoted with: $P(a \leq X \leq b) = F(b) - F(a)$

Note that the $X$ is now capitalized to distinguish it as a random variable and that the equation above defines the probability distribution of the random variable.

### The p-value

Knowing this distribution is incredibly useful in science. If we know the distribution of our data when the null hypothesis is true, referred to as the *null distribution*, we can compute the probability of observing a value as large as we did in our experiment, referred to as a *p-value*.

::: exercise-block
Example.

We have data from 24 mice fed two different diets. We want to know if the mice fed with high fat diet increased their body weight more than the mice fed with the control diet.

```{r}
femaleMiceWeights <- read.csv("data/femaleMiceWeights.csv")

head(femaleMiceWeights)
control<- femaleMiceWeights %>% filter (Diet=='chow')
treatment <-  femaleMiceWeights %>% filter (Diet=='hf')
tm<- mean(treatment$Bodyweight)
cm<- mean(control$Bodyweight)
obsdiff <- tm - cm
print(paste("treatment mean:", tm))
print(paste("control mean:", cm))
print(paste("difference =", obsdiff))

```

We can see that there is a difference between the two means but if we choose random samples of 12 individuals from the mice population, each of the samples would have different means just by chance. So how can we be sure if that difference is due to the change in diet or to the random sampling?

We are going to recreate that by choosing 24 mice and assigning them randomly to either treatment or control. We will repeat this 10000 times:

```{r}
n <- 10000
null <- vector("numeric",n)
for (i in 1:n) {
  control <- sample(femaleMiceWeights$Bodyweight,12)
  treatment <- sample(femaleMiceWeights$Bodyweight,12)
  null[i] <- mean(treatment) - mean(control)
}
head(null)
```

The values in `null` form what we call the *null distribution*.

What percentage of our 10000 experiments have a difference weight between control and treatment higher or equal to the one we observed for the different diets?

```{r}
mean(null >= obsdiff)
```

We see a difference as big as the one we observed only a small percentage of the time. This is what is known as a p-value. We can plot the null histogram and mark the line with our observed weight difference for the high fat diet.

```{r null_and_obs,fig.cap="Null distribution with observed difference marked with vertical red line.", fig.align='center', echo=FALSE}
hist(null, freq=TRUE)
abline(v=obsdiff, col="red", lwd=2)
```

:::

An important point to keep in mind here is that while we defined $\mbox{Pr}(a)$ by counting cases, we will learn that, in some circumstances, mathematics gives us formulas for $\mbox{Pr}(a)$ that save us the trouble of computing them as we did here. One example of this powerful approach uses the normal distribution approximation.

We will see later on [how to calculate the p-values](#pvalues)

# Normal distribution

A **normal distribution**, also known as a Gaussian distribution, is a continuous probability distribution that is symmetrical around its mean. This means that most of the observations cluster around the central peak, and the probabilities for values further away from the mean taper off equally in both directions. When plotted on a graph, it forms a bell-shaped curve, often referred to as a "bell curve".

Key properties of a normal distribution include

-   **Symmetry**: The left and right sides of the distribution are mirror images.

-   **Mean, Median, and Mode**: All three measures of central tendency are equal and located at the center of the distribution.

```{r, fig.align='center',echo=FALSE}
data <- data.frame (values = rnorm(1000, mean = 10, sd = 3))

gg<- ggplot(data, aes(x = values))+
  geom_histogram()
gg
```

probability density function (PDF) of a normal distribution

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$ {#eq-PDF}

we can use a mathematical formula to approximate the proportion of values or outcomes in any given interval:

$$
\mbox{Pr}(a < x < b) = \int_a^b \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left( \frac{-(x-\mu)^2}{2 \sigma^2} \right)} \, dx
$$ {#eq-pnorm}

While the formula may look intimidating, don't worry, you will never actually have to type it out, as it is stored in a more convenient form (as `pnorm` in R which sets *a* to $-\infty$, and takes *b* as an argument). We can compute the proportion of values below a value `x` with `pnorm(x,mu,sigma)`. A very useful characteristic of this approximation is that one only needs to know $\mu$ and $\sigma$ to describe the entire distribution. From this, we can compute the proportion of values in any interval.

## Standardizing data

A normal curve is determined by the mean $\bar{x}$ and the standard deviation $s$ . If the data follow the normal curve, then knowing those two values mean we know the whole histogram. To compute areas under the normal curve, we first standardize the data by substracting $\bar{x}$ and dividing by $s$. The resulting value is called the standardized value or z-score $z$

$$
z = \frac{x_1 - \bar{x}}{s}
$$ {#eq-zscore}

After standardizing all points in our data we will get a distribution with mean = 0 and standard deviation =1

```{r,fig.align='center',echo=FALSE}
standardizedValues <- data.frame(values = scale(data$values))
mean_val <- mean(standardizedValues$values)
median_val <- median(standardizedValues$values)
sd_val <- sd(standardizedValues$values)

# Add vertical lines for mean, median, and standard deviation
gg <- ggplot(standardizedValues, aes(x = values))+
  geom_histogram()+
  geom_vline(xintercept = mean_val, color = "darkred", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = median_val, color = "green", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val + sd_val, color = "lightblue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - sd_val, color = "lightblue", linetype = "dashed", linewidth = 1)+
  geom_vline(xintercept = mean_val + 2*sd_val, color = "lightblue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - 2*sd_val, color = "lightblue", linetype = "dashed", linewidth = 1)+
  geom_text(aes(x = mean_val, y = 10, label = "Mean"), color = "darkred", vjust = -0.5, angle=90) +
  geom_text(aes(x = median_val, y = 10, label = "Median"), color = "green", vjust = -0.5,hjust = -2, angle=90) +
  geom_text(aes(x = mean_val + sd_val, y = 10, label = "SD1"), color = "lightblue", vjust = -0.5, angle=90)  +
  geom_text(aes(x = mean_val - sd_val, y = 10, label = "SD1"), color = "lightblue", vjust = -0.5, angle=90)  +
  geom_text(aes(x = mean_val + 2*sd_val, y = 10, label = "SD2"), color = "lightblue", vjust = -0.5, angle=90) +
  geom_text(aes(x = mean_val - 2*sd_val, y = 10, label = "SD2"), color = "lightblue", vjust = -0.5, angle=90)
gg

```

So now, the value of z for a specific value $x_1$ is indicating how many standard deviations our value is away from the mean, and just by simple approximation using the empirical rule, if for example you standardize your height and you get a value of 1, you already know without having to look into more detail that you are approximately higher than 84% of the population.

```{r empiricalRule, fig.height=4,fig.width=6, fig.align='center',echo=FALSE}

# Plot the cumulative percentages
# Define the range for the x-axis
x <- seq(-4, 4, length = 1000)

# Calculate the density of the normal distribution
y <- dnorm(x)

# Define the z-values
z_values <- c(-3, -2, -1, 0, 1, 2, 3)

# Calculate the cumulative percentages
cumulative_percentages <- pnorm(z_values) * 100

# Plot the normal distribution curve
plot(x, y, type = "l", lwd = 2, col = "lightblue", ylab = "Density", xlab = "Z-score",
     main = "Standard Normal Distribution with Cumulative Percentages", ylim = c(0, 0.7))

# Add vertical lines at the z-values
abline(v = z_values, col = "darkred", lty = 2)

# Annotate the cumulative percentages
text(z_values, dnorm(z_values), labels = paste0(round(cumulative_percentages, 2), "%"), pos = 3, col = "darkred")
grid()

```

## Normal approximation

When the histogram of a list of numbers approximates the normal distribution we can use a convenient mathematical formula to approximate the proportion of individuals in any given interval. We already saw this equation (@eq-pnorm)

$$
P(a < x < b) = \int_{a}^{b} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(\frac{-(x - \mu)^2}{2\sigma^2}\right) \, dx
$$ {#eq-pnorm1}

If this approximation holds for our data then the population mean and variance of our data can be used in the formula above.

::: exercise-box
Example

*Let's calculate the number of individuals that are taller than 72 inches.* 
First we are going to calculate the number manually:

```{r}
n_individuals = length(heights)
taller = length(which(heights>72))
propTaller = taller/n_individuals
propTaller
```

so we have roughly 6.4% of the individuals taller than 72 inches. 
Now wee can calculate it using normal approximation and it will gives us an similar number:

```{r}
1-pnorm(72,mean(heights),rafalib::popsd(heights)) 
```
:::

if we do this check for different values and continue to see that the two values are similar we can assume a normal distribution, but a better way to do it is using a QQ plot:

## QQ plot

To corroborate that the normal distribution is in fact a good approximation we can use **quantile-quantile plots (QQ-plots)**. Quantiles are best understood by considering the special case of percentiles. The p-th percentile of a list of a distribution is defined as the number q that is bigger than p% of numbers. For example, the median 50-th percentile is the median. We can compute the percentiles for our list and for the normal distribution.

```{r, fig.align='center'}
#generate the percentiles
ps <- seq(0.01,0.99,0.01)
# The quantile function returns the values below which a given percentage of data falls. 
qs <- quantile(heights,ps)
#calculates the theoretical quantiles from a normal distribution with the same mean and standard deviation as the heights data. The qnorm function returns the quantiles of the normal distribution for the given probabilities (ps), mean, and standard deviation
normalqs <- qnorm(ps,mean(heights),rafalib::popsd(heights))
plot(normalqs,qs,xlab="Normal percentiles",ylab="Height percentiles")
abline(0,1) ##identity line
```

This line adds an identity line (a line with slope 1 and intercept 0) to the plot. The abline(0, 1) function draws a 45-degree line through the origin. This line helps to visually assess how closely the data follows a normal distribution. If the points lie close to this line, it suggests that the data is approximately normally distributed. This code is creating a Normal Quantile-Quantile (QQ) Plot. A QQ plot is used to compare the distribution of a dataset to a theoretical distribution---in this case, the normal distribution. If the heights data is normally distributed, the points in the plot will lie approximately along the identity line.

We can generate the same plot with a simplified code:

```{r, fig.align='center'}
qqnorm(heights)
qqline(heights) 
```

Data is not always normally distributed. Income is widely cited example. In these cases the average and standard deviation are not necessarily informative since one can't infer the distribution from just these two numbers. The properties described above are specific to the normal. For example, the normal distribution does not seem to be a good approximation for the direct compensation for 199 United States CEOs in the year 2000

```{r, fig.align='center', echo=FALSE}
hist(exec.pay)
```

the QQ plot would look like this:

```{r, fig.align='center', echo=FALSE}
qqnorm(exec.pay)
qqline(exec.pay)
```

## Use of standardadization in binomial probability

We know that the probability in any giving birth to have a girl is 0.49%. What is the probability of having 2 girls out of 3 births?\
In binomial probability we call the outcome we are interested in a 'success' and the other outcome 'failure' in our example, having a girl is success, having a boy is failure. When the number of repetitions in our experiment is small, we can just draw the possible outcomes.

Given that each birth is independent from the others, we have these possibilities for getting the outcome we are interested:

$GGB | GBG | BGG (G=Girl, B=Boy)$

The probability would be $P= P(G)P(G)P(B)+P(G)P(B)P(G)+P(B)P(G)P(G)$

That is the same as $P= 3(PG)P(G)P(B) = 3*0.49*0.49*0.51$

```{r}
pG = 0.49
pB =1-pG
p = 3*pG*pG*pB
p
```

But when the number of experiments grows it gets complicated to just find out all the combinations by hand, so we can use the binomial coefficient to know the number of ways one can arrange k success in n experiments:

$$
\binom{n}{k} = \frac{n!}{k!(n-k)!}
$$ {#eq-binomialCoefficient}

and to calculate the probability we use the coefficient with the *binomial probability formula. It’s used to find the probability of getting exactly k successes in n independent Bernoulli trials, where each trial has a success probability p.

$$
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
$$ {#eq-binomialProbabilityFormula}

if we calculate this manually:

```{r}
# Parameters
n <- 3
k <- 2
p <- 0.49

# Calculate binomial coefficient
binom_coeff <- factorial(n) / (factorial(k) * factorial(n - k))

# Calculate probability
probability <- binom_coeff * (p^k) * ((1 - p)^(n - k))

# Print the result
print(probability)

```

but in r we would do it like this

```{r #binomialprobability}

# Calculate binomial probability
probability <- dbinom(k, n, p)
print(probability)

```

If we have more than two outcomes but we are interested in one of them only we can still use the binomial formula. 

**Example**
*We are playing an online game, the probability of winning a big prize is 10%, the probability of winning a small prize is 20% and the probability of not winning anything is 70%. We want to know, in 10 repetitions, what is the probability that we win two small prizes:*

-   success = win small prize

-   failure = anything else.

```{r}
# Parameters
n <- 10
k <- 2
p <- 0.20

# Calculate binomial coefficient
binom_coeff <- factorial(n) / (factorial(k) * factorial(n - k))

# Calculate probability
probability <- binom_coeff * (p^k) * ((1 - p)^(n - k))

# Print the result
print(probability)
```

*but now, what is the probability of winning **at most**, 12 small prizes in 50 repetitions?* 
we would have to calculate the probability of winning 12 small prizes, 11 small prizes etc. but we can use standardization instead to achieve the same goal:

To standardize data for a binomial experiment, you typically use the **z-score** formula. This formula converts the binomial distribution to a standard normal distribution. The z-score (@eq-zscore) formula is: 
$$
z=\frac{X-\mu}{\sigma}
$$ {#eq-zscore2}

-   ( $X$ ) is the number of successes.

-   ( $\mu$ ) is the mean of the binomial distribution, calculated as ( $\mu = np$ ). where $n$ is the number of repetitions and $p$ is the the probability of success.

-   ( $\sigma$ ) is the **standard deviation of the binomial distribution**, calculated as 
$$\sigma = \sqrt{np(1-p)}$${#eq-standardDeviationBinomialDistribution}

So, the complete formula for the z-score in a binomial experiment is:

$$
z = \frac{X - np}{\sqrt{np(1-p)}}
$$ {#eq-zscoreBinomial}

This formula allows you to standardize the number of successes ( X ) by subtracting the mean and dividing by the standard deviation, converting it to a z-score.

```{r}
# Parameters
n <- 50
k <- 12
p <- 0.20

sigma <- sqrt(n*p*(1-p))
mu <- n*p
zScore <- (k-mu)/sigma
print(zScore)

```

## Calculating the p-values{#pvalues}
once we have calculated the z-Score we can calculate the area on the left of that number under the normal distribution curve, and that will be the probability we are looking for.

```{r, fig.align='center', echo=FALSE}

# Create a sequence of x values
x <- seq(-4, 4, length=100)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue", xlab="Z", ylab="Density", main="Standard Normal Distribution")
# Add a vertical line at x = 0.71
abline(v = 0.71, col = "red", lwd = 2, lty = 2)
```

We can now can find manually the probability using z-score tables.
In R, we may use the `pnorm()` function to find the p-value associated with a z-score, which has the following syntax.

`pnorm(q, mean = 0, sd = 1, lower.tail = TRUE)`

Where 
- `q = z-Score value`. 
- lower.tail: If TRUE, the probability in the normal distribution to the left of z-Score is returned. The probability to the right is returned if FALSE. TRUE is the default value.

To find the **p-value for the two-tailed test**:
       
`2*pnorm(q=0.71, lower.tail=FALSE)` 


```{r}
# Calculate the area to the left of the z-score
(area_left <- pnorm(zScore))

# Calculate the area to the right of the z-score
(area_right <- 1 - area_left)
#or
(area_rigt<- pnorm(zScore,lower.tail=FALSE))

# Calculate the cumulative probability using the z-score, in our case we use the left area:
probability <- pnorm(zScore)
print(probability)
```

Instead of plugging in the values and calculate the z-score first, we can simplify our operations in r like this:

```{r}
# Parameters
n <- 50
k <- 12
p <- 0.20
# Calculate the cumulative probability using an expression
probability <- pnorm((k - n * p) / sqrt(n * p * (1 - p)))
print(probability)
```


# Introduction to Inference

```{r, echo=FALSE}
#| echo: false
file <- here("data", "optical_full.xlsx")
optical <- read_excel(file)
```

Drawing conclusions from data is difficult because we almost never have complete information about any variable of interest. For instance, we may have only been able to send a satisfaction survey to a small subset of our customers.

-   The **population** of a study consists of all possible observations of interest.

-   A **sample** in a study consists of all the observations we actually have.

We are almost always interested in a population, but only have information about a sample. Statistical inference is the process of reasoning from the one to the other.

-   A **parameter** is a number that describes a population

-   A **statistic** is a number that describes a sample

The mean of the eye difference variable in our optical dataset is `r mean(optical$eye_difference)`. The **sample mean** is found by adding together all the values of a variable in the sample data, and dividing this total by $n$, the sample size.

If we extract 100 samples with three observations each from our data and visually show their means we can see every time we got a different value, sometimes they will be very close to the target parameter, but sometimes they can be quite far.

```{r, fig.align='center', echo=FALSE}
#| eval: true

kable(head(optical))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()
as.data.frame(report(optical$eye_difference))

sample <- slice_sample(optical, n = 3)
mean(sample$eye_difference)

samples <- rep_slice_sample(optical, n = 3, reps = 100)

sample_summary <- samples |> 
  group_by(replicate) |> 
  dplyr::summarize(sample_mean = mean(eye_difference))

ggplot(sample_summary, aes(x = sample_mean)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical$eye_difference),
             linetype = "dotted")

```

**Variability** is the natural tendency for a statistic to be different across different samples.

A **Biased statistics**, on the other hand, misses the mark on average, for example if we increase the value of all our sample means by 1:

```{r, fig.align='center', echo=FALSE}
ggplot(sample_summary, aes(x = sample_mean+1)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical$eye_difference),
             linetype = "dotted")
```

::: {.callout-orange appearance="simple" icon="false"}
**Reproducibility and Replicability**

Replicating means to get the same conclusion with slightly different samples, procedures, and data analysis methods.

Related to that is the problem of reproducibility. That means to get the same results then you simply use the same data and same methods that were claimed in the analysis.
:::

::: {.callout-orange appearance="simple" icon="false"}
::: centered-text
-   Shortcomings of statistical analysis \*
:::

Statistical inference can handle variability, but not bias. Bias comes from incorrect data collection and only good data collection and management practices can prevent bias.

Statistical inference can lead to wildly wrong conclusions when misused or abused for example:

-   Sample bias: not choosing the sample randomly.

-   Undercoverage: your procedure miss out keep portions of the population.

-   Overgeneralization: you need to limit the conclusions that you draw to the population that you have sampled from.

Significance test are particulary easy to abuse:

-   If you run a test just because you've spotted a pattern in your data, you're almost certain to get a positive result even if the trend is just due to random chance. Patterns exist just by chance.

-   p-hacking: repeated testing of the same hypothesis will eventually yield a positive result just by random chance.

-   Confusing significance for importance. Just because a difference is significant does not mean it is important.

No amount of statistical savvy can fully remediate bad data. All of the techniques we'll discuss in this course are designed to address variability in data, not bias. Before attempting any sort of analysis, you should always take some time to think critically about the overall quality of the data. A few questions to consider:

-   Is the sample representative of the population of interest, or might some subgroups be under or over represented?

-   How was the data collected? Are any of the variables self-reported? Were there points in the process where those collecting the data made subjective judgments?

-   How will conclusions from the data analysis be used? How might they be misinterpreted or abused?
:::

# Expected Value and Standard Error:

If we are interested in the average height of adult males in USA and we extract an adult male at random, we expect his height to be around the population average, give or take about one standard deviation $\sigma$ . **The expected value is the population average** $\mu$ . The expected value of the sample average is still the population average, but remember that the population mean is a random variable because sampling is a random process, so its mean won't be exactly the population mean. How far off can we expect this sample mean to the population mean?

**The standard error (SE)** of a statistic tell you roughly how far off the sample mean will be from its expected value (population mean). It indicates how much the sample mean is expected to vary from the true population mean.

$$ 
\text{SE} (\bar{x_n})= \frac{\sigma}{\sqrt{n}} 
$$ {#eq-standardErrorMean}

where n is the sample size. **We can actually use this formula to decide what is the sample size we need to use for a desired accuracy in our statistic**.

It plays a similar role as the *standard deviation* for one observation from the population mean, remember that the Standard deviation measures the dispersion of individual data points around the mean of a dataset. It tells you how spread out the values in a dataset are. In summary, the standard error of an estimate is the standard deviation of the sampling distribution of an estimate.

::: exercise-box
**Exercise:**

*A town has 10,000 registered voters, of whom 6,000 are voting for the Democratic party. A survey organization is taking a sample of 100 registered voters (assume sampling with replacement). The percentage of Democratic voters in the sample will be around \_\_\_\_\_, give or take \_\_\_\_. (You may use the fact that the standard deviation is about 0.5)*

The percentage of Democratic voters in the sample is equal to the mean of the sample, and so the "around" value is the **expected value of the sample mean**, which in turn is equal to the population mean, 0.60 or 60%. Similarly, the "give or take" value is the *standard error* (SE) of the mean, which is equal to $\frac{\sigma}{\sqrt{n}}$ , where σ is the standard deviation of the population and n is the size of the sample. We're told that σ is about 0.5 and n is 100, so the standard error of the mean is 0.05 or 5%.
:::

### Expected value and standard error for the sum

If we are interested in the sum of n draws $S_n$, the sum and the average are related by $S_n = n\bar{x_n}$ so the standard error of the sum:
$$SE(S_n)=\sqrt{n}\sigma$${#eq-standardErrorSum} 
which tells us that the variability of the sum of n draws increases at the rate of $\sqrt{n}$ so **while increasing the sample size reduces the standard error of the average, it actually increases the standard error for the sum.**

::: exercise-box
Exercise1 (Expected Value):

*You solicit 100 pledges for a charitable organization. Each pledge is equally likely to be \$10, \$50, or \$100. You may use the fact that the standard deviation of the three amounts \$10, \$50 and \$100 is \$37.*

*What is the expected value of the sum of the 100 pledges?*

The expected value of the sum of a sample is nμ, where n is the size of the sample and μ is the mean of the population. Here we're told that n is 100 and μ must be the average of \$10, \$50, and \$100, which is \$53.33, so the expected value of the sample sum is \$5333.
:::

::: exercise-box
Exercise2 (Standard error for the sum)

*You solicit 100 pledges for a charitable organization. Each pledge is equally likely to be \$10, \$50, or \$100. You may use the fact that the standard deviation of the three amounts \$10, \$50 and \$100 is \$37.*

*What are the chances that the 100 pledges total more than \$5,700 ?*

The standard error of the sample sum is $SE(S_n)=\sqrt{n}\sigma$  where n is the sample size and σ is the standard deviation of the population. 

Since n is 100 and we're told that the standard deviation of the population is \$37, we know the standard error of the sample sum is \$370.

Using the normal approximation to the sampling distribution, this tells us that \$5700 is roughly one standard deviation (5700 - 5333 = 366) above the mean of the sampling distribution. From the **empirical rule**, we know that one half of 68% of the data lies within one standard deviation to the right of the mean, so that 50 + (1/2)68 = 84% of the data lies to the left of \$5700. That is, 16% percent of the data lies to right of \$5700.
:::

### Expected value and standard error for percentages.

For percentages, the expected value is 
$$E = \mu * 100\%$${#eq-expectedValuePerc} 
and the Standard Error 
$$SE=\frac{\sigma}{\sqrt{n}} *100\%$${#eq-standardErrorPerc}

All the above formulas are for sampling with replacement, but they are still approximately true when sampling without replacement if the sample size is much smaller than the size of the population.

------------------------------------------------------------------------

::: exercise-box
Exercise:

*We toss a coin 100 times. How many tails do you expect to see?*

we assing tails a value of 1, and heads a value of 0.

p(1)=0.5, p(0)=0.5

Number of expected tails = $E(sum) = 100 * \mu$

and $\mu = 0*p(0)+1*p(1)$ , so $\mu =0.5$ and $E= 100*0.5$ =50 tails.

The Standard Error will be $SE(sum) = \sqrt{100}*\sigma$

to calculate sigma, we know that $\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2}$ where:

-   ($\sigma$) is the standard deviation,

-   ($N$) is the total number of observations,

-   ($X_i$) represents each individual observation,

-   ($\mu$) is the mean of the observations.

In our exercise, without having to toss the coing 100 times we calculate it like this:

$\sigma =\sqrt{(x_1-\mu)^{2} * p(1) +(x_2-\mu)^{2}*p(2)}$

$\sigma =\sqrt{(0-0.5)^{2} * 0.5 +(1-0.5)^{2}*0.5}=0.5$

$\sigma^2 ={(0-0.5)^{2} * 0.5 +(1-0.5)^{2}*0.5}=0.25$

$SE(sum) = \sqrt{100}\sigma^2=10*0.25=5$

so our answer is that we expect to see 50 tails, give or take 5 tails.

and in percentages we would expect to see E= 0.5\*100 =50% tails with an standard error of SE =0.5/10\*100 = 5%

in r code, an example of the same problem with n=200 tosses. See the difference in the percentages.

```{r}
# Define the probabilities 
p <- c(0.5, 0.5)  
# Define the values (0 for heads, 1 for tails) 
x <- c(0, 1)  
# Calculate the mean (μ) mu <- sum(x * p)  
# Calculate the variance (σ^2) 
variance <- sum((x - mu)^2 * p)  
# Calculate the standard deviation (σ) 
sigma <- sqrt(variance)  
# Number of coin tosses n <- 200  
# Calculate the standard error (SE) 
SE <- sqrt(n) * sigma  
 
cat("Expected percentage of tails:", n * mu, "\n") 
cat("Standard error:", SE, "\n") 
cat("Expected number of tails:", mu*100, " % \n") 
cat("Expected standard error percentage:", sigma /sqrt(n), "% \n")
```
:::

------------------------------------------------------------------------

## The sampling distribution

If we toss a fair coin 100 times, we can get any number of tails from 0 to 100. How likely is each outcome?

The number of tails has the binomial distribution with `n=100` and `p=0.5` where `success = tails`. So if the statistic of interest is $S_n=number\ of \ tails$ , then $S_n$ is a random variable whose probability histogram is given by the binomial distribution. This is called the *sampling distribution of the statistic $S_n$*. 
The sampling distribution provides more detailed information about the chance properties of $S_n$ than the summary numbers given by the expected value and the standard error alone.

There are three histograms to take into consideration:

### Probability histogram

The probability histogram for producing the data. Since both tails and heads have the same probability, our histogram would be two columns with the same height.

```{r, fig.align='center'}
#| echo: false
heads = rep(0,50)
tails = rep(1,50)
theoreticalP = c(heads,tails)
theoreticalP
# Convert the vector to a data frame
df <- data.frame(theoreticalP)


# Create the histogram
gg <- ggplot(df, aes(x = theoreticalP)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  labs(title = "Histogram of Theoretical Probabilities", x = "Value", y = "Frequency")

# Print the plot
print(gg)
```

### Empirical histogram

If we toss the coins, we will have am empirical histogram

```{r, fig.align='center', echo=FALSE}
# Set the number of coin tosses
num_tosses <- 100

# Simulate the coin tosses
coin_tosses <- sample(c(0, 1), size = num_tosses, replace = TRUE, prob = c(0.5, 0.5))

# Create a dataframe
df <- data.frame(toss = 1:num_tosses, result = coin_tosses)

# Create the histogram
gg <- ggplot(df, aes(x = result)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  labs(title = "empirical histogram", x = "Value", y = "Frequency")

# Print the plot
print(gg)
```

### Probability histogram of the sample

And finally we can do the probability histogram of the statistic $s_{100}$, which shows the sampling distribution:

```{r, fig.align='center' , echo=FALSE}
# Set the parameters
n <- 100
p <- 0.5

# Calculate the probabilities for each number of tails (0 to 100)
tails <- 0:n
probabilities <- dbinom(tails, size = n, prob = p)

# Create a dataframe
df <- data.frame(tails = tails, probability = probabilities)

# Create the histogram
gg <- ggplot(df, aes(x = tails, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability Distribution of Number of Tails in 100 Coin Tosses",
       x = "Number of Tails",
       y = "Probability") +
  theme_minimal()

# Print the plot
print(gg)

```

# Central limit theorem (CLT)

When sampling with replacement and n is large, then the sampling distribution of the sample average approximately follows the normal curve.

For example, going back to the example of of the online gambling where we had a probability of winning a small prize with p=0.2, we can see how the probability histogram for the binomial distribution approximates more and more to the normal distribution as we increase the number of trials n.

```{r, fig.align='center', echo=FALSE}
# Set the parameters
n <- 1
p <- 0.2

# Calculate the probabilities for each number of tails (0 to 100)
success <- 0:n
probabilities <- dbinom(success, size = n, prob = p)

# Create a dataframe
df <- data.frame(success = success, probability = probabilities)

# Create the histogram
gg <- ggplot(df, aes(x = success, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability histogram for the binomial n=1",
       x = "Number of successes with n=2",
       y = "Probability") +
  theme_minimal()

# Print the plot
print(gg)

n <- 10
p <- 0.2

# Calculate the probabilities for each number of tails (0 to 100)
success <- 0:n
probabilities <- dbinom(success, size = n, prob = p)

# Create a dataframe
df <- data.frame(success = success, probability = probabilities)

gg <- ggplot(df, aes(x = success, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability histogram for the binomial n=10",
       x = "Number of successes with n=10",
       y = "Probability") +
  theme_minimal()

print(gg)
n <- 100
p <- 0.2

# Calculate the probabilities for each number of tails (0 to 100)
success <- 0:n
probabilities <- dbinom(success, size = n, prob = p)

df <- data.frame(success = success, probability = probabilities)

gg <- ggplot(df, aes(x = success, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability histogram for the binomial n=100",
       x = "Number of successes with n=100",
       y = "Probability") +
  theme_minimal()

print(gg)
```

That means that we can use normal approximation to compute probabilities. The key point of the theorem is that we know that the sampling distribution of the statistic is normal no matter what the population histogram is.

The mathematical theory behind that:

The CLT is one of the most frequently used mathematical results in science. It tells us that when the sample size is large, the average $\bar{y}$ of a random sample follows a normal distribution centered at the population average μ and with standard deviation equal to the population standard deviation σ, divided by the square root of the sample size N. We refer to the standard deviation of the distribution of a random variable as the random variable's standard error. We have seen this formula before when studying the Standard Error (@eq-standardErrorMean): 
$$ 
\text{SE} (\bar{x_n})= \frac{\sigma}{\sqrt{n}} 
$$ 
This is telling us that when we take more samples and plot their results in a histogram, the spread of the histogram becomes smaller (closer to the population average) as we saw in the histograms above.

**Standardized sample mean**

This implies that if we take many samples of size $N$, then the quantity is approximated with a normal distribution centered at 0 and with standard deviation 1

$$
\frac{\bar{Y} - \mu}{\sigma_Y/\sqrt{N}}
$${#eq-standardizedSampleMean} 

-   $\bar{Y}$: The sample mean, which is the average of your sample data.

-   $\mu$: The population mean, which is the average of the entire population from which the sample is drawn.

-   $\sigma_Y$: The population standard deviation, which measures the spread of the population data.

-   $N$: The sample size, or the number of observations in your sample.

The standardized sample mean is a way to transform the sample mean into a standard normal distribution (mean 0, standard deviation 1). Standardizing the sample mean by subtracting the population mean and dividing by the standard error $\sigma_Y/\sqrt{N}$ allows us to compare it to the standard normal distribution. This is useful because the properties of the normal distribution are well understood and widely applicable in statistical inference.

**Central limit theorem in practice.** The central limit theorem allow us, when we don't have access to the population data, to use the normal approximation so we can compute p-values.

We are going to use the mice bodyweight data for our example. We willwork only with female mice because the bodyweight of female and male mice are different. We calculate the mean and the standard deviation of control and treatment groups and those will be the parameteres of our population.

The CLT tells us that when the samples are large, the random variable is normally distributed. Thus we can compute p-values using the function `pnorm`

```{r miceexampleCLT}
dat <- read.csv("data/mice_pheno.csv") 
table(dat$Sex, dat$Diet)

controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  dplyr::select(Bodyweight) %>% unlist
hfPopulation <- filter(dat,Sex == "F" & Diet == "hf") %>%  
  dplyr::select(Bodyweight) %>% unlist

mu_hf <- mean(hfPopulation)
mu_control <- mean(controlPopulation)
print(mu_hf - mu_control)
sd_hf <- sd(hfPopulation)
sd_hf
sd_control <- sd(controlPopulation)
sd_control
```

Remember that in practice we do not get to compute these population parameters. These are values we never see. In general, we have to estimate them from samples.

As we described, the CLT tells us that for large N, each of these is approximately normal with average population mean and standard error population variance divided by N. We mentioned that a rule of thumb is that N should be 30 or more. However, that is just a rule of thumb since the preciseness of the approximation depends on the population distribution. Here we can actually check the approximation and we do that for various values of N.

We are going to create 10,000 samples of N number of mice for N= 3,12,25 and 50 and calculate the difference for the means between treatment and control bodyweight. We then calculate the t-statistic for each sample size (t-statistic is explained later)

Statistical theory tells us that if we divide a random variable by its SE, we get a new random variable with an SE of 1.This ratio is what we call the t-statistic.

$$
t = \frac{\bar{x} - \mu}{SE}
$${#eq-tstatistic} 
where $SE = \frac{s}{\sqrt{n}}$

In the specific case of our experiment (comparing the means of two samples) this translates into: 
$$ 
t = \frac{\bar{y} - \bar{x}}{SE}=\frac{\bar{y} - \bar{x}}{\sqrt{\frac{s_y^2}{n} + \frac{s_x^2}{n}}} 
$$

Finally we will plot a QQ plot for each sample size against the normal distribution to see their fit:

```{r, fig.align='center'}
Ns <- c(3,12,25,50)
B <- 10000 #number of simulations
##function to compute a t-stat
computetstat <- function(n) {
  y <- sample(hfPopulation,n)
  x <- sample(controlPopulation,n)
  (mean(y)-mean(x))/sqrt(var(y)/n+var(x)/n)
}
res <-  sapply(Ns,function(n) {
  replicate(B,computetstat(n))
})
mypar(2,2)
for (i in seq(along=Ns)) {
  qqnorm(res[,i],main=Ns[i])
  qqline(res[,i],col=2)
}
```

So we see that for $N=3$, the CLT does not provide a usable approximation. For $N=12$, there is a slight deviation at the higher values, although the approximation appears useful. For 25 and 50, the approximation good.

This simulation only proves that $N=12$ is large enough in this case, not in general.The further the population distribution is from the normal distribution, the higher the sample size we will require.

For populations that are already normally distributed, even small sample sizes will result in sample means that are approximately normally distributed. For populations that are not normally distributed, larger sample sizes are required for the sample means to approximate a normal distribution.

We will see later that we don't usually calculate our p-values like this using the CLT, instead we will use a *t-test* that will give us a slightly different result. This is to be expected because our CLT approximation considered the denominator of t-stat practically fixed (with large samples it practically is), while the t-distribution approximation takes into account that the denominator (the standard error of the difference) is a random variable. The smaller the sample size, the more the denominator varies.

::: {.callout-orange appearance="simple" icon="false"}
**When does the central limit theorem apply?** For the normal approximation to work, the key requirements are:

-   we sample with replacement, or we simulate independent random variables from the same distribution. (If the sample size is much smaller than the population size, then sampling with replacement and sampling without replacement is approximately the same so this also applies)

-   The statistic of interest is a sum, average or percentage.

-   The sample size is large enough. The more skewed the population histogram is, the larger the required sample size n. If there is no strong skewness, then n=15 is sufficient.
:::

::: exercise-box
Exercise:

Suppose we are interested in the proportion of times we see a 6 when rolling n=100 dice. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies.

We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance $\frac{p(1-p)}{n}$. So according to the CLT: 
$$
z = \frac{observed_p - p}{\sqrt{(p\times(1-p))/n)}}
$$

z should be normal with mean 0 and SD 1.

Perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05).

```{r}
n=100
N=10000
p=1/6
SE<- sqrt(p*(1-p)/n)
set.seed(1)

res<- replicate(N,sample(1:6,n,replace=TRUE))

prop = apply(res, 2, function(x) mean(x == 6))
hist(prop, freq=TRUE)
z_scores <- (prop-p)/SE
plot(z_scores,dnorm(z_scores))

proportion_greater_than_2 <- mean(abs(z_scores) > 2)
print(proportion_greater_than_2)
```
:::

# Significance test vs Confidence intervals

There are two fundamental sorts of questions we might ask our sample data:

-   How can we estimate a parameter while taking into account the variability of the data in an honest way?

-   How can we know if our data supports a specific conclusion, given its inherent variability?

The first of these questions leads to the idea of a **confidence interval**, where we specify not only an estimate of a parameter but also a reasonable margin of error. The latter question gives rise to **significance testing**.

::: {#confidenceIntvsSignifTest style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
-   Use a confidence interval if you wish to estimate a parameter from a sample in a way that describes not only the observed mean but also the uncertainty surrounding it.

-   Use a significance test if there is one particular value of interest, for instance representing whether a piece of machinery is properly calibrated.
:::

Significance tests and confidence intervals are two sides of the same coin.

Significance tests consider specific individual values, while confidence intervals give more general information about the parameter of interest. The latter are more flexible and are generally easier to interpret. Consider a significance test only when there is a single parameter value of interest which you could identify before looking at the data.

# Confidence intervals

A confidence interval is a way of reporting an estimate of a parameter that includes information about how much variability could reasonably be expected due to random chance. A confidence interval for the mean of a quantitative variable has the form

$$
\mu = \bar{x} \pm ME
$$

where $\mu$ is the population mean, $\bar{x}$ is the observed sample mean and $ME$ is a **margin of error.**

## Manual calculation:

A confidence interval gives a range of plausible values for a population parameter. Usually the confidence interval is centered at an estimate for $\mu$ which is an average. Since the central limit theorem applies for averages, the confidence interval has a simple form:

$$CI = estimate \mp ME$$

where **ME is the margin of error** and can be decomposed like this:

$$CI=estimate \mp z\ * SE = \mu \mp z*SE$${#eq-confidenceInterval}

where SE is the *standard error* of the sample. If that is an average or a percentage then it is $SE= \frac{\sigma}{\sqrt{n}}$ as we saw in (@eq-standardErrorMean).

And $z$ is the z-score corresponding to the desired confidence level. For example, if we would like to have a 95% confidence level, our $z=1.96$

This comes from calculating the value of z where 95 of our data is in the middle:

```{r, fig.align='center'}
#| echo = FALSE
x <- seq(-4, 4, length=100)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue", xlab="Z", ylab="Density", main="Standard Normal Distribution")

# Highlight the area between -1.96 and 1.96
polygon(c(-1.96, seq(-1.96, 1.96, length=100), 1.96), 
        c(0, dnorm(seq(-1.96, 1.96, length=100)), 0), 
        col=rgb(0.1, 0.1, 0.9, 0.2), border=NA)

# Add vertical lines at z = ±1.96
abline(v = 1.96, col = "red", lwd = 2, lty = 2)
abline(v = -1.96, col = "red", lwd = 2, lty = 2)

# Add annotation at z = 1.96
text(1.96, dnorm(1.96), labels = "z = 1.96", pos = 4, col = "red")

# Add annotation for 95% confidence interval
text(0, 0.1, labels = "95%", pos = 3, col = "blue")
```

To get to that z value we use tables or we can use the quantile function qnorm:

```{r}
# Calculate the z-score for a 95% confidence interval
z_score <- qnorm(0.975)
z_score
```

The `0.975`value is used because for a 95% confidence interval, you need to capture the central 95% of the distribution, leaving 2.5% in each tail. Therefore, you look up the 97.5th percentile (0.975) to get the z-score.

For 90% confidence interval we are looking for the z value in the normalized distribution where 90% of the data falls in our center range and 10% outside, so 5% in each tail.

```{r}
# Calculate the z-score for a 95% confidence interval
desiredConfidence <- 90
tails = (100-desiredConfidence )/2
percentileOfinterest <- desiredConfidence+tails
z_score <- qnorm(percentileOfinterest/100)
z_score
```

For a 99 confidence level:

```{r}

# Calculate the z-score for a 95% confidence interval
desiredConfidence <- 99
tails = (100-desiredConfidence )/2
percentileOfinterest <- desiredConfidence+tails
z_score <- qnorm(percentileOfinterest/100)
z_score
```

**Estimating the SE with bootstrap principle**

We still have a problem for calculating the confidence interval following this, and it is that we need to know the standard deviation of the population $\sigma$ and we usually don't know it, but the [bootstrap principle](#bootstrap) states that we can calculate sigma by its sample version $s$ and still get an approximately correct confidence interval.

::: exercise-box
Example:
We poll 1000 likely voters and find that 58% approve of the way the president handles his job.

$SE= \frac{\sigma}{\sqrt{n}} * 100$ where $\sigma = \sqrt{p(1-p)}$ where $p$ is the proportion of **all voters** who approve, but we don't know p, but the bootstrap principle tells us that we can replace $\sigma$ by $s$ so we can plug in the values from our survey here: $=\sqrt{0.58(1-0.58)} = 0.49$

So a 95% confidence interval for p is:
$$
58\% \mp 1.96 \frac{0.49}{\sqrt{1000}}*100
$$

 which is approximately \[54.9%,61.1%\]
:::

The width of the confidence interval is determined by z and the standard error SE. To reduce that margin of error we have two options, we can increase the sample size or decrease the confidence level. The sample size is square rooted so this means that to cut the width of the confidence level in half we need four times the sample size, and to reduce it 10 times we would need 100 times the sample size.

## Getting the confidence intervals from a test result

```{r}
#simulate our survey results
success<- rep(1,580)
failure<- rep(0,420)
mydata <- c(success,failure)
#do a te.test.
testResult <- t.test(mydata)
confInt95_low <- testResult$conf.int[1]
confInt95_upp <- testResult$conf.int[2]
confInt95_upp
cat('confidence interval = [',confInt95_low,confInt95_upp,']')
margin_of_error <- (confInt95_upp - confInt95_low) / 2
cat('margin of error: ',margin_of_error)
```

For example in our optical dataset, if we do a t.test over that variable we can get the confidence intervals for a level of confidence 95%:

```{r, fig.align='center'}
file1 <- here("data", "optical_sample.xlsx")
optical_sample <- read_excel(file1)

testResult <- t.test(optical_sample$eye_difference)

confInt95_low <- testResult$conf.int[1]
confInt95_upp <- testResult$conf.int[2]

margin_of_error <- (confInt95_upp - confInt95_low) / 2 

ggplot(optical_sample, aes(x = eye_difference)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical_sample$eye_difference),
             linetype = "dashed")+
  geom_vline(xintercept = confInt95_low,
           linetype = "dotted", color = "green")+
  geom_vline(xintercept = confInt95_upp,
           linetype = "dotted", color = "green")
```

the margin of error will be:

```{r}
margin_of_error <- (confInt95_upp - confInt95_low) / 2 
margin_of_error
```

Now we can calculate the confidence intervals for other 90% and 99% level of confidence and see their differences in a graph:

```{r, fig.align='center'}
#level of confidence 90%. 
testResult <- t.test(optical_sample$eye_difference,
       conf.level = .90)
confInt90_low <- testResult$conf.int[1]
confInt90_upp <- testResult$conf.int[2]

#level of confidence 99%. 
testResult <- t.test(optical_sample$eye_difference,
                   conf.level = .99)
confInt99_low <- testResult$conf.int[1]
confInt99_upp <- testResult$conf.int[2]

ggplot(optical_sample, aes(x = eye_difference)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical_sample$eye_difference),
             linetype = "dashed")+
  geom_vline(xintercept = confInt90_low,
             linetype = "dotted", color = "orange")+
  geom_vline(xintercept = confInt90_upp,
             linetype = "dotted", color = "orange")+
  geom_vline(xintercept = confInt95_low,
           linetype = "dotted", color = "green")+
  geom_vline(xintercept = confInt95_upp,
           linetype = "dotted", color = "green")+
  geom_vline(xintercept = confInt99_low,
             linetype = "dotted", color = "yellow")+
  geom_vline(xintercept = confInt99_upp,
             linetype = "dotted", color = "yellow")
```

Three factors feed into the size of the margin of error:

1.  The **confidence level of the interval**. That is, the proportion of the time our interval would correctly capture the parameter of interest. Higher confidence requires a larger margin of error.

2.  The **spread of the observations** in the data set. More spread in the data implies more sampling variability and therefore a larger margin of error.

3.  The **size of the sample**.

::: exercise-box
Exercises:

Using the pm_sample data set,

1.  Find a 95% confidence interval for the mean salary of the project managers in this population. Interpret the results in plain language. Would a 99% confidence interval be wider of more narrow?

2.  Find a 95% confidence interval for the man non-salary compensation of project managers in this population. Why is the margin of error different than in the first problem?

```{r}

file <- here("data", "pm_survey.xlsx") 
pm_survey <- read_excel(file) 

kable(head(pm_survey))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()
as.data.frame(report(pm_survey$annual_salary))

t.test(pm_survey$annual_salary) 

```

The confidence interval at the (default) 95% is `r t.test(pm_survey$annual_salary)$conf.int` which means in plain language that if we were to repeat the sampling and the test multiple times, this confidence interval would capture the true value of the parameter 95% of the time. Although not extrictly true, we can say that there's a 95% chance that this confidence interval includes the true population mean.

For the second exercise

```{r}
pm_survey <- pm_survey |>    
  mutate(other_monetary_comp = replace_na(other_monetary_comp, 0))  

as.data.frame(report(pm_survey$other_monetary_comp))

t.test(pm_survey$other_monetary_comp)  
```

our confidence interval is `r t.test(pm_survey$other_monetary_comp)$conf.int` and so, the margin of error in both examples are:

```{r}
monetary_marginError<- (t.test(pm_survey$annual_salary)$conf.int[2] - 
                          t.test(pm_survey$annual_salary)$conf.int[1]) / 2 
monetary_marginError

nonMonetary_marginError<- (t.test(pm_survey$other_monetary_comp)$conf.int[2] - 
                             t.test(pm_survey$other_monetary_comp)$conf.int[1]) / 2
nonMonetary_marginError
```

The difference in both is due to the variability of the data, that we can calculate using the standar deviation:

```{r}
sd(pm_survey$annual_salary) 
sd(pm_survey$other_monetary_comp)
```
:::

### Interpreting confidence intervals: practical example

We are going to calculate the confidence interval for 100 samples of 30 observations calculated over the same population and plot it on a graph with a line marking the true mean. We will see how some of them (for a 95% confidence level it should be around 5% of the times) will still miss the population parameter:

```{r, fig.align='center'}

#calculate the sample mean and confidence interval from a sample of 100 
low = numeric()
high = numeric()
for (n in 1:100){
  sample <- slice_sample(optical, n = 30)
  test <- t.test(sample$eye_difference)
  low[n] <- test$conf.int[1]
  high[n] <- test$conf.int[2]
}

ci_reps <- data.frame("replicate" = 1:100,
                      low,
                      high)
ggplot(ci_reps, aes(x = low, 
                    xend = high,
                    y = replicate, 
                    yend = replicate)) + 
  geom_segment() +
  geom_vline(xintercept = mean(optical$eye_difference), 
             linetype = "dashed")
```

When interpreting a confidence interval, remember that not all values in an interval are equal. The center is always more likely than the ends.

# Significance testing

A t-significance test is used to consider whether an individual parameter value of interest is plausible in light of sample data.

The situation in which the parameter has that value is referred to as the **null hypothesis**, and the situation in which it does not is referred as to the **alternative hypothesis**. For example: could be the difference between left eye value and right eye value actually be zero? Does a certain large company pay better than the national average, or could salaries there just be different through random chance?

A test statistic measures how far away the data in our sample are from what we would expect if the null hypothesis $H_0$ were true.

The most common test statistic is the z-statistic. It determines how far an observed value is from the expected value, measured in standard errors.We already saw this formula before (@eq-zscore)

$$
z= \frac{observed - expected}{SE}
$${#eq-zscore3}

Observed is a statistic that is appropriate for assessing $H_0$. For example, if we toss a coin ten times and want to know if it is a fair coin, the appropriate statistic would be the number of tails or the percentage of tails.

Expected and SE are the expected value and the SE of this statistic computed under the assumption that $H_0$ is true.

::: exercise-block
Example:

We toss a coin 10 times, and get 7 tails. Using the formulas for the expected values of sums we have: Number of expected tails if the coin is fair:

For a binomial distribution we have: $E=n * p$ where $n$ is the number of repetitions and $p$ is the the probability of success (0,5), and the standard error for binomial scenarios is $SE = \sqrt{np(1-p)}$ .

$$
expected = 10 * \frac{1}{2} = 5
$$

$$
SE = \sqrt{10}\sqrt{\frac{1}{2} * \frac{1}{2}} =1.58
$$

$$
z= \frac{7-5}{1.58}=1.27
$$

By the central limit theorem, the p-value can be computed with normal approximation.

```{r}
z <- 1.27

# Calculate the cumulative probability up to z
p_value <- (1 - pnorm(z))*2

# Print the p-value
p_value
```

In other words, we have a 20.4% probability of observing 7 tails in 10 tosses given that the coin is fair.
:::

If the z-value is large, that means that there is a great difference between the observed and the expected, so large values of z are evidence against $H_0$. The strength of the evidence is measured by the p-value or observed significance level.

The p-value is the probability of getting a value of z as extreme or more extreme than the observed, assuming the null hypothesis is true. As we can see in the graph below, as z becomes larger, there is less probability of finding data outside of its limits, so the p-value will be smaller. We multiply the p value times 2 because we have to consider the two tails of the graph for this experiment.

The Z-score tells you how far an observed value is from the expected value in terms of standard errors. The p-value tells you the probability of observing a test statistic as extreme as, or more extreme than, the observed statistic, under the null hypothesis.

```{r, fig.align='center', echo=FALSE}
#| echo = FALSE
x <- seq(-4, 4, length=100)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue", xlab="", ylab="Density", main="Standard Normal Distribution", xaxt='n')

# Highlight the area before -1.27
polygon(c(-4, seq(-4, -1.27, length=100), -1.27), 
        c(0, dnorm(seq(-4, -1.27, length=100)), 0), 
        col=rgb(0.1, 0.1, 0.9, 0.2), border=NA)

# Highlight the area after 1.27
polygon(c(1.27, seq(1.27, 4, length=100), 4), 
        c(0, dnorm(seq(1.27, 4, length=100)), 0), 
        col=rgb(0.1, 0.1, 0.9, 0.2), border=NA)

# Add vertical lines at z = ±1.27
abline(v = 1.27, col = "red", lwd = 2, lty = 2)
abline(v = -1.27, col = "red", lwd = 2, lty = 2)

# Add annotation at z = 1.27
text(1.27, dnorm(1.27), labels = "z = 1.27", pos = 4, col = "red")
text(-1.27, dnorm(-1.27), labels = "z = -1.27", pos = 4, col = "red")


```

The **result of a significance test is a p-value,** which measures how plausible the value of interest is given the sample data. A p-value close to zero indicates the value isn't compatible with the data. As a general rule, if p\<0.05, the value can be considered questionable.

Ideally, you should identify a value of interest (the technical term is null hypothesis) before collecting the data. While this isn't always done in practice, doing so helps prevent wrong conclusions that come from the shotgun effect

> If you run a test because you observed an interesting pattern in your sample data, the overall chance of encountering a significant result increases. Data is like firing a shotgun with many pellets---some are bound to hit the target purely due to randomness, if you create your null theory based on a characteristic observed in your sample you are likely going to find significance, even if that is not characteristics of the overall population and it was there just by chance.

While a p-value might tell you that a parameter is different from a hypothesized value, it won't ever say how different it might be or if that difference is important.

A t-significance test is appropriate for quantitative data under the exact same circumstances as a t-confidence interval: unless the sample is very small and the data is highly non-symmetric. If the data is relatively symmetric and there are no extreme outliers, n=10 is usually enough. Regardless of the shape of the data, n=30 is a safe threshold.

The value of $\mu$ (mu) is the value we want to test against (our null hypothesis)

```{r}
file1 <- here("data", "optical_sample.xlsx")
optical_sample <- read_excel(file1)

testResult <- t.test(optical_sample$eye_difference, mu = 0) 

testResult$p.value
```

*p-value* express the probability of observing the values we have in the sample if the eye difference of the population were in fact 0. The smaller the p-value, the smaller the probability. Usually we take the threshold of 5% or p-value \<0.05 to reject the null theory, as it means that it is unlikely that the mean observed in our sample can come from a population whose true mean is 0.

We can change the null theory to whatever value we want to measure against. For example, can we say that the average age of the population is NOT 59?

```{r}
testResult <- t.test(optical_sample$Age, mu = 59)
testResult$p.value 
```

our p-value is `{r}testResult$p.value` which does not allow us to reject the null theory.

::: {#SignificanceTestQuickFacts .callout-orange}
Quick Facts

1.  A p-value below 0.05 doesn't mean there's a 5% chance the null hypothesis is true. Instead, it means if the null were true, there's a 5% chance of observing data as extreme as, or more extreme than the one that was observed in the sample.

2.  Statistical power, the ability to detect a true effect is often overlooked. A study can have a high chance of missing real effects (Type II error) even if its p-value threshold is stringent.

3.  In proportional analysis, Simpson's Paradox can occur where a trend seen in several groups reverses when the groups are combined. It underscores the importance of scrutinizing aggregated data.
:::

### One sided vs two sided test.

One has to carefully consider whether the alternative should be one-sided or two-sided test (we are considering the values on the left of -z and the right of z) or only the values on one side. If we are considering a two sided test, the p-value gets doubled. It is not ok to change the alternative afterwards in order to get the p-value under 5%.

Deciding whether to use a one-sided or two-sided t-test depends on the hypothesis you want to test. Here are some guidelines to help you make that decision:

Use a **one-sided t-test** when you have a specific direction in mind for your hypothesis. This means you are testing whether the mean is either greater than or less than a certain value, but not both.

**Examples**:

-   **Greater than**: You want to test if the mean score of a new teaching method is greater than the mean score of the traditional method.

-   **Less than**: You want to test if the mean time to complete a task using a new software is less than the mean time using the old software.

Use a **two-sided t-test** when you are interested in any difference from the specified value, regardless of direction. This means you are testing whether the mean is different from a certain value, without specifying the direction of the difference.

**Examples**:

-   You want to test if the mean weight of a sample of apples is different from a known standard weight.

-   You want to test if the mean score of a new drug is different from the mean score of a placebo.

How to Decide

1.  **Define Your Research Question**: Clearly state what you are trying to find out.

2.  **Determine the Direction of Interest**: Decide if you are only interested in deviations in one direction (one-sided) or in both directions (two-sided).

3.  **Formulate Your Hypotheses**: Based on your research question and direction of interest, formulate your null and alternative hypotheses.

**Example Scenario**

Suppose you are testing a new drug and want to know if it has a different effect on blood pressure compared to a placebo. If you only care whether the drug lowers blood pressure, you would use a one-sided test. If you care whether the drug either lowers or raises blood pressure, you would use a two-sided test.

```{r}
# Sample data
sample_data <- c(78, 82, 85, 90, 76, 79, 81, 77, 74, 88)

# Perform one-sided t-test
t_test_result <- t.test(sample_data, mu = 75, alternative = "greater")

# Print the result
print(t_test_result)

# Perform two-sided t-test
t_test_result <- t.test(sample_data, mu = 75, alternative = "two.sided")

# Print the result
print(t_test_result)

```

### Common errors in significance testing

At the start of a significance test, the hypothesized value might be true or false. At the end, it may be rejected or not. Altogether, there are four possible combinations of these outcomes:

|                       |           $H_0$ is true           |           $H_0$ is false            |
|-----------------:|:------------------------:|:--------------------------:|
|     $H_0$ is rejected | **Type I error** -false positive- |            True Negative            |
| $H_0$ is not rejected |           True Positive           | **Type II error** -False negative - |

Bear in mind, however, that we usually don't know whether the null theory is true or not.

We call the *event* of rejecting the null hypothesis, when it is in fact true, a *Type I error*, we call the *probability* of making a Type I error, the *Type I error rate*, and we say that rejecting the null hypothesis when the p-value is less than $\alpha$, *controls* the Type I error rate so that it is equal to $\alpha$

We may observe a p-value higher than alpha even when the null hypothesis is false. This is a type II error. We ask ourselfs this question: How big does N have to be in order to detect that the absolute value of the difference is greater than zero? Type II error control plays a major role in designing data collection procedures before you actually see the data, so that you know the test you will run has enough sensitivity or power. Power is 1 minus Type II error rate, or the probability that you will reject the null hypothesis when the alternative hypothesis is true.

There are several aspects of a hypothesis test that affect its power for a particular effect size. Intuitively, setting a lower $\alpha$ decreases the power of the test for a given effect size because the null hypothesis will be more difficult to reject (for example from 0.05 to 0.01). This means that for an experiment with fixed parameters (i.e., with a predetermined sample size, recording mechanism, etc), the power of the hypothesis test trades off with its Type I error rate, no matter what effect size you target.

### Power calculations

we are going to use our mice dataset to see how we can calculate the power of our t test for type II errors. We consider that the data in the file is our entire population. If we look at the difference average weight for control vs treatment we appreciate that there is in fact a difference or around 9%:

```{r}
dat <- read.csv("data/mice_pheno.csv") 
controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  dplyr::select(Bodyweight) %>% unlist

hfPopulation <- filter(dat,Sex == "F" & Diet == "hf") %>%  
  dplyr::select(Bodyweight) %>% unlist

mu_hf <- mean(hfPopulation)
mu_control <- mean(controlPopulation)
print(mu_hf - mu_control)
print((mu_hf - mu_control)/mu_control * 100) #percent increase
```

Depending on our sample size, this difference will be perceived by our test or not. Let's see an example with sample size = 12 and alpha = 0.05. We will do first a single test and see that we cannot reject the null hypothesis based on the p-value:

```{r}
set.seed(1)
N <- 12
hf <- sample(hfPopulation,N)
control <- sample(controlPopulation,N)
t.test(hf,control)$p.value

```

If we ran this experiment multiple times we can get the percentage of times that our p-value manage to actually reject the null hypothesis:

```{r}
repetitions<-2000
N<- 12
alpha<- 0.05
reject<- function(N, alpha=0.05){
  hf <- sample(hfPopulation, N)
  control <- sample(controlPopulation, N)
  pval<- t.test(hf,control)$p.value
  pval < alpha
}

rejections<- replicate(repetitions,reject(N))
mean(rejections)

```

In this case we see that 25% of the time we correctly rejected the null hypothesis and this is the power of our test. To increase this percentage we should increase the sample size. In the next code section we are going to run the same simulation for various values of N

```{r, fig.align='center'}
Ns<- seq(5,50,5)
power <- sapply(Ns,function(N){
  rejections<- replicate(repetitions,reject(N))
  mean(rejections)
})
plot(Ns,power, type="b")
```

There are on the internet several tools that allow you to calculate the the power you need for a particular standard deviation, sizes of n or the effect size you want to detect.

### Student's t test

So far we have been using the t-test without explaining the student's t-distribution that is used under the hood. Let's explain it now.

We will explain the student's t test with an example: The health guideline for lead in drinking water is a concentration of not more than 15 parts per billion (ppb). Five independent samples from a reservoir average 15.6 ppb. Is this sufficient evicence to conclude that the concentration $\mu$ in the reservoir is above the standard of 15 ppb?

Our null hypothesis is that no, the concentration is just on the standard $H_0:\mu=15$ and the alternative hypothesis is that the concentration is higher than 15 ppb.

$$
z= \frac{observed - expected}{SE} = \frac{15.6-15}{SE}
$$

SE of average = $\frac{\sigma}{\sqrt{n}}$ but sigma is unknown. By the boostrap principle we know that we should be able to substitute the standard deviation of the population $\sigma$ with the standard deviation of our sample $s$ however for sample size less or equal to 20, then the normal curve is not a good enough approximation to the distribution of the z-statistic. Rather, an appropriate approximation is the Student's t-distribution with n-1 degrees of freedom.

```{r, fig.align='center', fig.height=4, fig.width=5}
#| echo = FALSE


# Create a sequence of x values
x_values <- seq(-4, 4, by = 0.01)

# Calculate the density of the t-distribution for different degrees of freedom
df1 <- dt(x_values, df = 1)
df2 <- dt(x_values, df = 2)
df5 <- dt(x_values, df = 5)
df_inf <- dnorm(x_values) # Normal distribution as t-distribution with infinite degrees of freedom

# Create a data frame for plotting
data_to_plot <- data.frame(
    x = c(x_values, x_values, x_values, x_values),
    density = c(df1, df2, df5, df_inf),
    degree_of_freedom = factor(c(rep(1, length(df1)), rep(2, length(df2)), rep(5, length(df5)), rep('∞', length(df_inf))))
)

# Plot using ggplot
ggplot(data_to_plot, aes(x = x, y = density, color = degree_of_freedom)) +
    geom_line() +
    labs(title = "Student's t-Distribution", x = "x", y = "Density") +
    scale_color_discrete(name = "ν")

```

In the graph we can see the purple line where the degrees of freedom are high (large sample) is just the normal curve. The rest of the lines are showing how with less degrees of freedom the tails of the curve get bigger, representing higher uncertainty.
We saw the **sample standard deviation formula**  (@eq-sampleStandarDeviation): 

$$
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2}
$$

In this case we also need to adjust our **confidence interval** formula from: $CI=estimate\mp z\  SE$

to $$CI= estimate (\mu) \mp t_{n-1}SE$${#eq-confidenceIntervalTStudent}

::: {.callout-orange appearance="simple" icon="false"}
::: centered-text
**z-test vs t-test**
:::

The main differences between a t-test and a z-test lie in their assumptions and applications:

**T-Test**

-   **Distribution**: Uses the Student's t-distribution.

-   **Population Variance**: Unknown and estimated from the sample.

-   **Sample Size**: Typically used for small sample sizes (n \< 30).

-   **Degrees of Freedom**: Required for the calculation.

-   **Application**: Used when comparing the means of two groups, especially when the sample size is small and the population variance is unknown.

**Z-Test**

-   **Distribution**: Uses the standard normal distribution (z-distribution).

-   **Population Variance**: Known.

-   **Sample Size**: Typically used for large sample sizes (n \> 30).

-   **Degrees of Freedom**: Not required.

-   **Application**: Used for hypothesis testing of means and proportions when the sample size is large and the population variance is known.

[**Key Differences**]{.underline}

1.  **Distribution**:

    -   T-test: Student's t-distribution.

    -   Z-test: Standard normal distribution.

2.  **Population Variance**:

    -   T-test: Unknown and estimated from the sample.

    -   Z-test: Known.

3.  **Sample Size**:

    -   T-test: Small sample sizes (n \< 30).

    -   Z-test: Large sample sizes (n \> 30).

4.  **Degrees of Freedom**:

    -   T-test: Required.

    -   Z-test: Not required.

[**Example Scenarios**]{.underline}

-   **T-Test**: Comparing the average test scores of two small classes where the population variance is unknown.

-   **Z-Test**: Testing the average height of a large population where the population variance is known.
:::

T-test are the most commonly used test in real life, but if we meet all the criteria to perform a z-test we can do it like this in r:

```{r}
library(BSDA)
# Sample data
sample_data <- c(88, 92, 94, 94, 96, 97, 97, 97, 99, 99, 105, 109, 109, 109, 110, 112, 112, 113, 114, 115)
population_mean <- 100
population_sd <- 15

# Perform a one-sample z-test
z_test_result <- z.test(sample_data, mu = population_mean, sigma.x = population_sd)
print(z_test_result)
```

# Categorical data

In 1912 the Titanic sank and more than 1500 of the 2229 people on board died. Did the chances of survival depend on the ticket class?

|          | First | Second | Third | Crew |
|----------|-------|--------|-------|------|
| Survived | 202   | 118    | 178   | 215  |
| Died     | 123   | 167    | 528   | 698  |

This is an example of categorical data. The data are counts for a fixed number of categories. Here the data is tabulated in a 2x4 table. Such table is called a **Contingency Table** because it shows the survival counts for each category of ticket class, i.e. contingent on ticket class.

## Confidence Intervals for Proportions.

Confidence intervals for proportions provide a range of plausible values for population proportions, enabling estimation with a desired level of confidence.

Proportional analysis techniques allow for the assessment and comparison of proportions across different categories, providing valuable insights into categorical data.

There are any number of statistical methods for computing confidence intervals for proportions. By far the simplest is:

$$
p = \hat{p} \pm \sqrt{\frac{\hat{p} \cdot (1 - \hat{p})}{n}}
$${#eq-confidenceIntervalProportions}

where $p$ is the actual population proportion and $\hat{p}$ is the observed proportion.

As long as the sample size isn't too small, the difference between methods should be minor. If your sample includes at least 5 of each sort of possible outcomes, you are fine with whatever default method your software uses to calculate the proportions.

In R we will use $prop.test(n_s , sample size)$ where $n_s$ is the number of successes.

Practice:

We are going to calculate the proportion of smokers in our population based on our optical_sample data

```{r}
file1 <- here("data", "optical_sample.xlsx")
optical_sample <- read_excel(file1)

as.data.frame(report(optical_sample$IsSmoker))

successNumber<- sum(optical_sample$IsSmoker)
sampleSize <- NROW(optical_sample$IsSmoker)
testResult <- prop.test(successNumber, sampleSize)
testResult

#proportion of success 
proportion<- testResult$estimate %>% print()
conf_int <- testResult$conf.int%>% print()

```

We can use the formula we studied at the beginning of the chapter to manually calculate the confidence intervals as well and see the difference with what the software used:

```{r}
ci_lower<- proportion - sqrt(proportion*(1-proportion)/sampleSize) 
ci_upper<- proportion + sqrt(proportion*(1-proportion)/sampleSize) 
ci_lower
ci_upper
```

Another way of manually calculating the confidence interval is using a z-score

In the example below we use 95%: (qnorm(0.975) is the $z-score$ corresponding for 95% conf. level)

```{r}
margin_of_error <- qnorm(0.975) * sqrt(proportion * (1 - proportion) / sampleSize)

# Calculate the confidence interval bounds
proportion - margin_of_error
proportion + margin_of_error
```

For different confidence levels you will need to find the correct z-score

### Non-binary categorical variables

If our categorical variable is not binary, we still can use this same test to calculate the proportion of one single category against the rest.

For example, to calculate the proportion of women in our pm_survey dataset:

```{r}
file2 <- here("data", "pm_survey.xlsx")
pm_survey <- read_excel(file2)

kable(head(pm_survey))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()
as.data.frame(report(pm_survey$gender))

pm_survey <- pm_survey |> 
  mutate(across(.cols = c(highest_level_of_education_completed, gender), as.factor))

numberOfWomen<- NROW(pm_survey[pm_survey$gender == 'Woman',])
sampleSize <- NROW(pm_survey)
prop.test(numberOfWomen, sampleSize)

```

### Few observations for each outcome

While there are statistical methods for dealing with samples with fewer than 5 or each sort of outcome, you should be conservative about making decisions based on them. That said, there are two methods that might be helpful in such situations:

-   The wilson score interval works well even for observed proportions near zero or one and has other good theoretical properties as well. Unlike other confidence intervals we've seen it isn't symmetric.

-   The rule of three is a great rought-and-ready way to compute a 95% confidence interval for a proportion when no positive results have been observed. If you have failures only, the interval will go from 0 to 3/n where n is the sample size, and if you have observed successes only, the confidence interval will go from 1 - (3/n) to 0

## Significance testing for proportions

We want to know if the proportion of the people taking medication in our sample is the same as the proportion of people in USA taking medication. This data we know is 66% of the USA population so we can plug in that value in our test just like that in the formula using $p=$ where p, in this case is the proportion for our null theory.

```{r}
file <- here("data", "optical_sample.xlsx")
optical_sample <- read_excel(file)

as.data.frame(report(optical_sample$TakingMedication))

medicated<- sum(optical_sample$TakingMedication == "yes")
sampleSize <- nrow(optical_sample)

testResult <- prop.test(medicated, sampleSize, p = .66)

testResult$p.value
testResult$conf.int
```

In our case we can reject the null hypothesis and say that it is not reasonable to believe that this sample was drawn from a population with sample mean of 66%

::: exercise-box
Exercises

**Exercise 1.** A media outlet claims that 60% of project managers in the U.S have a college degree as their highest level of education. Is that claim plausible using our pm_survey data set?

```{r}

file <- here("data", "pm_survey.xlsx")
pm_survey <- read_excel(file)

pmWithCollegeDegree <- sum(pm_survey$highest_level_of_education_completed == 'College degree')
sampleSize <- nrow(pm_survey)

testResult <- prop.test(pmWithCollegeDegree, sampleSize, p= 0.6)

testResult$p.value
testResult$conf.int
```

In this case we cannot reject the null hypothesis.

**Exercise 2**. Use that data set to construct a 95% confidence interval for the proportion of project managers that are under the age of 35.

```{r}
pm_under35 <- nrow (pm_survey[pm_survey$how_old_are_you %in% c("18-24","25-34"),])

testResult <- prop.test(pm_under35, sampleSize)
testResult$conf.int 
testResult$estimate
```

The confidence interval shows that between 44% and 57% of respondents are under 35
:::

## Goodness of Fit

Consider genetic data where you have two groups of genotypes (A or B) for cases and controls for a given disease. The statistical question is if genotype and disease are associated. As in the examples we have been studying previously, we have two populations (A and B) and then numeric data for each, where disease status can be coded as 0 or 1. So why can't we perform a t-test? Note that the data is either 0 (control) or 1 (cases). It is pretty clear that this data is not normally distributed so the t-distribution approximation is certainly out of the question. We could use CLT if the sample size is large enough, otherwise, we can use *association tests*.

Imagine we have 250 individuals, where some of them have a given disease and the rest do not. We observe that 20% of the individuals that are homozygous for the minor allele (group B) have the disease compared to 10% of the rest. Would we see this again if we picked another 250 individuals?

```{r}
disease=factor(c(rep(0,180),rep(1,20),rep(0,40),rep(1,10)),
               labels=c("control","cases"))
genotype=factor(c(rep("A",200),rep("B",50)),
                levels=c("A","B"))
dat <- data.frame(disease, genotype)
dat <- dat[sample(nrow(dat)),] #shuffle them up

tab<- table(genotype,disease)
tab
```

The typical statistics we use to summarize these results is the odds ratio (OR). We compute the odds of having the disease if you are an "B": 10/40, the odds of having the disease if you are an "A": 20/180, and take the ratio: $(10/40) / (20/180)$

```{r}
(tab[2,2]/tab[2,1]) / (tab[1,2]/tab[1,1])
```

To compute a p-value, we don't use the `OR` directly. We instead assume that there is no association between genotype and disease, and then compute what we expect to see in each cell of the table under the null hypothesis: ignoring the groups, the probabilty of having the disease according to our data is:

```{r}
p=mean(disease=="cases")
p
```

according to this probability, under our null hypothesis we expect to see a table like this:

```{r}
expected <- rbind(c(1-p,p)*sum(genotype=="A"),
                  c(1-p,p)*sum(genotype=="B"))
dimnames(expected)<-dimnames(tab)
expected
```

The Chi-square test uses an asymptotic result (similar to the CLT) related to the sums of independent binary outcomes. Using this approximation, we can compute the probability of seeing a deviation from the expected table as big as the one we saw. The p-value for this table is:

```{r}
chisq.test(tab)$p.value
```

so we would expect to find this difference in disease ratio by chance approximately 8.8% of the times, which is not enough to reject our null hypothesis.

**Large Samples, Small p-values**

Reporting only p-values is not an appropriate way to report the results of your experiment. Studies with large sample sizes will have impressively small p-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1.

To demonstrate, we recalculate the p-value keeping all the proportions identical, but increasing the sample size by 10, which reduces the p-value substantially:

```{r}
tab<-tab*10
chisq.test(tab)$p.value
```

::: {.callout-orange appearance="simple" icon="false"}
1.  **Fitting Fallacies**: Goodness of fit doesn't guarantee predictive power. A model can fit past data perfectly yet fail miserably on new, unseen data, highlighting the dangers of overfitting.

2.  **The Sample Size Paradox**: Doubling your sample size doesn't necessarily halve the error. In fact, to do so, you'd need to quadruple the sample, given the square root relationship between sample size and margin of error.

3.  **Two-Sample Twists**: When comparing two samples, it's possible for each sample's individual data to appear random, yet their combined data can reveal a distinct pattern or significant difference.
:::

A Goodness-of-fit test, also called chi-squared or Pearson goodness-of-fit test, considers whether a categorical variable has a hypothesized distribution

Warning! **A goodness of fit test doesn't give any information about which specific categories might be out of line with the hypothesized distribution.** While you might be able to make an educated guess by looking at the data, this test shouldn't be used to support that kind of intuition.

In 208 the manufacturer of M&Ms published their last color distribution:

| Blue | Orange | Green | Yellow | Red | Brow |
|------|--------|-------|--------|-----|------|
| 24%  | 20%    | 16%   | 14%    | 13% | 13%  |

We open several packages of M&Ms and count the colors:

| Blue | Orange | Green | Yellow | Red | Brow |
|------|--------|-------|--------|-----|------|
| 85   | 79     | 56    | 64     | 58  | 68   |

Are these counts consistent with the last published percentages? is there sufficient evidence to claim that the color distribution is different? This question requires a test of goodness-of-fit for the six categories.

Our null hypothesis is that the color distribution is the same. The idea is to compare the observed counts to the numbers we would expect if \$H_0 \$ is true, so for our sample of 410 M&Ms we would expect:

| Blue | Orange | Green | Yellow | Red  | Brow |
|------|--------|-------|--------|------|------|
| 98.4 | 82     | 65.6  | 57.4   | 53.3 | 53.3 |

We look at the difference of the observed and the expected values, we square that difference and we standardize it by dividing by the expected

$$
\chi^2 =\sum_{categories} \frac{(observed-expected)^2}{expected}
$$

$$
\frac{(85 - 98.4)^2}{98.4} + \frac{(79 - 82)^2}{82} + \cdots + \frac{(68 - 53.3)^2}{53.3} = 8.57
$$

Large values of the chi-square statistic $\chi^2$ are evidence against the null hypothesis. To calculate the p-value we use the chi-square distribution. The p-value is the right tail of the $\chi^2$ distribution with degrees of freedom = number of categories -1.

```{r, fig.align='center'}
df <- 5
# Create the chi-square distribution curve
curve(dchisq(x, df = df), from = 0, to = 40, 
      main = paste("Chi-Square Distribution (df =", df, ")"),
      ylab = "Density", lwd = 2, col = "steelblue")

# Draw a vertical line at x = 8.57
abline(v = 8.57, col = "red", lwd = 2, lty = 2)

# Highlight the area to the right of x = 8.57
x_vector <- seq(8.57, 40, length.out = 1000)
y_vector <- dchisq(x_vector, df = df)
polygon(c(8.57, x_vector, 40), c(0, y_vector, 0), 
        col = adjustcolor("red", alpha.f = 0.3), border = NA)
```

in our example, this p-value happens to be 12.7%, which does not allow us to reject the null hypothesis. In r we can calculate the p-value like this:

```{r}
# Set the chi-square value and degrees of freedom
x <- 8.57
df <- 5

# Calculate the p-value
p_value <- pchisq(x, df, lower.tail = FALSE)

# Print the p-value
p_value
```

::: exercise-box
Exercise:

In the exercise below we want to know if the age population in our project managers survey data matches the distribution of ages for the USA population. We get the distribution of USA from wikipedia and do some adjustments so the ranges matches those in our survey:

```{r}
file1 <- here("data", "pm_survey.xlsx")
pm_survey <- read_excel(file1)

us_ages <- c(.117, .176, .168, .158, .166, .215)

table(pm_survey$how_old_are_you)
obs_ages <- as.numeric(table(pm_survey$how_old_are_you))

testResult <- chisq.test(obs_ages, p = us_ages)
testResult %>% report()

testResult$p.value
```

The very small p-value indicates that is extremely unlikely that our pm_survey data was extracted at random from a population with the us_ages distribution.

As mentioned, the goodness of fit test does not tell us what categories show the discrepancies in the data, if we want to find out what is the difference between our sample range of ages and the USA data we can just subtract the proportions from each and see what categories are sub represented and vice versa

```{r}
obs_ages / sum(obs_ages)
obs_ages / sum(obs_ages) - us_ages
```

Now we want to see if in our optical sample the customers are evenly distributed between the opticians. If we don't pass a p attribute to the chi squared test it will assume you are asking for even distribution between all categories:

```{r}
file2 <- here("data", "optical_full.xlsx")
optical_full <- read_excel(file2)

counts <- as.numeric(table(optical_full$`Optician Last Name`))

testResult<- chisq.test(counts)
testResult %>% report()

testResult$p.value

```

In this case we cannot reject the null hypothesis, which in this case is that the count of patients is evenly distributed among all opticians.
:::

::: exercise-box
Exercise

Does the distribution of populations of towns in the United States follow [Benford's law](https://en.wikipedia.org/wiki/Benford%27s_law)? Check using the towns data set.

```{r}
theme_set(theme_minimal())

file <- here("data", "towns.xlsx")
towns <- read_excel(file)

benford <- c(.301, .176, .125, .097, .079, .067, .058, .051, .046)
sum(benford)
#we calculate the frequency of each first digit in the population of towns.
table(towns$first_digit)
digits_freq <- as.numeric(table(towns$first_digit))
digits_freq

testResult<- chisq.test(digits_freq, p = benford)
testResult %>% report()
testResult$p.value


# An illustrative plot
benford_df <- data.frame(distribution = c(rep("Towns", 9), rep("Benford", 9)),
                         first_digit = rep(1:9, 2),
                         frequency = c(digits_freq/sum(digits_freq), benford))

ggplot(benford_df, aes(x = first_digit, 
                       y = frequency,
                       fill = distribution)) + 
  geom_col(position = "dodge") +
  scale_x_continuous(breaks = 1:9) + 
  labs(x = "First digit",
       y = "Relative frequency",
       fill = "Distribution") +
  scale_fill_brewer(palette = "Dark2")

```

our p-value indicates that we cannot reject the null hypothesis, meaning in this case that the distribution of the first digit in towns across US is compatible with Benford's law.
:::

### Statistical power

Statistical power is the ability of a test to detect a specified effect. A test with low power is unlikely to rule out a hypothesized value, even if the value is false. That is, it has a high probability of type II error. Increasing the power of an inference technique requires a trade-off of one sort or another. In practice, this usually means using a larger sample. A preliminary power analysis can give decision-makers information about the study size needed to detect an effect of interest.

A very simple example of how to do that: study planners can estimate the sample size needed to reduce the margin of error in the estimate of a population proportion using this formula:\
$$
n = \left( \frac{1.96}{2E} \right)^2
$$

where $E$ is the required margin of error. According to this formula, a simple political poll requiring a 3% margin of error would need a sample size of approximately n=1067 respondents.

# Two-Sample testing

Two-sample testing allows for the comparison of two independent groups or populations to assess if there are statistically significant differences between their means, proportions, or other relevant measures.

*Confidence intervals* for two-sample comparison provide a range of plausible values for the difference in means or proportions, allowing for estimation with a desired level of confidence.

*Significance testing* for two-sample comparison involves evaluating the evidence against the null hypothesis and determining if the observed differences between groups are statistically significant.

In this simple exercise we are going to generate ratings for two products at random. We have not changed the probability of each ranking so both products should have the same rating so the difference of their mean ratings should be 0 if we have enough samples.

Creating many samples at random, calculating their differences and plotting those differences will show how, although we can see that the bell curve distributes more or less evenly from 0 as expected, many of the samples showed a difference

```{r, fig.align='center'}
theme_set(theme_minimal())

set.seed(27)

rating <- sample(1:5, 27, replace = TRUE)
product <- c(rep("A", 15), rep("B", 12))
AB_testing <- data.frame(product,
                         rating)

AB_testing |> 
  group_by(product) |> 
  dplyr::summarize(avg_rating = mean(rating))

AB_testing |> 
  group_by(product) |> 
  dplyr::summarize(avg_rating = mean(rating)) |> 
  ggplot(aes(x = product, 
             y = avg_rating,
             fill = product)) + 
  geom_col() +
  scale_fill_brewer(palette = "Dark2") +
  theme(legend.position = "none") +
  labs(x = "Product",
       y = "Average rating")


# repeat the samples 10000 times:
difference <- integer()
for (rep in 1:10000){
  rating <- sample(1:5, 27, replace = TRUE)
  difference[rep] <- mean(rating[1:15]) - mean(rating[16:27]) 
}
qplot(difference, binwidth = .2, xlab = "Difference in ratings")
```

In reality we will only have access to one of the samples, and we have to be able to tell if it's reasonable to draw a conclusion about the population just based on the sample or whether or not we should just attribute these sorts of differences to random chance.

## Significance testing for Two-Samples data

### z-test

The significance test for two samples uses the null hypothesis that there is no difference in the means of the two population means. The p-value will be used to reject or not that null hypothesys.

we can use a z-test for the difference between the two means:

$$
z = \frac{observed\ difference- expected\ difference}{SE\ of\ difference} = \frac{(\bar{x_2} - \bar{x_1})-0}{SE\ of\ difference}
$$

our expected difference is 0 because that's our null hypothesis (no difference). An important fact is that if $\bar{x_1}$ and $\bar{x_2}$ are independent, then:

$$ 
SE(\bar{x_2} - \bar{x_1}) = \sqrt{ (SE(\bar{x_1}))^2 +(SE(\bar{x_2}))^2}
$$

::: exercise-box
Exercise: two-sample z-test (proportions.)

*Last month, the president's approval rating in a sample of 1000 likely voters was 55%. This month, a poll of 1,500 likely voters resulted in a rating of 58%. Is this sufficient evidence to conclude that the rating has changed?*

$\hat{p_1} = 55%$ and $\hat{p_2} = 58%$ $$
z = \frac{(\hat{p_2}-\hat{p_1})-0}{SE_{diff}}=\frac{(\hat{p_2}-\hat{p_1})-0}{\sqrt{ (SE(\bar{x_1}))^2 +(SE(\bar{x_2}))^2}}
$$ The formula for the standard error for the proportion is

$$ 
SE= \sqrt{\frac{p(1-p)}{n}}
$$ Calculate the standard errors:

For $\hat{p_1}$: $$
SE(\hat{p_1}) = \sqrt{\frac{\hat{p_1}(1 - \hat{p_1})}{n_1}} = \sqrt{\frac{0.55 \times 0.45}{1000}}= \sqrt{\frac{0.2475}{1000}} = \sqrt{0.0002475} \approx 0.0157
$$

For $\hat{p_2}$: $$
SE(\hat{p_2}) = \sqrt{\frac{\hat{p_2}(1 - \hat{p_2})}{n_2}} = \sqrt{\frac{0.58 \times 0.42}{1500}} = \sqrt{\frac{0.2436}{1500}} = \sqrt{0.0001624} \approx 0.0127
$$ $$
SE_{diff} = \sqrt{(0.0157)^2 + (0.0127)^2} = \sqrt{0.00024649 + 0.00016129} = \sqrt{0.00040778} \approx 0.0202
$$

$$
z = \frac{(0.58 - 0.55) - 0}{0.0202} = \frac{0.03}{0.0202} \approx 1.49
$$ The calculated z-value is approximately 1.49. To determine if this is statistically significant, you would compare this z-value to the critical value from the standard normal distribution (typically 1.96 for a 95% confidence level). Since 1.49 is less than 1.96, you would fail to reject the null hypothesis at the 95% confidence level, indicating that there is not sufficient evidence to conclude that the president's approval rating has changed significantly.

The p-value can be calculated using standard normal distribution tables or software

```{r}
pValue <- pnorm(1.49)
pValue
```

Since this is a two-tailed test (we are checking if the approval rating has changed, not just increased or decreased), we need to consider both tails of the distribution, so we double our values: The p-value is calculated as $2 \times (1 - \text{cumulative probability})$. $p = 2 \times (1 - 0.9318) = 2 \times 0.0682 = 0.1364$

The p-value for the z-value of 1.49 is approximately 0.1364. Since this p-value is greater than the typical significance level of 0.05, we fail to reject the null hypothesis. This means there is not sufficient evidence to conclude that the president's approval rating has changed significantly.
:::

The exercise above shows how to calculate the p value for proportions, if we are working with average values instead of proportions the calculation is the same, only thing to consider is that in this case is that Standard deviation of each individual sample will be calculated using the formula for the standard deviation for the mean $SE(\bar{x_1})= \frac{\sigma_1}{\sqrt{n_1}}= \frac{s_1}{\sqrt{n_1}}$ . If the sample sizes are not large, then the p-value needs to be computed from the t-distribution instead.

### The Welch Two Sample t-test

In the example below we are going to use the attrition dataset to see if the average age of employees in two different departments is different?

```{r}
file <- here("data", "attrition1.xlsx")
attrition1 <- read_excel(file)

kable(head(attrition1))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()

attrition1 |> 
  group_by(Department) |> 
  dplyr::summarize(avg_age = mean(Age))

testResult <- t.test(Age ~ Department, 
       data = attrition1)

testResult$p.value

report(testResult)

```

## Confidence intervals for Two-Samples data.

We can use the formula for the standard error of the difference to also do a confidence interval calculation. The confidence interval for $p_2-p_1$ is $(\hat{p_2}-\hat{p_1}) \mp z \times SE(\hat{p_2}-\hat{p_1})$

were z is the z-score for the confidence level we are interested in.

::: exercise-box
in our example about the voters approval of the president:

the z-score value for a confidence level of 95% is 1.959964

$$
58-55 \mp 1.96 \times 0.0202  \approx [-0.0705,0.0105]
$$

If we want to resolve the same exercise using r code it gives similar but not exactly the same results:

```{r}
successes <- c(0.55 * 1000, 0.58 * 1500)

# Define the sample sizes
sample_sizes <- c(1000, 1500)

# Perform the two-sample z-test for proportions
test_result <- prop.test(successes, sample_sizes)

# Print the result
print(test_result)
```
:::

There are many statistical techniques for describing the difference between a single variable across two populations. The most universal is the **Welch two-sample confidence interval**, which is a statistical technique used to compare means between two independent groups, taking into account the unequal variances often encountered in real-world data, so it valid in nearly all circumstances. A few things to bear in mind:

-   It is a multi-sample procedure, not a multi-variable one. Use it when you're asking how much two samples differ in a single variable.

-   Like every other statistical tool in this course, it requires that all the observations be independent of one another (not to be used with time-line analysis)

-   Unless the data has extreme outliers or is highly asymmetric, it will give good results when both samples are of size 10 or more. If the samples are of size at least 30, it is fine under all but the most extreme circumstances.

The Welch two-sample confidence interval does not require that the samples are equal in size or that the populations have equal variances, unlike some other procedures.

We are going to use the AB_testing data we generated in the previous section and calculate the confidence intervals for the differences observed in one of our samples

```{r}
set.seed(27)  
rating <- sample(1:5, 27, replace = TRUE) 
product <- c(rep("A", 15), rep("B", 12)) 
AB_testing <- data.frame(product,rating)  
AB_testing |>    group_by(product)  %>%     
  dplyr::summarize(avg_rating = mean(rating))  

as.data.frame(report(AB_testing))  

testResult<- t.test(rating ~ product, data = AB_testing)  
testResult %>% report()  
testResult$conf.int  
```

The confidence interval says that with 95% confidence, the population mean difference is between `r testResult$conf.int[1]` and `r testResult$conf.int[2]`. This is actually saying that the difference in the means could be 0.

If you know or can assume that the variance of the two populations is equal, then you can use var.equal = TRUE in the t.test, and in this case instead of using a Welch Two sample t-test, it will use a Two sample t-test

```{r}
t.test(rating ~ product,         data = AB_testing,        var.equal = TRUE)  
testResult<- t.test(rating ~ product, data = AB_testing) 
testResult %>% report()  
testResult$conf.int  
```

In the example below we want to use our substance_abuse data set and know if there is a difference in the variable DLA_improvement based on the program that the patient was following. We are assuming that the variance is equal in both cases:

We can change the confidence level manually if we want:

```{r}
file <- here("data", "substance_abuse.xlsx") 
substance_abuse <- read_excel(file) 
substance_abuse$DLA_improvement <- substance_abuse$DLA2 - substance_abuse$DLA1 
t.test(DLA_improvement ~ Program,         data = substance_abuse,        conf.level = .99)
```

::: exercise-box
Exercises

**Exercise 1**: *Generate a 95% confidence interval for the difference in average monthly income between the research and development and sales departments of the company*

```{r}
testResult <- t.test(MonthlyIncome ~Department, 
                     data= attrition1)
testResult$conf.int
report(testResult)
```

The confidence interval does not include 0, which means that there is a difference in the means of the populations of both groups

**Exercise 2**: *It is reasonable to claim that the montly rate is the same bewteen these two departments?*

```{r}
testResult <- t.test(MonthlyRate ~Department, 
                      data= attrition1,
                      mu =0)
testResult$p.value
report(testResult)
```

In light of the results we cannot reject the null hypothesis that they have the same monthly rate.
:::

### Pooled estimate:

going back to our recent exercise of the voters's approval rate, we concluded that the two different surveys did not significantly differed.

Since we could not reject the null hypothesis that the two samples are representing the same approval for the candidate, we can combine the two of them to find a better estimate of our Standard Error

in the first sample we have 0,55 x 1000 voters who approved, in the second sample we have 0.58x 1500 so in total we have 1420 approvals out of 2500 people surveyed, so our ppoled estimate will be $\frac{1420}{2500}=56.8\%$ so now we can calculate the Standard Error using that new value:

$$ 
SE(\hat{p_2}-\hat{p_1}) = \sqrt{\frac{0.568(1-0.568)}{1000}+\frac{0.568(1-0.568)}{1500}}=0.02022 
$$

### Pooled standard deviation:

If we know (or there is reason to assume) that the standard deviation of the two populations is the same $\sigma_1=\sigma_2$ then we can use the pooled estimate:

$$ 
s^2_{pooled}=\frac{(n_1-1)s^2_1+(n_2-1)s^2_2}{n_1+n_2-2} 
$$

::: {.callout-orange appearance="simple" icon="false"}
**Two-Sample Z-Test vs Two sample T-test vs Welch's Two-Sample T-Test**

**Two sample Z-test**

When to Use:

-   **Known Population Variances**: The population variances are known.

-   **Large Sample Sizes**: Typically used when the sample sizes are large (n \> 30).

-   **Normal Distribution**: Assumes that the data follows a normal distribution.

**Two sample T-test**

-   **Unknown Population Variances**: The population variances are unknown and assumed to be equal.

-   **Small Sample Sizes**: Typically used when the sample sizes are small (n \< 30).

-   **Normal Distribution**: Assumes that the data follows a normal distribution.

**Welch's Two-Sample T-Test**

When to Use:

-   **Unknown and Unequal Population Variances**: The population variances are unknown and not assumed to be equal.

-   **Small or Unequal Sample Sizes**: Can be used for small or unequal sample sizes.

-   **Normal Distribution**: Assumes that the data follows a normal distribution.

**Summary:**

-   **Population Variances**:

    -   **Z-Test**: Assumes known and equal population variances.

    -   **Welch's T-Test**: Does not assume equal variances and uses sample variances.

-   **Sample Size**:

    -   **Z-Test**: Typically used for large sample sizes.

    -   **Welch's T-Test**: Can be used for small or unequal sample sizes.
:::

## Paired-t test

What do we do when we have two samples, but they are not independent from each other? In this case we cannot use the classical two-sample z-test or two-sample t-test

We want to answer the question: Do husbands tend to be older than their wives?

| Husband's age | Wife's age | age difference |
|---------------|------------|----------------|
| 43            | 41         | 2              |
| 71            | 70         | 1              |
| 32            | 31         | 1              |
| 68            | 66         | 2              |
| 27            | 26         | 1              |

In a scenario like this, even if the samples were independent, the test would not give us a significant difference because the difference between the two pairs is always small, while the difference in the values in each sample (standard deviations) are large and what the two samples test does is to compare the differences to the fluctuation within each population.

In this case we will use the paired t-test. Our $H_0$ is that the population difference is 0. The formula for this test is:

$$
t=\frac{\bar{d}-0}{SE_{(d)}}
$$

where $\bar{d}$ is the average of the differences,

0 is the expected difference that under our null hypothesis is 0 and $SE_{\bar{(d)}}$ is the standard error for the difference:

$$
SE_{(d)}= \frac{\sigma_d}{\sqrt{n}} = \frac{s_d}{\sqrt{n}}
$$

In the example with our data:

$$
t=\frac{1.4}{\frac{0.55}{\sqrt{5}}}=5.69
$$

and now we have to use a table of a student t-distribution with 4 degrees of freedom to find the area under the curve of the normal distribution to the right of our t value.

we can use R code to calculate it like this:

```{r}
# Given values
t_value <- 5.69
degreesfreedom <- 4

# Calculate the p-value for a two-tailed test
p_value <- 2 * pt(-abs(t_value), degreesfreedom)
p_value

```

In this case our result means that we can reject the null hypothesis.

## The sign test

Image that we don't know the age difference, we only know if the husbands are older or not. We can follow here the same approach as with a binomial distribution, like the coin toss. In this specific case our null hypothesis $H_0$ is that half of the husbands are older than their wifes (no difference). We assign labels to the results, for example 1 if the husband is older than the wife and 0 otherwise.

$$
z=\frac{sum\ of\ 1s -\frac{n}{2}}{SE\ of\ sum} =\frac{sum\ of\ 1s -\frac{n}{2}}{\sqrt{n}\times\sigma_{H_0}}
$$

in our case we have 5 husbands being older than their wifes, so 5 1s and the standard deviation for our null hypothesis will be $\frac{1}{2}$ because we expect half the husbands to be older then their wives.

$$
z= \frac{5-\frac{5}{2}}{\sqrt{5}\frac{1}{2}}= 2.24
$$

now we can find the p value using a table or software:

```{r}
z_score<- 2.24
# Calculate the p-value for a two-tailed test
p_value <- 2 * (1 - pnorm(z_score))
p_value
```

we can see that the result of this test is less significant than the test we did before, this is because we have less data to work with.

If we want to resolve the problem using software we can do the calculations:

```{r}
# Number of successes (husbands older than wives) 
successes <- 5  
# Total number of pairs 
n <- 5 
# Z-test (Sign test approximation) 
z_score <- (successes - 0.5 * n) / sqrt(0.25 * n) 
z_p_value <- 2 * (1 - pnorm(z_score)) 
z_p_value
```

but if we use the corresponding binomial test instead that would apply for this scenario, we get to quite a different result:

```{r}
# Number of successes (husbands older than wives) 
successes <- 5  
# Total number of pairs 
n <- 5  
# Perform the binomial test 
test_result <- binom.test(successes, n, p = 0.5, alternative = "two.sided")  

print(test_result)
```

::: {.callout-orange, appearance="simple", icon="false"}

Both the two-sample paired t-test and the sign test are used to compare paired data, but they are applied in different situations based on the assumptions and characteristics of the data.

**Two-Sample Paired T-Test**

**When to Use**:

-   **Normal Distribution**: The differences between the paired observations should be approximately normally distributed.

-   **Interval or Ratio Data**: The data should be measured on an interval or ratio scale.

-   **Parametric Test**: This test is parametric, meaning it relies on assumptions about the distribution of the data.

**Example**:

-   Comparing the blood pressure of patients before and after a treatment.

-   Measuring the weight of individuals before and after a diet program.

**Sign test**

1.  **Paired Observations**: When you have paired data (e.g., before and after measurements) and you want to test if there is a consistent difference between the pairs. For example, testing if a treatment has an effect by comparing measurements before and after the treatment.

2.  **Median Differences**: When you want to test if the median of differences between pairs is zero. This is useful when the data does not meet the assumptions required for parametric tests like the paired t-test. It does not rely on assumptions about the distribution of the data.

3.  **Non-Normal Data**: When the data does not follow a normal distribution, making parametric tests inappropriate. The sign test does not assume any specific distribution for the data.

4.  **Ordinal Data**: When the data is ordinal (ranked) rather than interval or ratio. The sign test can handle data that can only be compared as greater than, less than, or equal to.

[**Example Scenarios**]{.underline}

-   **Medical Studies**: Comparing the effectiveness of a treatment by measuring patient conditions before and after the treatment.

-   **Quality Control**: Testing if a new manufacturing process consistently produces better results than the old process.

-   **Behavioral Studies**: Comparing responses before and after an intervention. Comparing the number of days patients feel better before and after a new medication.

[**Summary:**]{.underline}

-   **Use a paired t-test** when the differences between paired observations are normally distributed and you have interval or ratio data.

-   **Use a sign test** when the data does not meet the normality assumption, is ordinal, or you prefer a non-parametric approach. :::

## Wilcoxon Rank Sum Test

We learned how the sample mean and SD are susceptible to outliers. The t-test is based on these measures and is susceptible as well. The Wilcoxon rank test (equivalent to the Mann-Whitney test) provides an alternative. In the code below, we perform a t-test on data for which the null is true. However, we change one sum observation by mistake in each sample and the values incorrectly entered are different. Here we see that the t-test results in a small p-value, while the Wilcoxon test does not:

```{r}
set.seed(779) ##779 picked for illustration purposes
N=25
x<- rnorm(N,0,1)
y<- rnorm(N,0,1)
```

Create outliers:

```{r}
x[1] <- 5
x[2] <- 7
cat("t-test pval:",t.test(x,y)$p.value)
cat("Wilcox test pval:",wilcox.test(x,y)$p.value)
```

The basic idea is to 1) combine all the data, 2) turn the values into ranks 3) separate them back into their groups, and 4) compute the sum or average rank and perform a test.

```{r rank-test-illustration, fig.cap="Data from two populations with two outliers. The left plot shows the original data and the right plot shows their ranks. The numbers are the w values ",fig.width=10.5,fig.height=5.25}
library(rafalib)
mypar(1,2)

stripchart(list(x,y),vertical=TRUE,ylim=c(-7,7),ylab="Observations",pch=21,bg=1)
abline(h=0)

xrank<-rank(c(x,y))[seq(along=x)]
yrank<-rank(c(x,y))[-seq(along=x)]

stripchart(list(xrank,yrank),vertical=TRUE,ylab="Ranks",pch=21,bg=1,cex=1.25)

ws <- sapply(x,function(z) rank(c(z,y))[1]-1)
text( rep(1.05,length(ws)), xrank, ws, cex=0.8)
W <-sum(ws) 
```

`W` is the sum of the ranks for the first group relative to the second group. We can compute an exact p-value for $W$ based on combinatorics. We can also use the CLT since statistical theory tells us that this `W` is approximated by the normal distribution. We can construct a z-score as follows:

```{r}
n1<-length(x);n2<-length(y)
Z <- (mean(ws)-n2/2)/ sqrt(n2*(n1+n2+1)/12/n1)
print(Z)
```

Here the `Z` is not large enough to give us a p-value less than 0.05. These are part of the calculations performed by the R function `wilcox.test`.

we are not going to get into mathematical detail about these calculations, but the formula for the z score here is $$
 z=\frac{U - \frac{n_2}{2}}{\sqrt{\frac{n_2(n_1+n_2+1)}{12n_1}}}
$$ and to perform this test in r we use: `wilcox.test(x,y)`

## Two-samples of binary data

Two observed proportions for a single binary variable can be compared directly using the **two-proportion z-confidence interval** and **two-proportion z-test**. These apply when there are at least 5 observations of each type in each of the two groups. The formulas are relatively simple. For example a 95% confidence interval for the difference between proportions is

$$
(p_2-p_1) = (\hat p_2-\hat p_1) \pm 1.96 \sqrt{\frac{\hat p_2(1-\hat p_2)}{n_2}+\frac{\hat p_1(1-\hat p_1)}{n_1}} 
$$

where $p_1$ and $p_2$ are the population proportions, $\hat p_1$ and $\hat p_2$ are the observed sample proportions and $n_1$ and $n_2$ are the sample sizes. R will use a improved version of this formula when computing the proportions.

In R we will use $prop.test(n_s , sample size)$ where $n_s$ is the number of successes.

::: exercise-block
Examples:

in the attrition dataset. *Are the attrition proportions different for the two departments?*

```{r}
kable(head(attrition1))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()

table(attrition1$Department)
t<- table(attrition1$Department,
      attrition1$Attrition)
t
yes_counts<- as.numeric(t[,2])
sampleSize <-as.numeric(table(attrition1$Department))

testResult <- prop.test(yes_counts, sampleSize)
testResult
```

Our p-value `r testResult$p.value` is very low which means we can reject the default null hypothesis (the difference in the two samples is 0). It is also giving us the proportions for the samples in the two different departments: `r testResult$estimate` . And the confidence intervals is giving us the difference in the proportion of the two departments (as in prop1 - prop2). In this case being negative means that the second department has a higher attrition rate. The order of the variables is as entered in the vectors, so if *yes_counts* had *Research_Development* first and *Sales* second, prop1 will be for *Research_Development* and prop2 for *Sales*
:::

## Two-samples of categorical variables

Two samples of a single categorical variable can be compared with the $x^2-test$ for homogeneity (Chi-squared). Under the hood, this is just a goodness-of-fit test of the hypothesis that the observed proportions in one of the samples are the same as those in the pooled sample.

### Testing homogeneity

The $\chi^2$ test of homogeneity test the null hypothesis that the distribution of a categorical variable is the same for several populations. It assumes that the samples are drawn independently within and across populations.

See how we can apply this logic to our Titanic survival data:

|          | First | Second | Third | Crew |
|----------|-------|--------|-------|------|
| Survived | 202   | 118    | 178   | 215  |
| Died     | 123   | 167    | 528   | 698  |

Note that in this case we are not sampling from a population. The data are not a random sample of the people on board, rather the data represent the whole population. Son in this case the chance process resulting in survival or death is not the sampling, but the result of random events occurring when looking for a way out of the ship, like getting into a life boat or into the water, being rescued from the water on time, etc. Then the 325 observations of first class passengers represent 325 independent draws from a probability histogram that gives a certain chance for survival. The 285 observations about second class passengers are drawn from the probability histogram for second class passengers, which may be different. The null hypothesis says that the probability of survival is the same for all four probability histograms. According to this hypothesis we can calculate the probability of survival by pooling all the data = $\frac{713}{2229}=32\%$ with this number we can calculate the expected number of surviving passengers for each class:

| Surviving | First | Second | Third | Crew  |
|-----------|-------|--------|-------|-------|
| Observed  | 202   | 118    | 178   | 215   |
| Expected  | 104.0 | 91.2   | 225.8 | 292.1 |

| Died     | First | Second | Third | Crew  |
|----------|-------|--------|-------|-------|
| Expected | 221.0 | 193.8  | 480.1 | 620.8 |
| Observed | 123   | 167    | 528   | 698   |

Now we can compare our chi statistic as we learned, using all the differences between expected and observed values:

$$
\chi^2 =\sum_{categories} \frac{(observed-expected)^2}{expected}
$$

$$
=\frac{(202-104)^2}{104}+\frac{(123-221)^2}{221}+\cdots =192 
$$

in this case our degrees of freedom are calculated like this: (4-1)\*(2-1) = 3 where 4 is for the number of categories, and 2 is for the two rows of results we are dealing with (surviving and died)

In our case our p value will be extremely small, sugesting we should reject the null hypothesis that all ticket classes had the same posibility of survival.

```{r}
x <- 192.2
df <- 3

# Calculate the p-value
p_value <- pchisq(x, df, lower.tail = FALSE)

# Print the p-value
p_value

```

The test in r:

```{r}
# Create the contingency table
titanic_data <- matrix(c(202, 118, 178, 215, 123, 167, 528, 698), 
                       nrow = 2, 
                       byrow = TRUE,
                       dimnames = list(Survival = c("Survived", "Died"),
                                       Class = c("First", "Second", "Third", "Crew")))

# Print the contingency table
print(titanic_data)

# Perform the chi-squared test of homogeneity
chi_squared_test <- chisq.test(titanic_data)

# Print the test results
print(chi_squared_test)

```

::: exercise-box
Exercise:

*Use the substance_abuse data set to decide if the distribution of mental health diagnosis is the same for those with and without at least one psychiatric admission.*

```{r}
as.data.frame(report(substance_abuse$MHDx))
as.data.frame(report(substance_abuse$PsychAdmit))

substance_abuse <- substance_abuse %>% mutate(previousAdmit = ifelse(substance_abuse$PsychAdmit == 0, FALSE, TRUE))

t<- table( substance_abuse$previousAdmit,substance_abuse$MHDx)
t
chisq.test(t)
```

In this case according to our p-value we cannot say that patients that had at least one psychiatric admission in the past have the same proportion of diagnosis than patients without any admission.
:::

Chi squared test for homogeneity is not saying anything specific about any of these diagnosis, it is simply saying that the distribution of these diagnosis between those two groups (admitted vs not admitted) is not the same.

Is the distribution of SUDx the same for both men and women in our Substance Abuse data?

```{r}
head(substance_abuse)
as.data.frame(report(substance_abuse$SUDx))
```

```{r}

t <- table(substance_abuse$Gender, 
           substance_abuse$SUDx)
head(t)
```

Our null hypothesis is that the distribution between gender is the same.

```{r}
result <- chisq.test(t)
report(result)

```

In this case the p-value of the test is over 0.05 so that indicates that there is no significance difference by sex for the SUDx variable, so men and women are abusing the different categories of substances in the same proportions. The df in our results are the degrees of freedom that is the number of categories minus one.

In reality this is the same as doing a Goodness of fit test as we saw before, if we remember it was $chisq.test(p_s , p)$ where $p_s$ was the proportions of the different categories in our sample and $p$ the proportion of the population we wanted to test against. Back to our example what we are measuring here is the proportion of women or men against the proportion of the full sample, that will tell us if there is a difference between men and women, so we can also write the test like this getting similar results:

```{r}
sudxProp <- table(substance_abuse$SUDx)/sum(table(substance_abuse$SUDx))

women <- t[1,]

result<- chisq.test(women, p= sudxProp)
report(result)
```

### Independence Testing of Categorical Variables

We want to answer this question: Is gender (M/F) related to voting preference (liberal/conservative)? Now we have two categorical variables: gender and voting preference. The null hypothesis is that the two variables are independent. The alternative hypothesis is that there is some kind of association.

The tool we are going to use is the chi-square test ( $x^2$ ) for independence. This test can be used to test whether two categorical variables are independent or not. That is, whether knowledge about one provides information about the other. The math is exactly the same as for the chi-square test for homogeneity, hence, so is the sintax in most statistical packages, including R, but with a few things to bear in mind:

-   Technically, the null hypothesis of a $x^2$ test for independence is that in the larger population, the probability of an observation being in any specific pair of categories is equal to the product of the probabilities of being in each of the individual categories.

-   This is another omnibus test: it says nothing about particular categories.

-   Larger cell counts are better, as usual. Do not do this test if any of the counts are less than 5.

We are going to use our product_rating dataset to see the relationship between age groups and the product the user purchased.

first we can just see the contingency table for those two variables

```{r}
file <- here("data", "product_ratings.xlsx")
product_ratings <- read_excel(file)
t <- table(product_ratings$age,
           product_ratings$product)
t
```

```{r}
chisq.test(t)
```

The low p-value from this test is indicating that the theory that these two categorical variables are independent is implausible.

::: {.callout-orange appearance="simple" icon="false"}
Chi-Squared is used for testing the independence of categorical variables. It compares observed frequencies in a contingency table to expected frequencies under independence.
:::

::: exercise-box
Exercises

*Referring to the substance_abuse data set:*

-   *Is there any evidence of an association between Substance Use Diagnosis and Mental Health Diagnosis among patients receiving an intervention?*

```{r}
treatment <- filter(substance_abuse, 
                    Program == "Intervention")
t<- table (treatment$SUDx, treatment$MHDx)
chisq.test(t)
```

According to this result we cannot conclude that there is a relationship between these two variables.

-   *Is there any evidence of an association between SUDx and DLA_improvement among patients receiving an intervention?*

```{r, fig.align="center"}
ggplot(treatment, aes(x= SUDx, y = DLA_improvement))+
  geom_boxplot()

testResult <- aov( DLA_improvement ~ SUDx, data = treatment)
summary(testResult)
```

The difference is significant but not by large as the p value shows that in 3% of the cases we could observe these data differences in a sample from the a population where their category means is the same.
:::

If we run a TukeyHSD test on these we can compare each pair individually and we find out that actually the only pair that shows a statistically significant difference is alcohol with opioids.

```{r}
TukeyHSD(testResult)
```

It is easy to confuse the testing for homogeneity and the testing for independence.

### Comparing the different uses of the chi-square test:

|                 | sample                                                  | research question                                                                   |
|------------------|----------------------|--------------------------------|
| goodness of fit | single categorical variable. one sample                 | Are the counts of the different categories matching our expected results            |
| homogeneity     | single categorical variable measured on several samples | Are the groups homogeneous (have the same distribution of the categorical variable) |
| independence    | two categorical variables measured on one sample        | Are the two categorical variables independent?                                      |

Examples:

-   we want to know if there is more births in different week days (Monday, Tuesday...). We record the week day of 300 births. What test should we use to know if there is a difference per day? --\> We would use chi-square test for goodness of fit.

-   A food delivery start-up decides to advertise its service by placing ads on web pages. They wonder whether the percentage of viewers who click on the ad changes depending on how often the viewers were shown the ad. They randomly select 100 viewers from among those who were shown the add once, 135 from among those who were shown the add twice, and 150 from among those who were shown the ad three times. --\> We would use chi-square test of homogeneity.

-   An airline wants to find out whether there is a connection between the customer's status in its frequent flyer program and the class of ticket that the customer buys. It samples 1,000 ticket records at random and for each ticket notes the status level ('none', 'silver', 'gold') and the ticket class ('economy', 'business','first'). --\> chi-square test of independence

-   A county wants to check whether the racial composition of the teachers in the county corresponds to that of the population in the county. It samples 500 teachers at random and wants to compare that sample with the census numbers about the racial groups in that county. --\> chi-square test for goodness of fit

# Monte Carlo Method

The Monte Carlo method is a powerful statistical technique used to understand the impact of risk and uncertainty in prediction and forecasting models.

If for example we want to know the average height of all people living in USA, we estimate the parameter we are interested in (average height of the population) using a statistic estimator that is the average height of a sample of the population drawn at random. $\theta$ is the parameter we want to calculate and we use $\hat{\theta}$ that is the average o four sample. By the law of large numbers, the approximation error can be made arbitrarily small by using a large enough sample size.

What if we are interested in an estimator $\hat{\theta}$ for some parameter $\theta$ of the population and the normal approximation is not valid for that estimator? In such situations, simulations can often be used to estimate the parameters.

The Monte Carlo method relies on repeated random sampling to obtain numerical results. The core idea is to use randomness to solve problems that might be deterministic in nature. This method is named after the Monte Carlo Casino in Monaco, reflecting the element of chance and randomness.

In our example to estimate the height of the population of USA adults we get many (let's say 1000) samples of n=100 observations. We then compute the estimator $\hat{\theta}$ for each sample (in our case the average height), resulting in 1000 estimates. Now we compute the standard deviation of these 1000 estimates and it results to be close to the standard error of my estimator. :

$$
SE(\hat{\theta})=\sqrt{E(\hat{\theta}-E(\hat{\theta}))^ 2}
$$

$$
s(\hat{\theta}_1,...,\hat{\theta}_{1000})= \sqrt{\frac{1}{999}\sum^{1000}_{i=1}(\hat{\theta}-mean(\hat{\theta_i}))^ 2} \approx SE(\hat{\theta})
$$

The caveat of this method is that it will only work where we can extract as many samples as we wish.

Usually we cannot have as many samples as we wish, when we want to use the Monte Carlo Method in practice, it is typical to assume a parametric distirbution and generate a population from this, which is called a **parametric simulation**. This means that we take parameters estimated from the real data and using the mean and standard deviation, we plug these into a model of normal distribution and generate new samples.

For example for the case of mice weights, we know that mice typically weight 24 gr with a SD of about 3.5 gr. and that the distribution is approximately normal, to generate population data:

```{r}
controls<- rnorm(5000, mean=24, sd=3.5) 
```

# The bootstrap principle {#bootstrap}

The bootstrap method will work were we cannot extract as many samples.

We have an estimate $\hat{\theta}$ for a parameter $\theta$ and want to know how accurate our estimate is. For that we need to know the SE of the estimate or give a confidence interval for the parameter $\theta$. The bootstrap can do this in quite general settings:

Example $\theta$= average heigth of all people in the US. It is unknown but estimated with the average height of 100 randomly selected people. We can't compute the population mean because we can't access the whole population. So we ' plug in' the sample in place of the population and compute the mean of the sample instead. The rationale for the plug-in principle is that the sample mean will be close to the population mean because the sample histogram is close to the population histogram.

The bootstrap uses the plug-in principle and the Monte Carlo Method to approximate quantities such as $SE(\hat{ \theta})$ when we cannot get many samples. The bootstrap pretends that the sample histogram is the population histogram and uses the Monte Carlo to simulate the quantity of interest. What we do is we draw n samples with replacement from the original sample, and with those samples we use the Monte Carlo principle to calculate the quantity of interest.

Drawing a bootstrap sample by sampling with replacement from the data is called nonparametric bootstrap. Sometimes, we know more about the data. For example, we may know that the data follow a normal distribution, but we don't know the mean on the standard deviation. In that case, we may want to use that information. Of course, the obvious thing to do there is to simply estimate the unknown mean and standard deviation, and then simply sample from the normal distribution, with that mean and standard deviation. That's called parametric bootstrap.

This type of sampling works if the data are independent, that is, X1 to Xn are really generated independently. But oftentimes, there's dependence in the data. For example, the data are observed over time.

If the sampling distribtuion of $\hat{\theta}$ is approximately normal, then the confidence interval formula is :

$$
\left[ \hat{\theta} - z_{\alpha/2} SE(\hat{\theta}), \hat{\theta} + z_{\alpha/2} SE(\hat{\theta}) \right]
$$

This formula represents a confidence interval for an estimator ( $\hat{\theta}$ ), where:

-   ( $\hat{\theta}$ ) is the point estimate.

-   ( $z_{\alpha/2}$ ) is the critical value from the standard normal distribution for a given confidence level. For example for a confidence level of 95% $\alpha =(1-0.95)=0.05$ ; $\frac{\alpha}{2}=0.025$, $z_{\frac{\alpha}{2}}=1.96$

-   ( $SE(\hat{\theta})$ ) is the standard error of the estimator.

If the distribution is not normal, we can still use the bootstrap by estimating that the distribution of the estimate can be approximated by the distribution of the bootstrap copies extracted from it, so we can draw a distribution curve and calculate the 95% confidence interval using that curve:

$$
\left[ \hat{\theta}^*_{\alpha/2}, \hat{\theta}^*_{1-\alpha/2} \right]
$$

where $\hat{\theta}^*_{\alpha/2}$ is the $\alpha/2$ percentile of the $\hat{\theta}^*_{1},....\hat{\theta}^*_{B}$

## Use of Monte Carlo Simulation and Bootstrap

Simulations can also be used to check theoretical or analytical results. Also, many of the theoretical results we use in statistics are based on asymptotics: they hold when the sample size goes to infinity. In practice, we never have an infinite number of samples so we may want to know how well the theory works with our actual sample size. Sometimes we can answer this question analytically, but not always. Simulations are extremely useful in these cases.

As an example we are going to use simulation to compare the Central Limit Theorem to the t-distribution for different sample sizes. We will use our mice data and extrac control and 'fake' treatment samples. The fake treatment samples are just samples labeled as treatment but extracted from the control population.

We will build a function that automatically generates a t-statistic under the null hypothesis for a sample size of `n`.

```{r}
dat <- read.csv("data/mice_pheno.csv")
controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  dplyr::select(Bodyweight) %>% unlist

ttestgenerator <- function(n) {
  cases <- sample(controlPopulation,n)
  controls <- sample(controlPopulation,n)
  tstat <- (mean(cases)-mean(controls)) / 
      sqrt( var(cases)/n + var(controls)/n ) 
  return(tstat)
  }
ttests <- replicate(1000, ttestgenerator(10))
```

```{r ttest_hist, fig.cap="Histogram of 1000 Monte Carlo simulated t-statistics.", fig.align='center'}
hist(ttests)
```

we can use quantile-quantile plots to see how well this distribution approximates to the normal:

```{r ttest_qqplot, fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics to theoretical normal distribution.", fig.align='center'}
qqnorm(ttests)
abline(0,1)
```

This looks like a very good approximation. For this particular population, a sample size of 10 was large enough to use the CLT approximation. How about sample size 3?

```{r, ttest_df3_qqplot, fig.align='center',fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical normal distribution."}
ttests <- replicate(1000, ttestgenerator(3))
qqnorm(ttests)
abline(0,1)
```

Now we see that the large quantiles, referred to by statisticians as the *tails*, are larger than expected (below the line on the left side of the plot and above the line on the right side of the plot). In the previous module, we explained that when the sample size is not large enough and the *population values* follow a normal distribution, then the t-distribution is a better approximation. Our simulation results seem to confirm this:

In the code below first we generate the percentiles: `ps <- (seq(0,999)+0.5)/1000` seq(0,999) generates a sequence of numbers from 0 to 999. Adding 0.5 to each element centers the sequence values. Dividing by 1000 scales the values to be between 0 and 1. This results in 1000 equally spaced percentiles between 0.0005 and 0.9995. `qt(ps, df=2*3-2)` computes the quantiles of the t-distribution for the given percentiles `ps` with degrees of freedom `df=2*3-2` qqplot creates a QQ plot comparing the quantiles of our t-test results to the theoretical quantiles of the t-distribution.

```{r, ttest_v_tdist_qqplot,fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical t-distribution.", fig.align='center'}
ps <- (seq(0,999)+0.5)/1000
qqplot(qt(ps,df=2*3-2),ttests,xlim=c(-6,6),ylim=c(-6,6))
abline(0,1)
```

The QQ plot helps you visually assess how well your t-test results follow the theoretical t-distribution. The t-distribution is a much better approximation in this case, but it is still not perfect. This is due to the fact that the original data is not that well approximated by the normal distribution. We can see this plotting the qqplot for the full population data:

```{r, dat_qqplot, fig.cap="Quantile-quantile of original data compared to theoretical quantile distribution.", fig.align='center'}
qqnorm(controlPopulation)
qqline(controlPopulation)
```

# Permutation Tests

Suppose we have a situation in which none of the standard mathematical statistical approximations apply. We have computed a summary statistic, such as the difference in mean, but do not have a useful approximation, such as that provided by the CLT. In practice, we do not have access to all values in the population so we can't perform a simulation. Permutation tests can be useful in these scenarios.

We are back to the scenario were we only have 10 measurements for each group.

```{r, fig.align='center'}
dat=read.csv("data/femaleMiceWeights.csv")

control <- filter(dat,Diet=="chow") %>% dplyr::select(Bodyweight) %>% unlist
treatment <- filter(dat,Diet=="hf") %>% dplyr::select(Bodyweight) %>% unlist
obsdiff <- mean(treatment)-mean(control)
obsdiff
```

In previous sections, we showed parametric approaches that helped determine if the observed difference was significant. Permutation tests take advantage of the fact that if we randomly shuffle the cases and control labels, then the null is true. So we shuffle the cases and control labels and assume that the ensuing distribution approximates the null distribution. Here is how we generate a null distribution by shuffling the data 1,000 times:

```{r, fig.align='center'}
N <- 12
avgdiff <- replicate(1000, {
    all <- sample(c(control,treatment))
    newcontrols <- all[1:N]
    newtreatments <- all[(N+1):(2*N)]
  return(mean(newtreatments) - mean(newcontrols))
})
hist(avgdiff)
abline(v=obsdiff, col="red", lwd=2)
```

How many of the null means are bigger than the observed value? That proportion would be the p-value for the null. We add a 1 to the numerator and denominator to account for misestimation of the p-value. The p-value here is calculated directly as the percentage of values higher than our number of interest.

```{r}
#the proportion of permutations with larger difference
(sum(abs(avgdiff) > abs(obsdiff)) + 1) / (length(avgdiff) + 1)

```

Now let's repeat this experiment for a smaller dataset. We create a smaller dataset by sampling:

```{r, fig.align='center'}
N <- 5
control <- sample(control,N)
treatment <- sample(treatment,N)
obsdiff <- mean(treatment)- mean(control)
obsdiff

avgdiff <- replicate(1000, {
    all <- sample(c(control,treatment))
    newcontrols <- all[1:N]
    newtreatments <- all[(N+1):(2*N)]
  return(mean(newtreatments) - mean(newcontrols))
})
hist(avgdiff)
abline(v=obsdiff, col="red", lwd=2)
```

Now the observed difference is not significant using this approach. Keep in mind that there is no theoretical guarantee that the null distribution estimated from permutations approximates the actual null distribution. For example, if there is a real difference between the populations, some of the permutations will be unbalanced and will contain some samples that explain this difference. This implies that the null distribution created with permutations will have larger tails than the actual null distribution. This is why permutations result in conservative p-values. For this reason, when we have few samples, we can't do permutations.

Note also that permutations tests still have assumptions: samples are assumed to be independent and "exchangeable". If there is hidden structure in your data, then permutation tests can result in estimated null distributions that underestimate the size of tails because the permutations may destroy the existing structure in the original data.

# Multiple test corrections

The **Bonferroni correction** is a statistical method used to address the problem of multiple comparisons. When you perform multiple hypothesis tests, the chance of making a Type I error (false positive) increases. The Bonferroni correction helps control this by adjusting the significance level.

What we do is, if there are m test, we multiply the p-values by m. This correction makes sure that P(any of the m test rejects in error) is still smaller than 5%. The Bonferroni correction is often very restrictive. It guards against having even one false positive among the m test. As a consequence the adjusted p-values may not be significant any more even if a noticeable effect is present, so it will only work if you don't have a large number of test.

Alternatively if the number of tests is large, it is better to use the **False Discovery Proportion (FDP)**:

$$
FDP= \frac{number\ of\ false\ discoveries}{total\ number\ of\ discoveries}
$$

where a discovery occurs when a test rejects the null hypothesis. If no hypothesis are rejected, FDP is defined to be 0.

The **False Discovery Rate (FDR)** is the expected proportion of false discoveries among the discoveries. One common method to control the FDR is the Benjamini-Hochberg procedure. This method adjust the p-values to control the FDR at a desired alpha level. The procedure is like this:

1.  Sort the p-values in ascending order. $p_{(1)}, p_{(2)}, \ldots, p_{(m)}$

2.  Find the largest k value such as $p_k=\frac{k}{m}\alpha$

3.  Reject the null hypothesis for all $p_i$ where $i\leq k$

In r:

```{r}
# Example p-values
p_values <- c(0.01, 0.04, 0.03, 0.002, 0.05, 0.20)

# Apply the Benjamini-Hochberg procedure
p.adjusted <- p.adjust(p_values, method = "BH")

print(p.adjusted)

```

This will give you the adjusted p-values, and you can compare them to your desired FDR level to decide which hypotheses to reject.

The **validation set approach**. With this approach you split your data set into two parts, one is a model building set and a validation set before the analysis. You may use data snooping (multiple testing) in the first set of data to find something interesting. And then you test this hypothesis on the validation set..

# Statistical Relationship and Analysis

## Introduction to Regression Models

The methods described above relate to **univariate** variables. In sciences it is common to be interested in the relationship between two or more variables.

Suppose that we observe a quantitative response Y and p different predictors or inputs (X1,X2, . . . ,Xp). We assume that there is some relationship between Y and X = (X1,X2, . . . ,Xp), which can be written in the very general form: $$
Y = f(X)+\epsilon
$$

$f$ represents the systematic information that X provides about Y and $\epsilon$ is a random error term, which is independent from X. In essence, statistical learning and regression techniques refers to a set of approaches for estimating $f$

There are two main reasons that we may wish to estimate $f$: prediction and inference.

### Prediction

In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y: $$
\hat{Y}= \hat{f}(X)
$$ where $\hat{f}$ represents our estimate for $f$ and $\hat{Y}$ represents the resulting prediction for Y. In this setting $\hat{f}$ is often treated as a black box and we are not tipically concerned with the extact function, provided that it yiedls accurate predictions of Y.

### Inference

We are often interested in understanding the association between Y and X1, . . . ,Xp. In this situation we wish to estimate f, but our goal is not necessarily to make predictions for Y . Now ˆ f cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions: • Which predictors are associated with the response? It is often the case that only a small fraction of the available predictors are substantially associated with Y . Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application. • What is the relationship between the response and each predictor? Some predictors may have a positive relationship with Y , in the sense that larger values of the predictor are associated with larger values of Y . Other predictors may have the opposite relationship. Depending on the complexity of f, the relationship between the response and a given predictor may also depend on the values of the other predictors. • Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Historically, most methods for estimating f have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.

Relationships between variables are usually better visualized using scatterplots:

```{r, fig.align='center'}

data(father.son,package="UsingR") 
x <- father.son$fheight
y <- father.son$sheight
plot(x,y,xlab="Father's height in inches",ylab="Son's height in inches",main=paste("correlation =",signif(cor(x,y),2)))
```

The scatter plot shows a general trend: the taller the father the taller to son. A summary of this trend is the correlation coefficient which in this case is 0.5. We motivate this statistic by trying to predict son's height using the father's.

Quick facts:

::: {#CorrelationFunFacts style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
-   Correlation's curveball: Correlation doesn't imply causation. Two variables can move together without one causing the other, often due to lurking third variables.

-   Residual Revelations: Residuals plots, which showcase deviations from a model, can often tell more about data relationships than the fit itself, hinting at non-linearity, heteroscedasticity, or other intricacies.

-   Multicollinearity Maze: In multiple regression, if predictors are too related, it can muddy interpretations.
:::

# Linear Models

Linear models are the simplest structural models. It assumes that the dependence of Y on the predictors $X_1,X_2,\dots X_p$ is linear. We represent our function like this: $$
f_L(X)=\beta_0+\beta_1X_1+\beta_2X_2+\dots\beta_pX_p
$$ it has $p$ features and $p+1$ parameters $\beta$. We can estimate the parameters of the model by fitting the model to training data.

we assume a model:

$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+\dots\beta_pX_p+\epsilon
$$

Where $\epsilon$ is some noise or errors in the function, 
$\beta_0$ and $\beta_p$ are unknown constants that represent the intercept and the slope, also known as coefficients or parameters. 

Given some estimates $\hat{\beta_0}$ and $\hat{\beta_p}$ for the model coefficients we predict the values of Y using the model:
$\hat{y} = \hat{\beta_0}+\hat{\beta_1}x+\dots\hat{\beta_p}x$
the hat symbol over the variables indicate that they are an estimated value. 

A linear model is almost never correct, but it is usually a good approximation to the real function of the data. There are other models that may fit the data better, but there is a trade off in interpretability. Linear model have less prediction accuracy but are easy to interpret.

## Correlation coefficient {#correlation-coefficient}

If we standardize the values of our data, the line that we use to predict one value from the other follows a slope. That slope is the correlation.

In our substance_abuse dataset we have two variables for each observation DLA1 and DLA2 that we can plot in a scatter plot to see their relationship:

```{r, fig.align="center"}

treatment <- dplyr::filter(substance_abuse,
                    Program == "Intervention")

ggplot(treatment, aes(x = DLA1, y = DLA2)) + 
  geom_point() # Linear

```

It shows a linear relationship between the two variables: one one increase the other tend to increase as well.

To show other possible relationships between data we will generate a dataset that shows a non-linear relationship

```{r, fig.align='center'}
set.seed(7)
x <- abs(rchisq(100, 10) - 2.7)
y <- 1/x + rnorm(100, 1, .1) - .9
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()
```

The **correlation coefficient formula**:

$$
r = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s_x} \right) \times \left( \frac{y_i - \bar{y}}{s_y} \right)
$$

This formula standardizes the values of ( x ) and ( y ) by subtracting their means $\bar{x}$ and $\bar{y}$ and dividing by their standard deviations $s_x$ and $s_y$, then calculates the average product of these standardized values.

**Sample correlation** measures the strength of an observed **linear** relationship between two variables in a data set. Fundamentally, it measures the spread (or variability) of sample data along a line of best fit

As a convention, the variable on the horizontal axis is called **explanatory variable or predictor** and the one in the vertical axis is called **response variable.**

```{r, fig.align='center'}
ggplot(treatment, aes(x = DLA1, y = DLA2)) + 
  geom_point() +
  geom_smooth(method = "lm",
                           se = FALSE,
                           color = "grey")
```

To calculate the correlation we just have to use the command 'cor' and it's value will go from -1 to 1. The sign of r gives the direction of the association and its absolute value gives the strength.

Since both x and y were standardized when computing r, r has no units ans it is not affected by changing the center or the scale of either variable.

```{r}
cor(treatment$DLA1,
    treatment$DLA2)
```

Be careful to **refer to correlation only when a pair of variables has a linear relationship**. For instance, the following variables have a clear association, but their coefficient of correlation is close to 0:

```{r fig.align='center'}
set.seed(2)
x <- rnorm(100, 0, .5)
y <-  (x^2-1) + rnorm(1.5, -2.5, .8)
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()

cor(x,y)

```

We need to be aware that when we are working with samples, we may find correlation between two variables just by random chance. For example, if we plot the variable DLA_improvement against Age we can see that there is no correlation between the two:

```{r, fig.align='center'}
ggplot(treatment, aes(Age, y = DLA_improvement)) + 
  geom_point() # No association

cor(substance_abuse$Age, substance_abuse$DLA_improvement)
```

but if we get a random sample of those variables, we may find some correlation:

```{r, fig.align='center'}
set.seed(0)

sample <- slice_sample(treatment, n = 25)
cor(sample$Age, sample$DLA_improvement)
ggplot(sample, aes(x = Age, y = DLA_improvement)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "grey")
```

In the next chapters we will learn how to find out if the correlation found in our sample shows a true relationship in the population or if it is due to random chance.

Remember that we already saw that the correlation coefficient is very sensitive to outliers. (See [spearman correlation](#spearman-correlation))

## Regression Line and the Method of Least Squares {#Residualsumofsquares}

If the scatterplot shows a linear association, then this relationship between our data points can be summarized by a line. The equation for a line is $\hat{y}*i=a+bx_*i$. The idea is to choose the line that minimizes the sum of the squared distances between the observed $y_i$ and the predicted $\hat{y_i}$

The method of least squares is the method we use to minimize this **Residual sum of squares (RSS)**

$$
RSS=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left( y_i - (\hat{\beta_o} + \hat{\beta_1} x_i) \right)^2
$${#eq-RSSsimpleRegression} 

$$
RSS = (y_1 - \beta_0 - \beta_1 x_{1})^2 + (y_2 - \beta_0 - \beta_1 x_{2})^2 + \cdots + (y_n - \beta_0 - \beta_1 x_{n})^2
$$

The intercept $\beta_0$ is the expected value of Y when X=0, in other words, the value at which the regression line crosses the Y axis.

$$
\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$
$$
\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}
$$
where $\bar{x}$ and $\bar{y}$ are the sample means

It turns out that $\beta_1=r\frac{s_y}{s_x}$ where $r$ is the correlation coefficient and $s_y$ and $s_x$ are the two standard deviations and $\hat{\beta_0} = \bar{y} - \hat{\beta_i}\bar{x}$.


The main use of regression is to predict y from x. If we are given x, then we need $r, \bar{x},\bar{y},s_x, s_y$ to calculate the regression line. In code, we can compute that line using 'lm' in r language

::: exercise-box
Exercise: midterm vs final exam student's grades (cont)

*We know that the average score for the midterm exams was 49.5 and the average score for the final exam is 69.1, and the standard deviation for the midterm exams was 10.2 while the standard deviation for the final exams was 11.8. We also know that the correlation coefficient is 0.67.*

*Predict the final exam score of a student who scored 41 on the midterm.*

Sol: 41 is 8,5 below the midterm average. We want to standardize this value. We already know how to calculate this: 
$$z = \frac{x_1-\bar{x}}{sd} = \frac{41-49.5}{10.2}=-0.83$$ 
so now looking at the formula for the regression line we expect $y$ to be r times 0.83 times the standard deviation of the final exam below the average final score. $$\bar{y}\pm r*sd_{final}*0.83 =69.1 -0.67*11.8*0.83=62.5$$
:::

### Predicting x from y:

When predicting x from y it is a mistake to use the regression line that we calculated for regressing y on x and solve for x. This is because regressing x on y will result in a different regression line.

## Normal approximation in regression

Linear regression requires that the data in the scatter plot is more or less following an elypse shape. Once we have the scatter plot with the regression line, we know that given a value of X, the value of Y will be close to the y point of the regression line for the x value. In the image below, given x, we trace a vertical line to the regression line and then we trace a perpendicular line to the y axis (green line) and that is approximately the expected value of y.

```{r, fig.align='center', echo=FALSE}

set.seed(412)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Create a data frame
data <- data.frame(x, y)

# Fit a linear model
model <- lm(y ~ x, data = data)

# Choose a random value in x
random_x <- sample(x, 1)

# Predict the y value from the linear model
predicted_y <- predict(model, newdata = data.frame(x = random_x))

# Create the scatter plot with regression line and vertical/horizontal lines
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_vline(xintercept = random_x, linetype = "dashed", color = "red") +
  geom_hline(yintercept = predicted_y, linetype = "dashed", color = "green") 

```

but we also know that $y$ will not be necessarily exactly at that point, but at certain distance of it, and that approximation follows the normal curve, so we can then use normal approximation to calculate the value of $y$: subtract off the predicted value $\hat{y}$, then divide by $\sqrt{1-r^2} * s_y$.

$$ 
z = \frac{y_1-\bar{y}}{\sqrt{1-r^2}\cdot s_y} 
$$

::: exercise-box
Exercise:

*Among the students who scored around 41 on the midterm, what percentage scored above 60 on the final?*

We already predicted that the expected value for the score on the final exam for a student that scored 41 in midterm is 62.5. That means that the normal curve is centered at 62.5 so the percentage of students that will score above 60 is the area bellow that normal curve to the right of 60 (red line).

```{r, fig.align='center'}
#| echo =FALSE
# Create a sequence of x values
x <- seq.int(from =1,to=  125, by=1)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x, mean = 62.5, sd = 14, log=F)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue")
# Add a vertical line at x = 0.71
abline(v = 60, col = "red", lwd = 2, lty = 2)
abline(v = 62.5, col = "purple", lwd = 2, lty = 2)
```

For that we standardize 60 like this: $z = \frac{y_1 - \bar{y}}{\sqrt{1 - r^2} \cdot s_y} = \frac{60 - 62.5}{\sqrt{1 - 0.67^2} \cdot 11.8} = -0.29$ so the standardized curve would look like this:

```{r, fig.align='center'}
#| echo =FALSE
# Create a sequence of x values
x <- seq(from = -3, to = 3, by = 0.01)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x, mean = 0, sd = 1)

# Plot the bell curve
plot(x, y, type = "l", lwd = 2, col = "blue",
     main = "Standard Normal Distribution",
     xlab = "X-axis",
     ylab = "Density")

# Add a vertical line at x = 0.71
abline(v = -0.20, col = "red", lwd = 2, lty = 2)

```

now we just have to use a table or software to calculate that value

```{r, fig.align='center'}
zscore <- -0.29

# Calculate the area to the left of the z-score
area_left <- pnorm(zscore)

# Calculate the area to the right of the z-score
area_right <- 1 - area_left

# Print the result
area_right
```

in our case we are interested to the area to the right, so our answer is 61.41%
:::

::: exercise-box
Exercise: midterm vs final exam student's grades 

*In a biology class, both the midterm scores and the final exam scores have an average of 50 and a standard deviation of 10. The scatterplot looks football-shaped and the correlation coefficient is 0.6.*

*Emily got exactly the mean score of 50 on the midterm. What is your prediction for Emily's score on the final?*

sol:

Since the distance in standard deviations of Emily's midterm score from the average midterm score is 0, the corresponding distance of Emily's predicted final score from the average final score is r\*0 = 0, so our prediction is the average =50

*What is the "give or take" number for your prediction?*

sol:

$$
10\sqrt{1-0.6^2} = 8
$$
:::

## Residuals {#residualplots}

As mentioned before, the observed values of $Y$ will not fall directly over the regression line. At each value of $X=x$ there is typically a distribution of possible $Y$ values.

For each observation we will have the predicted value $\hat{y}$ and the observed value $y$ . That difference is called the residual. The **residual plot** is a scatter plot of the residuals against the x-values. It should show an unstructured horizontal band and it is used to check if the regression you are using is appropriate.

For example if we have data following a linear correlation:

```{r, fig.align='center'}
#|echo = FALSE
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Create the scatter plot
plot(x, y, main = "Scatter Plot with Linear Regression Line", 
     xlab = "X-axis", ylab = "Y-axis", pch = 19, col = "blue")

# Add the linear regression line
abline(lm(y ~ x), col = "red", lwd = 2)
```

the residual scatter plot will look like this:

```{r, fig.align='center'}
model <- lm(y ~ x)

# Get the residuals and fitted values
residuals <- resid(model)
fitted_values <- fitted(model)

# Create the residual scatter plot
plot(fitted_values, residuals, 
     main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     pch = 19, 
     col = "blue")

# Add a horizontal line at 0
abline(h = 0, col = "red", lwd = 2)
```

now, if we have data that does not follow a linear correlation:

```{r, fig.align='center'}
#|echo = FALSE
set.seed(7)
x <- abs(rchisq(100, 10) - 2.7)
y <- 1/x + rnorm(100, 1, .1) - .9
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()
```

the residual will look different:

```{r, fig.align='center'}
#|echo = FALSE
model <- lm(y ~ x)

# Get the residuals and fitted values
residuals <- resid(model)
fitted_values <- fitted(model)

# Create the residual scatter plot
plot(fitted_values, residuals, 
     main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     pch = 19, 
     col = "blue")

# Add a horizontal line at 0
abline(h = 0, col = "red", lwd = 2)
```

that is an indication that the relation is not linear, and we should not use regression for this data.

### Heteroscedascidity {#heteroscedasticity}

Another variation is when the scatter shows in a fan way, this means that the variability increases with the value of X, this is called heteroscedastic.

```{r, fig.align='center'}
#|echo = FALSE

# Generate some example data with heteroscedasticity
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100, sd = 0.9 * x)

plot(x, y, 
     main = "(Heteroscedasticity)", 
     pch = 19, 
     xlim = c(0, max(x)),
     col = "blue")

```

Sometimes we can fix a non linear relationship by modifying y values, x values or both, for example applying log transformation.

### Outliers, leverage and influential points

Points with very large residuals are called outliers and they should be examined to see if they represent an interesting phenomena or an error in the data.
{#highLeverage}
When the value that deviates is the $x$ value and not the $y$ we say that it is a high leverage point, and it has the potential to cause a big change in the regression line.

```{r, fig.align='center'}
#|echo = FALSE
# Generate some example data
set.seed(123)
x <- rnorm(20)
y <- 2 * x + rnorm(20)

# Add a high leverage point
x <- c(x, 10)
y <- c(y,4)

# Fit linear models
model_with_point <- lm(y ~ x)
model_without_point <- lm(y ~ x, subset = -21)

# Create the scatter plot
plot(x, y, 
     main = "High Leverage Point Influence", 
     xlab = "X", 
     ylab = "Y", 
     pch = 19, 
     col = ifelse(1:21 == 21, "red", "blue"))

# Add regression lines
abline(model_with_point, col = "red", lwd = 2)
abline(model_without_point, col = "green", lwd = 2, lty = 2)

# Add a legend
legend("bottomright", legend = c("With High Leverage Point", "Without High Leverage Point"), 
       col = c("red", "green"), lty = c(1, 2), lwd = 2)
```
In order to quantify and observation's leverage we can compute the *leverage statistic*. A large value of this statistic indicates an observation with high leverage. The formula for simple linear regression is:
$$
h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum^n_{i'=1}(x_{i'}-\bar{x})^2}
$${#leverageStatistic}
There is another formula for multiple predictors, but we won't see the formula here. 
The leverage statistic is always between $1/n$ and 1 and the average leverage for all the observations is always equal to $(p+1)/n$. So if a given observation has a leverage statistic that greatly exceeds $(p+1)/n$ then we may suspect that the corresponding point has leverage. 




::: {.callout-orange #summarieswarn }
Beware of data that are summaries (e.g. averages of data). Those are less variable than individual observations and correlations between averages tend to overstate the strength of the relationship
:::

## Assessing the Accuracy of the Coefficient Estimates and Confidence Intervals
The *standard error* of an estimator reflects how it varies under repeated sampling. 

$$
SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^ n(x_i-\bar{x})^2}
$$
What the denominator in this formula is telling us is that the more spread out our values are along the x axis, the better we will be able to predict the correct slope. 

$$
SE(\hat{\beta_0})^2 = \sigma^2 \left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^ n(x_i-\bar{x})^2}\right]
$$
where $\sigma^2$ is the variance of the error $\sigma^2=Var(\epsilon)$

These standard errors can be used to compute *confidence intervals* For example for 95 confidence:

$$
\hat{\beta_1} \mp 2 * SE(\hat{\beta_1}) = \left[\hat{\beta_1}-2 * SE(\hat{\beta_1}),\hat{\beta_1}+2 * SE(\hat{\beta_1})\right]
$$

## Hypothesis testing and significance of correlation

The most important thing to remember about correlation testing is that **it only applies to quantitative variables that have a generally linear relationship**.

A correlation test checks the **null hypothesis that the population correlation** $\rho$ **is equal to 0**. This is, there is no relationship between X and Y. $H_0:\beta_1=0$

To test the null hypothesis, we compute a *t-statistic* 
$$
t= \frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}
$$
This will have a t-distribution with n-2 degrees of freedom. Using statistical software, it is easy to compute the probability of observing any value equal to *t* or larger. We call this probability the *p-value*.
The confidence intervals can also tell us if we should reject the null hypothesis. If 0 falls in between the range of the confidence interval, that means that we cannot exclude the possibility that the slope is 0, meaning that there is no relation between the parameters. The confidence interval is also going to tell you how big the effect is, so it is always a good practice to compute confidence intervals as well as doing a hypothesis testing. 

The test **assumes that the data isn't too skewed in any other direction and that there aren't extreme outliers**. As usual, a larger sample provides a certain degree of protection.

**If the sample is very small, the test will be unlikely to rule out the possibility that** $\rho = 0$ even if that is the case. That is, the test will be underpowered.

Similarly, **if the sample is very large, the test will be likely to conclude that** $\rho \neq 0$ **. This gets to an important idea that we've touched on before which is that there is a difference between a sample statistic being statistically significant and it actually being important or meaningful. For instance in a very large sample you may get a sample correlation of 0.001 but it may come back as statistically significant. In the real world, you should always take into consideration not just statistical significance but also effect size when you make real world decisions.

```{r}
testResult<- cor.test(treatment$DLA2,
         treatment$DLA1)
report(testResult)
```

In this type of test our null hypothesis is that the true correlation between those two variables is 0. In our example, the test is giving us a sample correlation of `r testResult$estimate` and a p-value `r testResult$p.value` which in simple terms mean that the probability of getting this correlation results in our sample if the true correlation in the population was actually 0 would be less than 0.001, so highly unlikely. The test also gives us a confidence interval, in our case `r testResult$conf.int` and its interpretation is exactly the same as it always is for our confidence interval: if we were to go out and get many many samples from the same population and compute correlations of them, 95% of the time that confidence interval would capture the true parameter, in our case the true population correlation.

With this new knowledge we are going to test the sample we created before showing a relationship between age and DLA_improvement and compare it with the correlation for the population (all records in our file)

```{r, fig.align='center'}
#population correlation between age and dla improvement

ggplot(treatment, aes(x = Age, y = DLA_improvement)) + 
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "grey")

report(cor.test(treatment$Age,
         treatment$DLA_improvement))

#small sample correlation:
set.seed(0)
sample <- slice_sample(treatment, n = 25)
report(cor.test(sample$Age,
         sample$DLA_improvement))

```

::: {.callout-orange appearance="simple" icon="false"}
A significant correlation means that the likelihood of observing such a correlation (or stronger) by random chance is low, given the null hypothesis of no correlation.
:::

## Interpretation of Linear Regression results:

Any statistical package will have a function to calculate the line that fits the linear relationship in our data. In r it is lm command. We put the response variable first, and then the explanatory variable, finally the dataset.

```{r}
lm(DLA2 ~ DLA1, data = treatment)
```

It is read DLA2 is explained by DLA1 and we get two coefficients called the slope and the intercept. The interpretation of these numbers is very important. When you have a straight line, the slope shows the increase in $y$ when $x$ increases by 1. The intercept means where is the $y$ value when $x=0$

The most important use of a regression line is that it allows you to make predictions.

In our example, if we get a new patient with DLA1 of 3.6 we can use these values to calculate what it is the expected DLA2 value:

```{r}
DLA1 <- 3.6
DLA2 <- .07243 + .9660 *DLA1
DLA2
```

A very important thing to bear in mind is that **a regression line should only be used to make predictions on individuals whose explanatory variable falls in the range of the values used to calculate the linear regression model**, for example in our case our model was trained with DLA1 between 2.5 and 5 so we should not use this model to make predictions on individuals whose DLA1 is 6, for example.

We can get more information about our model if we ask for their summary:

```{r}
summary(lm(DLA2 ~ DLA1, data = treatment))
```

the p-value that corresponds with the intercept is a the p-value of a test done against the null hypothesis that the intercept is actually 0.

the p-value that corresponds with the dependent variable is the result of a test done against the null hypothesis that the slope is actually 0.

we can retrieve those p-values using the coefficients table from our lm result:

```{r}
testResult<- summary(lm(DLA2 ~ DLA1, data = treatment))
testResult$coefficients[,4]
```

We can use the `names()` function in order to find out what other pieces of information  are stored.
Although we can extract these quantities by name---e.g. `testResults$coefficients`---it is safer to use the extractor functions like `coef()` to access them.

```{r }
lm.fit<- lm(DLA2 ~ DLA1, data = treatment)
names(lm.fit)
coef(lm.fit)
``` 
In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command.

```{r }
confint(lm.fit)
```

The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of Y for a given value of X.

```{r chunk8}
# Choose a random value in x
random_x <- sample(x, 1)

predict(lm.fit, newdata = data.frame(x = random_x),
    interval = "confidence")
predict(lm.fit, newdata = data.frame(x = random_x),
    interval = "prediction")
```
R is also retrieving a p value for the overall model that's coming from an Anova F-Test. that corresponds with the coefficient for the slope.

our diagnostic plots are automatically
produced by applying the `plot()` function directly to the output
from `lm()`. In general, this command will produce one plot at a
time, and hitting *Enter* will generate the next plot. However,
it is often convenient to view all four plots together. We can achieve
this by using the `par()` and `mfrow()` functions, which tell `R` to split
the display screen into separate panels so that multiple plots can be
viewed simultaneously. For example,  `par(mfrow = c(2, 2))` divides the plotting
region into a $2 \times 2$ grid of panels.

```{r, fig.align='center', fig.width=7}
par(mfrow = c(2, 2))
plot(lm.fit)
```

::: exercise-box
Exercises:

*Using the mpg dataset:*

-   *Is there a linear relationship between city mileage and highway mileage in this set?*

    ```{r, fig.align='center'}
    file2 <- here("data", "mpg_2008.xlsx")
    mpg_2008 <- read_excel(file2)

    ggplot(mpg_2008, aes(x = cty, y = hwy))+
      geom_point()+
      geom_smooth(method ='lm',
                  se = FALSE,
                  color = 'purple' )
    ```

-   *What is the sample correlation?*

    ```{r}
    cor(mpg_2008$cty, mpg_2008$hwy)
    ```

-   *Does the sample correlation provide evidence that the population correlation is different from zero?*

    ```{r}
    testResult<- cor.test(mpg_2008$cty, mpg_2008$hwy)
    testResult
    report(testResult)
    ```

    Yes.

-   *Find the equation of the regression line*

```{r}
lm (cty ~hwy, data= mpg_2008)
lm (hwy ~ cty, data= mpg_2008)
```

the equation will be $cty = 0.6687 * hwy + 1.017$ or $hwy = 1.39 *cty + .2388$

*Estimate the highway mileage of an unknown car in this population with a city mileage of 24 miles per gallon.*

```{r}
1.29*24+.2388
```
:::

::: {.callout-orange appearance="simple" icon="false"}
Linear regression models the relationship between a dependent variable and one or more independent variables. Remember a well-fitting model doesn't always imply causality.
:::

## Assessing Model Accuracy

In this section, we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specific data set.

### Measuring the Quality of Fit. Mean Squared Error (MSE) {#MeanSquaredError}

In order to evaluate the performance of a statistical method on a given dataset, we need some way to measure how well its predictions actually match the observed data. In regression, the most commonly-used measure is the *mean squared error (MSE)* 
$$
MSE = \frac{1}{n}\sum^n_{i=1}(y_i-\hat{f}(x_i))^2 =\frac{1}{n}\sum^n_{i=1}(y_i-\hat{y_i})^2
$${#eq-MSEMeanSquaredError}

that is the squared difference between the observed values and the predicted values divided by the number of data points. The MSE is computed using the training data that was used to fit the model, and should be referred as training MSE, but we are more interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE. There is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be small, but the test MSE is often much larger.

```{r, fig.align='center', echo=FALSE}

set.seed(123)
X <- seq(0, 10, length.out = 30)
Y <- 2 + 5 * X + X^2 + rnorm(30, sd = 5)
data <- data.frame(X, Y)

# Fit models
linear_model <- lm(Y ~ X, data = data)
parametric_model <- lm(Y ~ poly(X, 5), data = data)  # Increased polynomial degree
overfitted_model <- lm(Y ~ poly(X, 20), data = data)

# Predict values
data <- data %>%
  mutate(
    linear_pred = predict(linear_model),
    polynomial_pred = predict(parametric_model),
    overfitted_pred = predict(overfitted_model)
  )

# Plot
ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  geom_line(aes(y = linear_pred), color = "blue") +
  geom_line(aes(y = polynomial_pred), color = "red") +
  geom_line(aes(y = overfitted_pred), color = "green") +
  labs(title = "Scatter Plot with 3 different Regression Lines", x = "X", y = "Y")
```

the graph above shows a linear regression line, and two polynomial lines one of them over fitted. If we calculate the Mean Square Error of each regression we can see that the over-fit model is not the best predictor over new data:

```{r, fig.align='center'}
# Function to calculate MSE
calculate_mse <- function(model, new_data) {
  mean((new_data$Y - predict(model, new_data))^2)
}

# Generate new training data
set.seed(456)
X <- seq(0, 10, length.out = 30)
Y <- 2 + 5 * X + X^2 + rnorm(30, sd = 5)
new_data <- data.frame(X, Y)

# Calculate MSE for each model
mse_linear <- calculate_mse(linear_model, new_data)
mse_parametric <- calculate_mse(parametric_model, new_data)
mse_overfitted <- calculate_mse(overfitted_model, new_data)

# Create a dataframe for MSE values
mse_data <- data.frame(
  Model = c("Linear", "Polynomial", "Overfitted"),
  MSE = c(mse_linear, mse_parametric, mse_overfitted)
)

# Plot MSE values
ggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "MSE for Different Models on New Training Data", x = "Model", y = "Mean Squared Error") +
  theme_minimal()

```

### Bias-Variance Trade-off

Imagine you're trying to build a model to predict house prices. You could use a simple model or a complex one. The bias-variance trade-off is all about balancing simplicity and complexity.

Bias is error introduced by assuming a model is too simple, capturing only a rough approximation of the real relationship. High bias means the model might miss important patterns (underfitting). Think of a straight line when a curve is more appropriate.

Variance is error introduced by the model being too complex, capturing noise instead of the actual pattern. High variance means the model is too sensitive to small fluctuations in the training data, leading to poor performance on new data (overfitting). Imagine fitting every little bump in the data.

The trade-off part comes in because as you try to decrease bias by making the model more complex, you often increase variance, and vice versa. The goal is to find the sweet spot where the model captures the essential patterns without overreacting to noise.

Low Bias, High Variance: A very flexible model that might fit the training data really well but fails on new data.

High Bias, Low Variance: A very simple model that doesn't capture the data well, missing patterns.

Optimal Bias-Variance Trade-Off: A balanced model that captures the underlying trend without overfitting.

### R-squared {#Rsquare}
We already talked about [how to assess the slope of an individual predictor](#MeanSquaredError)

The [Residual Standard Error (RSE)](#ResidualStandardError) provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of Y , it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a proportion —the proportion of variance explained— and so it always takes on a value between 0 and 1, and is independent of the scale of Y .

We already saw the [residual sum of squared (RSS)](#Residualsumofsquares)
RSS is a measure of the discrepancy between the actual data and the model's predictions. It represents the sum of the squared differences between the observed values and the predicted values.

$$
RSS=\sum^n_{i=1}(y_1-\hat{y_1})^2 
$$ {#eq-rss}

RSS measures the amount of variability that is left unexplained after performing the regression. 

The **total Sum of Squares (TSS)**:
TSS is a measure of the total variability in the observed data. It represents the sum of the squared differences between the observed values and the mean of the observed values. TSS measures the total variance in the response Y and can be thourhg of as the amount of variability inherent in the response before the regression is performed. 

$$
TSS=\sum^n_{i=1}(y_1-\bar{y_1})^2
$${#eq-tss}

TSS - RSS measures the amount of variability in the response that is explained (or removed) by performing the regressin, and $R^2$ measures the proportion of variability in Y that can be explained using X.
$$
R^2 = \frac{TSS-RSS}{TSS}= 1-\frac{RSS}{TSS}
$${#eq-rsquared}
The R-squared value measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It's calculated as:

A larger R-squared value indicates a better fit of the model to the data. An R-squared value closer to 1 means that the model explains a large proportion of the variance in the dependent variable, signifying a good fit. Conversely, a smaller R-squared value indicates that the model explains less of the variance, signifying a poorer fit, or the error variance is high, or both. 

However, it can still be challenging to determine what is a good R2 value, and in general, this will depend on the application. For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error. In this case, we would expect to see an R2 value that is extremely close to 1, and a substantially smaller R2 value might indicate a serious problem with the experiment in which the data were generated. On the other hand, in typical applications in biology, psychology, marketing,
and other domains, the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R2 value well below 0.1 might be more realistic.

The $R^2$ statistic is a measure of the linear relationship between X and Y, and if we recall [correlation](#correlation-coefficient), it also measures the linear relationship between X and Y, this suggest that we might be able to use correlation $r-Cor(X,Y)$ instead of $R^2$ in order to assess the fit of the model, and in fact, it can be shown that in the simple linear regression setting, $R^2 = r^2$.


### Residual Standard Error (RSE) {#ResidualStandardError}
Another measure of the model fit is the **Residual Standard Error (RSE)** The RSE is an estimate of the standard deviation of the error term, representing the average amount that the observed values deviate from the model's predictions. Essentially, it gives you an idea of the typical size of the residuals (errors) made by the model. It’s like a measure of how well the model fits the data.

$$
RSE = \sqrt{\frac{1}{n-p-1}RSS}
$${#eq-RSE}

where $n$ is the number of observations and $p$ is the number of predictors or independent variables in the model. $n-p-1$ is the *degrees of freedom*.
A smaller RSE indicates a better fit, implying that the model’s predictions are close to the actual data.
 - RSS helps calculate RSE, giving you the scale of the errors.
 - TSS is used to calculate $R^2$, which, together with RSE, helps you understand the overall model fit.

Both RSE and $R^2$ provide insight into the model's performance, but from slightly different angles. While $R^2$ tells you how much variance in the dependent variable is explained by the model, RSE tells you about the typical size of the prediction errors.

By using these metrics together, you can get a comprehensive view of how well your regression model is performing

## Multiple Linear regression. 

When we have more than one predictor, the model will create a plane instead of a curve that tries to minimize the sum of squared errors:

```{r, fig.align='center', echo=FALSE}
# Set seed for reproducibility
set.seed(123)

# Generate sample data with a stronger linear relationship
n <- 100
X1 <- runif(n, 0, 10)
X2 <- runif(n, 0, 10)
Y <- 5 + 1.5*X1 + 2*X2 + rnorm(n, sd = 2)  # Adjusted coefficients and smaller noise

# Create data frame
data <- data.frame(X1, X2, Y)

# Fit the linear model
model <- lm(Y ~ X1 + X2, data = data)

# Predict values for plotting the hyperplane
x1_seq <- seq(min(data$X1), max(data$X1), length.out = 30)
x2_seq <- seq(min(data$X2), max(data$X2), length.out = 30)
grid <- expand.grid(X1 = x1_seq, X2 = x2_seq)
grid$Y_pred <- predict(model, newdata = grid)

p <- plot_ly(data, x = ~X1, y = ~X2, z = ~Y, type = 'scatter3d', mode = 'markers', marker = list(size = 3)) %>%
  add_trace(x = grid$X1, y = grid$X2, z = grid$Y_pred, type = 'mesh3d', opacity = 0.5, color = 'blue') %>%
  layout(scene = list(
    xaxis = list(title = 'X1'),
    yaxis = list(title = 'X2'),
    zaxis = list(title = 'Y')
  ))
p
```

Remember the formula for RSS for a simple linear model was explained already (@eq-RSSsimpleRegression) now we have a similar version including all the predictors:

$$
RSS=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left( y_i - \hat{\beta_0} - \hat{\beta_1} x_{i1} - \hat{\beta_2} x_{i2}\dots... - - \hat{\beta_p} x_{ip}\right)^2
$${#eq-RSSmultipleRegression} 

This is done using statistical software. The values $\hat{\beta_o} ,\hat{\beta_1},\dots ,\hat{\beta_p}$ that minimize RSS are the **multiple least square regression coefficient estimates** 

The ideal scenario is when the predictors are uncorrelated: each coefficient can be estimated and tested separately, a unit change in $X_j$ is associated with a $\beta_j$ change in $Y$ while all the other variables stay fixed. 

Correlations amongst predictors cause problems: the variance of all coefficients tends to increase, sometimes dramatically and interpretations become hazardous, when $X_j$ changes, everything else changes. We cannot attribute the change to a specific predictor. For example height and weight are two predictors that are correlated. 

When looking at multiple regression models we have to ask ourselves some questions:
- Is at least one of the predictors useful in predicting the response
- Do all the predictors help to explain Y or only some of them are useful?

- How well does the model fit the data?

### F-statistic. 
For the first question we can use the **F-statistic**
$$
F= \frac{TSS-RSS/p}{RSS/(n-p-1)}
$${#eq-fStatistic}

The F-statistic in a multiple regression model assesses the overall significance of the model. It tells you whether your model's variables are jointly significant in explaining the variation in the dependent variable.
High F-statistic: Indicates that the group of predictors has a significant relationship with the dependent variable. The larger the F, the more likely the observed relationship is not due to random chance.
Low F-statistic: Suggests that the group of predictors does not have a significant explanatory power.

Whether an F value is high or low is determined by comparing it to a critical value from the F-distribution table, which depends on the degrees of freedom and the chosen significance level (commonly 0.05).
Steps to Determine High or Low F-value:
- Degrees of Freedom: Calculate the degrees of freedom for the numerator (df1 = number of predictors) and the denominator (df2 = total number of observations - number of predictors - 1).

F-distribution Table: Use the degrees of freedom to find the critical F-value from the F-distribution table at your chosen significance level (usually 0.05).

Compare: If the calculated F-value from your regression output is greater than the critical F-value from the table, it means your predictors are collectively significant (high F-value).

Example:
Suppose you have 3 predictors in your model and a sample size of 100. The degrees of freedom are:

df1 (numerator) = 3

df2 (denominator) = 100 - 3 - 1 = 96

Looking up the critical value for df1 = 3 and df2 = 96 at the 0.05 significance level in an F-table, you might find it to be around 2.70.

If your calculated F-value is, say, 5.2, since 5.2 > 2.70, your model is significant (high F-value).

If it were 1.8, since 1.8 < 2.70, your model isn't significant (low F-value).

In r, the f-value will also come with its p-value, indicating its significance. 

### Deciding on the important variables. 
The most direct approach to answer this question is called *all subsets* or *best subsets* regression. We compute teh least squares fit for all possible subsets and then choose between them based on some criterion that balances training error with model size. This gets really hard when the number of variables is high, the number of possible models is $2^p$ so for example for p=40 there are over a billion models. Instead, we need an automated approach that searches through a subset of them. We discuss two commonly use approaches next.

#### Forward selection
We start with the *null model* that is a model that contains the intercept but no predictors. The intercept is the mean of $y$
Now you start adding variables one at a time: for each variable you fit $p$ simple linear models each with one of the variables and the intercept, and you look at each of them. After that you choose the variable that results in the lowest RSS.
Now, having picked that, you fix that variable in the model and now you add one by one all the variables to this model again, and choose the one that best improve the residual sum of squares. 
You can continue like this until some stopping rule is satisfied, for example, when all the remaining variables have a p value above some threshold.

#### Backward selection
It runs a similar process but starting with a model with all the variables, then you run the model removing each variable and see which one is the least statistically significant one. 
The new (p-1) variable model is fit, and the variable iwth the largest p-value is removed. 
Continue until a stopping rule is reached, for instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold. 

### Model fit
Two of the most common numerical measures of model fit are the RSE and
$R^2$, the fraction of variance explained. These quantities are computed and interpreted in the same fashion as for simple linear regression.
Recall that [in simple regression, $R^2$ is the square of the correlation of the response and the variable](#Rsquare). In multiple linear regression, it turns out that it equals $Cor(Y, \hat{Y} )^2$, the square of the correlation between the response and the fitted linear model.
It turns out that $R^2$ will always increase when more variables are introduced, even if those variables are only weekly associated with the response. 
If we notice that the increase in $R^2$ is minimal after adding another variable, it is probably best to leave that variable out of the model. 
We can check RSE in combination with $R^2$, as RSE will increase despite RSS increasing as well if the new variable is not adding enough value. Remember the formula for RSE (#eq-RSE):
$$
RSE = \sqrt{\frac{1}{n-p-1}RSS}
$$
Thus, models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p.

:::{.exercise-box}
Example:
We run a campaign to sell a product, and we advertise in three mediums, newspapers, radio and tv. We have the amount spent in each of the mediums and the sales. We will first run a simple linear model over the three variables:
```{r}
Advertising <- read_csv("data/Advertising.csv")
tv<- lm(sales ~ TV, data = Advertising)
summary(tv)

radio<- lm(sales ~ radio, data = Advertising)
summary(radio)

newspaper<- lm(sales ~ newspaper, data = Advertising)
summary(newspaper)
```

We see that independently, all three variables have a p-value below or significance level. 

If we fit the model to the three variables instead newspaper stops showing as significant. 
```{r}
lm.fit<- lm(sales ~ TV+radio+newspaper, data = Advertising)
summary(lm.fit)

```
The Adjusted Rsquared is 0.8956 and the RSE 1.686

If we fit the model with only the two significant variables:
```{r}
lm.fit<- lm(sales ~ TV+radio, data = Advertising)
summary(lm.fit)

```
The adjusted R-squared is 0.8962 instead of 0.8956 but the RSE has decreased from 1.686 in the model with three predictors to 1.681 in the simpler model. 
So, the model sales ~ TV + radio is more parsimonious and just as effective. Plus, it’s always good to avoid unnecessary complexity when you can.
:::

Sometimes visualizing the data is helpful:
```{r, fig.align='center', fig.width=10}
# Calculate predicted values
Advertising$predicted_sales <- predict(lm.fit)

# Create 3D scatter plot
s3d <- scatterplot3d(Advertising$TV, Advertising$radio, Advertising$sales, 
                     pch = 16, highlight.3d = TRUE, type = "p", 
                     main = "3D Scatter Plot with Regression Plane",
                     xlab = "TV", ylab = "Radio", zlab = "Sales")

# Add regression plane
s3d$plane3d(lm.fit, col = "lightblue")

# Plot lines from data points to the regression plane
for (i in 1:nrow(Advertising)) {
  s3d$points3d(x = c(Advertising$TV[i], Advertising$TV[i]), 
               y = c(Advertising$radio[i], Advertising$radio[i]), 
               z = c(Advertising$sales[i], Advertising$predicted_sales[i]), 
               type = "l", col = "green")
}

```

## Collinearity {#collinearity}
Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.
Since collinearity reduces the accuracy of estimates of the regression coefficients, it causes the standard error to grow. Recall that the t-statistic for each predictor is calculated by dividing $\hat{\beta_j}$ by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject the null hypothesis, this means that the probability of correctly detecting a non-zero coefficient is reduced. 
A simple way to detect collinearty between two variables is to look at the correlation matrix of the predictors, but it is also possible that there is collinearity between three variables, even if no pair of variables have a high correlation, and this will not be detected in the correlation matrix. For this we can calculate the *variance inflaction factor (VIF)* but this is out of the scope here. 


## Categorical predictors in regression models.
Sometimes we may want to include in the model a variable that is not numeric, for example gender. To do that we create what we call a dummy variable, for example $x_i = 1$ if the person is female and $x_i = 0$ if it is a male. 
The resulting model will be:
$$
y_i=\beta_o+\beta_1x_i+\epsilon_i
$$
which will result in this if the person is female:
$$
y_i=\beta_o+\beta_1+\epsilon_i
$$
and if the person is male:
$$
y_i=\beta_o+\epsilon_i
$$
so what this is telling us is that $\beta_1$ is the effect of being female vs the baseline (in this case male)

If we have more than two levels what we do is create more dummy variables. For example we look at three different etnicities, Asian, Caucasian and African American, we create:

$x_{i1} = 0$ if person is Asian and $x_{i1} = 1$ if the person is not Asian
$x_{i2} = 0$ if person is Caucasian and $x_{i2} = 1$ if the person is not.
We don't need a level for African American because that will be deducted when $x_{i1} = 0$ and $x_{i2} = 0$.
So for categorical variables we create $k-1$ dummy variables. The level with no dummy variable is known as *baseline*. The choice of a baseline will not affect the fit of the model, the residual sum of squares  would be the same, but the coefficient and the p-values will change because each other category will be compared with the baseline. 

The equation will now look like this:
$$
y_1 = \beta_o+\beta_1x_{i1}+\beta_2x_{i2}+\epsilon_i
\begin{cases} 
\beta_0 + \beta_1 + e_1 & \text{if condition 1} \\
\beta_0 + \beta_2 + e_1 & \text{if condition 2} \\
\beta_0 + e_1 & \text{if condition 3}
\end{cases}
$$

:::exercise-box
Example
*The Carseats data from the library ISLR2 includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. *

```{r}
lm.fit <- lm(Sales ~ ShelveLoc, 
    data = Carseats)
summary(lm.fit)
contrasts(Carseats$ShelveLoc)
```
The contrasts() function returns the coding that R uses for the dummy variables.

R has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.
:::


## Interactions
In our previous examples, we have assume independence from one parameter to the others, but that is not always the case, sometimes the change in one predictor affects the results in another. 

:::{exercise-box}
Example
Imagine we run a campaign to sell a product, and we advertise in three mediums, newspapers, radio and tv. It could be that the effect of radio increases the effectiveness of the adds in tv, this in marketing is called synergy effect, and in statistics we refer to it as interaction effect. If we detect this interaction, spending part of our budget in radio and part on tv could be more effective than spending all in only the media with the most effect. 

```{r}
lm.fit<- lm(sales ~ TV + radio, data = Advertising)
summary(lm.fit)
```

to include interactions in our model we create a new variable with the product of the two predictors. If we ignore newspaper our equation would be:

$$
sales = \beta_0+\beta_1\times TV+\beta_2\times Radio + \beta_3 \times(radio\times TV)+ \epsilon
$$
$$
sales = \beta_0+(\beta_1 + \beta_3\times Radio) \times TV+\beta_2\times Radio + \epsilon
$$
and if we get a summary of the linear model we will see if the interaction between tv and radio is indeed significant or not:

```{r}
lm.fit_interaction <- lm(sales ~ TV * radio , data = Advertising)
summary(lm.fit_interaction)
# or
lm.fit_interaction <- lm(sales ~ TV + radio + TV:radio , data = Advertising)
summary(lm.fit_interaction)

```

the p-value of the interaction indicates that in our example this interaction is significant, we can also see how our R-squared is higher now (0.9678) than when we did not include the interaction in our model (0.8972).
This means that (96.8-89.7)/(100-89.7)=69% of the variability in sales that remains after fitting the initial model has been explained by the interaction term.

The coefficients estimates in the table suggest that an increase in radio advertising of \$1000 is associated with increased sales of $(\hat{\beta_2}+\hat{\beta_3}\times TV)\times 1000 = 29+1.1\times TV units$
An increase in TV advertising of \$1000 is associated with an increase of sales of $(\hat{\beta_1}+\hat{\beta_3}\times radio)\times 1000 = 19+1.1\times radio units$
:::

Sometimes it is the case that an interaction term has a very small p-value, but the associated main effects do not. The *hierarchy principle* states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.

## Non-linear transformations of the predictors. 
If a linear model does not quite fit our data:

```{r, fig.align='center'}

lm.fit<- lm(mpg~ hp, data= mtcars)
plot(mtcars$hp, mtcars$mpg)
abline(lm.fit)
summary(lm.fit)
```
we can transform this into a polynomial regression by making extra variables to accomodate polynomials, for example we add another variable horsepower squared:

```{r}
lm.fit <- lm(mpg ~ hp + I(hp^2), data = mtcars)

plot(mtcars$hp, mtcars$mpg, xlab = "Horsepower", ylab = "Miles per Gallon")

points(mtcars$hp, fitted(lm.fit), col = "pink", lwd = 2)

#or if we want to show a line:
# Create a sequence of hp values for prediction
hp_seq <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 100)

# Predict mpg for each hp value in the sequence
predicted_mpg <- predict(lm.fit, newdata = data.frame(hp = hp_seq))

lines(hp_seq, predicted_mpg, col = "blue", lwd = 2)

summary(lm.fit)

```
but there is a better way of fitting polynomials with r using the `poly()` function:
```{r, fig.align='center'}
lm.fit <- lm(mpg ~ poly(hp,4), data = mtcars) 
plot(mtcars$mpg ~ mtcars$hp)
points(mtcars$hp, fitted(lm.fit), col = "pink", lwd = 2)

```


## Potential problems in linear models
We already talked about most of them, but let's do a summary:

:::{.callout-orange}

1. **Non linearity**: The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. We saw how we can make use of the residual plot to see if the relation is liner or not when we talked about [residuals]({#residualplots}) 

2. **Correlation of error terms**: An important assumption of the linear regression model is that the error terms are uncorrelated.If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors and this will affect the p-values, confidence intervals etc. Why might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. time series. In many cases, observations that are obtained at adjacent time points will
have positively correlated errors. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no discernible pattern.
On the other hand, if the error terms are positively correlated, then we may see tracking in the residuals.Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals’ heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family, eat the same diet,or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regression
as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.

3. **Non-constant variance of error terms [(heteroscedasticity)](#heteroscedasticity) **: error terms have a constant variance, Var(ϵi) = σ2. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. Unfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response

4. **Outliers**: An outlier is a point for which $y$ is far from the value predicted by the model. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance it affects the RSE. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Residual plots can be used to identify outliers.

5. **High-leverage points**: Observations with [high leverage](#highLeverage) have an unusual value for $x_i$. Removing a high leverage point has much more impact than removing an outlier. In a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors.

6. **Collinearity**: we already dedicated [a section](#collinearity) to these problem. 
:::


# Introduction to ANOVA (Analysis of Variance)

Analysis of variance, or ANOVA, is used to test **the relationship between a categorical and a quantitative variable**. Specifically, it test the null hypothesis that the population mean of the quantitative variable is the same in each of the groups. A low p-value indicates that the data would be unlikely if in fact those population means where equal.

ANOVA requires that the data be relatively symmetric in each group and not include extreme outliers. Additionally, the spread of the data within the groups should be similar. Always plot your data before running ANOVA to check these assumptions.

ANOVA is an omnibus test: on it's own, it does not say anything about which group or groups might be interesting.

We are going back to our attrition dataset and analyse if the variable of work life balance gives us any information about the money they make (MonthlyRate). A first intuitive approach is to calculate the average MontlyRate per group:

```{r}
attrition1 %>% group_by(WorkLifeBalance) %>% 
  summarise (mean(MonthlyRate)) 
```

this shows that there is in fact a difference but, is this difference important or significative?

One first approach to answer this question is observing the data in a boxplot:

```{r, fig.align='center', echo=FALSE}
ggplot (data= attrition1, aes(x = WorkLifeBalance, y= MonthlyRate))+
  geom_boxplot()
```

This already shows us that the spread of the data in each of the groups is big compared to the the spread of the data between the groups, but one way we can test this is doing an ANOVA test that is going to compare the variances within the groups to the variance between the groups.

```{r}
testResult<- (aov(MonthlyRate ~ WorkLifeBalance, data = attrition1))
sumRes<- summary(testResult)
sumRes
```

if we look at our p-value that is testing if in fact all of these groups have the same mean it is `` r sumRes[[1]]$`Pr(>F)` `` which means that in almost 65% of the cases we would see these kind of data even if the population means for these groups were the same.

Our null hypothesis is that all group means are equal. If we had only two groups, we would use a t test

$$
t=\frac{difference\ in \ sample\ means}{SE\ of\ difference}
$$

now we generalize this idea to the instance when we have several groups. If the differences between the samples means are large relative to the variability within the groups, this suggest that our null hypothesis is not true. In contrast, if the differences in the means are quite small compared with the variability within each groups, that suggest that the differences in the means are due to variability in the sample.

```{r , fig.height=6, fig.width=12, fig.align='center'}
#|echo: FALSE
# Set seed for reproducibility
set.seed(123)

# Generate data for large differences between group means
group_A1 <- rnorm(100, mean = 10, sd = 2)
group_B1 <- rnorm(100, mean = 20, sd = 2)
group_C1 <- rnorm(100, mean = 30, sd = 2)

# Generate data for small differences between group means
group_A2 <- rnorm(100, mean = 15, sd = 10)
group_B2 <- rnorm(100, mean = 17, sd = 10)
group_C2 <- rnorm(100, mean = 19, sd = 10)

# Create data frames
data_large_diff <- data.frame(value = c(group_A1, group_B1, group_C1),
                              group = rep(c("A", "B", "C"), each = 100))

data_small_diff <- data.frame(value = c(group_A2, group_B2, group_C2),
                              group = rep(c("A", "B", "C"), each = 100))

# Create boxplots
p1 <- ggplot(data_large_diff, aes(x = group, y = value)) + 
  geom_boxplot() + 
  ggtitle("Large Differences Between Group Means") +
  theme_minimal()

p2 <- ggplot(data_small_diff, aes(x = group, y = value)) + 
  geom_boxplot() + 
  ggtitle("Small Differences Between Group Means") +
  theme_minimal()

# Combine plots side by side
combined_plot <- p1 + p2

# Print the combined plot
print(combined_plot)

```

But unfortunately things are not as easy as looking at the boxplots, the reason is that according to the square root law, the chance variability in the sample mean is smaller than the chance variability in the data, so the evidence against $H_0$ is not obvious from the boxplots. Computation is necessary.

We have k groups with n observations:

| observation | group 1  | group 2  | group k  |
|-------------|----------|----------|----------|
| 1           | $y_{11}$ | $y_{12}$ | $y_{1k}$ |
| 2           | $y_{21}$ | $y_{22}$ | $y_{2k}$ |
| ...         | ...      | ...      | ...      |
| n           | $y_{n1}$ | $y_{n2}$ | $y_{nk}$ |

in total there are $N=n_1+n_2+\cdots+n_k$ observations. The sample mean of one group is $\bar{y_j}=\frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}$ and the overal sample mean or grand mean is $\bar{\bar{y}}= \frac{1}{N}\sum_{j=1}^{k}\sum_{i=1}^{n_j}y_{ij}$

The analysis of variance compute two important variables, one is the **treatment sum of squares (SST)**{#SSTTreatmentSumSquares} that look at the difference between the group means to the overall mean

$$
SST=\sum_j\sum_i(\bar{y_j}-\bar{\bar{y}})^2 
$${#eq-SSTTreatmentSumSquares}

and has k-1 degrees of freedom.

If we divide this by its degrees of freedom we get what is called the **treatment mean square (MST)**{#MSTTreatmentMeanSquare} and measures the variability of the treatment means $\bar{y_j}$

$$
MST= \frac{SST}{k-1}
$${#eq-MSTTreatmentMeanSquare}

The other quantity we are interested in is the **error sum of squares (SSE)**{#SSEErrorSumSquares} where we look at the difference between each observation and the group mean.

$$
SSE= \sum_j\sum_i({y_{ij}}-\bar{y_j})^2
$${#eq-SSEErrorSumSquares}

and has N-k degrees of freedom.

Dividing the error sum of square by its degrees of freedom we get what is called the **error mean square or Mean Square for Error (MSE)** and measures the variability within the groups. Not to be confused with the the [MSE (Mean Squared Error)](#MeanSquaredError) that we saw in linear model. In both cases, MSE is a measure of variability of the errors, but In ANOVA, MSE (Mean Square for Error) assesses within-group variance. In linear models, MSE (Mean Squared Error) assesses the goodness of fit of the model by looking at prediction errors.

$$
MSE= \frac{SSE}{N-k}
$${#eq-MSEErrorMeanSquare}

Since we want to compare the variation between the groups to the variation within the groups we look at the ratio

$$
F= \frac{MST}{MSE}
$${#eq-Fstatistic}

Under the null hypothesis of equal group means this ratio should be about 1. It will not be exactly one due to sampling variability. It follows a F-distribution with k-1 and N-k degrees of freedom. Large values of F suggest that the variation between groups is unusually large. We reject the null hypothesis if F is the right 5% of the tail, i.e. when the p-value is smaller than 5%.

The result of the test is summarized in the ANOVA table:

| Source    | df  | Sum of Squares | Mean Square | F       | p-value |
|-----------|-----|----------------|-------------|---------|---------|
| Treatment | k-1 | SST            | MST         | MST/MSE |         |
| Error     | N-k | SSE            | MSE         |         |         |
| Total     | N-1 | TSS            |             |         |         |

Where **TSS is the Total Sum of Squares** and measures the total variability in the data (the overall variation of the observed data points around their mean):

$$
TSS=\sum_j\sum_i({y_j}-\bar{\bar{y}})^2 
$${#eq-TSS1}

### The one-way ANOVA model

The idea behind the anova table is that each observation is generated as the sum of a treatment $\mu_j$ plus and error term $\epsilon_{ij}$ (measurment of error) that follow the normal curve with mean 0 and common variance $\sigma^ 2$

$$
y_{ij}=\mu_j+\epsilon_{ij}
$$

our null hypothesis is that all treatment means are the same $\mu_1=\mu_2=\cdots=\mu_k$ Instead of looking at the group means it is helpful to look at deviations from an overall mean $\mu$ that deviation is represented by the greek letter 'tau' and called the **treatment effect** $\tau_j=\mu_j-\mu$ so the model is the overall mean plus the deviation plus an error term 
$$
y_{ij}=\mu+\tau_j+\epsilon_{ij}
$$

We estimate the overall mean $\mu$ by the grand mean $\bar{\bar{y}}$. then the estimate $\tau_j=\bar{y_j}-\bar{\bar{y}}$ and the estimate of the error is the residual $\epsilon_{ij}=y_{ij}-\bar{y_j}$. so we can write the previous equation like this:\
$$
y_{ij}=\mu+\tau_j+\epsilon_{ij}=\bar{\bar{y}}+(\bar{y_j}-\bar{\bar{y}})+(y_{ij}-\bar{y_j})
$$

now if we move $\bar{\bar{y}}$ to the left of the equation:

$$
y_{ij}-\bar{\bar{y}}=(\bar{y_j}-\bar{\bar{y}})+(y_{ij}-\bar{y_j})
$$

and we sum and square the terms:

$$
\sum_{j} \sum_{i} (y_{ij} - \bar{y}_{i})^2 = \sum_{j} \sum_{i} (y_{ij} - \bar{y}_{j})^2 + \sum_{j} \sum_{i} (\bar{y}_{j} - \bar{y}_{i})^2
$$

which is equal to say:

$$
TSS=SST+SSE
$${#eq-TSS2} 
Which means that total variation can be split into two sources, the treatment sum of squares and the error sum of squares. This is the decomposition that is behind the ANOVA table.

::: {#anovaassump style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;" icon="false"}
ANOVA Assumptions

-   Data are normally distributed

-   The F-test assumes that all the groups have the same variance. This can be roughly checked with side by side plots, but there are formal test we can perform as well.

-   The data are independent within and across groups. This would be the case if the subjects were assigned treatment at random. On the other hand, if the data was acquired from an observational study, we need to be very careful because this assumption would not be met and there could be cofounders so we will not be able to claim that there is causation with the treatment and the results of the tests.
:::

## Advanced ANOVA Techniques

Now we are going to use the mpg_2008 data set again to see if there is a difference in highway millage with respect to the 'Drive' categorical variable (front wheel, rear wheel and 4 wheel drive). Let's do the same we did above and calculate the average per group first

```{r}
mpg_2008 |> 
  group_by(drv) |> 
  dplyr::summarize(mean(hwy))
```

and we plot the data:

```{r, fig.align='center', echo=FALSE}
ggplot(mpg_2008, aes(x = drv, y = hwy)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter()
```

The boxplot allows us to see that the spread of the data in these groups is similar (the width of these boxes is about the same) so we can run an anova test on these data.

if we calculate the anova for this we find that the evidence support the hypothesis that there is in fact a difference in hwy between these groups:

```{r}
model <- aov(hwy ~ drv, data = mpg_2008)
summary(model)
```

As we mentioned, the ANOVA test does not tell us anything about each group in particular, for this we can run another test, one recommended is the **Tukey Honest Significant Difference** test and this will give us a p-value and a confidence interval for each pair of categories in our data.

```{r}
TukeyHSD(model)
```

The Tukey Honest Significant Difference test is better than running a Welch Two sample test by selecting just two variables in our dataset and testing one against the other. If we just run three different t-test comparing the three groups, we're potentially going to have an increased probability of a false positive, so the Tukey HSD test is specifically controlling for that multiple comparison problem.

Let's run a Welch Two sample test over one of the pairs in this dataset and see how it varies from the results from the TukeyHSD test:

```{r}
mpg_r4 <- filter(mpg_2008, drv != "f")
t.test(mpg_r4$hwy ~ mpg_r4$drv)
```

we can see that the p-value here is smaller than the one calculated for that same pair in the Tukey Test.

::: {.callout-orange appearance="simple" icon="false"}
ANOVA tests if there are statistically significant differences between the means of three or more groups. It essentially extends the t-test to multiple groups.
:::

Recap:

::: {#correlationkeypoints style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
1.  Sample correlation measures the strength and direction of the linear relationship between two variables, providing insights into their association.

2.  Hypothesis testing and significance of correlation allow you to determine whether the observed correlation is statistically significant, indicating a relationship beyond random chance.

3.  Linear regression enables you to model relationships between variables, predicting outcomes and understanding the impact of independent variables on the dependent variable.

4.  ANOVA is a statistical technique used to compare means between multiple groups, assessing whether there are significant differences among the groups.

5.  Independence testing of categorical variables examines whether there is a relationship between two categorical variables, determining if they are independent or associated.

6.  Known the assumptions: whether it's correlation, regression or ANOVA, always ensure that assumptions (like normality or homoscedasticity) are met before drawing any conclusions.
:::

# Classification Problems

Variables can be characterized as either quantitative or qualitative (also known as categorical). Quantitative variables take on numerical values. Examples include a person's age, height, or income or the value of a house. In contrast, qualitative variables take on values in one of K different classes, or categories. Examples of qualitative variables include a person's marital status (married or not), the brand of product class purchased (brand A, B, or C) etc. We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems.

Classification problems are about predicting discrete categories or labels for given inputs. Here our goal is to build up a classifier, that assigns a class label from our set `c` to a future, unlabeled observation `x` where `x` is the feature vector. We'd also like to assess the uncertainty in each classification and the roles of the different predictors amongst the x's in producing that classify. One example is using the words in an email to classify it as legit (ham) or spam.

To graphically visualize a very simple classification problem we have a range of X values and the Y can have only two categories, which we represent here as 0 or 1. The line in the graph will be the probability of 1 given a certain X:

```{r, fig.align='center', echo=FALSE}
# Generate sample data
set.seed(123)
X <- seq(0, 10, length.out = 30)
Y <- 2 + 5 * X + X^2 + rnorm(30, sd = 5)
data <- data.frame(X, Y)

# Fit models
linear_model <- lm(Y ~ X, data = data)
parametric_model <- lm(Y ~ poly(X, 5), data = data)  # Increased polynomial degree
overfitted_model <- lm(Y ~ poly(X, 20), data = data)

# Predict values
data <- data %>%
  mutate(
    linear_pred = predict(linear_model),
    polynomial_pred = predict(parametric_model),
    overfitted_pred = predict(overfitted_model)
  )

# Plot
ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  geom_line(aes(y = linear_pred), color = "blue") +
  geom_line(aes(y = polynomial_pred), color = "red") +
  geom_line(aes(y = overfitted_pred), color = "green") +
  labs(title = "Scatter Plot with Regression Lines", x = "X", y = "Y")

# Simulate random data
set.seed(123) # Setting seed for reproducibility
x <- runif(500, min = 1, max = 7) # Random uniform distribution between 1 and 7
y <- rbinom(500, size = 1, prob = plogis(x - mean(x))) # Binary outcome based on logistic probability

# Fit logistic regression model
model <- glm(y ~ x, family = binomial(link = "logit"))

# Create sequence for predictions
x_range <- seq(min(x), max(x), length.out = 500)

# Predict probabilities using the fitted model
probabilities <- predict(model, newdata = data.frame(x = x_range), type = "response")

# Create the plot
ggplot() +
    geom_point(aes(x = x[y == 1], y = rep(1, length(y[y == 1]))), colour = "blue", shape = 124) + 
    geom_point(aes(x = x[y == 0], y = rep(0, length(y[y == 0]))), colour = "orange", shape = 124) +
    geom_line(aes(x = x_range, y = probabilities), colour = "black") +
    labs(x = 'x', y = 'Probability') +
    theme_minimal()

```

In this simple example above we only have two k elements (0 and 1) but there may be many K elements, so we express the probability of k for a given value of x like this: 
$$
p_k(x)=Pr(Y=k|X=x),k=1,2,\dots,K
$${#eq-conditionalClassProbabilities}
These are the *conditional class probabilities* at x.

The *Bayes optimal classifier* is essentially the gold standard in classification. It represents the best possible classifier that can be achieved if we had perfect knowledge of the probability distribution of the data.

Imagine it like this: if you knew the exact probability of every possible classification given all possible features, you could make the optimal decision every time. This is what the Bayes optimal classifier does. It assigns each instance to the class with the highest posterior probability.

In formula terms, if $P(CxX)$ is the posterior probability of class C given the feature X, then the Bayes optimal decision rule assigns X to the class C $C(x)=j$ if $p_j(x)=max\{p_1(x),p_2(x),\dots,p_k(x)\}$ In our example, for X=5 we know that we have around 75% probability of a 1 and 25% probability of a 0, which means that we would classify that point as a 1

In practice, we often don't have perfect knowledge of these probabilities, so we use various methods to approximate the Bayes optimal classifier, like Naive Bayes, which assumes independence between features.

Typically we measure the performance of the classifier $\hat{C(x)}$ using the misclassification error rate, that is the number of mistakes we make and it's represented by the formula 
$$
Err_{Te}=Ave_{i\epsilon Te}I[y_i\ne\hat{C(x_i)]}
$${#eq-misclassificationErrorRate} 

