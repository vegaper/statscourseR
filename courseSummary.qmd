---
title: "Mastering Statistics: Fundamentals of Data Analysis."
format: 
  html: 
    toc: true
    toc-depth: 3
    toc-title: Contents
    number-sections: true
    number-depth: 3
    embed-resources: true
    fig-align: 'center'
    fig-cap-location: margin
    fig-width: 4
    fig-height: 4
    css: custom-style.css
    page-layout: full
    crossrefs-hover: true
    footnotes-hover: true
    citations-hover: true
    code-fold: true
    grid: 
      sidebar-width: 10px
      body-width: 1300px
      gutter-width: 0.5rem
    margin-left: 50px
    margin-right: 10px
    anchor-sections: true
  pdf: 
    toc: true
    number-sections: true
    colorlinks: true
editor: visual
html-math-method: mathjax
execute: 
  engine: knitr
  warning: false
  echo: true
knitr:
  opts_chunk:
    label: true
    number: true
    echo: true
---

```{r}
#| echo: false
library(dplyr)
library(tidyverse)
library(here)
library(readxl)
library(easystats)
library(infer)
library(kableExtra)
library(plotly)
library(ggplot2)
library(patchwork)
library(BSDA) 
library(MASS)
library(rafalib)
library(UsingR) #datasets
library(ISLR2) #datasets
library(scatterplot3d)
library(gridExtra)
library(caret) #confusionMatrix
library(pROC)
library(class)
library(boot) #crossvalidation
library(leaps) #best subset selection
library(glmnet) #ridge regression and lasso
library(survival) #survival 
library(survminer) #survival ggplots
library(splines) #splines 
theme_set(theme_minimal())
options(scipen= 999)
```

This document is a summary of different stats courses:

-   Mastering Statistics: Fundamentals of Data Analysis (Udemy)

-   Introduction to Statistics (Standford University [Coursera](https://www.coursera.org/learn/stanford-statistics))

-   Statistics and R (HarvardX PH525.1x [Edx](https://www.edx.org/learn/r-programming/harvard-university-statistics-and-r?index=product&queryID=4d888ad6a70a831a9f53424e6d16879f&position=1&results_level=first-level-results&term=HarvardX+PH525.1x&objectID=course-94796bd2-6c39-4189-96ac-fce68e613c57&campaign=Statistics+and+R&source=edX&product_category=course&placement_url=https%3A%2F%2Fwww.edx.org%2Fsearch))

    -   [e-book](https://leanpub.com/dataanalysisforthelifesciences)

-   Statistical Learning with R (Stanford Online STATSX0001 [Edx](https://www.edx.org/learn/statistics/stanford-university-statistical-learning))

    -   [additional resources](https://www.statlearning.com/resources-second-edition)

    -   [video Series in youtube](https://www.youtube.com/watch?v=LvySJGj-88U&list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e)

    -   [Companion book](https://r4ds.github.io/bookclub-islr/index.html)

# Exploratory Analysis (EDA)

## Mean

The mean (or arithmetic mean) is a measure of central tendency that represents the average value of a set of numbers. It is calculated by summing all the values in the dataset and then dividing by the number of values. $$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$ {#eq-mean} When we talk about the population mean we use the greek letter $\mu$ and when we talk about the sample mean we use our variable with a bar on top $\bar{x}$ .

## Variance

Variance measures the average squared deviations from the mean. To calculate it:

Find the mean of the data set. Subtract the mean from each data point and square the result. Average these squared differences.

Mathematically, for a population, the variance $\sigma^2$ is: $$
\sigma^2 = \frac{1}{N}\sum^N_{i=1}(x_i-\mu)^ 2
$$ {#eq-variance}

where (N) is the number of data points, $x_i$ are the data points, and $\mu$ is the mean.

## Standard deviation

Standard deviation is the square root of the variance. It provides a measure of spread in the same units as the data, making it more interpretable. It quantifies how much the values in a dataset deviate from the mean (average) of the dataset. A low standard deviation indicates that the values are close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.

Population standard deviation:

$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2}
$$ {#eq-standardDeviation}

where:

-   ($\sigma$) is the population standard deviation

-   \(N\) is the population size

-   ($X_i$) is the (i)-th observation in the population

-   ($\mu$) is the population mean

We can calculate the population standard deviation like this:

```{r}
heights=father.son$fheight
popsd <- function(x) sqrt(mean((x-mean(x))^2))
popsd(heights)
```

Note the `sd` function in R gives us a sample estimate of the σ as opposed to the population σ

```{r}
standardDeviation <- sd(heights)
standardDeviation
```

The sample standard deviation formula is very similar:

$$
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2}
$$ {#eq-sampleStandarDeviation}

### The empirical rule:

If the data follows the normal curve then

-   about 2/3 (68%) of the data fall within one standard deviation of the mean.

-   about 95% fall within 2 standard deviations of the mean

-   99.7% fall within 3 standard deviations of the mean

```{r, fig.align='center', echo=FALSE}
#| warning= FALSE
data <- data.frame (values = rnorm(1000, mean = 10, sd = 3))
# Calculate mean, median, and standard deviation
mean_val <- mean(data$values)
median_val <- median(data$values)

sd_val <- sd(data$values)
gg<- ggplot(data, aes(x = values))+
  geom_histogram()

# Add vertical lines for mean, median, and standard deviation
gg <- gg +
  geom_vline(xintercept = mean_val, color = "red", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = median_val, color = "green", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val + sd_val, color = "blue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - sd_val, color = "blue", linetype = "dashed", linewidth = 1)+
  geom_text(aes(x = mean_val, y = 10, label = "Mean"), color = "red", vjust = -0.5, angle=90) +
  geom_text(aes(x = median_val, y = 10, label = "Median"), color = "green", vjust = -0.5,hjust = -2, angle=90) +
  geom_text(aes(x = mean_val + sd_val, y = 10, label = "SD1"), color = "blue", vjust = -0.5, angle=90) +
  geom_text(aes(x = mean_val - sd_val, y = 10, label = "SD1"), color = "blue", vjust = -0.5, angle=90)
gg

```

Biases, systematic errors and unexpected variability are common in data. Failure to discover these problems often leads to flawed analyses and false discoveries. As an example, consider that experiments sometimes fail and not all data processing pipelines, such as the \`t.test\` function in R, are designed to detect these. Yet, these pipelines still give you an answer. Furthermore, it may be hard or impossible to notice an error was made just from the reported results.

Graphing data is a powerful approach to detecting these problems. We refer to this as *exploratory data analysis* (EDA). Many important methodological contributions to existing techniques in data analysis were initiated by discoveries made via EDA.

## Median, Median Absolute Deviation (MAD) and outliers

The **median** is a measure of central tendency that represents the middle value in a sorted list of numbers. If the list has an odd number of observations, the median is the middle number. If the list has an even number of observations, the median is the average of the two middle numbers. The median is less affected by outliers and skewed data compared to the mean.

The *Median Absolute Deviation (MAD)* is a robust measure of statistical dispersion. It is defined as the median of the absolute deviations from the data's median. MAD is less sensitive to outliers compared to the standard deviation. $$
MAD= median(|X_i-median(X)|)
$$ {#eq-mad} For example, I create a vector with 100 values following a normal distribution with average 0 and standard deviation 1, but then I change one of the points to a value of 100.

```{r boxplot_showing_outlier, fig.cap="Normally distributed data with one point that is very large due to a mistake.", fig.align='center',echo=FALSE}
set.seed(1)
x=c(rnorm(100,0,1)) ##real distribution
x[23] <- 100 ##mistake made in 23th measurement
boxplot(x)
```

now we are going to see how this datapoint is affecting our statistics:

```{r,echo=FALSE}
cat("The average is",mean(x))
cat("The SD is",sd(x))
cat("The median is ", median(x))
cat("The MAD is ", mad(x))
```

## Spearman correlation {#spearman-correlation}

The correlation (it will be presented later in [another chapter](#confidenceIntvsSignifTest)) is also very sensitive to outliers. In this example we create two datasets with no correlation whatsoever:

```{r, fig.align='center',echo=FALSE}
set.seed(1)
x=rnorm(100,0,1) 
y=rnorm(100,0,1) 

plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),
     pch=21,bg=1)
abline(0,1)
```

now we introduce as we did before one datapoint of a much higher value for x and y. This is affecting our correlation coefficient:

```{r, scatter_plot_showing_outlier,fig.cap="Scatterplot showing bivariate normal data with one signal outlier resulting in large values in both dimensions.", fig.align='center',echo=FALSE}
set.seed(1)
x[23] <- 100 ##mistake made in 23th measurement
y[23] <- 84 ##similar mistake made in 23th measurement

plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),
     pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
abline(0,1)
```

The Spearman correlation follows the general idea of median and MAD, that of using quantiles. The idea is simple: we convert each dataset to ranks and then compute correlation:

```{r spearman_corr_illustration, fig.cap="Scatterplot of original data (left) and ranks (right). Spearman correlation reduces the influence of outliers by considering the ranks instead of original data.",fig.width=10.5,fig.height=5.25, fig.align='center',echo=FALSE}
mypar(1,2)
plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
plot(rank(x),rank(y), main=paste0("correlation=",round(cor(x,y,method="spearman"),3)),
     pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
abline(0,1)
```

In general, if we know there are outliers, then median and MAD are recommended over the mean and standard deviation counterparts.

## Empirical Cumulative Density Function (CDF)

What exactly is a distribution? The simplest way to think of a *distribution* is as a compact description of many numbers.

Although not as popular as the histogram for EDA, the empirical cumulative density function (CDF) (or cumulative distribution function) shows us the same information and does not require us to define bins. For any number a the empirical CDF reports the proportion of numbers in our list smaller or equal to a.

$$
F(a) \equiv Pr(x \leq a)
$$ {#eq-cdf}

This is called the cumulative distribution function (CDF). When the CDF is derived from data, as opposed to theoretically, we also call it the empirical CDF (ECDF).

R provides a function that has as out the empirical CDF function.

```{r}
heights=father.son$fheight 
round(sample(heights,20),1) 
myCDF <- ecdf(heights) 
```

The `ecdf` function is a function that returns a function, which is not typical behavior of R functions. We create a function called myCDF based on our data *heights* that can then be used to generate a plot:

```{r ecdffunction, fig.align='center',echo=FALSE}
##We will evaluate the function at these values:
xs<-seq(floor(min(heights)),ceiling(max(heights)),0.1) 
### and then plot them:
plot(xs,myCDF(xs),type="l",xlab="x=Height",ylab="F(x)")
```

## Boxplot

When the data is not normally distributed the mean and the standard deviation are not enough to summarise the data. A better approach would be to present the data in a boxplot.

```{r, fig.align='center',echo=FALSE}
# Create a not normally distributed sample (exponential distribution)
set.seed(123)  # For reproducibility
data <- rexp(100, rate = 0.2)

# Create a boxplot with ggplot2
p <- ggplot(data.frame(value = data), aes(x = "", y = value)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 3, color = "blue") +
  labs(title = "Boxplot of Exponentially Distributed Data",
       y = "Values") +
  theme_minimal()

# Add labels for the important parts of the boxplot
p + annotate("text", x = 1.2, y = quantile(data, 0.75)+1.5, label = "Q3 (75th percentile)", hjust = 0) +
  annotate("text", x = 1.2, y = quantile(data, 0.25)-1.5, label = "Q1 (25th percentile)", hjust = 0) +
  annotate("text", x = 1.2, y = median(data)+1, label = "Median", hjust = 0) +
  annotate("text", x = 1.2, y = mean(data)+0.5, label = "Mean", hjust = 4.5, color = "blue") +
  annotate("text", x = 1.2, y = max(data[data <= quantile(data, 0.75) + 1.5 * IQR(data)]), label = "Max (whisker)", hjust = 2) +
  annotate("text", x = 1.2, y = min(data[data >= quantile(data, 0.25) - 1.5 * IQR(data)]), label = "Min (whisker)", hjust = 2)


```

-   Q1 (25th percentile): The lower quartile.

-   Median: The middle value of the data.

-   Mean: The average value of the data (marked in blue).

-   Q3 (75th percentile): The upper quartile.

-   Box (between the 25th percentile and the 75th percentile) is showing the middle half of the data.

-   Max (whisker): The maximum value within 1.5 times the IQR above Q3.

-   Min (whisker): The minimum value within 1.5 times the IQR below Q1.

-   Outliers: Points beyond the whiskers, marked in red.

## Histogram {#histogram}

We can think of any given dataset as a list of numbers. Suppose you have measured the heights of all men in a population. Imagine you need to describe these numbers to someone that has no idea what these heights are, for example an alien that has never visited earth. One approach is to simply list out all the numbers for the alien to see. Here are 20 randomly selected heights of 1,078

From scanning through these numbers we start getting a rough idea of what the entire list looks like but it is certainly inefficient. We can quickly improve on this approach by creating bins, say by rounding each value to the nearest inch, and reporting the number of individuals in each bin. A plot of these heights is called a histogram

We can specify the bins and add better labels in the following way:

```{r histogramofnormaldist,fig.align='center',echo=FALSE}
bins <- seq(floor(min(heights)),ceiling(max(heights)))  
hist(heights,breaks=bins,xlab="Height",main="Adult men heights") 
```

Showing this plot is much more informative than showing the numbers. Note that with this simple plot we can approximate the number of individuals in any given interval. For example, there are about 70 individuals over six feet (72 inches) tall.

This denotes the probability that the random variable ( x ) is between ( a ) and ( b )

$$P(a \leq x \leq b) = F(b) - F(a)$$ {#eq-probabilityHistogram}

## Probability Distribution

Summarizing lists of numbers is one powerful use of distribution. An even more important use is describing the possible outcomes of a random variable. Unlike a fixed list of numbers, we don't actually observe all possible outcomes of random variables, so instead of describing proportions, we describe probabilities. For instance, if we pick a random height from our list, then the probability of it falling between $a$ and $b$ is denoted with: $P(a \leq X \leq b) = F(b) - F(a)$

Note that the $X$ is now capitalized to distinguish it as a random variable and that the equation above defines the probability distribution of the random variable.

### The $p$-value

Knowing this distribution is incredibly useful in science. If we know the distribution of our data when the null hypothesis is true, referred to as the *null distribution*, we can compute the probability of observing a value as large as we did in our experiment, referred to as a $p$-value.

::: exercise-block
Example.

We have data from 24 mice fed two different diets. We want to know if the mice fed with high fat diet increased their body weight more than the mice fed with the control diet.

```{r}
femaleMiceWeights <- read.csv("data/femaleMiceWeights.csv")

head(femaleMiceWeights)
control<- femaleMiceWeights %>% filter (Diet=='chow')
treatment <-  femaleMiceWeights %>% filter (Diet=='hf')
tm<- mean(treatment$Bodyweight)
cm<- mean(control$Bodyweight)
obsdiff <- tm - cm
print(paste("treatment mean:", tm))
print(paste("control mean:", cm))
print(paste("difference =", obsdiff))

```

We can see that there is a difference between the two means but if we choose random samples of 12 individuals from the mice population, each of the samples would have different means just by chance. So how can we be sure if that difference is due to the change in diet or to the random sampling?

We are going to recreate that by choosing 24 mice and assigning them randomly to either treatment or control. We will repeat this 10000 times:

```{r}
n <- 10000
null <- vector("numeric",n)
for (i in 1:n) {
  control <- sample(femaleMiceWeights$Bodyweight,12)
  treatment <- sample(femaleMiceWeights$Bodyweight,12)
  null[i] <- mean(treatment) - mean(control)
}
head(null)
```

The values in `null` form what we call the *null distribution*.

What percentage of our 10000 experiments have a difference weight between control and treatment higher or equal to the one we observed for the different diets?

```{r}
mean(null >= obsdiff)
```

We see a difference as big as the one we observed only a small percentage of the time. This is what is known as a $p$-value. We can plot the null histogram and mark the line with our observed weight difference for the high fat diet.

```{r null_and_obs,fig.cap="Null distribution with observed difference marked with vertical red line.", fig.align='center', echo=FALSE}
hist(null, freq=TRUE)
abline(v=obsdiff, col="red", lwd=2)
```
:::

An important point to keep in mind here is that while we defined $\mbox{Pr}(a)$ by counting cases, we will learn that, in some circumstances, mathematics gives us formulas for $\mbox{Pr}(a)$ that save us the trouble of computing them as we did here. One example of this powerful approach uses the normal distribution approximation.

We will see later on [how to calculate the $p$-values](#pvalues)

# Normal distribution

A **normal distribution**, also known as a Gaussian distribution, is a continuous probability distribution that is symmetrical around its mean. This means that most of the observations cluster around the central peak, and the probabilities for values further away from the mean taper off equally in both directions. When plotted on a graph, it forms a bell-shaped curve, often referred to as a "bell curve".

Key properties of a normal distribution include

-   **Symmetry**: The left and right sides of the distribution are mirror images.

-   **Mean, Median, and Mode**: All three measures of central tendency are equal and located at the center of the distribution.

```{r, fig.align='center',echo=FALSE}
data <- data.frame (values = rnorm(1000, mean = 10, sd = 3))

gg<- ggplot(data, aes(x = values))+
  geom_histogram()
gg
```

probability density function (PDF) of a normal distribution

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$ {#eq-PDF}

we can use a mathematical formula to approximate the proportion of values or outcomes in any given interval:

$$
\mbox{Pr}(a < x < b) = \int_a^b \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left( \frac{-(x-\mu)^2}{2 \sigma^2} \right)} \, dx
$$ {#eq-pnorm}

While the formula may look intimidating, don't worry, you will never actually have to type it out, as it is stored in a more convenient form (as `pnorm` in R which sets *a* to $-\infty$, and takes *b* as an argument). We can compute the proportion of values below a value `x` with `pnorm(x,mu,sigma)`. A very useful characteristic of this approximation is that one only needs to know $\mu$ and $\sigma$ to describe the entire distribution. From this, we can compute the proportion of values in any interval.

## Standardizing data

A normal curve is determined by the mean $\bar{x}$ and the standard deviation $s$ . If the data follow the normal curve, then knowing those two values mean we know the whole histogram. To compute areas under the normal curve, we first standardize the data by subtracting $\bar{x}$ and dividing by $s$. The resulting value is called the standardized value or z-score $z$

$$
z = \frac{x_1 - \bar{x}}{s}
$$ {#eq-zscore}

After standardizing all points in our data we will get a distribution with mean = 0 and standard deviation =1

```{r,fig.align='center',echo=FALSE}
standardizedValues <- data.frame(values = scale(data$values))
mean_val <- mean(standardizedValues$values)
median_val <- median(standardizedValues$values)
sd_val <- sd(standardizedValues$values)

# Add vertical lines for mean, median, and standard deviation
gg <- ggplot(standardizedValues, aes(x = values))+
  geom_histogram()+
  geom_vline(xintercept = mean_val, color = "darkred", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = median_val, color = "green", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val + sd_val, color = "lightblue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - sd_val, color = "lightblue", linetype = "dashed", linewidth = 1)+
  geom_vline(xintercept = mean_val + 2*sd_val, color = "lightblue", linetype = "dashed", linewidth = 1) +
  geom_vline(xintercept = mean_val - 2*sd_val, color = "lightblue", linetype = "dashed", linewidth = 1)+
  geom_text(aes(x = mean_val, y = 10, label = "Mean"), color = "darkred", vjust = -0.5, angle=90) +
  geom_text(aes(x = median_val, y = 10, label = "Median"), color = "green", vjust = -0.5,hjust = -2, angle=90) +
  geom_text(aes(x = mean_val + sd_val, y = 10, label = "SD1"), color = "lightblue", vjust = -0.5, angle=90)  +
  geom_text(aes(x = mean_val - sd_val, y = 10, label = "SD1"), color = "lightblue", vjust = -0.5, angle=90)  +
  geom_text(aes(x = mean_val + 2*sd_val, y = 10, label = "SD2"), color = "lightblue", vjust = -0.5, angle=90) +
  geom_text(aes(x = mean_val - 2*sd_val, y = 10, label = "SD2"), color = "lightblue", vjust = -0.5, angle=90)
gg

```

So now, the value of z for a specific value $x_1$ is indicating how many standard deviations our value is away from the mean, and just by simple approximation using the empirical rule, if for example you standardize your height and you get a value of 1, you already know without having to look into more detail that you are approximately higher than 84% of the population.

```{r empiricalRule, fig.height=4,fig.width=6, fig.align='center',echo=FALSE}

# Plot the cumulative percentages
# Define the range for the x-axis
x <- seq(-4, 4, length = 1000)

# Calculate the density of the normal distribution
y <- dnorm(x)

# Define the z-values
z_values <- c(-3, -2, -1, 0, 1, 2, 3)

# Calculate the cumulative percentages
cumulative_percentages <- pnorm(z_values) * 100

# Plot the normal distribution curve
plot(x, y, type = "l", lwd = 2, col = "lightblue", ylab = "Density", xlab = "Z-score",
     main = "Standard Normal Distribution with Cumulative Percentages", ylim = c(0, 0.7))

# Add vertical lines at the z-values
abline(v = z_values, col = "darkred", lty = 2)

# Annotate the cumulative percentages
text(z_values, dnorm(z_values), labels = paste0(round(cumulative_percentages, 2), "%"), pos = 3, col = "darkred")
grid()

```

## Normal approximation

When the histogram of a list of numbers approximates the normal distribution we can use a convenient mathematical formula to approximate the proportion of individuals in any given interval. We already saw this equation (@eq-pnorm)

$$
P(a < x < b) = \int_{a}^{b} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(\frac{-(x - \mu)^2}{2\sigma^2}\right) \, dx
$$ {#eq-pnorm1}

If this approximation holds for our data then the population mean and variance of our data can be used in the formula above.

::: exercise-box
Example

*Let's calculate the number of individuals that are taller than 72 inches.* First we are going to calculate the number manually:

```{r}
n_individuals = length(heights)
taller = length(which(heights>72))
propTaller = taller/n_individuals
propTaller
```

so we have roughly 6.4% of the individuals taller than 72 inches. Now wee can calculate it using normal approximation and it will gives us an similar number:

```{r}
1-pnorm(72,mean(heights),rafalib::popsd(heights)) 
```
:::

if we do this check for different values and continue to see that the two values are similar we can assume a normal distribution, but a better way to do it is using a QQ plot:

## QQ plot

To corroborate that the normal distribution is in fact a good approximation we can use **quantile-quantile plots (QQ-plots)**. Quantiles are best understood by considering the special case of percentiles. The p-th percentile of a list of a distribution is defined as the number q that is bigger than p% of numbers. For example, the median 50-th percentile is the median. We can compute the percentiles for our list and for the normal distribution.

```{r, fig.align='center'}
#generate the percentiles
ps <- seq(0.01,0.99,0.01)
# The quantile function returns the values below which a given percentage of data falls. 
qs <- quantile(heights,ps)
#calculates the theoretical quantiles from a normal distribution with the same mean and standard deviation as the heights data. The qnorm function returns the quantiles of the normal distribution for the given probabilities (ps), mean, and standard deviation
normalqs <- qnorm(ps,mean(heights),rafalib::popsd(heights))
plot(normalqs,qs,xlab="Normal percentiles",ylab="Height percentiles")
abline(0,1) ##identity line
```

This line adds an identity line (a line with slope 1 and intercept 0) to the plot. The abline(0, 1) function draws a 45-degree line through the origin. This line helps to visually assess how closely the data follows a normal distribution. If the points lie close to this line, it suggests that the data is approximately normally distributed. This code is creating a Normal Quantile-Quantile (QQ) Plot. A QQ plot is used to compare the distribution of a dataset to a theoretical distribution---in this case, the normal distribution. If the heights data is normally distributed, the points in the plot will lie approximately along the identity line.

We can generate the same plot with a simplified code:

```{r, fig.align='center'}
qqnorm(heights)
qqline(heights) 
```

Data is not always normally distributed. Income is widely cited example. In these cases the average and standard deviation are not necessarily informative since one can't infer the distribution from just these two numbers. The properties described above are specific to the normal. For example, the normal distribution does not seem to be a good approximation for the direct compensation for 199 United States CEOs in the year 2000

```{r, fig.align='center', echo=FALSE}
hist(exec.pay)
```

the QQ plot would look like this:

```{r, fig.align='center', echo=FALSE}
qqnorm(exec.pay)
qqline(exec.pay)
```

## Use of standardadization in binomial probability

We know that the probability in any giving birth to have a girl is 0.49%. What is the probability of having 2 girls out of 3 births?\
In binomial probability we call the outcome we are interested in a 'success' and the other outcome 'failure' in our example, having a girl is success, having a boy is failure. When the number of repetitions in our experiment is small, we can just draw the possible outcomes.

Given that each birth is independent from the others, we have these possibilities for getting the outcome we are interested:

$GGB | GBG | BGG (G=Girl, B=Boy)$

The probability would be $P= P(G)P(G)P(B)+P(G)P(B)P(G)+P(B)P(G)P(G)$

That is the same as $P= 3(PG)P(G)P(B) = 3*0.49*0.49*0.51$

```{r}
pG = 0.49
pB =1-pG
p = 3*pG*pG*pB
p
```

But when the number of experiments grows it gets complicated to just find out all the combinations by hand, so we can use the binomial coefficient to know the number of ways one can arrange k success in n experiments:

$$
\binom{n}{k} = \frac{n!}{k!(n-k)!}
$$ {#eq-binomialCoefficient}

and to calculate the probability we use the coefficient with the \*binomial probability formula. It's used to find the probability of getting exactly k successes in n independent Bernoulli trials, where each trial has a success probability p.

$$
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
$$ {#eq-binomialProbabilityFormula}

if we calculate this manually:

```{r}
# Parameters
n <- 3
k <- 2
p <- 0.49

# Calculate binomial coefficient
binom_coeff <- factorial(n) / (factorial(k) * factorial(n - k))

# Calculate probability
probability <- binom_coeff * (p^k) * ((1 - p)^(n - k))

# Print the result
print(probability)

```

but in r we would do it like this

```{r #binomialprobability}

# Calculate binomial probability
probability <- dbinom(k, n, p)
print(probability)

```

If we have more than two outcomes but we are interested in one of them only we can still use the binomial formula.

**Example** *We are playing an online game, the probability of winning a big prize is 10%, the probability of winning a small prize is 20% and the probability of not winning anything is 70%. We want to know, in 10 repetitions, what is the probability that we win two small prizes:*

-   success = win small prize

-   failure = anything else.

```{r}
# Parameters
n <- 10
k <- 2
p <- 0.20

# Calculate binomial coefficient
binom_coeff <- factorial(n) / (factorial(k) * factorial(n - k))

# Calculate probability
probability <- binom_coeff * (p^k) * ((1 - p)^(n - k))

# Print the result
print(probability)
```

*but now, what is the probability of winning **at most**, 12 small prizes in 50 repetitions?* we would have to calculate the probability of winning 12 small prizes, 11 small prizes etc. but we can use standardization instead to achieve the same goal:

To standardize data for a binomial experiment, you typically use the **z-score** formula. This formula converts the binomial distribution to a standard normal distribution. The z-score (@eq-zscore) formula is: $$
z=\frac{X-\mu}{\sigma}
$$ {#eq-zscore2}

-   ( $X$ ) is the number of successes.

-   ( $\mu$ ) is the mean of the binomial distribution, calculated as ( $\mu = np$ ). where $n$ is the number of repetitions and $p$ is the the probability of success.

-   ( $\sigma$ ) is the **standard deviation of the binomial distribution**, calculated as $$\sigma = \sqrt{np(1-p)}$$ {#eq-standardDeviationBinomialDistribution}

So, the complete formula for the z-score in a binomial experiment is:

$$
z = \frac{X - np}{\sqrt{np(1-p)}}
$$ {#eq-zscoreBinomial}

This formula allows you to standardize the number of successes ( X ) by subtracting the mean and dividing by the standard deviation, converting it to a z-score.

```{r}
# Parameters
n <- 50
k <- 12
p <- 0.20

sigma <- sqrt(n*p*(1-p))
mu <- n*p
zScore <- (k-mu)/sigma
print(zScore)

```

## Calculating the $p$-values {#pvalues}

once we have calculated the z-Score we can calculate the area on the left of that number under the normal distribution curve, and that will be the probability we are looking for.

```{r, fig.align='center', echo=FALSE}

# Create a sequence of x values
x <- seq(-4, 4, length=100)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue", xlab="Z", ylab="Density", main="Standard Normal Distribution")
# Add a vertical line at x = 0.71
abline(v = 0.71, col = "red", lwd = 2, lty = 2)
```

We can now can find manually the probability using z-score tables. In R, we may use the `pnorm()` function to find the $p$-value associated with a z-score, which has the following syntax.

`pnorm(q, mean = 0, sd = 1, lower.tail = TRUE)`

Where - `q = z-Score value`. - lower.tail: If TRUE, the probability in the normal distribution to the left of z-Score is returned. The probability to the right is returned if FALSE. TRUE is the default value.

To find the $p$-value for the two-tailed test:

`2*pnorm(q=0.71, lower.tail=FALSE)`

```{r}
# Calculate the area to the left of the z-score
(area_left <- pnorm(zScore))

# Calculate the area to the right of the z-score
(area_right <- 1 - area_left)
#or
(area_rigt<- pnorm(zScore,lower.tail=FALSE))

# Calculate the cumulative probability using the z-score, in our case we use the left area:
probability <- pnorm(zScore)
print(probability)
```

Instead of plugging in the values and calculate the z-score first, we can simplify our operations in r like this:

```{r}
# Parameters
n <- 50
k <- 12
p <- 0.20
# Calculate the cumulative probability using an expression
probability <- pnorm((k - n * p) / sqrt(n * p * (1 - p)))
print(probability)
```

# Introduction to Inference

```{r, echo=FALSE}
#| echo: false
file <- here::here("data", "optical_full.xlsx")
optical <- read_excel(file)
```

Drawing conclusions from data is difficult because we almost never have complete information about any variable of interest. For instance, we may have only been able to send a satisfaction survey to a small subset of our customers.

-   The **population** of a study consists of all possible observations of interest.

-   A **sample** in a study consists of all the observations we actually have.

We are almost always interested in a population, but only have information about a sample. Statistical inference is the process of reasoning from the one to the other.

-   A **parameter** is a number that describes a population

-   A **statistic** is a number that describes a sample

The mean of the eye difference variable in our optical dataset is `r mean(optical$eye_difference)`. The **sample mean** is found by adding together all the values of a variable in the sample data, and dividing this total by $n$, the sample size.

If we extract 100 samples with three observations each from our data and visually show their means we can see every time we got a different value, sometimes they will be very close to the target parameter, but sometimes they can be quite far.

```{r, fig.align='center', echo=FALSE}
#| eval: true

kable(head(optical))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()
as.data.frame(report(optical$eye_difference))

sample <- slice_sample(optical, n = 3)
mean(sample$eye_difference)

samples <- rep_slice_sample(optical, n = 3, reps = 100)

sample_summary <- samples |> 
  group_by(replicate) |> 
  dplyr::summarize(sample_mean = mean(eye_difference))

ggplot(sample_summary, aes(x = sample_mean)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical$eye_difference),
             linetype = "dotted")

```

**Variability** is the natural tendency for a statistic to be different across different samples.

A **Biased statistics**, on the other hand, misses the mark on average, for example if we increase the value of all our sample means by 1:

```{r, fig.align='center', echo=FALSE}
ggplot(sample_summary, aes(x = sample_mean+1)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical$eye_difference),
             linetype = "dotted")
```

::: {.callout-orange appearance="simple" icon="false"}
**Reproducibility and Replicability**

Replicating means to get the same conclusion with slightly different samples, procedures, and data analysis methods.

Related to that is the problem of reproducibility. That means to get the same results then you simply use the same data and same methods that were claimed in the analysis.
:::

::: {.callout-orange appearance="simple" icon="false"}
::: centered-text
-   Shortcomings of statistical analysis \*
:::

Statistical inference can handle variability, but not bias. Bias comes from incorrect data collection and only good data collection and management practices can prevent bias.

Statistical inference can lead to wildly wrong conclusions when misused or abused for example:

-   Sample bias: not choosing the sample randomly.

-   Undercoverage: your procedure miss out keep portions of the population.

-   Overgeneralization: you need to limit the conclusions that you draw to the population that you have sampled from.

Significance test are particulary easy to abuse:

-   If you run a test just because you've spotted a pattern in your data, you're almost certain to get a positive result even if the trend is just due to random chance. Patterns exist just by chance.

-   p-hacking: repeated testing of the same hypothesis will eventually yield a positive result just by random chance.

-   Confusing significance for importance. Just because a difference is significant does not mean it is important.

No amount of statistical savvy can fully remediate bad data. All of the techniques we'll discuss in this course are designed to address variability in data, not bias. Before attempting any sort of analysis, you should always take some time to think critically about the overall quality of the data. A few questions to consider:

-   Is the sample representative of the population of interest, or might some subgroups be under or over represented?

-   How was the data collected? Are any of the variables self-reported? Were there points in the process where those collecting the data made subjective judgments?

-   How will conclusions from the data analysis be used? How might they be misinterpreted or abused?
:::

# Expected Value and Standard Error:

If we are interested in the average height of adult males in USA and we extract an adult male at random, we expect his height to be around the population average, give or take about one standard deviation $\sigma$ . **The expected value is the population average** $\mu$ . The expected value of the sample average is still the population average, but remember that the population mean is a random variable because sampling is a random process, so its mean won't be exactly the population mean. How far off can we expect this sample mean to the population mean?

**The standard error (SE)** of a statistic tell you roughly how far off the sample mean will be from its expected value (population mean). It indicates how much the sample mean is expected to vary from the true population mean.

$$ 
\text{SE} (\bar{x_n})= \frac{\sigma}{\sqrt{n}} 
$$ {#eq-standardErrorMean}

where n is the sample size. **We can actually use this formula to decide what is the sample size we need to use for a desired accuracy in our statistic**.

It plays a similar role as the *standard deviation* for one observation from the population mean, remember that the Standard deviation measures the dispersion of individual data points around the mean of a dataset. It tells you how spread out the values in a dataset are. In summary, the standard error of an estimate is the standard deviation of the sampling distribution of an estimate.

::: exercise-box
**Exercise:**

*A town has 10,000 registered voters, of whom 6,000 are voting for the Democratic party. A survey organization is taking a sample of 100 registered voters (assume sampling with replacement). The percentage of Democratic voters in the sample will be around \_\_\_\_\_, give or take \_\_\_\_. (You may use the fact that the standard deviation is about 0.5)*

The percentage of Democratic voters in the sample is equal to the mean of the sample, and so the "around" value is the **expected value of the sample mean**, which in turn is equal to the population mean, 0.60 or 60%. Similarly, the "give or take" value is the *standard error* (SE) of the mean, which is equal to $\frac{\sigma}{\sqrt{n}}$ , where σ is the standard deviation of the population and n is the size of the sample. We're told that σ is about 0.5 and n is 100, so the standard error of the mean is 0.05 or 5%.
:::

### Expected value and standard error for the sum

If we are interested in the sum of n draws $S_n$, the sum and the average are related by $S_n = n\bar{x_n}$ so the standard error of the sum: $$SE(S_n)=\sqrt{n}\sigma$$ {#eq-standardErrorSum} which tells us that the variability of the sum of n draws increases at the rate of $\sqrt{n}$ so **while increasing the sample size reduces the standard error of the average, it actually increases the standard error for the sum.**

::: exercise-box
Exercise1 (Expected Value):

*You solicit 100 pledges for a charitable organization. Each pledge is equally likely to be \$10, \$50, or \$100. You may use the fact that the standard deviation of the three amounts \$10, \$50 and \$100 is \$37.*

*What is the expected value of the sum of the 100 pledges?*

The expected value of the sum of a sample is nμ, where n is the size of the sample and μ is the mean of the population. Here we're told that n is 100 and μ must be the average of \$10, \$50, and \$100, which is \$53.33, so the expected value of the sample sum is \$5333.
:::

::: exercise-box
Exercise2 (Standard error for the sum)

*You solicit 100 pledges for a charitable organization. Each pledge is equally likely to be \$10, \$50, or \$100. You may use the fact that the standard deviation of the three amounts \$10, \$50 and \$100 is \$37.*

*What are the chances that the 100 pledges total more than \$5,700 ?*

The standard error of the sample sum is $SE(S_n)=\sqrt{n}\sigma$ where n is the sample size and σ is the standard deviation of the population.

Since n is 100 and we're told that the standard deviation of the population is \$37, we know the standard error of the sample sum is \$370.

Using the normal approximation to the sampling distribution, this tells us that \$5700 is roughly one standard deviation (5700 - 5333 = 366) above the mean of the sampling distribution. From the **empirical rule**, we know that one half of 68% of the data lies within one standard deviation to the right of the mean, so that 50 + (1/2)68 = 84% of the data lies to the left of \$5700. That is, 16% percent of the data lies to right of \$5700.
:::

### Expected value and standard error for percentages.

For percentages, the expected value is $$E = \mu * 100\%$$ {#eq-expectedValuePerc} and the Standard Error $$SE=\frac{\sigma}{\sqrt{n}} *100\%$$ {#eq-standardErrorPerc}

All the above formulas are for sampling with replacement, but they are still approximately true when sampling without replacement if the sample size is much smaller than the size of the population.

------------------------------------------------------------------------

::: exercise-box
Exercise:

*We toss a coin 100 times. How many tails do you expect to see?*

we assing tails a value of 1, and heads a value of 0.

p(1)=0.5, p(0)=0.5

Number of expected tails = $E(sum) = 100 * \mu$

and $\mu = 0*p(0)+1*p(1)$ , so $\mu =0.5$ and $E= 100*0.5$ =50 tails.

The Standard Error will be $SE(sum) = \sqrt{100}*\sigma$

to calculate sigma, we know that $\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_i - \mu)^2}$ where:

-   ($\sigma$) is the standard deviation,

-   ($N$) is the total number of observations,

-   ($X_i$) represents each individual observation,

-   ($\mu$) is the mean of the observations.

In our exercise, without having to toss the coing 100 times we calculate it like this:

$\sigma =\sqrt{(x_1-\mu)^{2} * p(1) +(x_2-\mu)^{2}*p(2)}$

$\sigma =\sqrt{(0-0.5)^{2} * 0.5 +(1-0.5)^{2}*0.5}=0.5$

$\sigma^2 ={(0-0.5)^{2} * 0.5 +(1-0.5)^{2}*0.5}=0.25$

$SE(sum) = \sqrt{100}\sigma^2=10*0.25=5$

so our answer is that we expect to see 50 tails, give or take 5 tails.

and in percentages we would expect to see E= 0.5\*100 =50% tails with an standard error of SE =0.5/10\*100 = 5%

in r code, an example of the same problem with n=200 tosses. See the difference in the percentages.

```{r}
# Define the probabilities 
p <- c(0.5, 0.5)  
# Define the values (0 for heads, 1 for tails) 
x <- c(0, 1)  
# Calculate the mean (μ) mu <- sum(x * p)  
# Calculate the variance (σ^2) 
variance <- sum((x - mu)^2 * p)  
# Calculate the standard deviation (σ) 
sigma <- sqrt(variance)  
# Number of coin tosses n <- 200  
# Calculate the standard error (SE) 
SE <- sqrt(n) * sigma  
 
cat("Expected percentage of tails:", n * mu, "\n") 
cat("Standard error:", SE, "\n") 
cat("Expected number of tails:", mu*100, " % \n") 
cat("Expected standard error percentage:", sigma /sqrt(n), "% \n")
```
:::

------------------------------------------------------------------------

## The sampling distribution

If we toss a fair coin 100 times, we can get any number of tails from 0 to 100. How likely is each outcome?

The number of tails has the binomial distribution with `n=100` and `p=0.5` where `success = tails`. So if the statistic of interest is $S_n=number\ of \ tails$ , then $S_n$ is a random variable whose probability histogram is given by the binomial distribution. This is called the *sampling distribution of the statistic* $S_n$. The sampling distribution provides more detailed information about the chance properties of $S_n$ than the summary numbers given by the expected value and the standard error alone.

There are three histograms to take into consideration:

### Probability histogram

The probability histogram for producing the data. Since both tails and heads have the same probability, our histogram would be two columns with the same height.

```{r, fig.align='center'}
#| echo: false
heads = rep(0,50)
tails = rep(1,50)
theoreticalP = c(heads,tails)
theoreticalP
# Convert the vector to a data frame
df <- data.frame(theoreticalP)


# Create the histogram
gg <- ggplot(df, aes(x = theoreticalP)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  labs(title = "Histogram of Theoretical Probabilities", x = "Value", y = "Frequency")

# Print the plot
print(gg)
```

### Empirical histogram

If we toss the coins, we will have am empirical histogram

```{r, fig.align='center', echo=FALSE}
# Set the number of coin tosses
num_tosses <- 100

# Simulate the coin tosses
coin_tosses <- sample(c(0, 1), size = num_tosses, replace = TRUE, prob = c(0.5, 0.5))

# Create a dataframe
df <- data.frame(toss = 1:num_tosses, result = coin_tosses)

# Create the histogram
gg <- ggplot(df, aes(x = result)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  labs(title = "empirical histogram", x = "Value", y = "Frequency")

# Print the plot
print(gg)
```

### Probability histogram of the sample

And finally we can do the probability histogram of the statistic $s_{100}$, which shows the sampling distribution:

```{r, fig.align='center' , echo=FALSE}
# Set the parameters
n <- 100
p <- 0.5

# Calculate the probabilities for each number of tails (0 to 100)
tails <- 0:n
probabilities <- dbinom(tails, size = n, prob = p)

# Create a dataframe
df <- data.frame(tails = tails, probability = probabilities)

# Create the histogram
gg <- ggplot(df, aes(x = tails, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability Distribution of Number of Tails in 100 Coin Tosses",
       x = "Number of Tails",
       y = "Probability") +
  theme_minimal()

# Print the plot
print(gg)

```

# Central limit theorem (CLT)

When sampling with replacement and n is large, then the sampling distribution of the sample average approximately follows the normal curve.

For example, going back to the example of of the online gambling where we had a probability of winning a small prize with p=0.2, we can see how the probability histogram for the binomial distribution approximates more and more to the normal distribution as we increase the number of trials n.

```{r, fig.align='center', echo=FALSE}
# Set the parameters
n <- 1
p <- 0.2

# Calculate the probabilities for each number of tails (0 to 100)
success <- 0:n
probabilities <- dbinom(success, size = n, prob = p)

# Create a dataframe
df <- data.frame(success = success, probability = probabilities)

# Create the histogram
gg <- ggplot(df, aes(x = success, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability histogram for the binomial n=1",
       x = "Number of successes with n=2",
       y = "Probability") +
  theme_minimal()

# Print the plot
print(gg)

n <- 10
p <- 0.2

# Calculate the probabilities for each number of tails (0 to 100)
success <- 0:n
probabilities <- dbinom(success, size = n, prob = p)

# Create a dataframe
df <- data.frame(success = success, probability = probabilities)

gg <- ggplot(df, aes(x = success, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability histogram for the binomial n=10",
       x = "Number of successes with n=10",
       y = "Probability") +
  theme_minimal()

print(gg)
n <- 100
p <- 0.2

# Calculate the probabilities for each number of tails (0 to 100)
success <- 0:n
probabilities <- dbinom(success, size = n, prob = p)

df <- data.frame(success = success, probability = probabilities)

gg <- ggplot(df, aes(x = success, y = probability)) +
  geom_bar(stat = "identity", fill = "blue", color = "black") +
  labs(title = "Probability histogram for the binomial n=100",
       x = "Number of successes with n=100",
       y = "Probability") +
  theme_minimal()

print(gg)
```

That means that we can use normal approximation to compute probabilities. The key point of the theorem is that we know that the sampling distribution of the statistic is normal no matter what the population histogram is.

The mathematical theory behind that:

The CLT is one of the most frequently used mathematical results in science. It tells us that when the sample size is large, the average $\bar{y}$ of a random sample follows a normal distribution centered at the population average μ and with standard deviation equal to the population standard deviation σ, divided by the square root of the sample size N. We refer to the standard deviation of the distribution of a random variable as the random variable's standard error. We have seen this formula before when studying the Standard Error (@eq-standardErrorMean): $$ 
\text{SE} (\bar{x_n})= \frac{\sigma}{\sqrt{n}} 
$$ This is telling us that when we take more samples and plot their results in a histogram, the spread of the histogram becomes smaller (closer to the population average) as we saw in the histograms above.

**Standardized sample mean**

This implies that if we take many samples of size $N$, then the quantity is approximated with a normal distribution centered at 0 and with standard deviation 1

$$
\frac{\bar{Y} - \mu}{\sigma_Y/\sqrt{N}}
$$ {#eq-standardizedSampleMean}

-   $\bar{Y}$: The sample mean, which is the average of your sample data.

-   $\mu$: The population mean, which is the average of the entire population from which the sample is drawn.

-   $\sigma_Y$: The population standard deviation, which measures the spread of the population data.

-   $N$: The sample size, or the number of observations in your sample.

The standardized sample mean is a way to transform the sample mean into a standard normal distribution (mean 0, standard deviation 1). Standardizing the sample mean by subtracting the population mean and dividing by the standard error $\sigma_Y/\sqrt{N}$ allows us to compare it to the standard normal distribution. This is useful because the properties of the normal distribution are well understood and widely applicable in statistical inference.

**Central limit theorem in practice.** The central limit theorem allow us, when we don't have access to the population data, to use the normal approximation so we can compute $p$-values.

We are going to use the mice bodyweight data for our example. We willwork only with female mice because the bodyweight of female and male mice are different. We calculate the mean and the standard deviation of control and treatment groups and those will be the parameteres of our population.

The CLT tells us that when the samples are large, the random variable is normally distributed. Thus we can compute $p$-values using the function `pnorm`

```{r miceexampleCLT}
dat <- read.csv("data/mice_pheno.csv") 
table(dat$Sex, dat$Diet)

controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  dplyr::select(Bodyweight) %>% unlist
hfPopulation <- filter(dat,Sex == "F" & Diet == "hf") %>%  
  dplyr::select(Bodyweight) %>% unlist

mu_hf <- mean(hfPopulation)
mu_control <- mean(controlPopulation)
print(mu_hf - mu_control)
sd_hf <- sd(hfPopulation)
sd_hf
sd_control <- sd(controlPopulation)
sd_control
```

Remember that in practice we do not get to compute these population parameters. These are values we never see. In general, we have to estimate them from samples.

As we described, the CLT tells us that for large N, each of these is approximately normal with average population mean and standard error population variance divided by N. We mentioned that a rule of thumb is that N should be 30 or more. However, that is just a rule of thumb since the preciseness of the approximation depends on the population distribution. Here we can actually check the approximation and we do that for various values of N.

We are going to create 10,000 samples of N number of mice for N= 3,12,25 and 50 and calculate the difference for the means between treatment and control bodyweight. We then calculate the t-statistic for each sample size (t-statistic is explained later)

Statistical theory tells us that if we divide a random variable by its SE, we get a new random variable with an SE of 1.This ratio is what we call the t-statistic.

$$
t = \frac{\bar{x} - \mu}{SE}
$$ {#eq-tstatistic} where $SE = \frac{s}{\sqrt{n}}$

In the specific case of our experiment (comparing the means of two samples) this translates into: $$ 
t = \frac{\bar{y} - \bar{x}}{SE}=\frac{\bar{y} - \bar{x}}{\sqrt{\frac{s_y^2}{n} + \frac{s_x^2}{n}}} 
$$

Finally we will plot a QQ plot for each sample size against the normal distribution to see their fit:

```{r, fig.align='center'}
Ns <- c(3,12,25,50)
B <- 10000 #number of simulations
##function to compute a t-stat
computetstat <- function(n) {
  y <- sample(hfPopulation,n)
  x <- sample(controlPopulation,n)
  (mean(y)-mean(x))/sqrt(var(y)/n+var(x)/n)
}
res <-  sapply(Ns,function(n) {
  replicate(B,computetstat(n))
})
mypar(2,2)
for (i in seq(along=Ns)) {
  qqnorm(res[,i],main=Ns[i])
  qqline(res[,i],col=2)
}
```

So we see that for $N=3$, the CLT does not provide a usable approximation. For $N=12$, there is a slight deviation at the higher values, although the approximation appears useful. For 25 and 50, the approximation good.

This simulation only proves that $N=12$ is large enough in this case, not in general.The further the population distribution is from the normal distribution, the higher the sample size we will require.

For populations that are already normally distributed, even small sample sizes will result in sample means that are approximately normally distributed. For populations that are not normally distributed, larger sample sizes are required for the sample means to approximate a normal distribution.

We will see later that we don't usually calculate our $p$-values like this using the CLT, instead we will use a *t-test* that will give us a slightly different result. This is to be expected because our CLT approximation considered the denominator of t-stat practically fixed (with large samples it practically is), while the t-distribution approximation takes into account that the denominator (the standard error of the difference) is a random variable. The smaller the sample size, the more the denominator varies.

::: {.callout-orange appearance="simple" icon="false"}
**When does the central limit theorem apply?** For the normal approximation to work, the key requirements are:

-   we sample with replacement, or we simulate independent random variables from the same distribution. (If the sample size is much smaller than the population size, then sampling with replacement and sampling without replacement is approximately the same so this also applies)

-   The statistic of interest is a sum, average or percentage.

-   The sample size is large enough. The more skewed the population histogram is, the larger the required sample size n. If there is no strong skewness, then n=15 is sufficient.
:::

::: exercise-box
Exercise:

Suppose we are interested in the proportion of times we see a 6 when rolling n=100 dice. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies.

We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance $\frac{p(1-p)}{n}$. So according to the CLT: $$
z = \frac{observed_p - p}{\sqrt{(p\times(1-p))/n)}}
$$

z should be normal with mean 0 and SD 1.

Perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05).

```{r}
n=100
N=10000
p=1/6
SE<- sqrt(p*(1-p)/n)
set.seed(1)

res<- replicate(N,sample(1:6,n,replace=TRUE))

prop = apply(res, 2, function(x) mean(x == 6))
hist(prop, freq=TRUE)
z_scores <- (prop-p)/SE
plot(z_scores,dnorm(z_scores))

proportion_greater_than_2 <- mean(abs(z_scores) > 2)
print(proportion_greater_than_2)
```
:::

# Significance test vs Confidence intervals

There are two fundamental sorts of questions we might ask our sample data:

-   How can we estimate a parameter while taking into account the variability of the data in an honest way?

-   How can we know if our data supports a specific conclusion, given its inherent variability?

The first of these questions leads to the idea of a **confidence interval**, where we specify not only an estimate of a parameter but also a reasonable margin of error. The latter question gives rise to **significance testing**.

::: {#confidenceIntvsSignifTest style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
-   Use a confidence interval if you wish to estimate a parameter from a sample in a way that describes not only the observed mean but also the uncertainty surrounding it.

-   Use a significance test if there is one particular value of interest, for instance representing whether a piece of machinery is properly calibrated.
:::

Significance tests and confidence intervals are two sides of the same coin.

Significance tests consider specific individual values, while confidence intervals give more general information about the parameter of interest. The latter are more flexible and are generally easier to interpret. Consider a significance test only when there is a single parameter value of interest which you could identify before looking at the data.

# Confidence intervals

A confidence interval is a way of reporting an estimate of a parameter that includes information about how much variability could reasonably be expected due to random chance. A confidence interval for the mean of a quantitative variable has the form

$$
\mu = \bar{x} \pm ME
$$

where $\mu$ is the population mean, $\bar{x}$ is the observed sample mean and $ME$ is a **margin of error.**

## Manual calculation:

A confidence interval gives a range of plausible values for a population parameter. Usually the confidence interval is centered at an estimate for $\mu$ which is an average. Since the central limit theorem applies for averages, the confidence interval has a simple form:

$$CI = estimate \mp ME$$

where **ME is the margin of error** and can be decomposed like this:

$$CI=estimate \mp z\ * SE = \mu \mp z*SE$$ {#eq-confidenceInterval}

where SE is the *standard error* of the sample. If that is an average or a percentage then it is $SE= \frac{\sigma}{\sqrt{n}}$ as we saw in (@eq-standardErrorMean).

And $z$ is the z-score corresponding to the desired confidence level. For example, if we would like to have a 95% confidence level, our $z=1.96$

This comes from calculating the value of z where 95 of our data is in the middle:

```{r, fig.align='center', echo=FALSE}

x <- seq(-4, 4, length=100)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue", xlab="Z", ylab="Density", main="Standard Normal Distribution")

# Highlight the area between -1.96 and 1.96
polygon(c(-1.96, seq(-1.96, 1.96, length=100), 1.96), 
        c(0, dnorm(seq(-1.96, 1.96, length=100)), 0), 
        col=rgb(0.1, 0.1, 0.9, 0.2), border=NA)

# Add vertical lines at z = ±1.96
abline(v = 1.96, col = "red", lwd = 2, lty = 2)
abline(v = -1.96, col = "red", lwd = 2, lty = 2)

# Add annotation at z = 1.96
text(1.96, dnorm(1.96), labels = "z = 1.96", pos = 4, col = "red")

# Add annotation for 95% confidence interval
text(0, 0.1, labels = "95%", pos = 3, col = "blue")
```

To get to that z value we use tables or we can use the quantile function qnorm:

```{r}
# Calculate the z-score for a 95% confidence interval
z_score <- qnorm(0.975)
z_score
```

The `0.975`value is used because for a 95% confidence interval, you need to capture the central 95% of the distribution, leaving 2.5% in each tail. Therefore, you look up the 97.5th percentile (0.975) to get the z-score.

For 90% confidence interval we are looking for the z value in the normalized distribution where 90% of the data falls in our center range and 10% outside, so 5% in each tail.

```{r}
# Calculate the z-score for a 95% confidence interval
desiredConfidence <- 90
tails = (100-desiredConfidence )/2
percentileOfinterest <- desiredConfidence+tails
z_score <- qnorm(percentileOfinterest/100)
z_score
```

For a 99 confidence level:

```{r}

# Calculate the z-score for a 95% confidence interval
desiredConfidence <- 99
tails = (100-desiredConfidence )/2
percentileOfinterest <- desiredConfidence+tails
z_score <- qnorm(percentileOfinterest/100)
z_score
```

**Estimating the SE with bootstrap principle**

We still have a problem for calculating the confidence interval following this, and it is that we need to know the standard deviation of the population $\sigma$ and we usually don't know it, but the [bootstrap principle](#bootstrap) states that we can calculate sigma by its sample version $s$ and still get an approximately correct confidence interval.

::: exercise-box
Example: We poll 1000 likely voters and find that 58% approve of the way the president handles his job.

$SE= \frac{\sigma}{\sqrt{n}} * 100$ where $\sigma = \sqrt{p(1-p)}$ where $p$ is the proportion of **all voters** who approve, but we don't know p, but the bootstrap principle tells us that we can replace $\sigma$ by $s$ so we can plug in the values from our survey here: $=\sqrt{0.58(1-0.58)} = 0.49$

So a 95% confidence interval for p is: $$
58\% \mp 1.96 \frac{0.49}{\sqrt{1000}}*100
$$

which is approximately \[54.9%,61.1%\]
:::

The width of the confidence interval is determined by z and the standard error SE. To reduce that margin of error we have two options, we can increase the sample size or decrease the confidence level. The sample size is square rooted so this means that to cut the width of the confidence level in half we need four times the sample size, and to reduce it 10 times we would need 100 times the sample size.

## Getting the confidence intervals from a test result

```{r}
#simulate our survey results
success<- rep(1,580)
failure<- rep(0,420)
mydata <- c(success,failure)
#do a te.test.
testResult <- t.test(mydata)
confInt95_low <- testResult$conf.int[1]
confInt95_upp <- testResult$conf.int[2]
confInt95_upp
cat('confidence interval = [',confInt95_low,confInt95_upp,']')
margin_of_error <- (confInt95_upp - confInt95_low) / 2
cat('margin of error: ',margin_of_error)
```

For example in our optical dataset, if we do a t.test over that variable we can get the confidence intervals for a level of confidence 95%:

```{r, fig.align='center'}
file1 <- here::here("data","optical_sample.xlsx")
optical_sample <- read_excel(file1)

testResult <- t.test(optical_sample$eye_difference)

confInt95_low <- testResult$conf.int[1]
confInt95_upp <- testResult$conf.int[2]

margin_of_error <- (confInt95_upp - confInt95_low) / 2 

ggplot(optical_sample, aes(x = eye_difference)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical_sample$eye_difference),
             linetype = "dashed")+
  geom_vline(xintercept = confInt95_low,
           linetype = "dotted", color = "green")+
  geom_vline(xintercept = confInt95_upp,
           linetype = "dotted", color = "green")
```

the margin of error will be:

```{r}
margin_of_error <- (confInt95_upp - confInt95_low) / 2 
margin_of_error
```

Now we can calculate the confidence intervals for other 90% and 99% level of confidence and see their differences in a graph:

```{r, fig.align='center'}
#level of confidence 90%. 
testResult <- t.test(optical_sample$eye_difference,
       conf.level = .90)
confInt90_low <- testResult$conf.int[1]
confInt90_upp <- testResult$conf.int[2]

#level of confidence 99%. 
testResult <- t.test(optical_sample$eye_difference,
                   conf.level = .99)
confInt99_low <- testResult$conf.int[1]
confInt99_upp <- testResult$conf.int[2]

ggplot(optical_sample, aes(x = eye_difference)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(optical_sample$eye_difference),
             linetype = "dashed")+
  geom_vline(xintercept = confInt90_low,
             linetype = "dotted", color = "orange")+
  geom_vline(xintercept = confInt90_upp,
             linetype = "dotted", color = "orange")+
  geom_vline(xintercept = confInt95_low,
           linetype = "dotted", color = "green")+
  geom_vline(xintercept = confInt95_upp,
           linetype = "dotted", color = "green")+
  geom_vline(xintercept = confInt99_low,
             linetype = "dotted", color = "yellow")+
  geom_vline(xintercept = confInt99_upp,
             linetype = "dotted", color = "yellow")
```

Three factors feed into the size of the margin of error:

1.  The **confidence level of the interval**. That is, the proportion of the time our interval would correctly capture the parameter of interest. Higher confidence requires a larger margin of error.

2.  The **spread of the observations** in the data set. More spread in the data implies more sampling variability and therefore a larger margin of error.

3.  The **size of the sample**.

::: exercise-box
Exercises:

Using the pm_sample data set,

1.  Find a 95% confidence interval for the mean salary of the project managers in this population. Interpret the results in plain language. Would a 99% confidence interval be wider of more narrow?

2.  Find a 95% confidence interval for the man non-salary compensation of project managers in this population. Why is the margin of error different than in the first problem?

```{r}

file <- here::here("data", "pm_survey.xlsx") 
pm_survey <- read_excel(file) 

kable(head(pm_survey))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()
as.data.frame(report(pm_survey$annual_salary))

t.test(pm_survey$annual_salary) 

```

The confidence interval at the (default) 95% is `r t.test(pm_survey$annual_salary)$conf.int` which means in plain language that if we were to repeat the sampling and the test multiple times, this confidence interval would capture the true value of the parameter 95% of the time. Although not extrictly true, we can say that there's a 95% chance that this confidence interval includes the true population mean.

For the second exercise

```{r}
pm_survey <- pm_survey |>    
  mutate(other_monetary_comp = replace_na(other_monetary_comp, 0))  

as.data.frame(report(pm_survey$other_monetary_comp))

t.test(pm_survey$other_monetary_comp)  
```

our confidence interval is `r t.test(pm_survey$other_monetary_comp)$conf.int` and so, the margin of error in both examples are:

```{r}
monetary_marginError<- (t.test(pm_survey$annual_salary)$conf.int[2] - 
                          t.test(pm_survey$annual_salary)$conf.int[1]) / 2 
monetary_marginError

nonMonetary_marginError<- (t.test(pm_survey$other_monetary_comp)$conf.int[2] - 
                             t.test(pm_survey$other_monetary_comp)$conf.int[1]) / 2
nonMonetary_marginError
```

The difference in both is due to the variability of the data, that we can calculate using the standar deviation:

```{r}
sd(pm_survey$annual_salary) 
sd(pm_survey$other_monetary_comp)
```
:::

### Interpreting confidence intervals: practical example

We are going to calculate the confidence interval for 100 samples of 30 observations calculated over the same population and plot it on a graph with a line marking the true mean. We will see how some of them (for a 95% confidence level it should be around 5% of the times) will still miss the population parameter:

```{r, fig.align='center'}

#calculate the sample mean and confidence interval from a sample of 100 
low = numeric()
high = numeric()
for (n in 1:100){
  sample <- slice_sample(optical, n = 30)
  test <- t.test(sample$eye_difference)
  low[n] <- test$conf.int[1]
  high[n] <- test$conf.int[2]
}

ci_reps <- data.frame("replicate" = 1:100,
                      low,
                      high)
ggplot(ci_reps, aes(x = low, 
                    xend = high,
                    y = replicate, 
                    yend = replicate)) + 
  geom_segment() +
  geom_vline(xintercept = mean(optical$eye_difference), 
             linetype = "dashed")
```

When interpreting a confidence interval, remember that not all values in an interval are equal. The center is always more likely than the ends.

# Significance testing

A t-significance test is used to consider whether an individual parameter value of interest is plausible in light of sample data.

The situation in which the parameter has that value is referred to as the **null hypothesis**, and the situation in which it does not is referred as to the **alternative hypothesis**. For example: could be the difference between left eye value and right eye value actually be zero? Does a certain large company pay better than the national average, or could salaries there just be different through random chance?

A test statistic measures how far away the data in our sample are from what we would expect if the null hypothesis $H_0$ were true.

The most common test statistic is the z-statistic. It determines how far an observed value is from the expected value, measured in standard errors.We already saw this formula before (@eq-zscore)

$$
z= \frac{observed - expected}{SE}
$$ {#eq-zscore3}

Observed is a statistic that is appropriate for assessing $H_0$. For example, if we toss a coin ten times and want to know if it is a fair coin, the appropriate statistic would be the number of tails or the percentage of tails.

Expected and SE are the expected value and the SE of this statistic computed under the assumption that $H_0$ is true.

::: exercise-block
Example:

We toss a coin 10 times, and get 7 tails. Using the formulas for the expected values of sums we have: Number of expected tails if the coin is fair:

For a binomial distribution we have: $E=n * p$ where $n$ is the number of repetitions and $p$ is the the probability of success (0,5), and the standard error for binomial scenarios is $SE = \sqrt{np(1-p)}$ .

$$
expected = 10 * \frac{1}{2} = 5
$$

$$
SE = \sqrt{10}\sqrt{\frac{1}{2} * \frac{1}{2}} =1.58
$$

$$
z= \frac{7-5}{1.58}=1.27
$$

By the central limit theorem, the $p$-value can be computed with normal approximation.

```{r}
z <- 1.27

# Calculate the cumulative probability up to z
(p_value <- (1 - pnorm(z))*2)

```

In other words, we have a 20.4% probability of observing 7 tails in 10 tosses given that the coin is fair.
:::

If the z-value is large, that means that there is a great difference between the observed and the expected, so large values of z are evidence against $H_0$. The strength of the evidence is measured by the $p$-value or observed significance level.

The $p$-value is the probability of getting a value of z as extreme or more extreme than the observed, assuming the null hypothesis is true. As we can see in the graph below, as z becomes larger, there is less probability of finding data outside of its limits, so the $p$-value will be smaller. We multiply the p value times 2 because we have to consider the two tails of the graph for this experiment.

The Z-score tells you how far an observed value is from the expected value in terms of standard errors. The $p$-value tells you the probability of observing a test statistic as extreme as, or more extreme than, the observed statistic, under the null hypothesis.

```{r, fig.align='center', echo=FALSE}
#| echo = FALSE
x <- seq(-4, 4, length=100)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue", xlab="", ylab="Density", main="Standard Normal Distribution", xaxt='n')

# Highlight the area before -1.27
polygon(c(-4, seq(-4, -1.27, length=100), -1.27), 
        c(0, dnorm(seq(-4, -1.27, length=100)), 0), 
        col=rgb(0.1, 0.1, 0.9, 0.2), border=NA)

# Highlight the area after 1.27
polygon(c(1.27, seq(1.27, 4, length=100), 4), 
        c(0, dnorm(seq(1.27, 4, length=100)), 0), 
        col=rgb(0.1, 0.1, 0.9, 0.2), border=NA)

# Add vertical lines at z = ±1.27
abline(v = 1.27, col = "red", lwd = 2, lty = 2)
abline(v = -1.27, col = "red", lwd = 2, lty = 2)

# Add annotation at z = 1.27
text(1.27, dnorm(1.27), labels = "z = 1.27", pos = 4, col = "red")
text(-1.27, dnorm(-1.27), labels = "z = -1.27", pos = 4, col = "red")


```

The **result of a significance test is a** $p$-value, which measures how plausible the value of interest is given the sample data. A $p$-value close to zero indicates the value isn't compatible with the data. As a general rule, if p\<0.05, the value can be considered questionable.

Ideally, you should identify a value of interest (the technical term is null hypothesis) before collecting the data. While this isn't always done in practice, doing so helps prevent wrong conclusions that come from the shotgun effect

> If you run a test because you observed an interesting pattern in your sample data, the overall chance of encountering a significant result increases. Data is like firing a shotgun with many pellets---some are bound to hit the target purely due to randomness, if you create your null theory based on a characteristic observed in your sample you are likely going to find significance, even if that is not characteristics of the overall population and it was there just by chance.

While a $p$-value might tell you that a parameter is different from a hypothesized value, it won't ever say how different it might be or if that difference is important.

A t-significance test is appropriate for quantitative data under the exact same circumstances as a t-confidence interval: unless the sample is very small and the data is highly non-symmetric. If the data is relatively symmetric and there are no extreme outliers, n=10 is usually enough. Regardless of the shape of the data, n=30 is a safe threshold.

The value of $\mu$ (mu) is the value we want to test against (our null hypothesis)

```{r}
file1 <- here::here("data", "optical_sample.xlsx")
optical_sample <- read_excel(file1)

testResult <- t.test(optical_sample$eye_difference, mu = 0) 

testResult$p.value
```

$p$-value express the probability of observing the values we have in the sample if the eye difference of the population were in fact 0. The smaller the $p$-value, the smaller the probability. Usually we take the threshold of 5% or $p$-value \<0.05 to reject the null theory, as it means that it is unlikely that the mean observed in our sample can come from a population whose true mean is 0.

We can change the null theory to whatever value we want to measure against. For example, can we say that the average age of the population is NOT 59?

```{r}
testResult <- t.test(optical_sample$Age, mu = 59)
testResult$p.value 
```

our $p$-value is `{r}testResult$p.value` which does not allow us to reject the null theory.

::: {#SignificanceTestQuickFacts .callout-orange}
Quick Facts

1.  A $p$-value below 0.05 doesn't mean there's a 5% chance the null hypothesis is true. Instead, it means if the null were true, there's a 5% chance of observing data as extreme as, or more extreme than the one that was observed in the sample.

2.  Statistical power, the ability to detect a true effect is often overlooked. A study can have a high chance of missing real effects (Type II error) even if its $p$-value threshold is stringent.

3.  In proportional analysis, Simpson's Paradox can occur where a trend seen in several groups reverses when the groups are combined. It underscores the importance of scrutinizing aggregated data.
:::

### One sided vs two sided test.

One has to carefully consider whether the alternative should be one-sided or two-sided test (we are considering the values on the left of -z and the right of z) or only the values on one side. If we are considering a two sided test, the $p$-value gets doubled. It is not ok to change the alternative afterwards in order to get the $p$-value under 5%.

Deciding whether to use a one-sided or two-sided t-test depends on the hypothesis you want to test. Here are some guidelines to help you make that decision:

Use a **one-sided t-test** when you have a specific direction in mind for your hypothesis. This means you are testing whether the mean is either greater than or less than a certain value, but not both.

**Examples**:

-   **Greater than**: You want to test if the mean score of a new teaching method is greater than the mean score of the traditional method.

-   **Less than**: You want to test if the mean time to complete a task using a new software is less than the mean time using the old software.

Use a **two-sided t-test** when you are interested in any difference from the specified value, regardless of direction. This means you are testing whether the mean is different from a certain value, without specifying the direction of the difference.

**Examples**:

-   You want to test if the mean weight of a sample of apples is different from a known standard weight.

-   You want to test if the mean score of a new drug is different from the mean score of a placebo.

How to Decide

1.  **Define Your Research Question**: Clearly state what you are trying to find out.

2.  **Determine the Direction of Interest**: Decide if you are only interested in deviations in one direction (one-sided) or in both directions (two-sided).

3.  **Formulate Your Hypotheses**: Based on your research question and direction of interest, formulate your null and alternative hypotheses.

**Example Scenario**

Suppose you are testing a new drug and want to know if it has a different effect on blood pressure compared to a placebo. If you only care whether the drug lowers blood pressure, you would use a one-sided test. If you care whether the drug either lowers or raises blood pressure, you would use a two-sided test.

```{r}
# Sample data
sample_data <- c(78, 82, 85, 90, 76, 79, 81, 77, 74, 88)

# Perform one-sided t-test
t_test_result <- t.test(sample_data, mu = 75, alternative = "greater")

# Print the result
print(t_test_result)

# Perform two-sided t-test
t_test_result <- t.test(sample_data, mu = 75, alternative = "two.sided")

# Print the result
print(t_test_result)

```

### Common errors in significance testing

At the start of a significance test, the hypothesized value might be true or false. At the end, it may be rejected or not. Altogether, there are four possible combinations of these outcomes:

|                       |           $H_0$ is true           |           $H_0$ is false            |
|-----------------:|:------------------------:|:--------------------------:|
|     $H_0$ is rejected | **Type I error** -false positive- |            True Negative            |
| $H_0$ is not rejected |           True Positive           | **Type II error** -False negative - |

Bear in mind, however, that we usually don't know whether the null theory is true or not.

We call the *event* of rejecting the null hypothesis, when it is in fact true, a *Type I error*, we call the *probability* of making a Type I error, the *Type I error rate*, and we say that rejecting the null hypothesis when the $p$-value is less than $\alpha$, *controls* the Type I error rate so that it is equal to $\alpha$

We may observe a $p$-value higher than alpha even when the null hypothesis is false. This is a type II error. We ask ourselfs this question: How big does N have to be in order to detect that the absolute value of the difference is greater than zero? Type II error control plays a major role in designing data collection procedures before you actually see the data, so that you know the test you will run has enough sensitivity or power. Power is 1 minus Type II error rate, or the probability that you will reject the null hypothesis when the alternative hypothesis is true.

There are several aspects of a hypothesis test that affect its power for a particular effect size. Intuitively, setting a lower $\alpha$ decreases the power of the test for a given effect size because the null hypothesis will be more difficult to reject (for example from 0.05 to 0.01). This means that for an experiment with fixed parameters (i.e., with a predetermined sample size, recording mechanism, etc), the power of the hypothesis test trades off with its Type I error rate, no matter what effect size you target.

### Power calculations

we are going to use our mice dataset to see how we can calculate the power of our t test for type II errors. We consider that the data in the file is our entire population. If we look at the difference average weight for control vs treatment we appreciate that there is in fact a difference or around 9%:

```{r}
dat <- read.csv("data/mice_pheno.csv") 
controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  dplyr::select(Bodyweight) %>% unlist

hfPopulation <- filter(dat,Sex == "F" & Diet == "hf") %>%  
  dplyr::select(Bodyweight) %>% unlist

mu_hf <- mean(hfPopulation)
mu_control <- mean(controlPopulation)
print(mu_hf - mu_control)
print((mu_hf - mu_control)/mu_control * 100) #percent increase
```

Depending on our sample size, this difference will be perceived by our test or not. Let's see an example with sample size = 12 and alpha = 0.05. We will do first a single test and see that we cannot reject the null hypothesis based on the $p$-value:

```{r}
set.seed(1)
N <- 12
hf <- sample(hfPopulation,N)
control <- sample(controlPopulation,N)
t.test(hf,control)$p.value

```

If we ran this experiment multiple times we can get the percentage of times that our $p$-value manage to actually reject the null hypothesis:

```{r}
repetitions<-2000
N<- 12
alpha<- 0.05
reject<- function(N, alpha=0.05){
  hf <- sample(hfPopulation, N)
  control <- sample(controlPopulation, N)
  pval<- t.test(hf,control)$p.value
  pval < alpha
}

rejections<- replicate(repetitions,reject(N))
mean(rejections)

```

In this case we see that 25% of the time we correctly rejected the null hypothesis and this is the power of our test. To increase this percentage we should increase the sample size. In the next code section we are going to run the same simulation for various values of N

```{r, fig.align='center'}
Ns<- seq(5,50,5)
power <- sapply(Ns,function(N){
  rejections<- replicate(repetitions,reject(N))
  mean(rejections)
})
plot(Ns,power, type="b")
```

There are on the internet several tools that allow you to calculate the the power you need for a particular standard deviation, sizes of n or the effect size you want to detect.

### Student's t test

So far we have been using the t-test without explaining the student's t-distribution that is used under the hood. Let's explain it now.

We will explain the student's t test with an example: The health guideline for lead in drinking water is a concentration of not more than 15 parts per billion (ppb). Five independent samples from a reservoir average 15.6 ppb. Is this sufficient evicence to conclude that the concentration $\mu$ in the reservoir is above the standard of 15 ppb?

Our null hypothesis is that no, the concentration is just on the standard $H_0:\mu=15$ and the alternative hypothesis is that the concentration is higher than 15 ppb.

$$
z= \frac{observed - expected}{SE} = \frac{15.6-15}{SE}
$$

SE of average = $\frac{\sigma}{\sqrt{n}}$ but sigma is unknown. By the boostrap principle we know that we should be able to substitute the standard deviation of the population $\sigma$ with the standard deviation of our sample $s$ however for sample size less or equal to 20, then the normal curve is not a good enough approximation to the distribution of the z-statistic. Rather, an appropriate approximation is the Student's t-distribution with n-1 degrees of freedom.

```{r, fig.align='center', fig.height=4, fig.width=5}
#| echo = FALSE


# Create a sequence of x values
x_values <- seq(-4, 4, by = 0.01)

# Calculate the density of the t-distribution for different degrees of freedom
df1 <- dt(x_values, df = 1)
df2 <- dt(x_values, df = 2)
df5 <- dt(x_values, df = 5)
df_inf <- dnorm(x_values) # Normal distribution as t-distribution with infinite degrees of freedom

# Create a data frame for plotting
data_to_plot <- data.frame(
    x = c(x_values, x_values, x_values, x_values),
    density = c(df1, df2, df5, df_inf),
    degree_of_freedom = factor(c(rep(1, length(df1)), rep(2, length(df2)), rep(5, length(df5)), rep('∞', length(df_inf))))
)

# Plot using ggplot
ggplot(data_to_plot, aes(x = x, y = density, color = degree_of_freedom)) +
    geom_line() +
    labs(title = "Student's t-Distribution", x = "x", y = "Density") +
    scale_color_discrete(name = "ν")

```

In the graph we can see the purple line where the degrees of freedom are high (large sample) is just the normal curve. The rest of the lines are showing how with less degrees of freedom the tails of the curve get bigger, representing higher uncertainty. We saw the **sample standard deviation formula** (@eq-sampleStandarDeviation):

$$
s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2}
$$

In this case we also need to adjust our **confidence interval** formula from: $CI=estimate\mp z\  SE$

to $$CI= estimate (\mu) \mp t_{n-1}SE$$ {#eq-confidenceIntervalTStudent}

::: {.callout-orange appearance="simple" icon="false"}
::: centered-text
**z-test vs t-test**
:::

The main differences between a t-test and a z-test lie in their assumptions and applications:

**T-Test**

-   **Distribution**: Uses the Student's t-distribution.

-   **Population Variance**: Unknown and estimated from the sample.

-   **Sample Size**: Typically used for small sample sizes (n \< 30).

-   **Degrees of Freedom**: Required for the calculation.

-   **Application**: Used when comparing the means of two groups, especially when the sample size is small and the population variance is unknown.

**Z-Test**

-   **Distribution**: Uses the standard normal distribution (z-distribution).

-   **Population Variance**: Known.

-   **Sample Size**: Typically used for large sample sizes (n \> 30).

-   **Degrees of Freedom**: Not required.

-   **Application**: Used for hypothesis testing of means and proportions when the sample size is large and the population variance is known.

[**Key Differences**]{.underline}

1.  **Distribution**:

    -   T-test: Student's t-distribution.

    -   Z-test: Standard normal distribution.

2.  **Population Variance**:

    -   T-test: Unknown and estimated from the sample.

    -   Z-test: Known.

3.  **Sample Size**:

    -   T-test: Small sample sizes (n \< 30).

    -   Z-test: Large sample sizes (n \> 30).

4.  **Degrees of Freedom**:

    -   T-test: Required.

    -   Z-test: Not required.

[**Example Scenarios**]{.underline}

-   **T-Test**: Comparing the average test scores of two small classes where the population variance is unknown.

-   **Z-Test**: Testing the average height of a large population where the population variance is known.
:::

T-test are the most commonly used test in real life, but if we meet all the criteria to perform a z-test we can do it like this in r:

```{r}
library(BSDA)
# Sample data
sample_data <- c(88, 92, 94, 94, 96, 97, 97, 97, 99, 99, 105, 109, 109, 109, 110, 112, 112, 113, 114, 115)
population_mean <- 100
population_sd <- 15

# Perform a one-sample z-test
z_test_result <- z.test(sample_data, mu = population_mean, sigma.x = population_sd)
print(z_test_result)
```

# Categorical data

In 1912 the Titanic sank and more than 1500 of the 2229 people on board died. Did the chances of survival depend on the ticket class?

|          | First | Second | Third | Crew |
|----------|-------|--------|-------|------|
| Survived | 202   | 118    | 178   | 215  |
| Died     | 123   | 167    | 528   | 698  |

This is an example of categorical data. The data are counts for a fixed number of categories. Here the data is tabulated in a 2x4 table. Such table is called a **Contingency Table** because it shows the survival counts for each category of ticket class, i.e. contingent on ticket class.

## Confidence Intervals for Proportions.

Confidence intervals for proportions provide a range of plausible values for population proportions, enabling estimation with a desired level of confidence.

Proportional analysis techniques allow for the assessment and comparison of proportions across different categories, providing valuable insights into categorical data.

There are any number of statistical methods for computing confidence intervals for proportions. By far the simplest is:

$$
p = \hat{p} \pm \sqrt{\frac{\hat{p} \cdot (1 - \hat{p})}{n}}
$$ {#eq-confidenceIntervalProportions}

where $p$ is the actual population proportion and $\hat{p}$ is the observed proportion.

As long as the sample size isn't too small, the difference between methods should be minor. If your sample includes at least 5 of each sort of possible outcomes, you are fine with whatever default method your software uses to calculate the proportions.

In R we will use $prop.test(n_s , sample size)$ where $n_s$ is the number of successes.

Practice:

We are going to calculate the proportion of smokers in our population based on our optical_sample data

```{r}
file1 <- here::here("data", "optical_sample.xlsx")
optical_sample <- read_excel(file1)

as.data.frame(report(optical_sample$IsSmoker))

successNumber<- sum(optical_sample$IsSmoker)
sampleSize <- NROW(optical_sample$IsSmoker)
testResult <- prop.test(successNumber, sampleSize)
testResult

#proportion of success 
proportion<- testResult$estimate %>% print()
conf_int <- testResult$conf.int%>% print()

```

We can use the formula we studied at the beginning of the chapter to manually calculate the confidence intervals as well and see the difference with what the software used:

```{r}
ci_lower<- proportion - sqrt(proportion*(1-proportion)/sampleSize) 
ci_upper<- proportion + sqrt(proportion*(1-proportion)/sampleSize) 
ci_lower
ci_upper
```

Another way of manually calculating the confidence interval is using a z-score

In the example below we use 95%: (qnorm(0.975) is the $z-score$ corresponding for 95% conf. level)

```{r}
margin_of_error <- qnorm(0.975) * sqrt(proportion * (1 - proportion) / sampleSize)

# Calculate the confidence interval bounds
proportion - margin_of_error
proportion + margin_of_error
```

For different confidence levels you will need to find the correct z-score

### Non-binary categorical variables

If our categorical variable is not binary, we still can use this same test to calculate the proportion of one single category against the rest.

For example, to calculate the proportion of women in our pm_survey dataset:

```{r}
file2 <- here::here("data", "pm_survey.xlsx")
pm_survey <- read_excel(file2)

kable(head(pm_survey))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()
as.data.frame(report(pm_survey$gender))

pm_survey <- pm_survey |> 
  mutate(across(.cols = c(highest_level_of_education_completed, gender), as.factor))

numberOfWomen<- NROW(pm_survey[pm_survey$gender == 'Woman',])
sampleSize <- NROW(pm_survey)
prop.test(numberOfWomen, sampleSize)

```

### Few observations for each outcome

While there are statistical methods for dealing with samples with fewer than 5 or each sort of outcome, you should be conservative about making decisions based on them. That said, there are two methods that might be helpful in such situations:

-   The wilson score interval works well even for observed proportions near zero or one and has other good theoretical properties as well. Unlike other confidence intervals we've seen it isn't symmetric.

-   The rule of three is a great rought-and-ready way to compute a 95% confidence interval for a proportion when no positive results have been observed. If you have failures only, the interval will go from 0 to 3/n where n is the sample size, and if you have observed successes only, the confidence interval will go from 1 - (3/n) to 0

## Significance testing for proportions

We want to know if the proportion of the people taking medication in our sample is the same as the proportion of people in USA taking medication. This data we know is 66% of the USA population so we can plug in that value in our test just like that in the formula using $p=$ where p, in this case is the proportion for our null theory.

```{r}
file <- here::here("data", "optical_sample.xlsx")
optical_sample <- read_excel(file)

as.data.frame(report(optical_sample$TakingMedication))

medicated<- sum(optical_sample$TakingMedication == "yes")
sampleSize <- nrow(optical_sample)

testResult <- prop.test(medicated, sampleSize, p = .66)

testResult$p.value
testResult$conf.int
```

In our case we can reject the null hypothesis and say that it is not reasonable to believe that this sample was drawn from a population with sample mean of 66%

::: exercise-box
Exercises

**Exercise 1.** A media outlet claims that 60% of project managers in the U.S have a college degree as their highest level of education. Is that claim plausible using our pm_survey data set?

```{r}

file <- here::here("data", "pm_survey.xlsx")
pm_survey <- read_excel(file)

pmWithCollegeDegree <- sum(pm_survey$highest_level_of_education_completed == 'College degree')
sampleSize <- nrow(pm_survey)

testResult <- prop.test(pmWithCollegeDegree, sampleSize, p= 0.6)

testResult$p.value
testResult$conf.int
```

In this case we cannot reject the null hypothesis.

**Exercise 2**. Use that data set to construct a 95% confidence interval for the proportion of project managers that are under the age of 35.

```{r}
pm_under35 <- nrow (pm_survey[pm_survey$how_old_are_you %in% c("18-24","25-34"),])

testResult <- prop.test(pm_under35, sampleSize)
testResult$conf.int 
testResult$estimate
```

The confidence interval shows that between 44% and 57% of respondents are under 35
:::

## Goodness of Fit

Consider genetic data where you have two groups of genotypes (A or B) for cases and controls for a given disease. The statistical question is if genotype and disease are associated. As in the examples we have been studying previously, we have two populations (A and B) and then numeric data for each, where disease status can be coded as 0 or 1. So why can't we perform a t-test? Note that the data is either 0 (control) or 1 (cases). It is pretty clear that this data is not normally distributed so the t-distribution approximation is certainly out of the question. We could use CLT if the sample size is large enough, otherwise, we can use *association tests*.

Imagine we have 250 individuals, where some of them have a given disease and the rest do not. We observe that 20% of the individuals that are homozygous for the minor allele (group B) have the disease compared to 10% of the rest. Would we see this again if we picked another 250 individuals?

```{r}
disease=factor(c(rep(0,180),rep(1,20),rep(0,40),rep(1,10)),
               labels=c("control","cases"))
genotype=factor(c(rep("A",200),rep("B",50)),
                levels=c("A","B"))
dat <- data.frame(disease, genotype)
dat <- dat[sample(nrow(dat)),] #shuffle them up

tab<- table(genotype,disease)
tab
```

The typical statistics we use to summarize these results is the odds ratio (OR). We compute the odds of having the disease if you are an "B": 10/40, the odds of having the disease if you are an "A": 20/180, and take the ratio: $(10/40) / (20/180)$

```{r}
(tab[2,2]/tab[2,1]) / (tab[1,2]/tab[1,1])
```

To compute a $p$-value, we don't use the `OR` directly. We instead assume that there is no association between genotype and disease, and then compute what we expect to see in each cell of the table under the null hypothesis: ignoring the groups, the probabilty of having the disease according to our data is:

```{r}
p=mean(disease=="cases")
p
```

according to this probability, under our null hypothesis we expect to see a table like this:

```{r}
expected <- rbind(c(1-p,p)*sum(genotype=="A"),
                  c(1-p,p)*sum(genotype=="B"))
dimnames(expected)<-dimnames(tab)
expected
```

The Chi-square test uses an asymptotic result (similar to the CLT) related to the sums of independent binary outcomes. Using this approximation, we can compute the probability of seeing a deviation from the expected table as big as the one we saw. The $p$-value for this table is:

```{r}
chisq.test(tab)$p.value
```

so we would expect to find this difference in disease ratio by chance approximately 8.8% of the times, which is not enough to reject our null hypothesis.

**Large Samples, Small** $p$-values

Reporting only $p$-values is not an appropriate way to report the results of your experiment. Studies with large sample sizes will have impressively small $p$-values. Yet when one looks closely at the results, we realize odds ratios are quite modest: barely bigger than 1.

To demonstrate, we recalculate the $p$-value keeping all the proportions identical, but increasing the sample size by 10, which reduces the $p$-value substantially:

```{r}
tab<-tab*10
chisq.test(tab)$p.value
```

::: {.callout-orange appearance="simple" icon="false"}
1.  **Fitting Fallacies**: Goodness of fit doesn't guarantee predictive power. A model can fit past data perfectly yet fail miserably on new, unseen data, highlighting the dangers of overfitting.

2.  **The Sample Size Paradox**: Doubling your sample size doesn't necessarily halve the error. In fact, to do so, you'd need to quadruple the sample, given the square root relationship between sample size and margin of error.

3.  **Two-Sample Twists**: When comparing two samples, it's possible for each sample's individual data to appear random, yet their combined data can reveal a distinct pattern or significant difference.
:::

A Goodness-of-fit test, also called chi-squared or Pearson goodness-of-fit test, considers whether a categorical variable has a hypothesized distribution

Warning! **A goodness of fit test doesn't give any information about which specific categories might be out of line with the hypothesized distribution.** While you might be able to make an educated guess by looking at the data, this test shouldn't be used to support that kind of intuition.

In 208 the manufacturer of M&Ms published their last color distribution:

| Blue | Orange | Green | Yellow | Red | Brow |
|------|--------|-------|--------|-----|------|
| 24%  | 20%    | 16%   | 14%    | 13% | 13%  |

We open several packages of M&Ms and count the colors:

| Blue | Orange | Green | Yellow | Red | Brow |
|------|--------|-------|--------|-----|------|
| 85   | 79     | 56    | 64     | 58  | 68   |

Are these counts consistent with the last published percentages? is there sufficient evidence to claim that the color distribution is different? This question requires a test of goodness-of-fit for the six categories.

Our null hypothesis is that the color distribution is the same. The idea is to compare the observed counts to the numbers we would expect if \$H_0 \$ is true, so for our sample of 410 M&Ms we would expect:

| Blue | Orange | Green | Yellow | Red  | Brow |
|------|--------|-------|--------|------|------|
| 98.4 | 82     | 65.6  | 57.4   | 53.3 | 53.3 |

We look at the difference of the observed and the expected values, we square that difference and we standardize it by dividing by the expected

$$
\chi^2 =\sum_{categories} \frac{(observed-expected)^2}{expected}
$$

$$
\frac{(85 - 98.4)^2}{98.4} + \frac{(79 - 82)^2}{82} + \cdots + \frac{(68 - 53.3)^2}{53.3} = 8.57
$$

Large values of the chi-square statistic $\chi^2$ are evidence against the null hypothesis. To calculate the $p$-value we use the chi-square distribution. The $p$-value is the right tail of the $\chi^2$ distribution with degrees of freedom = number of categories -1.

```{r, fig.align='center'}
df <- 5
# Create the chi-square distribution curve
curve(dchisq(x, df = df), from = 0, to = 40, 
      main = paste("Chi-Square Distribution (df =", df, ")"),
      ylab = "Density", lwd = 2, col = "steelblue")

# Draw a vertical line at x = 8.57
abline(v = 8.57, col = "red", lwd = 2, lty = 2)

# Highlight the area to the right of x = 8.57
x_vector <- seq(8.57, 40, length.out = 1000)
y_vector <- dchisq(x_vector, df = df)
polygon(c(8.57, x_vector, 40), c(0, y_vector, 0), 
        col = adjustcolor("red", alpha.f = 0.3), border = NA)
```

in our example, this $p$-value happens to be 12.7%, which does not allow us to reject the null hypothesis. In r we can calculate the $p$-value like this:

```{r}
# Set the chi-square value and degrees of freedom
x <- 8.57
df <- 5

# Calculate the p-value
p_value <- pchisq(x, df, lower.tail = FALSE)

# Print the p-value
p_value
```

::: exercise-box
Exercise:

In the exercise below we want to know if the age population in our project managers survey data matches the distribution of ages for the USA population. We get the distribution of USA from wikipedia and do some adjustments so the ranges matches those in our survey:

```{r}
file1 <- here::here("data", "pm_survey.xlsx")
pm_survey <- read_excel(file1)

us_ages <- c(.117, .176, .168, .158, .166, .215)

table(pm_survey$how_old_are_you)
obs_ages <- as.numeric(table(pm_survey$how_old_are_you))

testResult <- chisq.test(obs_ages, p = us_ages)
testResult %>% report()

testResult$p.value
```

The very small $p$-value indicates that is extremely unlikely that our pm_survey data was extracted at random from a population with the us_ages distribution.

As mentioned, the goodness of fit test does not tell us what categories show the discrepancies in the data, if we want to find out what is the difference between our sample range of ages and the USA data we can just subtract the proportions from each and see what categories are sub represented and vice versa

```{r}
obs_ages / sum(obs_ages)
obs_ages / sum(obs_ages) - us_ages
```

Now we want to see if in our optical sample the customers are evenly distributed between the opticians. If we don't pass a p attribute to the chi squared test it will assume you are asking for even distribution between all categories:

```{r}
file2 <- here::here("data", "optical_full.xlsx")
optical_full <- read_excel(file2)

counts <- as.numeric(table(optical_full$`Optician Last Name`))

testResult<- chisq.test(counts)
testResult %>% report()

testResult$p.value

```

In this case we cannot reject the null hypothesis, which in this case is that the count of patients is evenly distributed among all opticians.
:::

::: exercise-box
Exercise

Does the distribution of populations of towns in the United States follow [Benford's law](https://en.wikipedia.org/wiki/Benford%27s_law)? Check using the towns data set.

```{r}
theme_set(theme_minimal())

file <- here::here("data", "towns.xlsx")
towns <- read_excel(file)

benford <- c(.301, .176, .125, .097, .079, .067, .058, .051, .046)
sum(benford)
#we calculate the frequency of each first digit in the population of towns.
table(towns$first_digit)
digits_freq <- as.numeric(table(towns$first_digit))
digits_freq

testResult<- chisq.test(digits_freq, p = benford)
testResult %>% report()
testResult$p.value


# An illustrative plot
benford_df <- data.frame(distribution = c(rep("Towns", 9), rep("Benford", 9)),
                         first_digit = rep(1:9, 2),
                         frequency = c(digits_freq/sum(digits_freq), benford))

ggplot(benford_df, aes(x = first_digit, 
                       y = frequency,
                       fill = distribution)) + 
  geom_col(position = "dodge") +
  scale_x_continuous(breaks = 1:9) + 
  labs(x = "First digit",
       y = "Relative frequency",
       fill = "Distribution") +
  scale_fill_brewer(palette = "Dark2")

```

our $p$-value indicates that we cannot reject the null hypothesis, meaning in this case that the distribution of the first digit in towns across US is compatible with Benford's law.
:::

### Statistical power

Statistical power is the ability of a test to detect a specified effect. A test with low power is unlikely to rule out a hypothesized value, even if the value is false. That is, it has a high probability of type II error. Increasing the power of an inference technique requires a trade-off of one sort or another. In practice, this usually means using a larger sample. A preliminary power analysis can give decision-makers information about the study size needed to detect an effect of interest.

A very simple example of how to do that: study planners can estimate the sample size needed to reduce the margin of error in the estimate of a population proportion using this formula:\
$$
n = \left( \frac{1.96}{2E} \right)^2
$$

where $E$ is the required margin of error. According to this formula, a simple political poll requiring a 3% margin of error would need a sample size of approximately n=1067 respondents.

# Two-Sample testing

Two-sample testing allows for the comparison of two independent groups or populations to assess if there are statistically significant differences between their means, proportions, or other relevant measures.

*Confidence intervals* for two-sample comparison provide a range of plausible values for the difference in means or proportions, allowing for estimation with a desired level of confidence.

*Significance testing* for two-sample comparison involves evaluating the evidence against the null hypothesis and determining if the observed differences between groups are statistically significant.

In this simple exercise we are going to generate ratings for two products at random. We have not changed the probability of each ranking so both products should have the same rating so the difference of their mean ratings should be 0 if we have enough samples.

Creating many samples at random, calculating their differences and plotting those differences will show how, although we can see that the bell curve distributes more or less evenly from 0 as expected, many of the samples showed a difference

```{r, fig.align='center'}
theme_set(theme_minimal())

set.seed(27)

rating <- sample(1:5, 27, replace = TRUE)
product <- c(rep("A", 15), rep("B", 12))
AB_testing <- data.frame(product,
                         rating)

AB_testing |> 
  group_by(product) |> 
  dplyr::summarize(avg_rating = mean(rating))

AB_testing |> 
  group_by(product) |> 
  dplyr::summarize(avg_rating = mean(rating)) |> 
  ggplot(aes(x = product, 
             y = avg_rating,
             fill = product)) + 
  geom_col() +
  scale_fill_brewer(palette = "Dark2") +
  theme(legend.position = "none") +
  labs(x = "Product",
       y = "Average rating")


# repeat the samples 10000 times:
difference <- integer()
for (rep in 1:10000){
  rating <- sample(1:5, 27, replace = TRUE)
  difference[rep] <- mean(rating[1:15]) - mean(rating[16:27]) 
}
qplot(difference, binwidth = .2, xlab = "Difference in ratings")
```

In reality we will only have access to one of the samples, and we have to be able to tell if it's reasonable to draw a conclusion about the population just based on the sample or whether or not we should just attribute these sorts of differences to random chance.

## Significance testing for Two-Samples data

### z-test

The significance test for two samples uses the null hypothesis that there is no difference in the means of the two population means. The $p$-value will be used to reject or not that null hypothesys.

we can use a z-test for the difference between the two means:

$$
z = \frac{observed\ difference- expected\ difference}{SE\ of\ difference} = \frac{(\bar{x_2} - \bar{x_1})-0}{SE\ of\ difference}
$$

our expected difference is 0 because that's our null hypothesis (no difference). An important fact is that if $\bar{x_1}$ and $\bar{x_2}$ are independent, then:

$$ 
SE(\bar{x_2} - \bar{x_1}) = \sqrt{ (SE(\bar{x_1}))^2 +(SE(\bar{x_2}))^2}
$$

::: exercise-box
Exercise: two-sample z-test (proportions.)

*Last month, the president's approval rating in a sample of 1000 likely voters was 55%. This month, a poll of 1,500 likely voters resulted in a rating of 58%. Is this sufficient evidence to conclude that the rating has changed?*

$\hat{p_1} = 55%$ and $\hat{p_2} = 58%$ $$
z = \frac{(\hat{p_2}-\hat{p_1})-0}{SE_{diff}}=\frac{(\hat{p_2}-\hat{p_1})-0}{\sqrt{ (SE(\bar{x_1}))^2 +(SE(\bar{x_2}))^2}}
$$ The formula for the standard error for the proportion is

$$ 
SE= \sqrt{\frac{p(1-p)}{n}}
$$ Calculate the standard errors:

For $\hat{p_1}$: $$
SE(\hat{p_1}) = \sqrt{\frac{\hat{p_1}(1 - \hat{p_1})}{n_1}} = \sqrt{\frac{0.55 \times 0.45}{1000}}= \sqrt{\frac{0.2475}{1000}} = \sqrt{0.0002475} \approx 0.0157
$$

For $\hat{p_2}$: $$
SE(\hat{p_2}) = \sqrt{\frac{\hat{p_2}(1 - \hat{p_2})}{n_2}} = \sqrt{\frac{0.58 \times 0.42}{1500}} = \sqrt{\frac{0.2436}{1500}} = \sqrt{0.0001624} \approx 0.0127
$$ $$
SE_{diff} = \sqrt{(0.0157)^2 + (0.0127)^2} = \sqrt{0.00024649 + 0.00016129} = \sqrt{0.00040778} \approx 0.0202
$$

$$
z = \frac{(0.58 - 0.55) - 0}{0.0202} = \frac{0.03}{0.0202} \approx 1.49
$$ The calculated z-value is approximately 1.49. To determine if this is statistically significant, you would compare this z-value to the critical value from the standard normal distribution (typically 1.96 for a 95% confidence level). Since 1.49 is less than 1.96, you would fail to reject the null hypothesis at the 95% confidence level, indicating that there is not sufficient evidence to conclude that the president's approval rating has changed significantly.

The $p$-value can be calculated using standard normal distribution tables or software

```{r}
pValue <- pnorm(1.49)
pValue
```

Since this is a two-tailed test (we are checking if the approval rating has changed, not just increased or decreased), we need to consider both tails of the distribution, so we double our values: The $p$-value is calculated as $2 \times (1 - \text{cumulative probability})$. $p = 2 \times (1 - 0.9318) = 2 \times 0.0682 = 0.1364$

The $p$-value for the z-value of 1.49 is approximately 0.1364. Since this $p$-value is greater than the typical significance level of 0.05, we fail to reject the null hypothesis. This means there is not sufficient evidence to conclude that the president's approval rating has changed significantly.
:::

The exercise above shows how to calculate the p value for proportions, if we are working with average values instead of proportions the calculation is the same, only thing to consider is that in this case is that Standard deviation of each individual sample will be calculated using the formula for the standard deviation for the mean $SE(\bar{x_1})= \frac{\sigma_1}{\sqrt{n_1}}= \frac{s_1}{\sqrt{n_1}}$ . If the sample sizes are not large, then the $p$-value needs to be computed from the t-distribution instead.

### The Welch Two Sample t-test

In the example below we are going to use the attrition dataset to see if the average age of employees in two different departments is different?

```{r}
file <- here::here("data", "attrition1.xlsx")
attrition1 <- read_excel(file)

kable(head(attrition1))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()

attrition1 |> 
  group_by(Department) |> 
  dplyr::summarize(avg_age = mean(Age))

testResult <- t.test(Age ~ Department, 
       data = attrition1)

testResult$p.value

report(testResult)

```

## Confidence intervals for Two-Samples data.

We can use the formula for the standard error of the difference to also do a confidence interval calculation. The confidence interval for $p_2-p_1$ is $(\hat{p_2}-\hat{p_1}) \mp z \times SE(\hat{p_2}-\hat{p_1})$

were z is the z-score for the confidence level we are interested in.

::: exercise-box
in our example about the voters approval of the president:

the z-score value for a confidence level of 95% is 1.959964

$$
58-55 \mp 1.96 \times 0.0202  \approx [-0.0705,0.0105]
$$

If we want to resolve the same exercise using r code it gives similar but not exactly the same results:

```{r}
successes <- c(0.55 * 1000, 0.58 * 1500)

# Define the sample sizes
sample_sizes <- c(1000, 1500)

# Perform the two-sample z-test for proportions
test_result <- prop.test(successes, sample_sizes)

# Print the result
print(test_result)
```
:::

There are many statistical techniques for describing the difference between a single variable across two populations. The most universal is the **Welch two-sample confidence interval**, which is a statistical technique used to compare means between two independent groups, taking into account the unequal variances often encountered in real-world data, so it valid in nearly all circumstances. A few things to bear in mind:

-   It is a multi-sample procedure, not a multi-variable one. Use it when you're asking how much two samples differ in a single variable.

-   Like every other statistical tool in this course, it requires that all the observations be independent of one another (not to be used with time-line analysis)

-   Unless the data has extreme outliers or is highly asymmetric, it will give good results when both samples are of size 10 or more. If the samples are of size at least 30, it is fine under all but the most extreme circumstances.

The Welch two-sample confidence interval does not require that the samples are equal in size or that the populations have equal variances, unlike some other procedures.

We are going to use the AB_testing data we generated in the previous section and calculate the confidence intervals for the differences observed in one of our samples

```{r}
set.seed(27)  
rating <- sample(1:5, 27, replace = TRUE) 
product <- c(rep("A", 15), rep("B", 12)) 
AB_testing <- data.frame(product,rating)  
AB_testing |>    group_by(product)  %>%     
  dplyr::summarize(avg_rating = mean(rating))  

as.data.frame(report(AB_testing))  

testResult<- t.test(rating ~ product, data = AB_testing)  
testResult %>% report()  
testResult$conf.int  
```

The confidence interval says that with 95% confidence, the population mean difference is between `r testResult$conf.int[1]` and `r testResult$conf.int[2]`. This is actually saying that the difference in the means could be 0.

If you know or can assume that the variance of the two populations is equal, then you can use var.equal = TRUE in the t.test, and in this case instead of using a Welch Two sample t-test, it will use a Two sample t-test

```{r}
t.test(rating ~ product,         data = AB_testing,        var.equal = TRUE)  
testResult<- t.test(rating ~ product, data = AB_testing) 
testResult %>% report()  
testResult$conf.int  
```

In the example below we want to use our substance_abuse data set and know if there is a difference in the variable DLA_improvement based on the program that the patient was following. We are assuming that the variance is equal in both cases:

We can change the confidence level manually if we want:

```{r}
file <- here::here("data", "substance_abuse.xlsx") 
substance_abuse <- read_excel(file) 
substance_abuse$DLA_improvement <- substance_abuse$DLA2 - substance_abuse$DLA1 
t.test(DLA_improvement ~ Program,         data = substance_abuse,        conf.level = .99)
```

::: exercise-box
Exercises

**Exercise 1**: *Generate a 95% confidence interval for the difference in average monthly income between the research and development and sales departments of the company*

```{r}
testResult <- t.test(MonthlyIncome ~Department, 
                     data= attrition1)
testResult$conf.int
report(testResult)
```

The confidence interval does not include 0, which means that there is a difference in the means of the populations of both groups

**Exercise 2**: *It is reasonable to claim that the montly rate is the same bewteen these two departments?*

```{r}
testResult <- t.test(MonthlyRate ~Department, 
                      data= attrition1,
                      mu =0)
testResult$p.value
report(testResult)
```

In light of the results we cannot reject the null hypothesis that they have the same monthly rate.
:::

### Pooled estimate:

going back to our recent exercise of the voters's approval rate, we concluded that the two different surveys did not significantly differed.

Since we could not reject the null hypothesis that the two samples are representing the same approval for the candidate, we can combine the two of them to find a better estimate of our Standard Error

in the first sample we have 0,55 x 1000 voters who approved, in the second sample we have 0.58x 1500 so in total we have 1420 approvals out of 2500 people surveyed, so our ppoled estimate will be $\frac{1420}{2500}=56.8\%$ so now we can calculate the Standard Error using that new value:

$$ 
SE(\hat{p_2}-\hat{p_1}) = \sqrt{\frac{0.568(1-0.568)}{1000}+\frac{0.568(1-0.568)}{1500}}=0.02022 
$$

### Pooled standard deviation:

If we know (or there is reason to assume) that the standard deviation of the two populations is the same $\sigma_1=\sigma_2$ then we can use the pooled estimate:

$$ 
s^2_{pooled}=\frac{(n_1-1)s^2_1+(n_2-1)s^2_2}{n_1+n_2-2} 
$$

::: {.callout-orange appearance="simple" icon="false"}
**Two-Sample Z-Test vs Two sample T-test vs Welch's Two-Sample T-Test**

**Two sample Z-test**

When to Use:

-   **Known Population Variances**: The population variances are known.

-   **Large Sample Sizes**: Typically used when the sample sizes are large (n \> 30).

-   **Normal Distribution**: Assumes that the data follows a normal distribution.

**Two sample T-test**

-   **Unknown Population Variances**: The population variances are unknown and assumed to be equal.

-   **Small Sample Sizes**: Typically used when the sample sizes are small (n \< 30).

-   **Normal Distribution**: Assumes that the data follows a normal distribution.

**Welch's Two-Sample T-Test**

When to Use:

-   **Unknown and Unequal Population Variances**: The population variances are unknown and not assumed to be equal.

-   **Small or Unequal Sample Sizes**: Can be used for small or unequal sample sizes.

-   **Normal Distribution**: Assumes that the data follows a normal distribution.

**Summary:**

-   **Population Variances**:

    -   **Z-Test**: Assumes known and equal population variances.

    -   **Welch's T-Test**: Does not assume equal variances and uses sample variances.

-   **Sample Size**:

    -   **Z-Test**: Typically used for large sample sizes.

    -   **Welch's T-Test**: Can be used for small or unequal sample sizes.
:::

## Paired-t test

What do we do when we have two samples, but they are not independent from each other? In this case we cannot use the classical two-sample z-test or two-sample t-test

We want to answer the question: Do husbands tend to be older than their wives?

| Husband's age | Wife's age | age difference |
|---------------|------------|----------------|
| 43            | 41         | 2              |
| 71            | 70         | 1              |
| 32            | 31         | 1              |
| 68            | 66         | 2              |
| 27            | 26         | 1              |

In a scenario like this, even if the samples were independent, the test would not give us a significant difference because the difference between the two pairs is always small, while the difference in the values in each sample (standard deviations) are large and what the two samples test does is to compare the differences to the fluctuation within each population.

In this case we will use the paired t-test. Our $H_0$ is that the population difference is 0. The formula for this test is:

$$
t=\frac{\bar{d}-0}{SE_{(d)}}
$$

where $\bar{d}$ is the average of the differences,

0 is the expected difference that under our null hypothesis is 0 and $SE_{\bar{(d)}}$ is the standard error for the difference:

$$
SE_{(d)}= \frac{\sigma_d}{\sqrt{n}} = \frac{s_d}{\sqrt{n}}
$$

In the example with our data:

$$
t=\frac{1.4}{\frac{0.55}{\sqrt{5}}}=5.69
$$

and now we have to use a table of a student t-distribution with 4 degrees of freedom to find the area under the curve of the normal distribution to the right of our t value.

we can use R code to calculate it like this:

```{r}
# Given values
t_value <- 5.69
degreesfreedom <- 4

# Calculate the p-value for a two-tailed test
p_value <- 2 * pt(-abs(t_value), degreesfreedom)
p_value

```

In this case our result means that we can reject the null hypothesis.

## The sign test

Image that we don't know the age difference, we only know if the husbands are older or not. We can follow here the same approach as with a binomial distribution, like the coin toss. In this specific case our null hypothesis $H_0$ is that half of the husbands are older than their wifes (no difference). We assign labels to the results, for example 1 if the husband is older than the wife and 0 otherwise.

$$
z=\frac{sum\ of\ 1s -\frac{n}{2}}{SE\ of\ sum} =\frac{sum\ of\ 1s -\frac{n}{2}}{\sqrt{n}\times\sigma_{H_0}}
$$

in our case we have 5 husbands being older than their wifes, so 5 1s and the standard deviation for our null hypothesis will be $\frac{1}{2}$ because we expect half the husbands to be older then their wives.

$$
z= \frac{5-\frac{5}{2}}{\sqrt{5}\frac{1}{2}}= 2.24
$$

now we can find the p value using a table or software:

```{r}
z_score<- 2.24
# Calculate the p-value for a two-tailed test
p_value <- 2 * (1 - pnorm(z_score))
p_value
```

we can see that the result of this test is less significant than the test we did before, this is because we have less data to work with.

If we want to resolve the problem using software we can do the calculations:

```{r}
# Number of successes (husbands older than wives) 
successes <- 5  
# Total number of pairs 
n <- 5 
# Z-test (Sign test approximation) 
z_score <- (successes - 0.5 * n) / sqrt(0.25 * n) 
z_p_value <- 2 * (1 - pnorm(z_score)) 
z_p_value
```

but if we use the corresponding binomial test instead that would apply for this scenario, we get to quite a different result:

```{r}
# Number of successes (husbands older than wives) 
successes <- 5  
# Total number of pairs 
n <- 5  
# Perform the binomial test 
test_result <- binom.test(successes, n, p = 0.5, alternative = "two.sided")  

print(test_result)
```

::: {.callout-orange, appearance="simple", icon="false"}

Both the two-sample paired t-test and the sign test are used to compare paired data, but they are applied in different situations based on the assumptions and characteristics of the data.

**Two-Sample Paired T-Test**

**When to Use**:

-   **Normal Distribution**: The differences between the paired observations should be approximately normally distributed.

-   **Interval or Ratio Data**: The data should be measured on an interval or ratio scale.

-   **Parametric Test**: This test is parametric, meaning it relies on assumptions about the distribution of the data.

**Example**:

-   Comparing the blood pressure of patients before and after a treatment.

-   Measuring the weight of individuals before and after a diet program.

**Sign test**

1.  **Paired Observations**: When you have paired data (e.g., before and after measurements) and you want to test if there is a consistent difference between the pairs. For example, testing if a treatment has an effect by comparing measurements before and after the treatment.

2.  **Median Differences**: When you want to test if the median of differences between pairs is zero. This is useful when the data does not meet the assumptions required for parametric tests like the paired t-test. It does not rely on assumptions about the distribution of the data.

3.  **Non-Normal Data**: When the data does not follow a normal distribution, making parametric tests inappropriate. The sign test does not assume any specific distribution for the data.

4.  **Ordinal Data**: When the data is ordinal (ranked) rather than interval or ratio. The sign test can handle data that can only be compared as greater than, less than, or equal to.

[**Example Scenarios**]{.underline}

-   **Medical Studies**: Comparing the effectiveness of a treatment by measuring patient conditions before and after the treatment.

-   **Quality Control**: Testing if a new manufacturing process consistently produces better results than the old process.

-   **Behavioral Studies**: Comparing responses before and after an intervention. Comparing the number of days patients feel better before and after a new medication.

[**Summary:**]{.underline}

-   **Use a paired t-test** when the differences between paired observations are normally distributed and you have interval or ratio data.

-   **Use a sign test** when the data does not meet the normality assumption, is ordinal, or you prefer a non-parametric approach. :::

## Wilcoxon Rank Sum Test

We learned how the sample mean and SD are susceptible to outliers. The t-test is based on these measures and is susceptible as well. The Wilcoxon rank test (equivalent to the Mann-Whitney test) provides an alternative. In the code below, we perform a t-test on data for which the null is true. However, we change one sum observation by mistake in each sample and the values incorrectly entered are different. Here we see that the t-test results in a small $p$-value, while the Wilcoxon test does not:

```{r}
set.seed(779) ##779 picked for illustration purposes
N=25
x<- rnorm(N,0,1)
y<- rnorm(N,0,1)
```

Create outliers:

```{r}
x[1] <- 5
x[2] <- 7
cat("t-test pval:",t.test(x,y)$p.value)
cat("Wilcox test pval:",wilcox.test(x,y)$p.value)
```

The basic idea is to 1) combine all the data, 2) turn the values into ranks 3) separate them back into their groups, and 4) compute the sum or average rank and perform a test.

```{r rank-test-illustration, fig.cap="Data from two populations with two outliers. The left plot shows the original data and the right plot shows their ranks. The numbers are the w values ",fig.width=10.5,fig.height=5.25}
library(rafalib)
mypar(1,2)

stripchart(list(x,y),vertical=TRUE,ylim=c(-7,7),ylab="Observations",pch=21,bg=1)
abline(h=0)

xrank<-rank(c(x,y))[seq(along=x)]
yrank<-rank(c(x,y))[-seq(along=x)]

stripchart(list(xrank,yrank),vertical=TRUE,ylab="Ranks",pch=21,bg=1,cex=1.25)

ws <- sapply(x,function(z) rank(c(z,y))[1]-1)
text( rep(1.05,length(ws)), xrank, ws, cex=0.8)
W <-sum(ws) 
```

`W` is the sum of the ranks for the first group relative to the second group. We can compute an exact $p$-value for $W$ based on combinatorics. We can also use the CLT since statistical theory tells us that this `W` is approximated by the normal distribution. We can construct a z-score as follows:

```{r}
n1<-length(x);n2<-length(y)
Z <- (mean(ws)-n2/2)/ sqrt(n2*(n1+n2+1)/12/n1)
print(Z)
```

Here the `Z` is not large enough to give us a $p$-value less than 0.05. These are part of the calculations performed by the R function `wilcox.test`.

we are not going to get into mathematical detail about these calculations, but the formula for the z score here is $$
 z=\frac{U - \frac{n_2}{2}}{\sqrt{\frac{n_2(n_1+n_2+1)}{12n_1}}}
$$ and to perform this test in r we use: `wilcox.test(x,y)`

## Two-samples of binary data

Two observed proportions for a single binary variable can be compared directly using the **two-proportion z-confidence interval** and **two-proportion z-test**. These apply when there are at least 5 observations of each type in each of the two groups. The formulas are relatively simple. For example a 95% confidence interval for the difference between proportions is

$$
(p_2-p_1) = (\hat p_2-\hat p_1) \pm 1.96 \sqrt{\frac{\hat p_2(1-\hat p_2)}{n_2}+\frac{\hat p_1(1-\hat p_1)}{n_1}} 
$$

where $p_1$ and $p_2$ are the population proportions, $\hat p_1$ and $\hat p_2$ are the observed sample proportions and $n_1$ and $n_2$ are the sample sizes. R will use a improved version of this formula when computing the proportions.

In R we will use $prop.test(n_s , sample size)$ where $n_s$ is the number of successes.

::: exercise-block
Examples:

in the attrition dataset. *Are the attrition proportions different for the two departments?*

```{r}
kable(head(attrition1))%>%
  kable_styling(latex_options = "scale_down")%>%
  landscape()

table(attrition1$Department)
t<- table(attrition1$Department,
      attrition1$Attrition)
t
yes_counts<- as.numeric(t[,2])
sampleSize <-as.numeric(table(attrition1$Department))

testResult <- prop.test(yes_counts, sampleSize)
testResult
```

Our $p$-value `r testResult$p.value` is very low which means we can reject the default null hypothesis (the difference in the two samples is 0). It is also giving us the proportions for the samples in the two different departments: `r testResult$estimate` . And the confidence intervals is giving us the difference in the proportion of the two departments (as in prop1 - prop2). In this case being negative means that the second department has a higher attrition rate. The order of the variables is as entered in the vectors, so if *yes_counts* had *Research_Development* first and *Sales* second, prop1 will be for *Research_Development* and prop2 for *Sales*
:::

## Two-samples of categorical variables

Two samples of a single categorical variable can be compared with the $x^2-test$ for homogeneity (Chi-squared). Under the hood, this is just a goodness-of-fit test of the hypothesis that the observed proportions in one of the samples are the same as those in the pooled sample.

### Testing homogeneity

The $\chi^2$ test of homogeneity test the null hypothesis that the distribution of a categorical variable is the same for several populations. It assumes that the samples are drawn independently within and across populations.

See how we can apply this logic to our Titanic survival data:

|          | First | Second | Third | Crew |
|----------|-------|--------|-------|------|
| Survived | 202   | 118    | 178   | 215  |
| Died     | 123   | 167    | 528   | 698  |

Note that in this case we are not sampling from a population. The data are not a random sample of the people on board, rather the data represent the whole population. Son in this case the chance process resulting in survival or death is not the sampling, but the result of random events occurring when looking for a way out of the ship, like getting into a life boat or into the water, being rescued from the water on time, etc. Then the 325 observations of first class passengers represent 325 independent draws from a probability histogram that gives a certain chance for survival. The 285 observations about second class passengers are drawn from the probability histogram for second class passengers, which may be different. The null hypothesis says that the probability of survival is the same for all four probability histograms. According to this hypothesis we can calculate the probability of survival by pooling all the data = $\frac{713}{2229}=32\%$ with this number we can calculate the expected number of surviving passengers for each class:

| Surviving | First | Second | Third | Crew  |
|-----------|-------|--------|-------|-------|
| Observed  | 202   | 118    | 178   | 215   |
| Expected  | 104.0 | 91.2   | 225.8 | 292.1 |

| Died     | First | Second | Third | Crew  |
|----------|-------|--------|-------|-------|
| Expected | 221.0 | 193.8  | 480.1 | 620.8 |
| Observed | 123   | 167    | 528   | 698   |

Now we can compare our chi statistic as we learned, using all the differences between expected and observed values:

$$
\chi^2 =\sum_{categories} \frac{(observed-expected)^2}{expected}
$$

$$
=\frac{(202-104)^2}{104}+\frac{(123-221)^2}{221}+\cdots =192 
$$

in this case our degrees of freedom are calculated like this: (4-1)\*(2-1) = 3 where 4 is for the number of categories, and 2 is for the two rows of results we are dealing with (surviving and died)

In our case our p value will be extremely small, sugesting we should reject the null hypothesis that all ticket classes had the same posibility of survival.

```{r}
x <- 192.2
df <- 3

# Calculate the p-value
p_value <- pchisq(x, df, lower.tail = FALSE)

# Print the p-value
p_value

```

The test in r:

```{r}
# Create the contingency table
titanic_data <- matrix(c(202, 118, 178, 215, 123, 167, 528, 698), 
                       nrow = 2, 
                       byrow = TRUE,
                       dimnames = list(Survival = c("Survived", "Died"),
                                       Class = c("First", "Second", "Third", "Crew")))

# Print the contingency table
print(titanic_data)

# Perform the chi-squared test of homogeneity
chi_squared_test <- chisq.test(titanic_data)

# Print the test results
print(chi_squared_test)

```

::: exercise-box
Exercise:

*Use the substance_abuse data set to decide if the distribution of mental health diagnosis is the same for those with and without at least one psychiatric admission.*

```{r}
as.data.frame(report(substance_abuse$MHDx))
as.data.frame(report(substance_abuse$PsychAdmit))

substance_abuse <- substance_abuse %>% mutate(previousAdmit = ifelse(substance_abuse$PsychAdmit == 0, FALSE, TRUE))

t<- table( substance_abuse$previousAdmit,substance_abuse$MHDx)
t
chisq.test(t)
```

In this case according to our $p$-value we cannot say that patients that had at least one psychiatric admission in the past have the same proportion of diagnosis than patients without any admission.
:::

Chi squared test for homogeneity is not saying anything specific about any of these diagnosis, it is simply saying that the distribution of these diagnosis between those two groups (admitted vs not admitted) is not the same.

Is the distribution of SUDx the same for both men and women in our Substance Abuse data?

```{r}
head(substance_abuse)
as.data.frame(report(substance_abuse$SUDx))
```

```{r}

t <- table(substance_abuse$Gender, 
           substance_abuse$SUDx)
head(t)
```

Our null hypothesis is that the distribution between gender is the same.

```{r}
result <- chisq.test(t)
report(result)

```

In this case the $p$-value of the test is over 0.05 so that indicates that there is no significance difference by sex for the SUDx variable, so men and women are abusing the different categories of substances in the same proportions. The df in our results are the degrees of freedom that is the number of categories minus one.

In reality this is the same as doing a Goodness of fit test as we saw before, if we remember it was $chisq.test(p_s , p)$ where $p_s$ was the proportions of the different categories in our sample and $p$ the proportion of the population we wanted to test against. Back to our example what we are measuring here is the proportion of women or men against the proportion of the full sample, that will tell us if there is a difference between men and women, so we can also write the test like this getting similar results:

```{r}
sudxProp <- table(substance_abuse$SUDx)/sum(table(substance_abuse$SUDx))

women <- t[1,]

result<- chisq.test(women, p= sudxProp)
report(result)
```

### Independence Testing of Categorical Variables

We want to answer this question: Is gender (M/F) related to voting preference (liberal/conservative)? Now we have two categorical variables: gender and voting preference. The null hypothesis is that the two variables are independent. The alternative hypothesis is that there is some kind of association.

The tool we are going to use is the chi-square test ( $x^2$ ) for independence. This test can be used to test whether two categorical variables are independent or not. That is, whether knowledge about one provides information about the other. The math is exactly the same as for the chi-square test for homogeneity, hence, so is the sintax in most statistical packages, including R, but with a few things to bear in mind:

-   Technically, the null hypothesis of a $x^2$ test for independence is that in the larger population, the probability of an observation being in any specific pair of categories is equal to the product of the probabilities of being in each of the individual categories.

-   This is another omnibus test: it says nothing about particular categories.

-   Larger cell counts are better, as usual. Do not do this test if any of the counts are less than 5.

We are going to use our product_rating dataset to see the relationship between age groups and the product the user purchased.

first we can just see the contingency table for those two variables

```{r}
file <- here::here("data", "product_ratings.xlsx")
product_ratings <- read_excel(file)
t <- table(product_ratings$age,
           product_ratings$product)
t
```

```{r}
chisq.test(t)
```

The low $p$-value from this test is indicating that the theory that these two categorical variables are independent is implausible.

::: {.callout-orange appearance="simple" icon="false"}
Chi-Squared is used for testing the independence of categorical variables. It compares observed frequencies in a contingency table to expected frequencies under independence.
:::

::: exercise-box
Exercises

*Referring to the substance_abuse data set:*

-   *Is there any evidence of an association between Substance Use Diagnosis and Mental Health Diagnosis among patients receiving an intervention?*

```{r}
treatment <- filter(substance_abuse, 
                    Program == "Intervention")
t<- table (treatment$SUDx, treatment$MHDx)
chisq.test(t)
```

According to this result we cannot conclude that there is a relationship between these two variables.

-   *Is there any evidence of an association between SUDx and DLA_improvement among patients receiving an intervention?*

```{r, fig.align="center"}
ggplot(treatment, aes(x= SUDx, y = DLA_improvement))+
  geom_boxplot()

testResult <- aov( DLA_improvement ~ SUDx, data = treatment)
summary(testResult)
```

The difference is significant but not by large as the p value shows that in 3% of the cases we could observe these data differences in a sample from the a population where their category means is the same.
:::

If we run a TukeyHSD test on these we can compare each pair individually and we find out that actually the only pair that shows a statistically significant difference is alcohol with opioids.

```{r}
TukeyHSD(testResult)
```

It is easy to confuse the testing for homogeneity and the testing for independence.

### Comparing the different uses of the chi-square test:

|                 | sample                                                  | research question                                                                   |
|------------------|----------------------|--------------------------------|
| goodness of fit | single categorical variable. one sample                 | Are the counts of the different categories matching our expected results            |
| homogeneity     | single categorical variable measured on several samples | Are the groups homogeneous (have the same distribution of the categorical variable) |
| independence    | two categorical variables measured on one sample        | Are the two categorical variables independent?                                      |

Examples:

-   we want to know if there is more births in different week days (Monday, Tuesday...). We record the week day of 300 births. What test should we use to know if there is a difference per day? --\> We would use chi-square test for goodness of fit.

-   A food delivery start-up decides to advertise its service by placing ads on web pages. They wonder whether the percentage of viewers who click on the ad changes depending on how often the viewers were shown the ad. They randomly select 100 viewers from among those who were shown the add once, 135 from among those who were shown the add twice, and 150 from among those who were shown the ad three times. --\> We would use chi-square test of homogeneity.

-   An airline wants to find out whether there is a connection between the customer's status in its frequent flyer program and the class of ticket that the customer buys. It samples 1,000 ticket records at random and for each ticket notes the status level ('none', 'silver', 'gold') and the ticket class ('economy', 'business','first'). --\> chi-square test of independence

-   A county wants to check whether the racial composition of the teachers in the county corresponds to that of the population in the county. It samples 500 teachers at random and wants to compare that sample with the census numbers about the racial groups in that county. --\> chi-square test for goodness of fit

# Permutation Tests

Suppose we have a situation in which none of the standard mathematical statistical approximations apply. We have computed a summary statistic, such as the difference in mean, but do not have a useful approximation, such as that provided by the CLT. In practice, we do not have access to all values in the population so we can't perform a simulation. Permutation tests can be useful in these scenarios.

We are back to the scenario were we only have 10 measurements for each group.

```{r, fig.align='center'}
dat=read.csv("data/femaleMiceWeights.csv")

control <- filter(dat,Diet=="chow") %>% dplyr::select(Bodyweight) %>% unlist
treatment <- filter(dat,Diet=="hf") %>% dplyr::select(Bodyweight) %>% unlist
obsdiff <- mean(treatment)-mean(control)
obsdiff
```

In previous sections, we showed parametric approaches that helped determine if the observed difference was significant. Permutation tests take advantage of the fact that if we randomly shuffle the cases and control labels, then the null is true. So we shuffle the cases and control labels and assume that the ensuing distribution approximates the null distribution. Here is how we generate a null distribution by shuffling the data 1,000 times:

```{r, fig.align='center'}
N <- 12
avgdiff <- replicate(1000, {
    all <- sample(c(control,treatment))
    newcontrols <- all[1:N]
    newtreatments <- all[(N+1):(2*N)]
  return(mean(newtreatments) - mean(newcontrols))
})
hist(avgdiff)
abline(v=obsdiff, col="red", lwd=2)
```

How many of the null means are bigger than the observed value? That proportion would be the $p$-value for the null. We add a 1 to the numerator and denominator to account for misestimation of the $p$-value. The $p$-value here is calculated directly as the percentage of values higher than our number of interest.

```{r}
#the proportion of permutations with larger difference
(sum(abs(avgdiff) > abs(obsdiff)) + 1) / (length(avgdiff) + 1)

```

Now let's repeat this experiment for a smaller dataset. We create a smaller dataset by sampling:

```{r, fig.align='center'}
N <- 5
control <- sample(control,N)
treatment <- sample(treatment,N)
obsdiff <- mean(treatment)- mean(control)
obsdiff

avgdiff <- replicate(1000, {
    all <- sample(c(control,treatment))
    newcontrols <- all[1:N]
    newtreatments <- all[(N+1):(2*N)]
  return(mean(newtreatments) - mean(newcontrols))
})
hist(avgdiff)
abline(v=obsdiff, col="red", lwd=2)
```

Now the observed difference is not significant using this approach. Keep in mind that there is no theoretical guarantee that the null distribution estimated from permutations approximates the actual null distribution. For example, if there is a real difference between the populations, some of the permutations will be unbalanced and will contain some samples that explain this difference. This implies that the null distribution created with permutations will have larger tails than the actual null distribution. This is why permutations result in conservative $p$-values. For this reason, when we have few samples, we can't do permutations.

Note also that permutations tests still have assumptions: samples are assumed to be independent and "exchangeable". If there is hidden structure in your data, then permutation tests can result in estimated null distributions that underestimate the size of tails because the permutations may destroy the existing structure in the original data.

# Multiple test corrections

The **Bonferroni correction** is a statistical method used to address the problem of multiple comparisons. When you perform multiple hypothesis tests, the chance of making a Type I error (false positive) increases. The Bonferroni correction helps control this by adjusting the significance level.

What we do is, if there are m test, we multiply the $p$-values by m. This correction makes sure that P(any of the m test rejects in error) is still smaller than 5%. The Bonferroni correction is often very restrictive. It guards against having even one false positive among the m test. As a consequence the adjusted $p$-values may not be significant any more even if a noticeable effect is present, so it will only work if you don't have a large number of test.

Alternatively if the number of tests is large, it is better to use the **False Discovery Proportion (FDP)**:

$$
FDP= \frac{number\ of\ false\ discoveries}{total\ number\ of\ discoveries}
$$

where a discovery occurs when a test rejects the null hypothesis. If no hypothesis are rejected, FDP is defined to be 0.

The **False Discovery Rate (FDR)** is the expected proportion of false discoveries among the discoveries. One common method to control the FDR is the Benjamini-Hochberg procedure. This method adjust the $p$-values to control the FDR at a desired alpha level. The procedure is like this:

1.  Sort the $p$-values in ascending order. $p_{(1)}, p_{(2)}, \ldots, p_{(m)}$

2.  Find the largest k value such as $p_k=\frac{k}{m}\alpha$

3.  Reject the null hypothesis for all $p_i$ where $i\leq k$

In r:

```{r}
# Example p-values
p_values <- c(0.01, 0.04, 0.03, 0.002, 0.05, 0.20)

# Apply the Benjamini-Hochberg procedure
p.adjusted <- p.adjust(p_values, method = "BH")

print(p.adjusted)

```

This will give you the adjusted $p$-values, and you can compare them to your desired FDR level to decide which hypotheses to reject.

The **validation set approach**. With this approach you split your data set into two parts, one is a model building set and a validation set before the analysis. You may use data snooping (multiple testing) in the first set of data to find something interesting. And then you test this hypothesis on the validation set..

# Statistical Relationship and Analysis

## Introduction to Regression Models

The methods described above relate to **univariate** variables. In sciences it is common to be interested in the relationship between two or more variables.

Suppose that we observe a quantitative response Y and p different predictors or inputs (X1,X2, . . . ,Xp). We assume that there is some relationship between Y and X = (X1,X2, . . . ,Xp), which can be written in the very general form: $$
Y = f(X)+\epsilon
$$

$f$ represents the systematic information that X provides about Y and $\epsilon$ is a random error term, which is independent from X. In essence, statistical learning and regression techniques refers to a set of approaches for estimating $f$

There are two main reasons that we may wish to estimate $f$: prediction and inference.

### Prediction

In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y: $$
\hat{Y}= \hat{f}(X)
$$ where $\hat{f}$ represents our estimate for $f$ and $\hat{Y}$ represents the resulting prediction for Y. In this setting $\hat{f}$ is often treated as a black box and we are not tipically concerned with the extact function, provided that it yiedls accurate predictions of Y.

### Inference

We are often interested in understanding the association between Y and X1, . . . ,Xp. In this situation we wish to estimate f, but our goal is not necessarily to make predictions for Y . Now ˆ f cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions: • Which predictors are associated with the response? It is often the case that only a small fraction of the available predictors are substantially associated with Y . Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application. • What is the relationship between the response and each predictor? Some predictors may have a positive relationship with Y , in the sense that larger values of the predictor are associated with larger values of Y . Other predictors may have the opposite relationship. Depending on the complexity of f, the relationship between the response and a given predictor may also depend on the values of the other predictors. • Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Historically, most methods for estimating f have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.

Relationships between variables are usually better visualized using scatterplots:

```{r, fig.align='center'}

data(father.son,package="UsingR") 
x <- father.son$fheight
y <- father.son$sheight
plot(x,y,xlab="Father's height in inches",ylab="Son's height in inches",main=paste("correlation =",signif(cor(x,y),2)))
```

The scatter plot shows a general trend: the taller the father the taller to son. A summary of this trend is the correlation coefficient which in this case is 0.5. We motivate this statistic by trying to predict son's height using the father's.

Quick facts:

::: {#CorrelationFunFacts style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
-   Correlation's curveball: Correlation doesn't imply causation. Two variables can move together without one causing the other, often due to lurking third variables.

-   Residual Revelations: Residuals plots, which showcase deviations from a model, can often tell more about data relationships than the fit itself, hinting at non-linearity, heteroscedasticity, or other intricacies.

-   Multicollinearity Maze: In multiple regression, if predictors are too related, it can muddy interpretations.
:::

# Linear Models

Linear models are the simplest structural models. It assumes that the dependence of Y on the predictors $X_1,X_2,\dots X_p$ is linear. We represent our function like this: $$
f_L(X)=\beta_0+\beta_1X_1+\beta_2X_2+\dots\beta_pX_p
$$ it has $p$ features and $p+1$ parameters $\beta$. We can estimate the parameters of the model by fitting the model to training data.

we assume a model:

$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+\dots\beta_pX_p+\epsilon
$$

Where $\epsilon$ is some noise or errors in the function, $\beta_0$ and $\beta_p$ are unknown constants that represent the intercept and the slope, also known as coefficients or parameters.

Given some estimates $\hat{\beta_0}$ and $\hat{\beta_p}$ for the model coefficients we predict the values of Y using the model: $\hat{y} = \hat{\beta_0}+\hat{\beta_1}x+\dots\hat{\beta_p}x$ the hat symbol over the variables indicate that they are an estimated value.

A linear model is almost never correct, but it is usually a good approximation to the real function of the data. There are other models that may fit the data better, but there is a trade off in interpretability. Linear model have less prediction accuracy but are easy to interpret.

## Correlation coefficient {#correlation-coefficient}

If we standardize the values of our data, the line that we use to predict one value from the other follows a slope. That slope is the correlation.

In our substance_abuse dataset we have two variables for each observation DLA1 and DLA2 that we can plot in a scatter plot to see their relationship:

```{r, fig.align="center"}

treatment <- dplyr::filter(substance_abuse,
                    Program == "Intervention")

ggplot(treatment, aes(x = DLA1, y = DLA2)) + 
  geom_point() # Linear

```

It shows a linear relationship between the two variables: one one increase the other tend to increase as well.

To show other possible relationships between data we will generate a dataset that shows a non-linear relationship

```{r, fig.align='center'}
set.seed(7)
x <- abs(rchisq(100, 10) - 2.7)
y <- 1/x + rnorm(100, 1, .1) - .9
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()
```

The **correlation coefficient formula**:

$$
r = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s_x} \right) \times \left( \frac{y_i - \bar{y}}{s_y} \right)
$$

This formula standardizes the values of ( x ) and ( y ) by subtracting their means $\bar{x}$ and $\bar{y}$ and dividing by their standard deviations $s_x$ and $s_y$, then calculates the average product of these standardized values.

**Sample correlation** measures the strength of an observed **linear** relationship between two variables in a data set. Fundamentally, it measures the spread (or variability) of sample data along a line of best fit

As a convention, the variable on the horizontal axis is called **explanatory variable or predictor** and the one in the vertical axis is called **response variable.**

```{r, fig.align='center'}
ggplot(treatment, aes(x = DLA1, y = DLA2)) + 
  geom_point() +
  geom_smooth(method = "lm",
                           se = FALSE,
                           color = "grey")
```

To calculate the correlation we just have to use the command 'cor' and it's value will go from -1 to 1. The sign of r gives the direction of the association and its absolute value gives the strength.

Since both x and y were standardized when computing r, r has no units ans it is not affected by changing the center or the scale of either variable.

```{r}
cor(treatment$DLA1,
    treatment$DLA2)
```

Be careful to **refer to correlation only when a pair of variables has a linear relationship**. For instance, the following variables have a clear association, but their coefficient of correlation is close to 0:

```{r fig.align='center'}
set.seed(2)
x <- rnorm(100, 0, .5)
y <-  (x^2-1) + rnorm(1.5, -2.5, .8)
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()

cor(x,y)

```

We need to be aware that when we are working with samples, we may find correlation between two variables just by random chance. For example, if we plot the variable DLA_improvement against Age we can see that there is no correlation between the two:

```{r, fig.align='center'}
ggplot(treatment, aes(Age, y = DLA_improvement)) + 
  geom_point() # No association

cor(substance_abuse$Age, substance_abuse$DLA_improvement)
```

but if we get a random sample of those variables, we may find some correlation:

```{r, fig.align='center'}
set.seed(0)

sample <- slice_sample(treatment, n = 25)
cor(sample$Age, sample$DLA_improvement)
ggplot(sample, aes(x = Age, y = DLA_improvement)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "grey")
```

In the next chapters we will learn how to find out if the correlation found in our sample shows a true relationship in the population or if it is due to random chance.

Remember that we already saw that the correlation coefficient is very sensitive to outliers. (See [spearman correlation](#spearman-correlation))

## Covariance

Covariance measures the extent to which two variables change together. If the variables tend to increase together, the covariance is positive. If one tends to increase while the other decreases, the covariance is negative. For variables X and Y: $$ \text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})$$ {#eq-covariance}

Covariance and correlation are related, but they're not the same thing:

Covariance:

-   Measures: The degree to which two variables change together.

-   Scale: It's dependent on the units of the variables, making it difficult to interpret the strength of the relationship.

Correlation:

-   Measures: The strength and direction of the linear relationship between two variables.

-   Scale: Standardizes the measure of covariance, ranging between -1 and 1.

Summary:

Covariance tells you if two variables tend to increase or decrease together, but not how strong that relationship is.

Correlation standardizes this measure, making it easier to interpret the strength and direction of the relationship on a consistent scale.

So, while covariance gives you a raw measure of how two variables vary together, correlation refines it to a standardized measure of that relationship.

Calculating the *covariance matrix* in r is easy using the `cov()` function:

```{r}
# Sample data frame
data <- data.frame(X = c(4, 6, 8, 10, 12), Y = c(2, 4, 6, 8, 10), Z = c(1, 2, 3, 4, 5))

# Calculate covariance matrix
cov_matrix <- cov(data)
print(cov_matrix)
```

How to Interpret: Diagonal Elements: These are the variances of the individual variables.

-   Var(X): 10.00

-   Var(Y): 10.00

-   Var(Z): 2.50

Off-Diagonal Elements: These are the covariances between pairs of variables.

-   Cov(X, Y): 10.00

-   Cov(X, Z): 5.00

-   Cov(Y, Z): 5.00

Example Interpretation: Variance: The variance of X is 10, meaning X values are spread out with a variance of 10 around their mean.

Covariance (X and Y): The positive covariance of 10 between X and Y suggests that when X increases, Y tends to increase as well.

Covariance (X and Z): The positive covariance of 5 between X and Z indicates a positive relationship, though not as strong as between X and Y.

Covariance (Y and Z): The positive covariance of 5 between Y and Z also indicates a positive relationship.

In essence, the covariance matrix provides insights into how pairs of variables vary together, helping you understand relationships and dependencies in your data.

## Regression Line and the Method of Least Squares (RSS) {#Residualsumofsquares}

If the scatterplot shows a linear association, then this relationship between our data points can be summarized by a line. The equation for a line is $\hat{y}*i=a+bx_*i$. The idea is to choose the line that minimizes the sum of the squared distances between the observed $y_i$ and the predicted $\hat{y_i}$

The method of least squares is the method we use to minimize this **Residual sum of squares (RSS)**

$$
RSS=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 
$$ {#eq-RSSsimpleRegression}

$$
RSS=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left( y_i - (\hat{\beta_o} + \hat{\beta_1} x_i) \right)^2
$$

$$
RSS = (y_1 - \beta_0 - \beta_1 x_{1})^2 + (y_2 - \beta_0 - \beta_1 x_{2})^2 + \cdots + (y_n - \beta_0 - \beta_1 x_{n})^2
$$

The intercept $\beta_0$ is the expected value of Y when X=0, in other words, the value at which the regression line crosses the Y axis.

$$
\hat{\beta_1}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
$$ $$
\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}
$$ where $\bar{x}$ and $\bar{y}$ are the sample means

It turns out that $\beta_1=r\frac{s_y}{s_x}$ where $r$ is the correlation coefficient and $s_y$ and $s_x$ are the two standard deviations and $\hat{\beta_0} = \bar{y} - \hat{\beta_i}\bar{x}$.

The main use of regression is to predict y from x. If we are given x, then we need $r, \bar{x},\bar{y},s_x, s_y$ to calculate the regression line. In code, we can compute that line using 'lm' in r language

::: exercise-box
Exercise: midterm vs final exam student's grades (cont)

*We know that the average score for the midterm exams was 49.5 and the average score for the final exam is 69.1, and the standard deviation for the midterm exams was 10.2 while the standard deviation for the final exams was 11.8. We also know that the correlation coefficient is 0.67.*

*Predict the final exam score of a student who scored 41 on the midterm.*

Sol: 41 is 8,5 below the midterm average. We want to standardize this value. We already know how to calculate this: $$z = \frac{x_1-\bar{x}}{sd} = \frac{41-49.5}{10.2}=-0.83$$ so now looking at the formula for the regression line we expect $y$ to be r times 0.83 times the standard deviation of the final exam below the average final score. $$\bar{y}\pm r*sd_{final}*0.83 =69.1 -0.67*11.8*0.83=62.5$$
:::

### Predicting x from y:

When predicting x from y it is a mistake to use the regression line that we calculated for regressing y on x and solve for x. This is because regressing x on y will result in a different regression line.

## Normal approximation in regression

Linear regression requires that the data in the scatter plot is more or less following an elypse shape. Once we have the scatter plot with the regression line, we know that given a value of X, the value of Y will be close to the y point of the regression line for the x value. In the image below, given x, we trace a vertical line to the regression line and then we trace a perpendicular line to the y axis (green line) and that is approximately the expected value of y.

```{r, fig.align='center', echo=FALSE}

set.seed(412)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Create a data frame
data <- data.frame(x, y)

# Fit a linear model
model <- lm(y ~ x, data = data)

# Choose a random value in x
random_x <- sample(x, 1)

# Predict the y value from the linear model
predicted_y <- predict(model, newdata = data.frame(x = random_x))

# Create the scatter plot with regression line and vertical/horizontal lines
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_vline(xintercept = random_x, linetype = "dashed", color = "red") +
  geom_hline(yintercept = predicted_y, linetype = "dashed", color = "green") 

```

but we also know that $y$ will not be necessarily exactly at that point, but at certain distance of it, and that approximation follows the normal curve, so we can then use normal approximation to calculate the value of $y$: subtract off the predicted value $\hat{y}$, then divide by $\sqrt{1-r^2} * s_y$.

$$ 
z = \frac{y_1-\bar{y}}{\sqrt{1-r^2}\cdot s_y} 
$$

::: exercise-box
Exercise:

*Among the students who scored around 41 on the midterm, what percentage scored above 60 on the final?*

We already predicted that the expected value for the score on the final exam for a student that scored 41 in midterm is 62.5. That means that the normal curve is centered at 62.5 so the percentage of students that will score above 60 is the area bellow that normal curve to the right of 60 (red line).

```{r, fig.align='center'}
#| echo =FALSE
# Create a sequence of x values
x <- seq.int(from =1,to=  125, by=1)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x, mean = 62.5, sd = 14, log=F)

# Plot the bell curve
plot(x, y, type="l", lwd=2, col="blue")
# Add a vertical line at x = 0.71
abline(v = 60, col = "red", lwd = 2, lty = 2)
abline(v = 62.5, col = "purple", lwd = 2, lty = 2)
```

For that we standardize 60 like this: $z = \frac{y_1 - \bar{y}}{\sqrt{1 - r^2} \cdot s_y} = \frac{60 - 62.5}{\sqrt{1 - 0.67^2} \cdot 11.8} = -0.29$ so the standardized curve would look like this:

```{r, fig.align='center'}
#| echo =FALSE
# Create a sequence of x values
x <- seq(from = -3, to = 3, by = 0.01)

# Calculate the corresponding y values for the standard normal distribution
y <- dnorm(x, mean = 0, sd = 1)

# Plot the bell curve
plot(x, y, type = "l", lwd = 2, col = "blue",
     main = "Standard Normal Distribution",
     xlab = "X-axis",
     ylab = "Density")

# Add a vertical line at x = 0.71
abline(v = -0.20, col = "red", lwd = 2, lty = 2)

```

now we just have to use a table or software to calculate that value

```{r, fig.align='center'}
zscore <- -0.29

# Calculate the area to the left of the z-score
area_left <- pnorm(zscore)

# Calculate the area to the right of the z-score
area_right <- 1 - area_left

# Print the result
area_right
```

in our case we are interested to the area to the right, so our answer is 61.41%
:::

::: exercise-box
Exercise: midterm vs final exam student's grades

*In a biology class, both the midterm scores and the final exam scores have an average of 50 and a standard deviation of 10. The scatterplot looks football-shaped and the correlation coefficient is 0.6.*

*Emily got exactly the mean score of 50 on the midterm. What is your prediction for Emily's score on the final?*

sol:

Since the distance in standard deviations of Emily's midterm score from the average midterm score is 0, the corresponding distance of Emily's predicted final score from the average final score is r\*0 = 0, so our prediction is the average =50

*What is the "give or take" number for your prediction?*

sol:

$$
10\sqrt{1-0.6^2} = 8
$$
:::

## Residuals {#residualplots}

As mentioned before, the observed values of $Y$ will not fall directly over the regression line. At each value of $X=x$ there is typically a distribution of possible $Y$ values.

For each observation we will have the predicted value $\hat{y}$ and the observed value $y$ . That difference is called the residual. The **residual plot** is a scatter plot of the residuals against the x-values. It should show an unstructured horizontal band and it is used to check if the regression you are using is appropriate.

For example if we have data following a linear correlation:

```{r, fig.align='center'}
#|echo = FALSE
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Create the scatter plot
plot(x, y, main = "Scatter Plot with Linear Regression Line", 
     xlab = "X-axis", ylab = "Y-axis", pch = 19, col = "blue")

# Add the linear regression line
abline(lm(y ~ x), col = "red", lwd = 2)
```

the residual scatter plot will look like this:

```{r, fig.align='center'}
model <- lm(y ~ x)

# Get the residuals and fitted values
residuals <- resid(model)
fitted_values <- fitted(model)

# Create the residual scatter plot
plot(fitted_values, residuals, 
     main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     pch = 19, 
     col = "blue")

# Add a horizontal line at 0
abline(h = 0, col = "red", lwd = 2)
```

now, if we have data that does not follow a linear correlation:

```{r, fig.align='center'}
#|echo = FALSE
set.seed(7)
x <- abs(rchisq(100, 10) - 2.7)
y <- 1/x + rnorm(100, 1, .1) - .9
data <- data.frame(x, y)
ggplot(data, aes(x = x, y = y)) +
  geom_point()
```

the residual will look different:

```{r, fig.align='center'}
#|echo = FALSE
model <- lm(y ~ x)

# Get the residuals and fitted values
residuals <- resid(model)
fitted_values <- fitted(model)

# Create the residual scatter plot
plot(fitted_values, residuals, 
     main = "Residuals vs Fitted Values", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     pch = 19, 
     col = "blue")

# Add a horizontal line at 0
abline(h = 0, col = "red", lwd = 2)
```

that is an indication that the relation is not linear, and we should not use regression for this data.

### Heteroscedascidity {#heteroscedasticity}

Another variation is when the scatter shows in a fan way, this means that the variability increases with the value of X, this is called heteroscedastic.

```{r, fig.align='center'}
#|echo = FALSE

# Generate some example data with heteroscedasticity
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100, sd = 0.9 * x)

plot(x, y, 
     main = "(Heteroscedasticity)", 
     pch = 19, 
     xlim = c(0, max(x)),
     col = "blue")

```

Sometimes we can fix a non linear relationship by modifying y values, x values or both, for example applying log transformation.

### Outliers, leverage and influential points

Points with very large residuals are called outliers and they should be examined to see if they represent an interesting phenomena or an error in the data. {#highLeverage} When the value that deviates is the $x$ value and not the $y$ we say that it is a high leverage point, and it has the potential to cause a big change in the regression line.

```{r, fig.align='center'}
#|echo = FALSE
# Generate some example data
set.seed(123)
x <- rnorm(20)
y <- 2 * x + rnorm(20)

# Add a high leverage point
x <- c(x, 10)
y <- c(y,4)

# Fit linear models
model_with_point <- lm(y ~ x)
model_without_point <- lm(y ~ x, subset = -21)

# Create the scatter plot
plot(x, y, 
     main = "High Leverage Point Influence", 
     xlab = "X", 
     ylab = "Y", 
     pch = 19, 
     col = ifelse(1:21 == 21, "red", "blue"))

# Add regression lines
abline(model_with_point, col = "red", lwd = 2)
abline(model_without_point, col = "green", lwd = 2, lty = 2)

# Add a legend
legend("bottomright", legend = c("With High Leverage Point", "Without High Leverage Point"), 
       col = c("red", "green"), lty = c(1, 2), lwd = 2)
```

In order to quantify and observation's leverage we can compute the *leverage statistic*. A large value of this statistic indicates an observation with high leverage. The formula for simple linear regression is: $$
h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum^n_{i'=1}(x_{i'}-\bar{x})^2}
$${#leverageStatistic} There is another formula for multiple predictors, but we won't see the formula here. The leverage statistic is always between $1/n$ and 1 and the average leverage for all the observations is always equal to $(p+1)/n$. So if a given observation has a leverage statistic that greatly exceeds $(p+1)/n$ then we may suspect that the corresponding point has leverage.

::: {#summarieswarn .callout-orange}
Beware of data that are summaries (e.g. averages of data). Those are less variable than individual observations and correlations between averages tend to overstate the strength of the relationship
:::

## Assessing the Accuracy of the Coefficient Estimates and Confidence Intervals {#assessing-the-accuracy-of-the-coefficient-estimates-and-confidence-intervals}

The *standard error* of an estimator reflects how it varies under repeated sampling.

$$
SE(\hat{\beta_1})^2 = \frac{\sigma^2}{\sum_{i=1}^ n(x_i-\bar{x})^2}
$$ {#eq-standardErrorSlope} What the denominator in this formula is telling us is that the more spread out our values are along the x axis, the better we will be able to predict the correct slope.

$$
SE(\hat{\beta_0})^2 = \sigma^2 \left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^ n(x_i-\bar{x})^2}\right]
$$ {#eq-standardErrorIntercept} where $\sigma^2$ is the variance of the error $\sigma^2=Var(\epsilon)$

These standard errors can be used to compute *confidence intervals* For example for 95 confidence:

$$
\hat{\beta_1} \mp 2 * SE(\hat{\beta_1}) = \left[\hat{\beta_1}-2 * SE(\hat{\beta_1}),\hat{\beta_1}+2 * SE(\hat{\beta_1})\right]
$$

## Hypothesis testing and significance of correlation

The most important thing to remember about correlation testing is that **it only applies to quantitative variables that have a generally linear relationship**.

A correlation test checks the **null hypothesis that the population correlation** $\rho$ **is equal to 0**. This is, there is no relationship between X and Y. $H_0:\beta_1=0$

To test the null hypothesis, we compute a *t-statistic* $$
t= \frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}
$$ This will have a t-distribution with n-2 degrees of freedom. Using statistical software, it is easy to compute the probability of observing any value equal to *t* or larger. We call this probability the $p$-value. The confidence intervals can also tell us if we should reject the null hypothesis. If 0 falls in between the range of the confidence interval, that means that we cannot exclude the possibility that the slope is 0, meaning that there is no relation between the parameters. The confidence interval is also going to tell you how big the effect is, so it is always a good practice to compute confidence intervals as well as doing a hypothesis testing.

The test **assumes that the data isn't too skewed in any other direction and that there aren't extreme outliers**. As usual, a larger sample provides a certain degree of protection.

**If the sample is very small, the test will be unlikely to rule out the possibility that** $\rho = 0$ even if that is the case. That is, the test will be underpowered.

Similarly, **if the sample is very large, the test will be likely to conclude that** $\rho \neq 0$ \*\*. This gets to an important idea that we've touched on before which is that there is a difference between a sample statistic being statistically significant and it actually being important or meaningful. For instance in a very large sample you may get a sample correlation of 0.001 but it may come back as statistically significant. In the real world, you should always take into consideration not just statistical significance but also effect size when you make real world decisions.

```{r}
testResult<- cor.test(treatment$DLA2,
         treatment$DLA1)
report(testResult)
```

In this type of test our null hypothesis is that the true correlation between those two variables is 0. In our example, the test is giving us a sample correlation of `r testResult$estimate` and a $p$-value `r testResult$p.value` which in simple terms mean that the probability of getting this correlation results in our sample if the true correlation in the population was actually 0 would be less than 0.001, so highly unlikely. The test also gives us a confidence interval, in our case `r testResult$conf.int` and its interpretation is exactly the same as it always is for our confidence interval: if we were to go out and get many many samples from the same population and compute correlations of them, 95% of the time that confidence interval would capture the true parameter, in our case the true population correlation.

With this new knowledge we are going to test the sample we created before showing a relationship between age and DLA_improvement and compare it with the correlation for the population (all records in our file)

```{r, fig.align='center'}
#population correlation between age and dla improvement

ggplot(treatment, aes(x = Age, y = DLA_improvement)) + 
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "grey")

report(cor.test(treatment$Age,
         treatment$DLA_improvement))

#small sample correlation:
set.seed(0)
sample <- slice_sample(treatment, n = 25)
report(cor.test(sample$Age,
         sample$DLA_improvement))

```

::: {.callout-orange appearance="simple" icon="false"}
A significant correlation means that the likelihood of observing such a correlation (or stronger) by random chance is low, given the null hypothesis of no correlation.
:::

## Interpretation of Linear Regression results

Any statistical package will have a function to calculate the line that fits the linear relationship in our data. In r it is lm command. We put the response variable first, and then the explanatory variable, finally the dataset.

```{r}
lm(DLA2 ~ DLA1, data = treatment)
```

It is read DLA2 is explained by DLA1 and we get two coefficients called the slope and the intercept. The interpretation of these numbers is very important. When you have a straight line, the slope shows the increase in $y$ when $x$ increases by 1. The intercept means where is the $y$ value when $x=0$

The most important use of a regression line is that it allows you to make predictions.

In our example, if we get a new patient with DLA1 of 3.6 we can use these values to calculate what it is the expected DLA2 value:

```{r}
DLA1 <- 3.6
DLA2 <- .07243 + .9660 *DLA1
DLA2
```

A very important thing to bear in mind is that **a regression line should only be used to make predictions on individuals whose explanatory variable falls in the range of the values used to calculate the linear regression model**, for example in our case our model was trained with DLA1 between 2.5 and 5 so we should not use this model to make predictions on individuals whose DLA1 is 6, for example.

We can get more information about our model if we ask for their summary:

```{r}
summary(lm(DLA2 ~ DLA1, data = treatment))
```

The $p$-value that corresponds with the intercept is a the $p$-value of a test done against the null hypothesis that the intercept is actually 0.

The $p$-value that corresponds with the dependent variable is the result of a test done against the null hypothesis that the slope is actually 0.

We can retrieve those $p$-values using the coefficients table from our lm result:

```{r}
testResult<- summary(lm(DLA2 ~ DLA1, data = treatment))
testResult$coefficients[,4]
```

When looking at coefficients one must be aware that the units we used in our data will affect them. The z-statistic is not affected by units.

We can use the `names()` function in order to find out what other pieces of information are stored. Although we can extract these quantities by name---e.g. `testResults$coefficients`---it is safer to use the extractor functions like `coef()` to access them.

```{r }
lm.fit<- lm(DLA2 ~ DLA1, data = treatment)
names(lm.fit)
coef(lm.fit)
```

In order to obtain a confidence interval for the coefficient estimates, we can use the `confint()` command.

```{r }
confint(lm.fit)
```

The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of Y for a given value of X.

```{r chunk8}
# Choose a random value in x
random_x <- sample(x, 1)

predict(lm.fit, newdata = data.frame(x = random_x),
    interval = "confidence")
predict(lm.fit, newdata = data.frame(x = random_x),
    interval = "prediction")
```

R is also retrieving a p value for the overall model that's coming from an Anova F-Test. that corresponds with the coefficient for the slope.

our diagnostic plots are automatically produced by applying the `plot()` function directly to the output from `lm()`. In general, this command will produce one plot at a time, and hitting *Enter* will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the `par()` and `mfrow()` functions, which tell `R` to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, `par(mfrow = c(2, 2))` divides the plotting region into a $2 \times 2$ grid of panels.

```{r, fig.align='center', fig.width=7}
par(mfrow = c(2, 2))
plot(lm.fit)
```

::: exercise-box
Exercises:

*Using the mpg dataset:*

-   *Is there a linear relationship between city mileage and highway mileage in this set?*

    ```{r, fig.align='center'}
    file2 <- here::here("data", "mpg_2008.xlsx")
    mpg_2008 <- read_excel(file2)

    ggplot(mpg_2008, aes(x = cty, y = hwy))+
      geom_point()+
      geom_smooth(method ='lm',
                  se = FALSE,
                  color = 'purple' )
    ```

-   *What is the sample correlation?*

    ```{r}
    cor(mpg_2008$cty, mpg_2008$hwy)
    ```

-   *Does the sample correlation provide evidence that the population correlation is different from zero?*

    ```{r}
    testResult<- cor.test(mpg_2008$cty, mpg_2008$hwy)
    testResult
    report(testResult)
    ```

    Yes.

-   *Find the equation of the regression line*

```{r}
lm (cty ~hwy, data= mpg_2008)
lm (hwy ~ cty, data= mpg_2008)
```

the equation will be $cty = 0.6687 * hwy + 1.017$ or $hwy = 1.39 *cty + .2388$

*Estimate the highway mileage of an unknown car in this population with a city mileage of 24 miles per gallon.*

```{r}
1.29*24+.2388
```
:::

::: {.callout-orange appearance="simple" icon="false"}
Linear regression models the relationship between a dependent variable and one or more independent variables. Remember a well-fitting model doesn't always imply causality.
:::

# Assessing Model Accuracy

In this section, we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specific data set.

## Measuring the Quality of Fit. Mean Squared Error (MSE) {#MeanSquaredError}

In order to evaluate the performance of a statistical method on a given dataset, we need some way to measure how well its predictions actually match the observed data. In regression, the most commonly-used measure is the *mean squared error (MSE)*

$$
MSE = \frac{1}{n}\sum^n_{i=1}(y_i-\hat{f}(x_i))^2 =\frac{1}{n}\sum^n_{i=1}(y_i-\hat{y_i})^2
$$ {#eq-MSEMeanSquaredError}

that is the squared difference between the observed values and the predicted values divided by the number of data points.

If we remember the formula for the RSS(@eq-RSSsimpleRegression) we see that MSE formula can be written as:

$$
MSE = \frac{1}{n} RSS
$$

The MSE is computed using the training data that was used to fit the model, and should be referred as training MSE, but we are more interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE. There is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be small, but the test MSE is often much larger.

```{r, fig.align='center', echo=FALSE}

set.seed(123)
X <- seq(0, 10, length.out = 30)
Y <- 2 + 5 * X + X^2 + rnorm(30, sd = 5)
data <- data.frame(X, Y)

# Fit models
linear_model <- lm(Y ~ X, data = data)
parametric_model <- lm(Y ~ poly(X, 5), data = data)  # Increased polynomial degree
overfitted_model <- lm(Y ~ poly(X, 20), data = data)

# Predict values
data <- data %>%
  mutate(
    linear_pred = predict(linear_model),
    polynomial_pred = predict(parametric_model),
    overfitted_pred = predict(overfitted_model)
  )

# Plot
ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  geom_line(aes(y = linear_pred), color = "blue") +
  geom_line(aes(y = polynomial_pred), color = "red") +
  geom_line(aes(y = overfitted_pred), color = "green") +
  labs(title = "Scatter Plot with 3 different Regression Lines", x = "X", y = "Y")
```

the graph above shows a linear regression line, and two polynomial lines one of them over fitted. If we calculate the Mean Square Error of each regression we can see that the over-fit model is not the best predictor over new data:

```{r, fig.align='center'}
# Function to calculate MSE
calculate_mse <- function(model, new_data) {
  mean((new_data$Y - predict(model, new_data))^2)
}

# Generate new training data
set.seed(456)
X <- seq(0, 10, length.out = 30)
Y <- 2 + 5 * X + X^2 + rnorm(30, sd = 5)
new_data <- data.frame(X, Y)

# Calculate MSE for each model
mse_linear <- calculate_mse(linear_model, new_data)
mse_parametric <- calculate_mse(parametric_model, new_data)
mse_overfitted <- calculate_mse(overfitted_model, new_data)

# Create a dataframe for MSE values
mse_data <- data.frame(
  Model = c("Linear", "Polynomial", "Overfitted"),
  MSE = c(mse_linear, mse_parametric, mse_overfitted)
)

# Plot MSE values
ggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "MSE for Different Models on New Training Data", x = "Model", y = "Mean Squared Error") +
  theme_minimal()

```

## Bias-Variance Trade-off {#bias-variance-trade-off}

Imagine you're trying to build a model to predict house prices. You could use a simple model or a complex one. The bias-variance trade-off is all about balancing simplicity and complexity.

Bias is error introduced by assuming a model is too simple, capturing only a rough approximation of the real relationship. High bias means the model might miss important patterns (underfitting). Think of a straight line when a curve is more appropriate.

Variance is error introduced by the model being too complex, capturing noise instead of the actual pattern. High variance means the model is too sensitive to small fluctuations in the training data, leading to poor performance on new data (overfitting). Imagine fitting every little bump in the data.

The trade-off part comes in because as you try to decrease bias by making the model more complex, you often increase variance, and vice versa. The goal is to find the sweet spot where the model captures the essential patterns without overreacting to noise.

Low Bias, High Variance: A very flexible model that might fit the training data really well but fails on new data.

High Bias, Low Variance: A very simple model that doesn't capture the data well, missing patterns.

Optimal Bias-Variance Trade-Off: A balanced model that captures the underlying trend without overfitting.

## R-squared {#Rsquare}

We already talked about [how to assess the slope of an individual predictor](#MeanSquaredError)

The [Residual Standard Error (RSE)](#ResidualStandardError) provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of Y , it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a proportion ---the proportion of variance explained--- and so it always takes on a value between 0 and 1, and is independent of the scale of Y .

We already saw the [residual sum of squared (RSS)](#Residualsumofsquares) RSS is a measure of the discrepancy between the actual data and the model's predictions. It represents the sum of the squared differences between the observed values and the predicted values.

$$
RSS=\sum^n_{i=1}(y_1-\hat{y_1})^2 
$$ RSS measures the amount of variability that is left unexplained after performing the regression.

The **total Sum of Squares (TSS)**: TSS is a measure of the total variability in the observed data. It represents the sum of the squared differences between the observed values and the mean of the observed values. TSS measures the total variance in the response Y and can be thourhg of as the amount of variability inherent in the response before the regression is performed.

$$
TSS=\sum^n_{i=1}(y_1-\bar{y_1})^2
$$ {#eq-tss}

TSS - RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in Y that can be explained using X.

$$
R^2 = \frac{TSS-RSS}{TSS}= 1-\frac{RSS}{TSS}
$$ {#eq-rsquared}

The R-squared value measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It's calculated as:

A larger R-squared value indicates a better fit of the model to the data. An R-squared value closer to 1 means that the model explains a large proportion of the variance in the dependent variable, signifying a good fit. Conversely, a smaller R-squared value indicates that the model explains less of the variance, signifying a poorer fit, or the error variance is high, or both.

However, it can still be challenging to determine what is a good R2 value, and in general, this will depend on the application. For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error. In this case, we would expect to see an R2 value that is extremely close to 1, and a substantially smaller R2 value might indicate a serious problem with the experiment in which the data were generated. On the other hand, in typical applications in biology, psychology, marketing, and other domains, the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R2 value well below 0.1 might be more realistic.

The $R^2$ statistic is a measure of the linear relationship between X and Y, and if we recall [correlation](#correlation-coefficient), it also measures the linear relationship between X and Y, this suggest that we might be able to use correlation $r-Cor(X,Y)$ instead of $R^2$ in order to assess the fit of the model, and in fact, it can be shown that in the simple linear regression setting, $R^2 = r^2$.

## Residual Standard Error (RSE) {#ResidualStandardError}

Another measure of the model fit is the **Residual Standard Error (RSE)** The RSE is an estimate of the standard deviation of the error term, representing the average amount that the observed values deviate from the model's predictions. Essentially, it gives you an idea of the typical size of the residuals (errors) made by the model. It's like a measure of how well the model fits the data.

$$
RSE = \sqrt{\frac{1}{n-p-1}RSS}
$$ {#eq-RSE}

where $n$ is the number of observations and $p$ is the number of predictors or independent variables in the model. $n-p-1$ is the *degrees of freedom*. A smaller RSE indicates a better fit, implying that the model's predictions are close to the actual data. - RSS helps calculate RSE, giving you the scale of the errors. - TSS is used to calculate $R^2$, which, together with RSE, helps you understand the overall model fit.

Both RSE and $R^2$ provide insight into the model's performance, but from slightly different angles. While $R^2$ tells you how much variance in the dependent variable is explained by the model, RSE tells you about the typical size of the prediction errors.

By using these metrics together, you can get a comprehensive view of how well your regression model is performing

## Testing error and training error {#testingtrainingerror}

The *training error rate* is the proportion of mistakes that are made if we apply our model to the training data-set.

The *test error* is the average error that results from using a statistical learning method to predict the response on a new observation, that is, a measurement that was not used in training the method.

The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case.In contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training. But the training error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter.

## Multiple Linear regression.

When we have more than one predictor, the model will create a plane instead of a curve that tries to minimize the sum of squared errors:

```{r, fig.align='center', echo=FALSE}
# Set seed for reproducibility
set.seed(123)

# Generate sample data with a stronger linear relationship
n <- 100
X1 <- runif(n, 0, 10)
X2 <- runif(n, 0, 10)
Y <- 5 + 1.5*X1 + 2*X2 + rnorm(n, sd = 2)  # Adjusted coefficients and smaller noise

# Create data frame
data <- data.frame(X1, X2, Y)

# Fit the linear model
model <- lm(Y ~ X1 + X2, data = data)

# Predict values for plotting the hyperplane
x1_seq <- seq(min(data$X1), max(data$X1), length.out = 30)
x2_seq <- seq(min(data$X2), max(data$X2), length.out = 30)
grid <- expand.grid(X1 = x1_seq, X2 = x2_seq)
grid$Y_pred <- predict(model, newdata = grid)

p <- plot_ly(data, x = ~X1, y = ~X2, z = ~Y, type = 'scatter3d', mode = 'markers', marker = list(size = 3)) %>%
  add_trace(x = grid$X1, y = grid$X2, z = grid$Y_pred, type = 'mesh3d', opacity = 0.5, color = 'blue') %>%
  layout(scene = list(
    xaxis = list(title = 'X1'),
    yaxis = list(title = 'X2'),
    zaxis = list(title = 'Y')
  ))
p
```

Remember the formula for RSS for a simple linear model was explained already (@eq-RSSsimpleRegression) now we have a similar version including all the predictors:

$$
RSS=\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left( y_i - \hat{\beta_0} - \hat{\beta_1} x_{i1} - \hat{\beta_2} x_{i2}\dots... - - \hat{\beta_p} x_{ip}\right)^2
$$ {#eq-RSSmultipleRegression}

This is done using statistical software. The values $\hat{\beta_o} ,\hat{\beta_1},\dots ,\hat{\beta_p}$ that minimize RSS are the **multiple least square regression coefficient estimates**

The ideal scenario is when the predictors are uncorrelated: each coefficient can be estimated and tested separately, a unit change in $X_j$ is associated with a $\beta_j$ change in $Y$ while all the other variables stay fixed.

Correlations amongst predictors cause problems: the variance of all coefficients tends to increase, sometimes dramatically and interpretations become hazardous, when $X_j$ changes, everything else changes. We cannot attribute the change to a specific predictor. For example height and weight are two predictors that are correlated.

When looking at multiple regression models we have to ask ourselves some questions: - Is at least one of the predictors useful in predicting the response - Do all the predictors help to explain Y or only some of them are useful? - How well does the model fit the data?

## Model fit

Two of the most common numerical measures of model fit are the RSE and $R^2$, the fraction of variance explained. These quantities are computed and interpreted in the same fashion as for simple linear regression. Recall that [in simple regression, $R^2$ is the square of the correlation of the response and the variable](#Rsquare). In multiple linear regression, it turns out that it equals $Cor(Y, \hat{Y} )^2$, the square of the correlation between the response and the fitted linear model. It turns out that $R^2$ will always increase when more variables are introduced, even if those variables are only weekly associated with the response. If we notice that the increase in $R^2$ is minimal after adding another variable, it is probably best to leave that variable out of the model. We can check RSE in combination with $R^2$, as RSE will increase despite RSS increasing as well if the new variable is not adding enough value. Remember the formula for RSE (@eq-RSE):

$$
RSE = \sqrt{\frac{1}{n-p-1}RSS}
$$

Thus, models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p.

::: exercise-box
Example:

We run a campaign to sell a product, and we advertise in three mediums, newspapers, radio and tv. We have the amount spent in each of the mediums and the sales. We will first run a simple linear model over the three variables:

```{r}
Advertising <- readr::read_csv("data/Advertising.csv")
tv<- lm(sales ~ TV, data = Advertising)
summary(tv)

radio<- lm(sales ~ radio, data = Advertising)
summary(radio)

newspaper<- lm(sales ~ newspaper, data = Advertising)
summary(newspaper)
```

We see that independently, all three variables have a $p$-value below or significance level.

If we fit the model to the three variables instead newspaper stops showing as significant.

```{r}
lm.fit<- lm(sales ~ TV+radio+newspaper, data = Advertising)
summary(lm.fit)

```

The [*Adjusted Rsquared*](#adjusted-rsquared) is 0.8956 and the RSE 1.686

If we fit the model with only the two significant variables:

```{r}
lm.fit<- lm(sales ~ TV+radio, data = Advertising)
summary(lm.fit)

```

The *adjusted R-squared* is 0.8962 instead of 0.8956 but the RSE has decreased from 1.686 in the model with three predictors to 1.681 in the simpler model. So, the model sales \~ TV + radio is more parsimonious and just as effective. Plus, it's always good to avoid unnecessary complexity when you can.
:::

Sometimes visualizing the data is helpful:

```{r, fig.align='center', fig.width=10}
# Calculate predicted values
Advertising$predicted_sales <- predict(lm.fit)

# Create 3D scatter plot
s3d <- scatterplot3d(Advertising$TV, Advertising$radio, Advertising$sales, 
                     pch = 16, highlight.3d = TRUE, type = "p", 
                     main = "3D Scatter Plot with Regression Plane",
                     xlab = "TV", ylab = "Radio", zlab = "Sales")

# Add regression plane
s3d$plane3d(lm.fit, col = "lightblue")

# Plot lines from data points to the regression plane
for (i in 1:nrow(Advertising)) {
  s3d$points3d(x = c(Advertising$TV[i], Advertising$TV[i]), 
               y = c(Advertising$radio[i], Advertising$radio[i]), 
               z = c(Advertising$sales[i], Advertising$predicted_sales[i]), 
               type = "l", col = "green")
}

```

## Collinearity {#collinearity}

Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. Since collinearity reduces the accuracy of estimates of the regression coefficients, it causes the standard error to grow. Recall that the t-statistic for each predictor is calculated by dividing $\hat{\beta_j}$ by its standard error. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject the null hypothesis, this means that the probability of correctly detecting a non-zero coefficient is reduced. A simple way to detect collinearity between two variables is to look at the correlation matrix of the predictors, but it is also possible that there is collinearity between three variables, even if no pair of variables have a high correlation, and this will not be detected in the correlation matrix. For this we can calculate the *variance inflaction factor (VIF)* but this is out of the scope here.

## F-statistic.

$$
F= \frac{TSS-RSS/p}{RSS/(n-p-1)}
$$ {#eq-fStatistic}

The F-statistic in a multiple regression model assesses *the overall significance of the model*. It tells you whether your model's variables are jointly significant in explaining the variation in the dependent variable. High F-statistic: Indicates that the group of predictors has a significant relationship with the dependent variable. The larger the F, the more likely the observed relationship is not due to random chance. Low F-statistic: Suggests that the group of predictors does not have a significant explanatory power.

Whether an F value is high or low is determined by comparing it to a critical value from the F-distribution table, which depends on the degrees of freedom and the chosen significance level (commonly 0.05). Steps to Determine High or Low F-value: - Degrees of Freedom: Calculate the degrees of freedom for the numerator (df1 = number of predictors) and the denominator (df2 = total number of observations - number of predictors - 1).

F-distribution Table: Use the degrees of freedom to find the critical F-value from the F-distribution table at your chosen significance level (usually 0.05).

Compare: If the calculated F-value from your regression output is greater than the critical F-value from the table, it means your predictors are collectively significant (high F-value).

Example: Suppose you have 3 predictors in your model and a sample size of 100. The degrees of freedom are:

df1 (numerator) = 3

df2 (denominator) = 100 - 3 - 1 = 96

Looking up the critical value for df1 = 3 and df2 = 96 at the 0.05 significance level in an F-table, you might find it to be around 2.70.

If your calculated F-value is, say, 5.2, since 5.2 \> 2.70, your model is significant (high F-value).

If it were 1.8, since 1.8 \< 2.70, your model isn't significant (low F-value).

In r, the f-value will also come with its $p$-value, indicating its significance.

## Deciding on the important variables.

With today's big data there is more availability of data than ever, and it is becoming usual to have many many features to fit a model, and model accuracy can be challenging in those situations, specially when we have more features than number of samples (p\>n). By removing irrelevant features we can obtain a model that is more easily interpreted. We will present some approaches for automatically performing *feature selection*.

-   *Subset selection*: We identify a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.

-   *Shrinkage*: We fit a model involving all p predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as *regularization*) has the effect of reducing variance and can also perform variable selection.

-   *Dimension reduction*: We project the p predictors into a M-dimensional subspace, where M\<p. This is achieved by computing M different *linear combinations* or *projections*, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares.

### Best Subset selection

The most direct approach to answer this question is called *all subsets* or *best subsets* regression. We compute the least squares fit for all possible subsets and then choose between them based on some criterion that balances training error with model size. This gets really hard when the number of variables is high, the number of possible models is $2^p$ so for example for p=40 there are over a billion models. Instead, we need an automated approach that searches through a subset of them. We discuss two commonly use approaches next.

Although we are presenting this here in the context of linear models, the same ideas apply to other types of models, such as logistic regression. One difference will be that other models will work with a similar metric to the residual sum of squares that is called *deviance*.

A limitation of the subset selection method is the number of p. Most statistical packages have a limit of 30 to 40 features. Another problem of this method with high number of p is overfitting, so this method is only valid when p is small (\<10).

We start with the *null model* that is a model that contains the intercept but no predictors. The intercept is the mean of $y$ Now you start adding variables one at a time: for each variable you fit $p$ simple linear models each with one of the variables and the intercept, and you look at each of them. After that you choose the variable that results in the lowest RSS (that is the largest $R^2$). Now, having picked that, you fix that variable in the model and now you add one by one all the variables to this model again, and choose the one that best improve the residual sum of squares.

You can continue like this until some stopping rule is satisfied, for example, when all the remaining variables have a p value above some threshold.

Once we have a correct combination of predictors, to select if the best model is the model with 2,3,or n predictors, we cannot use RSS anymore because naturally RSS will decrease as we include more predictors, but that does not necessarily means it's a best model. We need to select the model that will have the smallest *test error*, and not the one with the smallest *training error*. Remember [we talked about these concepts already](#testingtrainingerror). Frequently for this we use [cross-validation](#crossvalidation)

::: exercise-box
Example

Here we apply the best subset selection approach to the `Hitters` data from ISLR2 library. We wish to predict a baseball player's Salary on the basis of various statistics associated with performance in the previous year. Note that the Salary variable is missing in some rows, so we are going to omit them

```{r}
#nubmer of nas. 
sum(is.na(Hitters$Salary))
#remove nas
Hitters<- na.omit(Hitters)
sum(is.na(Hitters$Salary))
```

The `regsubsets()` function (part of the `leaps` library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for 'lm()'. The 'summary()' command outputs the best set of variables for each model size.

```{r}
regfit.full <- leaps::regsubsets(Salary ~ ., Hitters)
(reg.summary<- summary(regfit.full))
```

For each subset size (number of features 1...n) it returns the best subset models. An asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI.

By default, `regsubsets()` only reports results up to the best eight-variable model. But the `nvmax` option can be used in order to return as many variables as are desired.

```{r}
regfit.full <- regsubsets(Salary ~ ., data = Hitters,
    nvmax = 19)
(reg.summary <- summary(regfit.full))
```

We can see the R squared using `reg.summary$rsq`. Here we can see that the R2 statistic increases from 32%, when only one variable is included in the model, to almost 55%, when all variables are included. As expected, the R2 statistic increases monotonically as more variables are included, so this is not a good statistic for choosing the right size for the model because it will always recommend the model with more variables.

```{r}
reg.summary$rsq
```

So now that we have the best combination of variables for each model size, we can use cross validation or other methods to select the model with the best number of predictors.

We will see study Mallow's Cp, AIC or BIC soon, and `regsubset` gives us the statistics for the different models for those methods:

```{r, fig.align='center', fig.width=10, fig.height=4}
par(mfrow = c(2, 2))
plot(reg.summary$bic, xlab = "Number of Variables",
    ylab = "BIC", type = "l")
plot(reg.summary$cp, xlab = "Number of Variables",
    ylab = "Cp", type = "l")
plot(reg.summary$rsq, xlab = "Number of Variables",
    ylab = "RSS", type = "l")
plot(reg.summary$rss, xlab = "Number of Variables",
    ylab = "rss", type = "l")
```

with this information we will be able to select the model with the correct number of variables.
:::

### Stepwise Selection

#### Forward Stepwise selection

The stepwise selection is very similar to the best subset as we start with a model with only the intercept and start aggregating predictors and selecting those with less RSS. The difference with best subset selection is that these models are nested and brings the number of models that we are considering from $2^p$ to $p^2$ which is more manageable. Because we are not considering every possible combination of all the predictors, this method is not guaranteed to find the model with less RSS.

Again, once we have the best version of the model with each number of predictors, we will choose among them using cross-validation or a similar technique.

#### Backward stepwise selection

It runs a similar process but starting with a model with all the variables, then you run the model removing each variable and see which one is the least statistically significant one. The new (p-1) variable model is fit, and the variable with the largest $p$-value is removed. Continue until a stopping rule is reached, for instance, we may stop when all remaining variables have a significant $p$-value defined by some significance threshold.

One limitation of the backward stepwise selection is that we can only do it with n\>p (more samples than predictors). This limitation does not apply to forward stepwise selection.

::: exercise-box
Example

We can also use the `regsubsets()` function to perform forward stepwise or backward stepwise selection, using the argument `method = "forward"` or `method="backward"`. We are going to format a bit the output of the summary so it is more friendly to the reader:

```{r}
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters,
    nvmax = 19, method = "forward")

regfit.bwd <- regsubsets(Salary ~ ., data = Hitters,
    nvmax = 19, method = "backward")

createSummaryTable <- function(regfitObject){

summary <- summary(regfitObject)
matrix_of_models <- as.data.frame(summary$which)
matrix_of_models <- matrix_of_models %>%
  mutate_all(~ifelse(., "**", ""))

kable(matrix_of_models, escape = FALSE) %>%
  kable_styling(full_width = FALSE) %>%
  row_spec(0, bold = TRUE) 
}

createSummaryTable(regfit.fwd)

createSummaryTable(regfit.bwd)
```

For instance, we see that using forwards stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best model for our number of variabless identified by forward and backward stepwise selection and best subset selection are different.

```{r}
coef(regfit.full, 7)

coef(regfit.fwd, 7)

coef(regfit.bwd, 7)
```

As we saw before, the summary() function also returns $R^2$, RSS, adjusted $R^2$, Cp, and BIC. We can examine these to try to select the best overall model.

```{r}
reg.summary <- summary(regfit.fwd)
names(reg.summary)
```

Plotting RSS, adjusted R Squared, Cp and BIC for all the models at once will help us decide which model to select. Note the `type ="1"` option tells R to connect the plotted points with lines. Here we are plotting RSS and Adjusted R Squared

```{r, fig.align='center', fig.width=10, fig.height=4}
par(mfrow = c(1, 2))
plot(reg.summary$rss, xlab = "Number of Variables",
    ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l")
```

The `points()` command works like the `plot()` command, except that it puts points on a plot that has already been created, instead of creating a new plot. The `which.max()` function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted R2 statistic, which would be the preferred one.

```{r, fig.align='center'}
max<- which.max(reg.summary$adjr2)
plot(reg.summary$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l")
points(max, reg.summary$adjr2[max], col = "red", cex = 2, 
    pch = 20)
```

Similarly we can plot the Cp and the BIC statistics and indicate the models with the smallest statistic:

```{r, fig.align='center'}
min<- which.min(reg.summary$cp)
plot(reg.summary$cp, xlab = "Number of Variables",
    ylab = "Cp", type = "l")
points(min,reg.summary$cp[min], col = "red", cex = 2, 
    pch = 20)
```

```{r, fig.align='center'}
min<- which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables",
    ylab = "BIC", type = "l")
points(min, reg.summary$bic[min], col = "red", cex = 2,
    pch = 20)

```

The 'regsubsets()' function has a built-in 'plot()' command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, Cp, adjusted R2, or AIC. To find out more about this function, type ?plot.regsubsets. We will see the BIC here:

```{r, fig.align='center', fig.width=10, fig.height=8}
plot(regfit.fwd, scale = "bic")
```

The top row of the plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to −150. However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the `coef()` function to see the coefficient estimates associated with this model.

```{r}
coef(regfit.fwd, 6)
```

As we did before we can plot the results for Cp and and see the difference between our bakwards and forward step wise selections:

```{r}
plot(regfit.fwd, scale="Cp")
plot(regfit.bwd, scale="Cp")
```
:::

## Estimating the test error

-   We can indirectly estimate the test error by making an adjustment to the training error to account for the bias due to overfitting.

-   We can directly estimate the test error, using validation set approach or a cross-validation approach (see chapter [Resampling Methods](#resampling-methods)). These involve fitting the model in part of the dataset and testing in with unseen data.

We are going to see here other methods:

### Mallow's Cp

This technique adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables. $$
C_p = \frac{1}{n}(RSS+2d\hat{\sigma}^2)
$$ {#eq-mallowscp}

Where d is the total number of parameters used and - $\hat{\sigma}^2$ is an estimate of the variance of the error term $\epsilon$ associated with each response measurement, calculated as: $$
\hat{\sigma}^2 = \frac{SSE}{n-p}
$$ {#eq-errorvariance} SSE is the *Sum of Squared Errors* or *Residual Sum of Squares*

$$
SSE = \sum^n_{i=1}(y_i-\hat{y_i})^2
$$ {#eq-SSESumSquaredErrors1}

This technique is restricted to those models where p\<n. Example using the mtcars dataset

```{r}
# Fit the model
model <- lm(mpg ~ hp + wt, data = mtcars)

# Extract the residuals
residuals <- model$residuals

# Calculate SSE
SSE <- sum(residuals^2)

# Number of observations
n <- length(mtcars$mpg)

# Number of predictors (including intercept)
p <- length(coefficients(model))

# Calculate \(\hat{\sigma}^2\)
sigma_hat_sq <- SSE / (n - p)

# Print the result
print(sigma_hat_sq)

```

Now we calculate Mallow's Cp for the same dataset for a model with with two parameters:

```{r}
# Fit the subset model
subset_model <- lm(mpg ~ hp + wt, data = mtcars)

# Calculate residuals and SSE for the subset model
residuals_subset <- subset_model$residuals
SSE_subset <- sum(residuals_subset^2)

# Number of predictors in the subset model (including intercept)
p_subset <- length(coefficients(subset_model))
print(SSE_subset)
# Calculate Mallows' Cp 
Cp <- (SSE_subset / sigma_hat_sq) + 2 * p_subset - n 
print(Cp)
```

When comparing two values for different models we want to choose the smallest Cp.

### AIC

The criterion is defined for a large class of models fit by maximum likelihood. We are not going to look at the formula right now, but it is similar to the previous and it is actually proportional to Cp, so again, we will choose the model with the smallest value.

### Bayesian Information Criterion (BIC)

$$
BIC = \frac{1}{n}(RSS+log(n)d\hat{\sigma}^2)
$$ {#eq-BIC}

We also look for the smallest value of the BIC for our models. The BIC statistic generally places a heavier penalty on models with many variables than the two methods seen before.

Because all these three methods rely on calculating hat sigma squared, all of them require that n\>p due to the formula to calculate it (@eq-errorvariance)

### Adjusted RSquared {#adjusted-rsquared}

For a least squared model with $d$ variables, the adjusted $R^2$ statistic is calculated as:

$$
\text{Adjusted } R^2= 1- \frac{RSS/(n-d-1)}{TSS/(n-1)}
$$ {#eq-adjustedRsquared}

where TSS is the total sum of squares.

Unlike Cp, AIC and BIC, for which a small value indicates a model with low test error, a large value of adjusted R squared indicates a model with small test error. Maximizing the adjusted $R^2$ is equivalent to minimizing RSS/(n-d-1). While RSS always decreases as the number of variables in the model increases, the presence of d in the denominator corrects for that, so the inclusion of unnecessary variables in the model pays a price.

The drawback of Adjusted Rsquared is that it cannot be used for other models, like logistic, for example.

::: exercise-box
Example: Validation Set Approach:

We saw how to choose our best models for the `Hitters` dataset using Best subset selection, Stepwise Selection and then using methods like BIC, AIC to select the best size for our model.

Now we will consider how to do this using the validation set and cross-validation approaches. In order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of the model-fitting, including variable selection. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error. In order to use the validation approach, we begin by splitting the observations into a training set and a test set. We choose the size of the train set as 180 (approx 2/3 of the data)

```{r}
objects_to_remove <- ls(pattern = "^regfit") 
rm(list = objects_to_remove)
n<-nrow(Hitters)
set.seed(1)
train <- sample(seq(1:n), 180, replace= FALSE)

```

now we apply `regsubsets()` to the training set in order to perform best subset selection.

```{r}
regfit.fwd<- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax= 19, method="forward")
createSummaryTable(regfit.fwd)

```

We now make a model matrix from the test data:

```{r}
test.mat <- model.matrix(Salary ~ ., data = Hitters[-train, ])
head(test.mat)
```

The `model.matrix()` function is used in many regression packages for building an X matrix from data. Now we run a loop, and for each size i, we extract the coefficients from `regfit.fwd` for the best model that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test Mean Square for Error (MSE).

First we will do a sample iteration showing some of the the values so we can see what the loop is doing we choose the model with three variables + the intercept:

```{r}
(coefi <- coef(regfit.fwd, id = 3))
 print(names(coefi))
 head(test.mat[, names(coefi)])
 pred <- test.mat[, names(coefi)] %*% coefi
 head(pred)
 salaryTestData<- Hitters$Salary[-train]
 meanSquareError<- mean((salaryTestData - pred)^2)
```

Let's run the loop now:

```{r}
val.errors <- rep(NA, 19)
for (i in 1:19){
  coefi <- coef(regfit.fwd, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((Hitters$Salary[-train] - pred)^2)
}

```

we can now plot the square root of MSE to see the results. Taking the square root of MSE gives us the *Root Mean Squared Error (RMSE)*, which is on the same scale as the original data. This makes it easier to interpret and compare. The model summary does not give us MSE directly, but we can calculate it: $MSE=RSS/n$. In our code: `regfit.fwd$rss[-1]/length(Hitters$Salary[test])` the minus one is to remove the intercept.

```{r, fig.align='center', fig.width=7}
plot(sqrt(val.errors), ylab= "Sqrt MSE", pch=1,type="b",ylim= c(250,450))
points(which.min(val.errors), sqrt(val.errors[which.min(val.errors)]), col="red", cex = 2,
    pch = 20, )

points(sqrt(regfit.fwd$rss[-1]/length(Hitters$Salary[train])),col="blue", pch =1, type="b")
legend("topright", legend=c("Training","Validation"), col= c("blue","black"), pch=1)

```

the best model is the one with the min error:

```{r}
val.errors
min<-which.min(val.errors)
coef(regfit.fwd, min)
```

This was a little tedious because there is no predict method for `regsubsets()`. Since we will be using this function again, we can write our own method:

```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]]) #extract call formula
  mat <- model.matrix(form, newdata) 
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
 }

```

Our function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to `regsubsets()`. We demonstrate how we use this function below, when we do cross-validation.

Finally, we perform best subset selection on the full data set, and select the best model for our number of variables. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates. Note that we perform best subset selection on the full data set and select the best model for our number of variables, rather than simply using the variables that were obtained from the training set, because the best model for our number of variables on the full data set may differ from the corresponding model on the training set.

In fact, we see that the best model for our number of variables on the full data set has a different set of variables than the best model for our number of variables on the training set.

```{r}
regfit.best <- regsubsets(Salary ~ ., data = Hitters,
    nvmax = 19)
coef(regfit.best, min)
```
:::

## Resampling Methods {#resampling-methods}

### Monte Carlo Method

The Monte Carlo method is a powerful statistical technique used to understand the impact of risk and uncertainty in prediction and forecasting models.

If for example we want to know the average height of all people living in USA, we estimate the parameter we are interested in (average height of the population) using a statistic estimator that is the average height of a sample of the population drawn at random. $\theta$ is the parameter we want to calculate and we use $\hat{\theta}$ that is the average o four sample. By the law of large numbers, the approximation error can be made arbitrarily small by using a large enough sample size.

What if we are interested in an estimator $\hat{\theta}$ for some parameter $\theta$ of the population and the normal approximation is not valid for that estimator? In such situations, simulations can often be used to estimate the parameters.

The Monte Carlo method relies on repeated random sampling to obtain numerical results. The core idea is to use randomness to solve problems that might be deterministic in nature. This method is named after the Monte Carlo Casino in Monaco, reflecting the element of chance and randomness.

In our example to estimate the height of the population of USA adults we get many (let's say 1000) samples of n=100 observations. We then compute the estimator $\hat{\theta}$ for each sample (in our case the average height), resulting in 1000 estimates. Now we compute the standard deviation of these 1000 estimates and it results to be close to the standard error of my estimator. :

$$
SE(\hat{\theta})=\sqrt{E(\hat{\theta}-E(\hat{\theta}))^ 2}
$$

$$
s(\hat{\theta}_1,...,\hat{\theta}_{1000})= \sqrt{\frac{1}{999}\sum^{1000}_{i=1}(\hat{\theta}-mean(\hat{\theta_i}))^ 2} \approx SE(\hat{\theta})
$$

The caveat of this method is that it will only work where we can extract as many samples as we wish.

Usually we cannot have as many samples as we wish, when we want to use the Monte Carlo Method in practice, it is typical to assume a parametric distribution and generate a population from this, which is called a **parametric simulation**. This means that we take parameters estimated from the real data and using the mean and standard deviation, we plug these into a model of normal distribution and generate new samples.

For example for the case of mice weights, we know that mice typically weight 24 gr with a SD of about 3.5 gr. and that the distribution is approximately normal, to generate population data:

```{r}
controls<- rnorm(5000, mean=24, sd=3.5) 
```

### Cross-validation {#crossvalidation}

As the models become more complex (the more features we add to our model), the [training error](#testingtrainingerror) becomes lower. On the other hand, the *test error* does not reduces at the same rate as the model gets more complex, it starts going down but then it starts increasing again as the model starts overfitting to the training dataset.

The prediction error comes from bias and variance. The *bias* is how var off on the average the model is from the truth. The *variance* is how much the estimate varies around its average. The less parameters we fit into our model, the more bias we have (we will find it more difficult to find the true mean) and as we increase the parameters, the bias reduces, but the variance goes up, because we have more and more parameters to estimate from the same amount of data. The solution for this would be a large designated test set, but this is not often available.

There are some methods to try to adjust the training error so it does not over fit and produces an increase in the testing error rate. We randomly divide the available set of samples into two parts: a *training set* and a *validation* or *hold out set*. The model is fit on the training set and the fitted model is used to predict responses for the observations in the validation set. The resulting validation-set error provides an estimate of the test error. This is typically assessed using [Mean Squared Error (MSE)](#MeanSquaredError) in the case of quantitative response and misclassification rate in the case of qualitative (discrete) responses.

```{r, fig.aligh ='center', fig.width=12, fig.height=6, fig.cap='two-fold cross validation'  }
# Set up the plotting area for two plots side by side
par(mfrow = c(1, 2))

# Plot 1: MSE with All Data
# Initialize a vector to store MSE errors for each polynomial degree
mse_error_all <- rep(0, 10)

# Loop to calculate MSE errors for polynomial degrees 1 to 10
for (i in 1:10) {
  glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  predictions <- predict(glm_fit)
  actuals <- Auto$mpg
  mse_error_all[i] <- mean((predictions - actuals)^2)
}

# Plot the MSE errors for all data
plot(1:10, mse_error_all, 
     type = "b", 
     col = 1, 
     ylim= c(0,(max(mse_error_all) + 1)),
     xlab = "Degree of Polynomial", 
     ylab = "MSE", 
     main = "MSE with All Data")

# Plot 2: MSE with Different Samples
# Initialize a list to store MSE errors for each sample
mse_error_samples <- list()

# Loop to create different samples and calculate MSE errors
for (j in 1:10) {
  # Create a different sample
  train <- sample(c(TRUE, FALSE), nrow(Auto), replace = TRUE, prob = c(0.6, 0.4))
  
  # Initialize a vector to store MSE errors for the current sample
  mse_error_samples[[j]] <- rep(0, 10)
  
  # Loop to calculate MSE errors for polynomial degrees 1 to 10
  for (i in 1:10) {
    glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto[train, ])
    predictions <- predict(glm_fit, newdata = Auto[!train, ])
    actuals <- Auto$mpg[!train]
    mse_error_samples[[j]][i] <- mean((predictions - actuals)^2)
  }
}

y_max_samples <- max(unlist(mse_error_samples)) + 1

# Plot the MSE errors for different samples
plot(1:10, mse_error_samples[[1]], type = "b", col = 1, xlab = "Degree of Polynomial", ylab = "MSE", main = "MSE with Different Samples",
  ylim = c(0, y_max_samples))
for (j in 2:10) {
  lines(1:10, mse_error_samples[[j]], type = "b", col = j)
}
legend("bottomleft", legend = paste("Sample", 1:10), col = 1:10, lty = 1, cex = 0.8)

```

In the first graph we can see that a polynomial of level 2 reduces drastically the MSE from a linear model, but more polynomial terms do not have the same effect. The second graph shows us how the MSE vary with different samples, so the choice of the sample will have an effect on the fit of the model. Another drawback of this method is that, by fitting the model to a smaller number of data points, the estimated error will be higher than fitting the model to the whole available data points.

Now we will see some techniques that try to solve some of those drawbacks.

#### K-fold Cross-validation

*K-fold cross validation* is a widely used approach for estimating test error. The idea is to randomly divide the data into K equal-sized parts. We leave out part $k$, fit the model to the other $K-1$ parts (combined) and then obtain predictions for the left-out $kth$ part.This is done in turn for each part k, and then the results are combined. K is usually between 5 and 10. Let's the K parts be $C_1,C_2,\dots,C_k$. where $C_k$ denotes the indices of the observations in part k. There are $n_k$ observations in part k. $$
CV_{(K)}= \sum^K_{k=1}\frac{n_k}{n} MSE_k
$$ {#eq-kfoldCrossValidationErrorRate}

Where: $CV_{(K)}$: This represents the cross-validation error estimate for K-fold cross-validation. $\sum^K_{k=1}$: This indicates that the sum is taken over all K folds. $\frac{n_k}{n}$: This is the proportion of the total data used in the k-th fold, where $n_k$ is the number of observations in the k-th fold and $n$ is the total number of observations. $MSE_k$: This is the Mean Squared Error for the k-th fold. $MSE_k=\sum_{i\epsilon C_k}(y_i-\hat{y_i})^2/n_k$ and $\hat{y_i}$ is the fit for the observation i, obtained from the data with the part k removed.

One special use of the k-fold cross validation is setting K=n, this is called *leave-one-out cross validation (LOOCV)*: each observation is left out and used as a validation set, while the whole set of other data points are used for training the model. Leave one out cross validation for linear or polynomial model uses this formula: $$
CV_{(n)}=\frac{1}{n} \sum_{i=1}^n \left(\frac{y_i-\hat{y_i}}{1-h_i} \right)^2
$$ {#eq-leveoneoutCrossValidationErrorRate}

where $\hat{y_i}$ is the ith fitted value for the original least squares fit and $h_i$ is the leverage (diagonal of the "hat" matrix). This is like the ordinary MSE except the ith residual is divided by $1-h_i$ The $h_i$ tells you how much influence a single point has on the its own fit, it will be a number between 0 and 1.

Cross-validation takes the average of the errors over the n-folds so in LOOCV each sample looks much more like the other ones, because it only leaves one point out. As a result, each error estimate are highly correlated, when you average these highly correlated errors, the resulting average has a high variance -it is more sensitive to slight changes or outliers- because there's little variation in the training data across folds. It's not about the individual errors being different; it's about them moving together because the training sets are too similar. Remember the trade off we talked about between bias and variance in the [Bias-variance trade off](#bias-variance-trade-off) section

For this reason it is often preferred to use k=5 or 10. Let's compute the cross-validation in r using the formula we saw above. First we are going to fit the model to a linear model. We can use the function glm and if we don't pass any family type it will fit to a linear model.

```{r, fig.cap='LOOCV linear model'  }

#linear model:
fit.lm<- glm(mpg ~ horsepower, data = Auto)
loocv<- function(fit){
  h <-lm.influence(fit)$h
  mean((residuals(fit)/(1-h))^2)
}

loocv(fit.lm)
```

But we have a handy function in r that will do this for us: We will use the `library(boot)` to calculate the cross validation errors for the Auto dataset using the function `cv.glm`. If we don't pass any value of k, it will default to leave-one-out cross-validation.

```{r, fig.cap='LOOCV linear model'  }

glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- boot::cv.glm(Auto, glm.fit)
cv.err$delta
```

And we see that we get the same result that the one we calculated manually above. The first Delta is the cross-validation prediction error, and the second one is a biased corrected version of it, taking into consideration that the dataset is smaller for the dataset in cross validation. This has little effect in leave one out, but it will have a bigger effect in k=n cross-validation.

Now we are going to fit polynomials of different degrees to our data because it looks non-linear, so we expect some of these to fit better than a linear model.

```{r, fig.aligh ='center',fig.width=12, fig.height=4, fig.cap='LOOCV for different polynomials'}
# Leave one out
cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- boot::cv.glm(Auto, glm.fit)$delta[1]
}
# Plot the MSE errors for all data
plot(1:10, cv.error, 
     type = "b", 
     col = 1, 
     ylim= c(2,(max(cv.error) + 1)),
     xlab = "Degree of Polynomial", 
     ylab = "MSE", 
     main = "Leave One Out")
```

Now let's try 10-k fold cross-validation.

```{r, fig.aligh ='center', fig.width=12, fig.height=4, fig.cap=' 10k-fold cross validation' }
## $k$-Fold Cross-Validation
# Initialize a list to store CV errors for each sample
cv.error.10 <- list()

# Loop to create different samples and calculate MSE errors
for (j in 1:10) {
  # Initialize a vector to store MSE errors for the current sample
  cv.error.10[[j]] <- rep(0, 10)
  
  # Loop to calculate MSE errors for polynomial degrees 1 to 10
  for (i in 1:10) {
    glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
    cv.error.10[[j]][i] <- boot::cv.glm(Auto, glm.fit, K = 10)$delta[1]
  }
}

y_max_samples <- max(unlist(cv.error.10)) + 1

# Plot the MSE errors for different samples
plot(1:10, cv.error.10[[1]], type = "b", col = 1, xlab = "Degree of Polynomial", ylab = "MSE", main = "k=10 fold CV",
  ylim = c(2, y_max_samples))
for (j in 2:10) {
  lines(1:10, cv.error.10[[j]], type = "b", col = j)
}
legend("bottomleft", legend = paste("k", 1:10), col = 1:10, lty = 1, cex = 0.8)

```

We can see how the variability in 10-k fold is much less than what we obtained before using half of the data (2-k)

For classification problems the k-fold cross-validation works exactly the same, the only difference is that instead of MSE to measure the cross validation error we use the \[misclassification error rate\] (#misclassificationErrorRate).

#### Potential problems with cross-validations.

Consider a simple classifier applied to some two-class data. We have 5000 predictors and 50 samples. With this, we find the 100 predictors having the largest correlation with the class labels. We then apply a classifier such as logistic regression using only those 100 predictors. If we now try to estimate the test set performance of this classifier using cross-validation, we will get a much lower error estimate than real. This is because when we choose the 100 best predictors, this is a form of training for the model, and must be included in the validation process. If for example we simulate data with the class labels independent of the outcome (so our test error is 50%) and then we choose the best predictors and create a model with them, when we calculate the cross-validation, our error estimate will be smaller than it should be.

To avoid this we will have to create our folds and remove our test fold before the process of fitting and choosing the predictors, and then apply the cross validation over the fold we set apart.

You can find more information in the [lectureVideo](#https://www.youtube.com/watch?v=jgoa28FR__Y)

::: exercise-box
Example: Model selection with Cross Validation:

We now try to choose among the models of different sizes using cross-validation. We must perform best subset selection *within each of the* $k$ training sets. Despite this, we see that with its clever subsetting syntax, `R` makes this job quite easy.

First, we create a vector that allocates each observation to one of $k=10$ folds, and we create a matrix in which we will store the results.

```{r }
k <- 10
n <- nrow(Hitters)

folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 19,
    dimnames = list(NULL, paste(1:19)))
```

Now we write a for loop that performs cross-validation. In the $j$th fold, the elements of `folds` that equal `j` are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new `predict()` method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix `cv.errors`. Note that in the following code `R` will automatically use our `predict.regsubsets()` function when we call `predict()` because the `best.fit` object has class `regsubsets`.

```{r }
for (j in 1:k) {
  best.fit <- regsubsets(Salary ~ .,
       data = Hitters[folds != j, ],
       nvmax = 19)
  for (i in 1:19) {
    pred <- predict(best.fit, Hitters[folds == j, ], id = i)
    cv.errors[j, i] <-
         mean((Hitters$Salary[folds == j] - pred)^2)
   }
}
```

This has given us a $10 \times 19$ matrix, of which the $(j,i)$th element corresponds to the test MSE for the $j$th cross-validation fold for the best $i$-variable model. We use the `apply()` function to average over the columns of this matrix in order to obtain a vector for which the $i$th element is the cross-validation error for the $i$-variable model.

```{r, fig.align='center'}
(mean.cv.errors <- apply(cv.errors, 2, mean))

par(mfrow = c(1, 1))

plot(sqrt(mean.cv.errors), type = "b", ylab="sqrt MSE")

```

We see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.

```{r}
reg.best <- regsubsets(Salary ~ ., data = Hitters,
    nvmax = 19)
coef(reg.best, 10)
```
:::

### The bootstrap principle {#bootstrap}

The *bootstrap* is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical method. For example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient.

We have an estimate $\hat{\theta}$ for a parameter $\theta$ and want to know how accurate our estimate is. For that we need to know the SE of the estimate or give a confidence interval for the parameter $\theta$. The bootstrap can do this in quite general settings:

Example $\theta$= average height of all people in the US. It is unknown but estimated with the average height of 100 randomly selected people. We can't compute the population mean because we can't access the whole population. So we ' plug in' the sample in place of the population and compute the mean of the sample instead. The rationale for the plug-in principle is that the sample mean will be close to the population mean because the sample histogram is close to the population histogram.

The bootstrap uses the plug-in principle and the Monte Carlo Method to approximate quantities such as $SE(\hat{ \theta})$ when we cannot get many samples.

The bootstrap pretends that the sample histogram is the population histogram and uses the Monte Carlo to simulate the quantity of interest.

What we do is we draw n samples **with replacement** from the original sample, each sample is the same size $n$ as the original dataset, and with those samples we use the Monte Carlo principle to calculate the quantity of interest. Example of 4 samples chosen at random from a dataset with three points:

```{r}
x <- rnorm(3,mean=5,sd=2)
Y <- 0.2+0.33*x+ rnorm(1, -1.1, .8)

pop<- data.frame(obs=c("A","B","C"),x,Y)
pop
#bootstrap samples:
set.seed(1)
s<- sample(c(1,2,3),3,replace=TRUE)
pop[s,]
set.seed(2)
s<- sample(c(1,2,3),3,replace=TRUE)
pop[s,]
set.seed(3)
s<- sample(c(1,2,3),3,replace=TRUE)
pop[s,]
set.seed(4)
s<- sample(c(1,2,3),3,replace=TRUE)
pop[s,]
```

Drawing a bootstrap sample by sampling with replacement from the data is called *nonparametric bootstrap*.

If we know that the data follow a normal distribution, but we don't know the mean on the standard deviation. The obvious thing to do there is to simply estimate the unknown mean and standard deviation, and then simply sample from the normal distribution, with that mean and standard deviation. That's called *parametric bootstrap*.

This type of sampling works if the data are independent, that is, $X_1$ to $X_n$ are really generated independently. But oftentimes, there's dependence in the data. For example, the data are observed over time.

If the sampling distribution of $\hat{\theta}$ is approximately normal, then the confidence interval formula is :

$$
\left[ \hat{\theta} - z_{\alpha/2} SE(\hat{\theta}), \hat{\theta} + z_{\alpha/2} SE(\hat{\theta}) \right]
$$ {#eq-conficenceIntervalEstimatorNormalDistribution}

This formula represents a confidence interval for an estimator $\hat{\theta}$, where:

-   $\hat{\theta}$ is the point estimate.

-   $z_{\alpha/2}$ is the critical value from the standard normal distribution for a given confidence level. For example for a confidence level of 95% $\alpha =(1-0.95)=0.05$ ; $\frac{\alpha}{2}=0.025$, $z_{\frac{\alpha}{2}}=1.96$

-   $SE(\hat{\theta})$ is the standard error of the estimator.

If the distribution is not normal, we can still use the bootstrap by estimating that the distribution of the estimate can be approximated by the distribution of the bootstrap-copies extracted from it, so we can draw a distribution curve and calculate the 95% confidence interval using that curve:

$$
\left[ \hat{\theta}^*_{\alpha/2}, \hat{\theta}^*_{1-\alpha/2} \right]
$$ {#eq-conficenceIntervalEstimatorNotGaussian}

where $\hat{\theta}^*_{\alpha/2}$ is the $\alpha/2$ percentile of the $\hat{\theta}^*_{1},....\hat{\theta}^*_{B}$

::: exercise-box
Estimating the accuracy of a linear regression Model

The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for $\beta_0$ and $\beta_1$ (intercept and slope) for a linear regression model that uses horsepower to predict mpg (miles per gallon) in the Auto data set.

We first create a simple function `boot.fn()` which takes in the Auto data set as well as the indices for the observations, and returns the intercept and slope estimates for the linear regression model.

```{r}
boot.fn<- function (data, index){
  coef(lm(mpg ~ horsepower, data= data, subset = index))
}
```

We then apply this function to the full set of 392 observations in order to compute get the slope and intersect for the full data set:

```{r}
boot.fn(Auto,1:392)
```

if we use bootstrap principle we just need to create a sample with replacement of the same size of the full data set. This is one sample example:

```{r}
boot.fn(Auto, sample(392,392, replace=TRUE))
```

but what we do in r is to use the `boot()` function from the boot library to compute the standard errors of 1,000 bootstrap estimates for the intercept and the slope:

```{r}
set.seed(1)
boot(Auto, boot.fn,1000)
```

This indicates that the bootstrap estimate for $SE(\beta_0)$ is 0.84 and for $SE(\beta_1)$ is 0.0073

Standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.

```{r}
summary(lm(mpg ~ horsepower, data= Auto))$coef
```

these standard errors are calculated using the formulas we saw in [Assessing the Accuracy of the Coefficient Estimates and Confidence Intervals](#assessing-the-accuracy-of-the-coefficient-estimates-and-confidence-intervals)

The formula for the slope: (@eq-standardErrorSlope) and for the intercept1:(@eq-standardErrorIntercept)

These formulas rely on certain assumptions (out of the scope for this document, check the ISLR book for more info). The bootstrap approach does not rely on any of these assumptions, and so, it is likely giving more accurate estimate of the standard errors than the summary function
:::

If the data is a time series, we can't simply sample the observations with replacement because the data is not independent. There's correlation in time series. For example we want to predict the value of a stock price using the previous days stock price. We expect the stock price for a given day to be correlated with the stock price from the previous day. This creates a problem for the bootstrap because bootstrap assumes the data are independent.

What people uses for time series is the *block boostrap*, it divides the data up into blocks and we assume that in between blocks, the data is independent.

Another instance where bootstrap is not a good approach is for estimating the prediction error in cross-validation problems. In cross-validation, each of the K validation folds is distinct from the other K-1 folds used for training, there is no overlap, this is crucial for its success. But each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. This will cause the bootstrap to seriously underestimate the true prediction error.

::: exercise-box
Estimating the Accuracy of a Statistic of Interest

We will be using the dataset Portfolio from LSLR package for this practice. Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of X and Y, respectively, where X and Y are random quantities. We will invest a fraction α of our money in X, and will invest the remaining 1 − α in Y . Since there is variability associated with the returns on these two assets, we wish to choose α to minimize the total risk, or variance, of our investment.In other words, we want to minimize Var(αX +(1−α)Y ). The value that minimizes the risk is given by the formula:

$$
\alpha= \frac{\sigma^2_Y-\sigma_{XY}}{\sigma^2_X+\sigma^2_Y-2\sigma_{XY}}
$$

where $\sigma^2_X$= VAR(X), $\sigma^2_Y$= VAR(Y) and $\sigma_{XY}$ = COV(X,Y). These quantities are unknown, so we will estimate them using our data. We can write that function in r:

```{r}

alpha.fn <- function(data, index) {
  X <- data$X[index]
  Y <- data$Y[index]
  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}

alpha.fn(Portfolio, 1:nrow(Portfolio))
```

but what is the sampling variability of α? we use boostrap for this:

```{r, fig.align='center', fig.width=7}
set.seed(1)
boot.out<- boot::boot(Portfolio, alpha.fn, R = 1000)
boot.out

plot(boot.out)

```

the statistic came as 0.5758 as before and we now know our standard error that came as 0.093

The plots show you the distribution of the samples and the QQ plot.
:::

### Use of Monte Carlo Simulation and Bootstrap

Simulations can also be used to check theoretical or analytical results. Also, many of the theoretical results we use in statistics are based on asymptotics: they hold when the sample size goes to infinity. In practice, we never have an infinite number of samples so we may want to know how well the theory works with our actual sample size. Sometimes we can answer this question analytically, but not always. Simulations are extremely useful in these cases.

As an example we are going to use simulation to compare the Central Limit Theorem to the t-distribution for different sample sizes. We will use our mice data and extract control and 'fake' treatment samples. The fake treatment samples are just samples labeled as treatment but extracted from the control population.

We will build a function that automatically generates a t-statistic under the null hypothesis for a sample size of `n`.

```{r}
dat <- read.csv("data/mice_pheno.csv")
controlPopulation <- filter(dat,Sex == "F" & Diet == "chow") %>%  
  dplyr::select(Bodyweight) %>% unlist

ttestgenerator <- function(n) {
  cases <- sample(controlPopulation,n)
  controls <- sample(controlPopulation,n)
  tstat <- (mean(cases)-mean(controls)) / 
      sqrt( var(cases)/n + var(controls)/n ) 
  return(tstat)
  }
ttests <- replicate(1000, ttestgenerator(10))
```

```{r ttest_hist, fig.cap="Histogram of 1000 Monte Carlo simulated t-statistics.", fig.align='center'}
hist(ttests)
```

we can use quantile-quantile plots to see how well this distribution approximates to the normal:

```{r ttest_qqplot, fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics to theoretical normal distribution.", fig.align='center'}
qqnorm(ttests)
abline(0,1)
```

This looks like a very good approximation. For this particular population, a sample size of 10 was large enough to use the CLT approximation. How about sample size 3?

```{r, ttest_df3_qqplot, fig.align='center',fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical normal distribution."}
ttests <- replicate(1000, ttestgenerator(3))
qqnorm(ttests)
abline(0,1)
```

Now we see that the large quantiles, referred to by statisticians as the *tails*, are larger than expected (below the line on the left side of the plot and above the line on the right side of the plot). In the previous module, we explained that when the sample size is not large enough and the *population values* follow a normal distribution, then the t-distribution is a better approximation. Our simulation results seem to confirm this:

In the code below first we generate the percentiles: `ps <- (seq(0,999)+0.5)/1000` seq(0,999) generates a sequence of numbers from 0 to 999. Adding 0.5 to each element centers the sequence values. Dividing by 1000 scales the values to be between 0 and 1. This results in 1000 equally spaced percentiles between 0.0005 and 0.9995. `qt(ps, df=2*3-2)` computes the quantiles of the t-distribution for the given percentiles `ps` with degrees of freedom `df=2*3-2` qqplot creates a QQ plot comparing the quantiles of our t-test results to the theoretical quantiles of the t-distribution.

```{r, ttest_v_tdist_qqplot,fig.cap="Quantile-quantile plot comparing 1000 Monte Carlo simulated t-statistics with three degrees of freedom to theoretical t-distribution.", fig.align='center'}
ps <- (seq(0,999)+0.5)/1000
qqplot(qt(ps,df=2*3-2),ttests,xlim=c(-6,6),ylim=c(-6,6))
abline(0,1)
```

The QQ plot helps you visually assess how well your t-test results follow the theoretical t-distribution. The t-distribution is a much better approximation in this case, but it is still not perfect. This is due to the fact that the original data is not that well approximated by the normal distribution. We can see this plotting the qqplot for the full population data:

```{r, dat_qqplot, fig.cap="Quantile-quantile of original data compared to theoretical quantile distribution.", fig.align='center'}
qqnorm(controlPopulation)
qqline(controlPopulation)
```

## Shrinkage Methods

*Ridge regression* and *lasso* The subset selection methods use least squares to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficients estimates, or equivalently, that shrinks the coefficient estimates towards zero.Shrinking the coefficient estimates can significantly reduce their variance.

**Ridge regression** Also called $L2$ Regularization. It addresses the problem of multicollinearity (when predictor variables are highly correlated). It is useful when you have many predictors and suspect multicollinearity. It works well when the number of predictors is larger than the number of observations.

$$
\sum^n_{i=1}\left( y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij}\right) + \lambda\sum^p_{j=1}\beta^2_j
$$ {#eq-ridgeRegression}

the first part of the function is RSS so we can write it like this: $$
RSS + \lambda\sum^p_{j=1}\beta^2_j
$$ We call $\lambda\sum^p_{j=1}\beta^2_j$ the *Ridge Penaly* where $\lambda$ is called the *tuning parameter*, to be determined separately.It controls the strength of the penalty. When $\lambda$ is 0, it is just ordinary least square regression, as $\lambda$ increases, the flexibility of the model decreases, helping to reduce overfitting.

As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small, however, the second term of the function, called shrinkage penalty, is small when the coefficients are close to zero, and so it has the effect of shrinking the estimates towards zero.

One thing to take into account with this methods is that while in our regression formulas the units or scales of our variables do not matter, (if we measure the weight in grams or in kilograms) because the coefficient will adapt to that scale, but in this method that actually matters, and as a result is is important to standardize the predictors before applying ridge regression. To standardize he predictors we do as usual, we divide the feature by the standard deviation of that feature. $$
\tilde x_{ij} = \frac{x_{ij}}{SD(x_{ij})} =\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n}(x_{ij-\bar{x_j}})^2}
$$ Ridge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model.

**Lasso** Also called $L1$ regularization. The Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The formula is very similar to the ridge regression but instead of taking the squares of the coefficients, now it is the sum of the absolute value: $$
RSS + \lambda\sum^p_{j=1}|\beta_j|
$$ The difference is that it actually sets the coefficients to 0 when lambda is large enough, so it subsets the selection removing the features that are not important.

**Selecting the Tuning Parameter for Ridge Regression and Lasso**

As in ridge regression, selecting a good value of lambda for the lasso is critical, we will use again cross validation for this. Lasso is particularly useful when you expect only a few of predictors to be relevant, leading to a simpler, more interpretable model. If most of the predictors are relevant, ridge regression will do better selecting the best model. We don't know this when we create the model, so it is usual to use both methods, and use cross-validation to determine the best model coming out of each method by comparing the cross-validated error.

Cross validation provides a simple way to find the value of $\labmda$ both for lasso and ridge regression. We choose a grid of lambda values, and compute the cross-validation error rate for each value of it. We then select the tuning parameter value for which the cross-validation error is smallest. Finally the model is re-fit using all of the available observations and the selected value of the tuning parameters.

::: exercise-box
Example: Ridge regression

We will use the `glmnet` package in order to perform ridge regression and the lasso. The main function in this package is `glmnet()`, which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far. In particular, we must pass in an x matrix as well as a y vector, and we do not use the y \~ x syntax. We will now perform ridge regression and the lasso in order to predict Salary on the Hitters data. Before proceeding ensure that the missing values have been removed from the data. The `model.matrix()` function is particularly useful for creating x; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because `glmnet()` can only take numerical, quantitative inputs.

```{r}
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary ~ . ,data= Hitters)[, -1]
y <- Hitters$Salary
```

The `glmnet()` function has an alpha argument that determines what type of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit. We first fit a ridge regression model.

By default the `glmnet()` function performs ridge regression for an automatically selected range of λ values. However, here we have chosen to implement the function over a grid of values ranging from $λ=10^{10}$ to $λ=10^{−2}$ , essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of λ that is not one of the original grid values. Note that by default, the `glmnet()` function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument `standardize = FALSE`.

```{r}
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```

Associated with each value of λ is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a 20×100 matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of λ).

```{r}
dim(coef(ridge.mod))
ridge.mod$lambda[50] #the value of lambda at possition 50
coef(ridge.mod)[,50] #coefficients for that value of lambda
plot(ridge.mod, xvar="lambda", label=TRUE)
```

This plot is showing how the coefficients shrink as we increase the value of lambda.

We can use predict() function for a number of purposes For instance we can obtain the ridge regression coefficients for a new value of lambda, say 50:

```{r}
predict(ridge.mod, s=50, type= 'coefficients')[1:20,]
```

We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and fit a ridge regression model on the training set and evaluate its MSE on the test set using lambda=4. Note the use of `predict()` this time we get predictions for a test set by replacing the `type="coefficients"` with the `newx` argument

```{r}
set.seed(1)
train<- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh= 1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx=x[test,])
(MSE<- mean((ridge.pred-y.test)^2))
```

the test MSE = `{r} MSE`

We now check whether there is any benefit to performing ridge regression with lambda=4 instead of just performing least squares regression (lambda=0). In order for `glmnet()` to yield exact least squares coefficients when lambda is 0, we use the argument `exact=T`

```{r}
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ],
    exact = T, x = x[train, ], y = y[train])
mean((ridge.pred - y.test)^2)

```

In general, instead of arbitrarily choosing a value for lambda, it would be better to use cross-validation to choose the tuning parameter. We can do this using the built in cross-validation function.

The `glmnet` package have a function `cv.glmnet()` that will do the cross validation. By default the function performs ten-fold cross-validation, though this can be changed using the argument `nfold`.

```{r}
set.seed(1)
cv.ridge<- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.ridge)

```

WE can see here that with higher values of lambda the mean squares errors are too high (the restricted coefficients are too small to fit the model properly). The vertical lines shows the minimum on the left and the other vertical line is at one standard error from the minimum. The top x axis shows the number of variables in the model. In this case what the graph is showing is that the model without adjustment with the full coefficients for all the variables is doing quite well to fit the model.

The best value for lambda that results in the smallest cross-validation error is 326. What is the MSE associated with this value of lambda?

```{r}
bestlam <- cv.ridge$lambda.min
bestlam
ridge.pred <- predict(ridge.mod, s = bestlam,
    newx = x[test, ])
mean((ridge.pred - y.test)^2)
```

This represents an improvement over lambda 0. Finally we refit our ridge regression model on the full data set, using the value of lambda chosen by cross-validation and examine the coefficient estimates.

```{r}
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20, ]
```
:::

::: exercise-box
Example: lasso

We saw that ridge regression with a wise choice of λ can outperform least squares on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the `glmnet()` function; however, this time we use the argument `alpha=1`. Other than that change, we proceed just as we did in fitting a ridge model.

```{r chunk39}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1,
    lambda = grid)
plot(lasso.mod, xvar="lambda", label= TRUE)
```

We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. The top x axes tells us how many variable coefficients are not 0 We now perform cross-validation and compute the associated test error.

```{r chunk40}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min

#test error:
lasso.pred <- predict(lasso.mod, s = bestlam,
    newx = x[test, ])
mean((lasso.pred - y.test)^2)
```

This is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with $\lambda$ chosen by cross-validation.

However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with $\lambda$ chosen by cross-validation contains only eleven variables.

We now run the model over the full dataset:

```{r }
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients",
    s = bestlam)[1:20, ]
lasso.coef[lasso.coef != 0]
```
:::

## Dimension Reduction Methods

To recapitulate, we have seen subset selection methods, where we were just taking a subset of the predictors and using least squares models choosing the best model. Then ridge regression and lasso, where we were taking all of the predictors, but it does not use least squares, but a shrinkage approach to fit the model. Now we are going to use least squares but not on the original predictors, instead we are going to come up with new predictors, which are linear combinations of the original ones.

Let $Z_1,Z_2,\dots,Z_M$ represent linear combination of some of the $p$ original predictors and use them to fit a linear regression model:

$$
y_i=\theta_0 + \sum_{m=1}^M \theta_mz_{im}+\epsilon_i
$$ {#eq-dimensionReductionFunction}

**Principal Components Regression (PCR)**

The first principal component is that (normalized) linear combination of the variables with the largest variance. The second principal component has largest variance, subject to being uncorrelated with the first etc.

**Partial Least Squares (PLS)** Like PCR, PLS is a dimension reduction method, which first identifies a new set of features that are linear combinations of the original features, and then fits a linear model. After standardizing the p predictors, PLS computes the first direction $Z_1$ by setting each $\phi_{ij}$ equal to the coefficient from the simple linear regression of Y onto $X_j$, hence, PLS places the highest weight on the variables that are most strongly related to the response. Subsequent directions (Z) are found by taking residuals and then repeating the above process.

# Extending the linear models

## Categorical predictors in regression models.

Sometimes we may want to include in the model a variable that is not numeric, for example gender. To do that we create what we call a dummy variable, for example $x_i = 1$ if the person is female and $x_i = 0$ if it is a male. The resulting model will be: $$
y_i=\beta_o+\beta_1x_i+\epsilon_i
$$ which will result in this if the person is female: $$
y_i=\beta_o+\beta_1+\epsilon_i
$$ and if the person is male: $$
y_i=\beta_o+\epsilon_i
$$ so what this is telling us is that $\beta_1$ is the effect of being female vs the baseline (in this case male)

If we have more than two levels what we do is create more dummy variables. For example we look at three different etnicities, Asian, Caucasian and African American, we create:

$x_{i1} = 0$ if person is Asian and $x_{i1} = 1$ if the person is not Asian $x_{i2} = 0$ if person is Caucasian and $x_{i2} = 1$ if the person is not. We don't need a level for African American because that will be deducted when $x_{i1} = 0$ and $x_{i2} = 0$. So for categorical variables we create $k-1$ dummy variables. The level with no dummy variable is known as *baseline*. The choice of a baseline will not affect the fit of the model, the residual sum of squares would be the same, but the coefficient and the $p$-values will change because each other category will be compared with the baseline.

The equation will now look like this: $$
y_1 = \beta_o+\beta_1x_{i1}+\beta_2x_{i2}+\epsilon_i
\begin{cases} 
\beta_0 + \beta_1 + e_1 & \text{if condition 1} \\
\beta_0 + \beta_2 + e_1 & \text{if condition 2} \\
\beta_0 + e_1 & \text{if condition 3}
\end{cases}
$$

::: exercise-box
Example

*The Carseats data from the library ISLR2 includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location---that is, the space within a store in which the car seat is displayed---at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically.*

```{r}
lm.fit <- lm(Sales ~ ShelveLoc, 
    data = Carseats)
summary(lm.fit)
contrasts(Carseats$ShelveLoc)
```

The contrasts() function returns the coding that R uses for the dummy variables.

R has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.
:::

## Interactions

In our previous examples, we have assume independence from one parameter to the others, but that is not always the case, sometimes the change in one predictor affects the results in another.

::: {exercise-box}
Example Imagine we run a campaign to sell a product, and we advertise in three mediums, newspapers, radio and tv. It could be that the effect of radio increases the effectiveness of the adds in tv, this in marketing is called synergy effect, and in statistics we refer to it as interaction effect. If we detect this interaction, spending part of our budget in radio and part on tv could be more effective than spending all in only the media with the most effect.

```{r}
lm.fit<- lm(sales ~ TV + radio, data = Advertising)
summary(lm.fit)
```

to include interactions in our model we create a new variable with the product of the two predictors. If we ignore newspaper our equation would be:

$$
sales = \beta_0+\beta_1\times TV+\beta_2\times Radio + \beta_3 \times(radio\times TV)+ \epsilon
$$

$$
sales = \beta_0+(\beta_1 + \beta_3\times Radio) \times TV+\beta_2\times Radio + \epsilon
$$ and if we get a summary of the linear model we will see if the interaction between tv and radio is indeed significant or not:

```{r}
lm.fit_interaction <- lm(sales ~ TV * radio , data = Advertising)
summary(lm.fit_interaction)
# or
lm.fit_interaction <- lm(sales ~ TV + radio + TV:radio , data = Advertising)
summary(lm.fit_interaction)

```

the $p$-value of the interaction indicates that in our example this interaction is significant, we can also see how our R-squared is higher now (0.9678) than when we did not include the interaction in our model (0.8972). This means that (96.8-89.7)/(100-89.7)=69% of the variability in sales that remains after fitting the initial model has been explained by the interaction term.

The coefficients estimates in the table suggest that an increase in radio advertising of \$1000 is associated with increased sales of $(\hat{\beta_2}+\hat{\beta_3}\times TV)\times 1000 = 29+1.1\times TV units$ An increase in TV advertising of \$1000 is associated with an increase of sales of $(\hat{\beta_1}+\hat{\beta_3}\times radio)\times 1000 = 19+1.1\times radio units$
:::

Sometimes it is the case that an interaction term has a very small $p$-value, but the associated main effects do not. The *hierarchy principle* states that if we include an interaction in a model, we should also include the main effects, even if the $p$-values associated with their coefficients are not significant.

## Non-linear transformations of the predictors.

If a linear model does not quite fit our data:

```{r, fig.align='center'}

lm.fit<- lm(mpg~ hp, data= mtcars)
plot(mtcars$hp, mtcars$mpg)
abline(lm.fit)
summary(lm.fit)
```

we can transform this into a polynomial regression by making extra variables to accommodate polynomials, for example we add another variable horsepower squared:

```{r}
lm.fit <- lm(mpg ~ hp + I(hp^2), data = mtcars)

plot(mtcars$hp, mtcars$mpg, xlab = "Horsepower", ylab = "Miles per Gallon")

points(mtcars$hp, fitted(lm.fit), col = "pink", lwd = 2)

#or if we want to show a line:
# Create a sequence of hp values for prediction
hp_seq <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 100)

# Predict mpg for each hp value in the sequence
predicted_mpg <- predict(lm.fit, newdata = data.frame(hp = hp_seq))

lines(hp_seq, predicted_mpg, col = "blue", lwd = 2)

summary(lm.fit)

```

but there is a better way of fitting polynomials with r using the `poly()` function:

```{r, fig.align='center'}
lm.fit <- lm(mpg ~ poly(hp,4), data = mtcars) 
plot(mtcars$mpg ~ mtcars$hp)
points(mtcars$hp, fitted(lm.fit), col = "pink", lwd = 2)

```

## Potential problems in linear models

We already talked about most of them, but let's do a summary:

::: callout-orange
1.  **Non linearity**: The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. We saw how we can make use of the residual plot to see if the relation is liner or not when we talked about [residuals](%7B#residualplots%7D)

2.  **Correlation of error terms**: An important assumption of the linear regression model is that the error terms are uncorrelated.If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors and this will affect the $p$-values, confidence intervals etc. Why might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. time series. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no discernible pattern. On the other hand, if the error terms are positively correlated, then we may see tracking in the residuals.Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals' heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family, eat the same diet,or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.

3.  **Non-constant variance of error terms [(heteroscedasticity)](#heteroscedasticity)** : error terms have a constant variance, Var(ϵi) = σ2. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. Unfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response

4.  **Outliers**: An outlier is a point for which $y$ is far from the value predicted by the model. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance it affects the RSE. Since the RSE is used to compute all confidence intervals and $p$-values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Residual plots can be used to identify outliers.

5.  **High-leverage points**: Observations with [high leverage](#highLeverage) have an unusual value for $x_i$. Removing a high leverage point has much more impact than removing an outlier. In a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor's values, but that is unusual in terms of the full set of predictors.

6.  **Collinearity**: we already dedicated [a section](#collinearity) to these problem.
:::

# Introduction to ANOVA (Analysis of Variance)

Analysis of variance, or ANOVA, is used to test **the relationship between a categorical and a quantitative variable**. Specifically, it test the null hypothesis that the population mean of the quantitative variable is the same in each of the groups. A low $p$-value indicates that the data would be unlikely if in fact those population means where equal.

ANOVA requires that the data be relatively symmetric in each group and not include extreme outliers. Additionally, the spread of the data within the groups should be similar. Always plot your data before running ANOVA to check these assumptions.

ANOVA is an omnibus test: on it's own, it does not say anything about which group or groups might be interesting.

We are going back to our attrition dataset and analyse if the variable of work life balance gives us any information about the money they make (MonthlyRate). A first intuitive approach is to calculate the average MontlyRate per group:

```{r}
attrition1 %>% group_by(WorkLifeBalance) %>% 
  summarise (mean(MonthlyRate)) 
```

this shows that there is in fact a difference but, is this difference important or significant?

One first approach to answer this question is observing the data in a boxplot:

```{r, fig.align='center', echo=FALSE}
ggplot (data= attrition1, aes(x = WorkLifeBalance, y= MonthlyRate))+
  geom_boxplot()
```

This already shows us that the spread of the data in each of the groups is big compared to the the spread of the data between the groups, but one way we can test this is doing an ANOVA test that is going to compare the variances within the groups to the variance between the groups.

```{r}
testResult<- (aov(MonthlyRate ~ WorkLifeBalance, data = attrition1))
sumRes<- summary(testResult)
sumRes
```

if we look at our $p$-value that is testing if in fact all of these groups have the same mean it is `` r sumRes[[1]]$`Pr(>F)` `` which means that in almost 65% of the cases we would see these kind of data even if the population means for these groups were the same.

Our null hypothesis is that all group means are equal. If we had only two groups, we would use a t test

$$
t=\frac{difference\ in \ sample\ means}{SE\ of\ difference}
$$

now we generalize this idea to the instance when we have several groups. If the differences between the samples means are large relative to the variability within the groups, this suggest that our null hypothesis is not true. In contrast, if the differences in the means are quite small compared with the variability within each groups, that suggest that the differences in the means are due to variability in the sample.

```{r , fig.height=6, fig.width=12, fig.align='center'}
#|echo: FALSE
# Set seed for reproducibility
set.seed(123)

# Generate data for large differences between group means
group_A1 <- rnorm(100, mean = 10, sd = 2)
group_B1 <- rnorm(100, mean = 20, sd = 2)
group_C1 <- rnorm(100, mean = 30, sd = 2)

# Generate data for small differences between group means
group_A2 <- rnorm(100, mean = 15, sd = 10)
group_B2 <- rnorm(100, mean = 17, sd = 10)
group_C2 <- rnorm(100, mean = 19, sd = 10)

# Create data frames
data_large_diff <- data.frame(value = c(group_A1, group_B1, group_C1),
                              group = rep(c("A", "B", "C"), each = 100))

data_small_diff <- data.frame(value = c(group_A2, group_B2, group_C2),
                              group = rep(c("A", "B", "C"), each = 100))

# Create boxplots
p1 <- ggplot(data_large_diff, aes(x = group, y = value)) + 
  geom_boxplot() + 
  ggtitle("Large Differences Between Group Means") +
  theme_minimal()

p2 <- ggplot(data_small_diff, aes(x = group, y = value)) + 
  geom_boxplot() + 
  ggtitle("Small Differences Between Group Means") +
  theme_minimal()

# Combine plots side by side
combined_plot <- p1 + p2

# Print the combined plot
print(combined_plot)

```

But unfortunately things are not as easy as looking at the boxplots, the reason is that according to the square root law, the chance variability in the sample mean is smaller than the chance variability in the data, so the evidence against $H_0$ is not obvious from the boxplots. Computation is necessary.

We have k groups with n observations:

| observation | group 1  | group 2  | group k  |
|-------------|----------|----------|----------|
| 1           | $y_{11}$ | $y_{12}$ | $y_{1k}$ |
| 2           | $y_{21}$ | $y_{22}$ | $y_{2k}$ |
| ...         | ...      | ...      | ...      |
| n           | $y_{n1}$ | $y_{n2}$ | $y_{nk}$ |

in total there are $N=n_1+n_2+\cdots+n_k$ observations. The sample mean of one group is $\bar{y_j}=\frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}$ and the overal sample mean or grand mean is $\bar{\bar{y}}= \frac{1}{N}\sum_{j=1}^{k}\sum_{i=1}^{n_j}y_{ij}$

The analysis of variance compute two important variables, one is the **treatment sum of squares (SST)**{#SSTTreatmentSumSquares} that look at the difference between the group means to the overall mean

$$
SST=\sum_j\sum_i(\bar{y_j}-\bar{\bar{y}})^2 
$$ {#eq-SSTTreatmentSumSquares}

and has k-1 degrees of freedom.

If we divide this by its degrees of freedom we get what is called the **treatment mean square (MST)**{#MSTTreatmentMeanSquare} and measures the variability of the treatment means $\bar{y_j}$

$$
MST= \frac{SST}{k-1}
$$ {#eq-MSTTreatmentMeanSquare}

The other quantity we are interested in is the **error sum of squares (SSE)** where we look at the difference between each observation and the group mean.

$$
SSE= \sum_j\sum_i({y_{ij}}-\bar{y_j})^2
$$ {#eq-SSEErrorSumSquares}

and has N-k degrees of freedom.

Dividing the error sum of square by its degrees of freedom we get what is called the **error mean square or Mean Square for Error (MSE)** and measures the variability within the groups. Not to be confused with the the [MSE (Mean Squared Error)](#MeanSquaredError) that we saw in linear model. In both cases, MSE is a measure of variability of the errors, but In ANOVA, MSE (Mean Square for Error) assesses within-group variance. In linear models, MSE (Mean Squared Error) assesses the goodness of fit of the model by looking at prediction errors.

$$
MSE= \frac{SSE}{N-k}
$$ {#eq-MSEErrorMeanSquare}

Since we want to compare the variation between the groups to the variation within the groups we look at the ratio

$$
F= \frac{MST}{MSE}
$$ {#eq-Fstatistic}

Under the null hypothesis of equal group means this ratio should be about 1. It will not be exactly one due to sampling variability. It follows a F-distribution with k-1 and N-k degrees of freedom. Large values of F suggest that the variation between groups is unusually large. We reject the null hypothesis if F is the right 5% of the tail, i.e. when the $p$-value is smaller than 5%.

The result of the test is summarized in the ANOVA table:

| Source    | df  | Sum of Squares | Mean Square | F       | p-value |
|-----------|-----|----------------|-------------|---------|---------|
| Treatment | k-1 | SST            | MST         | MST/MSE |         |
| Error     | N-k | SSE            | MSE         |         |         |
| Total     | N-1 | TSS            |             |         |         |

Where **TSS is the Total Sum of Squares** and measures the total variability in the data (the overall variation of the observed data points around their mean):

$$
TSS=\sum_j\sum_i({y_j}-\bar{\bar{y}})^2 
$$ {#eq-TSS1}

### The one-way ANOVA model

The idea behind the anova table is that each observation is generated as the sum of a treatment $\mu_j$ plus and error term $\epsilon_{ij}$ (measurment of error) that follow the normal curve with mean 0 and common variance $\sigma^ 2$

$$
y_{ij}=\mu_j+\epsilon_{ij}
$$

our null hypothesis is that all treatment means are the same $\mu_1=\mu_2=\cdots=\mu_k$ Instead of looking at the group means it is helpful to look at deviations from an overall mean $\mu$ that deviation is represented by the greek letter 'tau' and called the **treatment effect** $\tau_j=\mu_j-\mu$ so the model is the overall mean plus the deviation plus an error term $$
y_{ij}=\mu+\tau_j+\epsilon_{ij}
$$

We estimate the overall mean $\mu$ by the grand mean $\bar{\bar{y}}$. then the estimate $\tau_j=\bar{y_j}-\bar{\bar{y}}$ and the estimate of the error is the residual $\epsilon_{ij}=y_{ij}-\bar{y_j}$. so we can write the previous equation like this:\
$$
y_{ij}=\mu+\tau_j+\epsilon_{ij}=\bar{\bar{y}}+(\bar{y_j}-\bar{\bar{y}})+(y_{ij}-\bar{y_j})
$$

now if we move $\bar{\bar{y}}$ to the left of the equation:

$$
y_{ij}-\bar{\bar{y}}=(\bar{y_j}-\bar{\bar{y}})+(y_{ij}-\bar{y_j})
$$

and we sum and square the terms:

$$
\sum_{j} \sum_{i} (y_{ij} - \bar{y}_{i})^2 = \sum_{j} \sum_{i} (y_{ij} - \bar{y}_{j})^2 + \sum_{j} \sum_{i} (\bar{y}_{j} - \bar{y}_{i})^2
$$

which is equal to say:

$$
TSS=SST+SSE
$$ {#eq-TSS2} Which means that total variation can be split into two sources, the treatment sum of squares and the error sum of squares. This is the decomposition that is behind the ANOVA table.

::: {#anovaassump style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;" icon="false"}
ANOVA Assumptions

-   Data are normally distributed

-   The F-test assumes that all the groups have the same variance. This can be roughly checked with side by side plots, but there are formal test we can perform as well.

-   The data are independent within and across groups. This would be the case if the subjects were assigned treatment at random. On the other hand, if the data was acquired from an observational study, we need to be very careful because this assumption would not be met and there could be cofounders so we will not be able to claim that there is causation with the treatment and the results of the tests.
:::

## Advanced ANOVA Techniques

Now we are going to use the mpg_2008 data set again to see if there is a difference in highway millage with respect to the 'Drive' categorical variable (front wheel, rear wheel and 4 wheel drive). Let's do the same we did above and calculate the average per group first

```{r}
mpg_2008 |> 
  group_by(drv) |> 
  dplyr::summarize(mean(hwy))
```

and we plot the data:

```{r, fig.align='center', echo=FALSE}
ggplot(mpg_2008, aes(x = drv, y = hwy)) + 
  geom_boxplot(outlier.shape = NA) +
  geom_jitter()
```

The boxplot allows us to see that the spread of the data in these groups is similar (the width of these boxes is about the same) so we can run an anova test on these data.

if we calculate the anova for this we find that the evidence support the hypothesis that there is in fact a difference in hwy between these groups:

```{r}
model <- aov(hwy ~ drv, data = mpg_2008)
summary(model)
```

As we mentioned, the ANOVA test does not tell us anything about each group in particular, for this we can run another test, one recommended is the **Tukey Honest Significant Difference** test and this will give us a $p$-value and a confidence interval for each pair of categories in our data.

```{r}
TukeyHSD(model)
```

The Tukey Honest Significant Difference test is better than running a Welch Two sample test by selecting just two variables in our dataset and testing one against the other. If we just run three different t-test comparing the three groups, we're potentially going to have an increased probability of a false positive, so the Tukey HSD test is specifically controlling for that multiple comparison problem.

Let's run a Welch Two sample test over one of the pairs in this dataset and see how it varies from the results from the TukeyHSD test:

```{r}
mpg_r4 <- filter(mpg_2008, drv != "f")
t.test(mpg_r4$hwy ~ mpg_r4$drv)
```

we can see that the $p$-value here is smaller than the one calculated for that same pair in the Tukey Test.

::: {.callout-orange appearance="simple" icon="false"}
ANOVA tests if there are statistically significant differences between the means of three or more groups. It essentially extends the t-test to multiple groups.
:::

Recap:

::: {#correlationkeypoints style="border: 2px solid #f0ad4e;  border-radius: 8px;   background-color: #fff3cd;   padding: 10px;"}
1.  Sample correlation measures the strength and direction of the linear relationship between two variables, providing insights into their association.

2.  Hypothesis testing and significance of correlation allow you to determine whether the observed correlation is statistically significant, indicating a relationship beyond random chance.

3.  Linear regression enables you to model relationships between variables, predicting outcomes and understanding the impact of independent variables on the dependent variable.

4.  ANOVA is a statistical technique used to compare means between multiple groups, assessing whether there are significant differences among the groups.

5.  Independence testing of categorical variables examines whether there is a relationship between two categorical variables, determining if they are independent or associated.

6.  Known the assumptions: whether it's correlation, regression or ANOVA, always ensure that assumptions (like normality or homoscedasticity) are met before drawing any conclusions.
:::

# Classification Problems

Variables can be characterized as either quantitative or qualitative (also known as categorical). Quantitative variables take on numerical values. Examples include a person's age, height, or income or the value of a house. In contrast, qualitative variables take on values in one of K different classes, or categories. Examples of qualitative variables include a person's marital status (married or not), the brand of product class purchased (brand A, B, or C) etc. We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems.

Classification problems are about predicting discrete categories or labels for given inputs. Here our goal is to build up a classifier, that assigns a class label from our set `c` to a future, unlabeled observation `x` where `x` is the feature vector. We'd also like to assess the uncertainty in each classification and the roles of the different predictors amongst the x's in producing that classify. One example is using the words in an email to classify it as legit (ham) or spam.

To graphically visualize a very simple classification problem we have a range of X values and the Y can have only two categories, which we represent here as 0 or 1. The line in the graph will be the probability of 1 given a certain X:

```{r, fig.align='center', echo=FALSE}
# Generate sample data
set.seed(123)
X <- seq(0, 10, length.out = 30)
Y <- 2 + 5 * X + X^2 + rnorm(30, sd = 5)
data <- data.frame(X, Y)

# Fit models
linear_model <- lm(Y ~ X, data = data)
parametric_model <- lm(Y ~ poly(X, 5), data = data)  # Increased polynomial degree
overfitted_model <- lm(Y ~ poly(X, 20), data = data)

# Predict values
data <- data %>%
  mutate(
    linear_pred = predict(linear_model),
    polynomial_pred = predict(parametric_model),
    overfitted_pred = predict(overfitted_model)
  )

# Plot
ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  geom_line(aes(y = linear_pred), color = "blue") +
  geom_line(aes(y = polynomial_pred), color = "red") +
  geom_line(aes(y = overfitted_pred), color = "green") +
  labs(title = "Scatter Plot with Regression Lines", x = "X", y = "Y")

# Simulate random data
set.seed(123) # Setting seed for reproducibility
x <- runif(500, min = 1, max = 7) # Random uniform distribution between 1 and 7
y <- rbinom(500, size = 1, prob = plogis(x - mean(x))) # Binary outcome based on logistic probability

# Fit logistic regression model
model <- glm(y ~ x, family = binomial(link = "logit"))

# Create sequence for predictions
x_range <- seq(min(x), max(x), length.out = 500)

# Predict probabilities using the fitted model
probabilities <- predict(model, newdata = data.frame(x = x_range), type = "response")

# Create the plot
ggplot() +
    geom_point(aes(x = x[y == 1], y = rep(1, length(y[y == 1]))), colour = "blue", shape = 124) + 
    geom_point(aes(x = x[y == 0], y = rep(0, length(y[y == 0]))), colour = "orange", shape = 124) +
    geom_line(aes(x = x_range, y = probabilities), colour = "black") +
    labs(x = 'x', y = 'Probability') +
    theme_minimal()

```

In this simple example above we only have two k elements (0 and 1) but there may be many K elements, so we express the probability of k for a given value of x like this: $$
p_k(x)=Pr(Y=k|X=x),k=1,2,\dots,K
$$ {#eq-conditionalClassProbabilities} These are the *conditional class probabilities* at x.

The *Bayes optimal classifier* is essentially the gold standard in classification. It represents the best possible classifier that can be achieved if we had perfect knowledge of the probability distribution of the data.

Imagine it like this: if you knew the exact probability of every possible classification given all possible features, you could make the optimal decision every time. This is what the Bayes optimal classifier does. It assigns each instance to the class with the highest posterior probability.

In formula terms, if $P(CxX)$ is the posterior probability of class C given the feature X, then the Bayes optimal decision rule assigns X to the class C $C(x)=j$ if $p_j(x)=max\{p_1(x),p_2(x),\dots,p_k(x)\}$ In our example, for X=5 we know that we have around 75% probability of a 1 and 25% probability of a 0, which means that we would classify that point as a 1

In practice, we often don't have perfect knowledge of these probabilities, so we use various methods to approximate the Bayes optimal classifier, like Naive Bayes, which assumes independence between features.

Typically we measure the performance of the classifier $\hat{C(x)}$ using the misclassification error rate{#misclassificationErrorRate}, that is the number of mistakes we make and it's represented by the formula $$
Err_{Te}=Ave_{i\epsilon Te}I[y_i\ne\hat{C(x_i)]}
$$ {#eq-misclassificationErrorRate}

## Logistic regression

The formula for the logistic regression is: $$
{p}(X) = \frac{e^{(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n)}}{1 + e^{(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n)}}
$$ {#eq-logisticRegression} This reads as p of X for the probability that y is 1 given X\
-p(x) is the predicted probability of the dependent variable being 1 (or positive outcome) given the predictors $x_1,x_2,\dots,x_n$ - $\beta_0$ is the intercept of the linear model - $\beta_1,\beta_2\dots,\beta_n$ are the coefficients for the predictors. - $e$ is a mathematical constant (2.71828) (Euler's number) The result will be always between 0 and .

This formula can also be written as: $$
log\left( \frac{p(X)}{1-p(X)} \right)=\beta_0+\beta_1X_1+\dots+\beta_nX_n
$$ {#eq-logodds}

and it is called *log odds* or *logit transformacion of p(X)*

### Maximum Likelihood Estimation (MLE) {#MaximumLikelihoodEstimation}

Maximum Likelihood Estimation (MLE) is used to find the parameters (coefficients) in logistic regression. Here's the lowdown: MLE aims to find the parameter values that make the observed data most probable. It identifies the set of parameters (in our case, the regression coefficients) that maximize the likelihood function.

In Logistic Regression: For binary outcomes (0 or 1), logistic regression models the probability of the outcome. The likelihood Function: for logistic regression is: $$ L(\beta) = \prod_{i=1}^n \hat{p}(x_i)^{y_i} [1 - \hat{p}(x_i)]^{1 - y_i}
$$ {#eq-maximumLikelihoodFunction}

Where: $\hat{p}(xi)$ is the predicted probability for the i-th observation. $y_i$ is the observed outcome (0 or 1) for the i-th observation.

Log-Likelihood: For computational simplicity, we usually work with the natural logarithm of the likelihood function (log-likelihood): $$\ell(\beta) = \sum_{i=1}^n y_i \log(\hat{p}(x_i)) + (1 - y_i) \log(1 - \hat{p}(x_i))$$

Optimization: The goal is to find the parameter values (β0,β1,β2,...,βn) that maximize the log-likelihood function.

This is usually done using numerical optimization methods since there's no closed-form solution.

Intuition: MLE in logistic regression finds the best-fit model that maximizes the probability of observing the given data.

It adjusts the coefficients to maximize the match between the predicted probabilities and the actual outcomes.

In r we use the function `glm()`

The package ISLR2 has a dataset 'Default' and we want to use to see what variables (student, balance(credit balance), and income) are relevant to predict if a person is going to default their credit card payments.

First we are going to see each variable independently:

```{r}
head(Default)

glm.income<- glm(default ~ income, data=Default, family = binomial)
summary(glm.income)
glm.student<- glm(default ~ student, data=Default, family = binomial)
summary(glm.student)
glm.balance<- glm(default ~ balance, data=Default, family = binomial)
summary(glm.balance)
```

### Interpreting the results:

For the Default data, estimated coefficients of the logistic regression model that predicts the probability of default using balance. A one-unit increase in balance is associated with an increase in the log odds of default by 0.0055 units. Many aspects of the logistic regression output are similar to the linear regression. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic plays the same role as the t-statistic in the linear regression output: the z-statistic associated with $\beta_1$ is equal to $\hat{\beta_1}/SE(\hat{\beta_1})$, and so a large (absolute) value of the z-statistic indicates evidence against the null hypothesis $H_0:\beta_1=0$. This null hypothesis implies that the probability of default does not depend on balance. Since the `$p$-value` associated with balance is tiny, we can reject the null hypothesis. The intercept is typically not of interest. \### Making predictions Once the coefficients have been estimated, we can compute the probability of default for any given credit balance. we will use the logistic regression formula we already saw (@eq-logisticRegression) $$
\hat{p}(X) = \frac{e^{\hat{\beta_0} + \hat{\beta_1} X_1}}{1 + e^{\hat{\beta_0} + \hat{\beta_1} X_1 }}
$$ we just plug in the values from our `glm`, for example for a balance of 1000: $$
\hat{p}(X) = \frac{e^{-10.65 + 0.0055 \times 1000}}{1 + e^{-10.65 + 0.0055 \times 1000}} =0.00576
$$ using R we are going to calculate the probability of default for a balance of 1000 and 2000 USD:

```{r}
new_data<- data.frame(balance= c(1000,2000))
glm.probs <- predict(glm.balance,new_data, type = "response")
glm.probs
```

We can use qualitative predictors the same way, we will now see the student predictor: Student: Yes $$
\hat{p}(X) = \frac{e^{-3.50 + 0.4049 \times 1}}{1 + e^{-3.50 + 0.4049 \times 1}} =0.0431
$$ Student:No $$
\hat{p}(X) = \frac{e^{-3.50 + 0.4049 \times 0}}{1 + e^{-3.50 + 0.4049 \times 0}} =0.0292
$$ In this case this indicates that students tend to have higher default probabilities than non-students.

```{r}
new_data= data.frame(student= as.factor(c('Yes','No')))
glm.probs <- predict(glm.student,new_data, type = "response")
glm.probs
```

### Multiple Logistic Regression. {#MultipleLogisticRegression}

So far we have just considered one predictor at a time, but like in linear models, we can use them combined:

```{r}
glm.fit<- glm(default ~ income+balance+student, data=Default, family = binomial)
summary(glm.fit)
```

If we pay attention now to the coefficient of student, it is negative, indicating that students are less likely to default than non-students. This is not what we saw when we looked at the model with only the student variable. This is because The variables student and balance are correlated. Students tend to hold higher levels of debt, which is in turn associated with higher probability of default. In other words, students are more likely to have large credit card balances, which, as we know, tend to be associated with high default rates. Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall, students tend to default at a higher rate than non-students.

This simple example illustrates the dangers and subtleties associated with performing regressions involving only a single predictor when other predictors may also be relevant. As in the linear regression setting, the results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among the predictors. In general, the phenomenon seen here is known as *confounding*.

```{r, fig.align='center',fig.width=10, echo=FALSE}
pred <- predict(glm.fit, type = "response")
pred_data<- data.frame(
  balance = Default$balance,
  student = Default$student
)
p1 <- ggplot(Default, aes(x = balance, y = pred, color = student)) + 
  geom_line(size = 1) + 
  labs(title = "Probability of Defaulting",
       x = "Balance",
       y = "Probability of Defaulting") + 
  theme_minimal()

p2 <- ggplot(Default, aes(x = student, y = balance, fill = student)) + 
  geom_boxplot() + 
  labs(title = "Boxplot of Balance",
       x = "Student Status",
       y = "Balance") + 
  theme_minimal()

# Arrange plots side by side
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

#### Misclassification rate

How many values does our model get right and how many get wrong? for that we can use a *confusion matrix* :

```{r}
# Get predicted probabilities
glm.probs <- predict(glm.fit, type = "response")

# Convert probabilities to binary predictions (threshold = 0.5)
glm.pred <- ifelse(glm.probs > 0.5, "Yes", "No")

# Print confusion matrix
print('Confusion Matrix')

table(Default$default, glm.pred)


cat('misclassification rate: ', (228+40)/10000)

```

But this values are over estimated because we are using the same data that we used to train our model, so this is the training error. As we already know, to get the real fit of our model we should train our model with a subset of data and test it with unseen data.

#### Null rate

Although this misclassification rate might seem very small, we need to take into consideration, that the data has a majority of 'No' in the response, so if we would fit every single value to 'No' without using any model, we would only get 333/10000 errors, so only 3.33% misclassification. This is called the *null rate*

Another way of looking at it would be seeing what percentage of No's and Yes's we have missclassified.

For No's we have missclassified 40 out of 9667 = 0.4% For Yes's we have missclassified 228 out of 333 = 68.5%

#### Visualizing the relationship between variables

A way of checking the relationship between different predictors in our model is to visualize a scatter matrix using `pairs()`. We will use the `SAheart` dataset for this:

In this case we are not trying to predict the probability of getting a heart disease, we are going to assess the risk factors

```{r, fig.width=10, fig.height=10}
# Load the SAheart dataset
library(bestglm)
data(SAheart)

# Select columns to include in the pairs plot (excluding 'typea')
SAheart_selected <- SAheart %>%
  dplyr::select(-c(typea)) %>%
  mutate(famhist = ifelse(famhist == "Present", 1, 0))

# Define colors based on 'chd'
color_chd <- ifelse(SAheart_selected$chd == 1, "darkred", "darkblue")

# Exclude the 'chd' column from the pair plot
SAheart_selected <- SAheart_selected %>%
  dplyr::select(-chd)

# Create the scatter plot matrix
pairs(SAheart_selected, 
      col = color_chd, 
      pch = 19,           # Use solid points
      cex = 0.5,          # Reduce point size
      labels = colnames(SAheart_selected),
      cex.labels = 0.9,   # Reduce label text size
      main = "Scatter-plot Matrix",
      oma = c(2, 2, 2, 2)) # Reduce outer margins

```

to fit a model for this data:

```{r}
heartfit <- glm(chd~.,data=SAheart, family=binomial)
summary(heartfit)
```

Interestingly, obesity and alcohol does not show as significant and this is due to correlation.

::: exercise-box
Example:

We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, `lagone` through `lagfive`. We have also recorded volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and direction (whether the market was Up or Down on this date). Our goal is to predict direction (a qualitative response) using the other features.

```{r, fig.align='center', fig.height=10, fig.width=10}
head(Smarket)
pairs(Smarket[2:7], col = Smarket$Direction, pch = 20, cex = 0.5)

```

The pairs plot seem to not show a lot of correlation.

```{r}
glm.fits <- glm(
    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
    data = Smarket, family = binomial
  )
summary(glm.fits)
```

The 'contrast()\` function tells us what value of Direction is 0 and which one is 1:

```{r}
contrasts(Smarket$Direction)
```

The negative predictor in lag1, lag2 suggest that if the market had a positive return yesterday, then it is less likely to go up today, however, the $p$-value is large, so there is no clear association.

The null deviance and the residual deviance are very similar, which indicates that the model does not perform much better than just using the average.

The `predict()` function can be used to predict the probability that the market will go up, given the values of the predictors. The `type="response"` option tells R to output probabilities of the form $P(Y=1|X)$, as opposed to other information such as the logit. If no data set is supplied to the `predict()` function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down, because the `contrasts()` function indicates that `R` has created a dummy variable with a 1 for `Up`.

```{r }
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]

```

The first thing that calls our attention is that the values are very close to 50%, so the predictors are not very strong. In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, `Up` or `Down`. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than $0.5$.

```{r }
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
```

The first command creates a vector of 1,250 `Down` elements. The second line transforms to `Up` all of the elements for which the predicted probability of a market increase exceeds $0.5$. Given these predictions, the `table()` function can be used to produce a *confusion matrix* in order to determine how many observations were correctly or incorrectly classified.

```{r }
table(glm.pred, Smarket$Direction)
(507 + 145) / 1250
mean(glm.pred == Smarket$Direction)
```

The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on $507$ days and that it would go down on $145$ days, for a total of $507+145 = 652$ correct predictions. The `mean()` function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market $52.2$% of the time.

At first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of $1,250$ observations. In other words, $100\%-52.2\%=47.8\%$, is **the *training* error rate**. As we have seen previously, the training error rate is often overly optimistic -it tends to underestimate the test error rate-. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the *held out* data. This will yield a more realistic error rate, in the sense that in practice we will be interested in our model's performance not on the data that we used to fit the model, but rather on days in the future for which the market's movements are unknown. To implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005:

```{r}
train <- (Smarket$Year < 2005)
Smarket.2005 <- Smarket[!train, ]
dim(Smarket.2005)
Direction.2005 <- Smarket$Direction[!train]
```

We now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set---that is, for the days in 2005.

```{r}
glm.fits <- glm(
    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
    data = Smarket, family = binomial, subset = train
  )
glm.probs <- predict(glm.fits, Smarket.2005,
    type = "response")
```

Notice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.

```{r}
glm.pred <- rep("Down", 252)
glm.pred[glm.probs > .5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
```

and our test set error rate:

```{r}
mean(glm.pred != Direction.2005)
```

Suppose that we want to predict the returns associated with particular values. We do this using the predict() function.

```{r}

predict(glm.fits,
    newdata = data.frame(Lag1 = c(1.2, 1.5),  Lag2 = c(1.1, -0.8),  Lag3 = c(1.1, -0.8), Lag4 = c(1.1, -0.8), Lag5 = c(1.1, -0.8), Volume= c(10,100), type = "response"
  ))
```
:::

### Case-control sampling and logistic regression.

We know because medicine tells us, that the risk of heart disease in this type of population of South Africans is actually 5%. But in our dataset we used 160 cases (people with heart disease) and 302 controls. This results in an estimated proportion of $\tilde{\pi} = 0.35$, yet the prevalence of the heart disease is $\pi=0.05$

We can still use these data set to fit our model, and the model will be correct, but the constant term will be incorrect. We can correct the constant using a simple transformation. $$
\hat{\beta_0^*}=\hat{\beta_0}+log\frac{\pi}{1-\pi}-log\frac{\tilde{\pi}}{1-\tilde{\pi}}
$$ This formula is a re-calibration or re-centering of the logistic regression intercept $\hat{\beta_0}$ where: - $\hat{\beta_0}$ is the original estimated intercept. - $\hat{\beta_0^*}$ is the adjusted intercept after re-calibration. - $\pi$ is the baseline probability in the population. - $\tilde{\pi}$ is the target probability or prevalence rate you are adjusting to.

This is an approach often followed in epidemiology because the cases that we want to study are rare, so we want to take them all, up to five or six times that number of controls is sufficient. Another example is in advertising, where the click-through ratio for an add is very low, we need usually hundreds or thousands of people that saw the add but did not click on it, for just a few clicks, so following this approach we can reduce our dataset and not include all the control data.

### Multinomial regression

So far we have only considered two classes in the response, but we can extend the model to k number of classes: $$
Pr(Y = k \mid X = x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \cdots + \beta_{kp}x_p}}{1 + \sum_{l=1}^{K-1} e^{\beta_{l0} + \beta_{l1}x_1 + \cdots + \beta_{lp}x_p}}
$$ {#eq-multinomialLogisticRegression}

To do this, we first select a single class to serve as a baseline. The decision to treat one specific class as a baseline is unimportant. The coefficient estimates will differ between two fitted models to different baselines variables due to the differing choice of baseline, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same.

Although we can use multinomial regression for k values, it has its limitations: - When the classes are well separated, the parameter estimates for the logistic regression model are surprisingly unstable. - If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model that we will see next is more stable than the logistic regression model.

## Discriminant Analysis.

### Linear Discriminant Analysis(LDA)

Linear discriminant analysis is popular when we have more than two response classes.

We can model de distribution of X in each of the classes separately and then use Bayes theorem to flip things around to get the probability of Y given X. When we use normal distributions for each class, this leads to linear or quadratic discriminant analysis.

#### Bayes theorem for classification.

$$
P(Y=k \mid X=x) = \frac{Pr(X=x \mid Y=k) \times  Pr(Y=k)}{Pr(X=x)}
$$ {#eq-bayestheorem}

-   Posterior Probability \$P(Y=k \mid X=x) \$ this is the probability that the target variable Y is in class k given the observed data X=x
-   Likelihood: $Pr(X=x \mid Y=k)$ this is the probability of observing the data X=x given that the variable Y is in class k.
-   Prior Probability: $Pr(Y=k)$ This is the initial probability of class k before observing the data, it's your prior belief about the class distribution.
-   Marginal Likelihood: $Pr(X=x)$ this is the overall probability of observing the data X=x across all classes.

### k-Nearest Neighbour classification

In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. The k-nearest neighbor classification uses distance to try to classify the predictions: Given a positive integer K and a test observation $x_0$, the KNN classifier first identifies the K points in the training data that are closest to x0, represented by $N_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$: $$
P(Y=j|X=x_0)=\frac{1}{K}\sum_{i\epsilon N_0}I(y_i=j)
$$ {#eq-probabilityFunctionknearest}

The algorithm relies on a distance metric (commonly Euclidean distance) to measure the similarity between instances. The distance between two points $(x_1, y_1)$ and $(x_2, y_2)$ in a 2D space is calculated as: $d=(x_2-x_1)^2+(y_2-y_1)^2$

Choosing k: The parameter $k$ represents the number of nearest neighbors to consider when making a prediction. For example, if $k = 3$, the algorithm looks at the 3 closest training instances to the query point.

For classification, k-NN uses a majority voting mechanism. The class label that appears most frequently among the $k$ nearest neighbors is assigned to the query point.

k-NN is a non-parametric algorithm, meaning it makes no assumptions about the underlying data distribution. This makes it flexible but also computationally intensive, especially with large datasets.

::: exercise-box
Example:

We will need the library 'class' and work with the IRIS dataset. First we divide the dataset into training and test subsets and use the `knn` function to get the predicted values for the test dataset. We can use a confusion_matrix to see the successes and failures of the model guessing the species.

```{r}
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
train <- iris[sample,1:4 ]
test <- iris[!sample,1:4 ]
species<- iris[sample,5]
specietest <- iris[!sample,5]
knn.pred<- class::knn(train,test,species,k=1)
table(knn.pred,specietest)
mean(knn.pred ==specietest)
```
:::

#### Linear Discriminant Analysis for one variable.

To classify Y at the value X=x we only need to see which of the $p_x(x)$ is the largest. For this we use the *discriminant score*

$$
\delta_k(x) = x \frac{\mu_k}{\sigma^{2}} - \frac{\mu_k^2}{2\sigma^{2}}+ \log(\pi_k)
$$ {#eq-discriminantScore}

Where $\pi$ is the prior probability

Note that $\delta_k(x)$ is a linear function of x

If there are k=2 classes and the prior is the same for both (0.5) then the formula simplifies to: $$
x = \frac{\mu_1+\mu_2}{2}
$$ Below we have a visualization for two classes of Y and its probabilities given x when the prior is the same for both classes. The dashed line shows the decision boundary, anything on the left of the line will be classified as class 'blue' and anything on the right as class 'red'

```{r, echo=FALSE}
# Set seed for reproducibility
set.seed(123)

# Simulate data for normal density functions
x <- seq(-3, 6, length.out = 200)
mu1 <- 0
mu2 <- 3
y1 <- dnorm(x, mean = mu1, sd = 1)
y2 <- dnorm(x, mean = mu2, sd = 1)

dbound = (mu2-mu1)/2

# Plot normal density functions
p1 <- ggplot(data.frame(x, y1, y2), aes(x)) +
  geom_line(aes(y = y1), color = "blue") +
  geom_line(aes(y = y2), color = "red") +
  geom_vline(xintercept = dbound, linetype = "dashed") +  # Bayes decision boundary
  labs(title = "Normal Density Functions",
       y = "Density",
       x = "X") +
  theme_minimal()
```

When the priors are different, we have take them into consideration and compare $\pi_k f_x(x)$

We usually don't know the prior, so we have to estimate it looking at the data. The estimated prior in each class will be the total number of elements in that class divided by the total number of cases $$
\hat{\pi_k}=\frac{n_k}{n}
$$

The mean of each class is calculated normally. The estimated variance is a bit more complex because it is not using the variance of each class but the *pooled variance estimate*

$$
\hat{\sigma^2}= \frac{1}{n-K} \sum^K_{k=1} \sum_{y_i=k}(x_1-\hat{\mu_k})^2= \sum^K_{k=1}\frac{n_k-1}{n-K}\times\hat{\sigma^2_k}
$$ {#eq-pooledVarianceEstimate}

Where $\hat{\sigma^2_k}$ is the usual formula for the estimated variance in the kth class. $$
\frac{1}{n_k-1}\sum^N_{i:y_i=k}(x_i-\hat{\mu_k})^ 2
$$ so basically what we are doing in the pooled variance is to calculate the variance for each of the classes and then do a weighted average of them.

### Multivariate linear discrimination.

The discriminant score function for multivariate takes this quite complicated form: $$
\delta_k(x) = x^T\Sigma^{-1} \mu_k -\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+ \log(\pi_k)
$$ {#eq-discriminantScoreMulti}

Once we have the estimates for the different parameters $\hat{\delta}_k(x)$ we can turn these into estimates for class probabilities: $$
\hat{P_{r}}(Y = k | X = x) = \frac{e^{\hat{\delta_k} (x)}}{\sum_{l=1}^{K} e^{\hat{\delta_l}(x)}}
$$ {#eq-conditionalProbabilityFormula1}

With these estimates we will classify Y=k to the largest probability for the value of x we are interested in.

We can run a classification over the Default dataset that we already used in [multiple logistic regression](@MultipleLogisticRegression). To do so we use the function LDA from library MASS, but first we are going to check the levels of the response variable to know which value the reference (the first one):

```{r}
levels(Default$default)
```

So our reference level is default= 'No'

```{r}
LDA.fit = lda( default ~ income+balance+student, data=Default, )
LDA.fit
LDA.fit.p = predict(LDA.fit, newdata=Default[,2:4])
LDA.fit.Classes = predict(LDA.fit, newdata=Default[,2:4])$class

```

**How to interpret the results of the LDA**

-   *Prior Probabilities of Groups*: These are the probabilities assigned to each class (No and Yes) before observing any data, based on the proportion of each class in the training data.

    -   No: 0.9667 (96.67% of the data is in the "No" category)
    -   Yes: 0.0333 (3.33% of the data is in the "Yes" category)

-   *Group Means*: These are the mean values of each predictor variable for each class.

-   *Coefficients of Linear Discriminants*: These coefficients are used to form the linear discriminant functions (LD1 in this case). The linear discriminant function combines the predictor variables to maximize the separation between the classes. Larger absolute values of coefficients indicate more significant contributions to the discriminant function.

The negative coefficient of the Student(Yes) indicates that being a Student reduces the possibility of defaulting = 'yes'.

In a two-class LDA model with reference level A, a positive coefficient for a predictor indicates that an increase in that predictor increases the discriminant score. A higher discriminant score makes it more likely for the observation to be classified as B rather than A.

Conversely, a negative coefficient indicates that an increase in the predictor decreases the discriminant score, making it more likely for the observation to be classified as A.

LD1 is the *Linear Disciminative Function* LDFs are essentially the axes that best separate the different classes in your data. They are linear combinations of your original variables. (\@#eq-discriminantScoreMulti)

In this case we only have a LDF (LD1), but with more classess we will get more. The number of LDFs is always one less than the number of classes.

**Histogram of Linear Discriminants**: We can also use plots to see the overlapping of our groups: This plot shows a histogram of the discriminant function values for each class. The histograms should ideally show distinct peaks for each class, indicating good separation.

Overlapping histograms suggest some misclassification, as the discriminant function values are not clearly separated.

```{r, fig.align='center'}
plot(LDA.fit)
```

create a confusion matrix:

```{r}
table(Default$default,LDA.fit.Classes)
```

And as before, we can use the confusion matrix to calculate our misclassification rate:

```{r}
cat('misclassification rate: ', (254+22)/10000)
```

For No's we have missclassified 22 out of 9667 = 0.2% this is our *False positive rate* For Yes's we have missclassified 254 out of 333 = 76.3% this is our *False negative rate*

We can also use the function `confusionMatrix()` from the package `caret` to create and analyse the results. It takes two parameters the first one is the predicted values and the second one the real values (reference)

```{r}
# Create confusion matrix
confusion_matrix <- caret::confusionMatrix(data=factor(LDA.fit.Classes), reference=Default$default)

print(confusion_matrix)
```

**Interpreting the results of the confusion matrix** - *Accuracy*: It measures the overall correctness of the model. 97.24% of the predictions are correct.

-   *95% CI (Confidence Interval)*: It provides a range within which the true accuracy lies with 95% confidence.

-   *No Information Rate (NIR)*:It represents the accuracy if the model always predicted the majority class (No).

-   $p$-value \[Acc \> NIR\]: This $p$-value tests if the accuracy is significantly better than the NIR.

-   *Kappa*: It measures the agreement between the observed and predicted classifications, adjusted for chance. Values close to 1 indicate better agreement.

-   *Mcnemar's Test* $p$-value: Tests if there is a significant difference between the number of false positives and false negatives.

Class-Specific Metrics:

-   *Sensitivity (Recall)*:Proportion of actual positives correctly identified.

-   *Specificity*: Proportion of actual negatives correctly identified.

-   *Positive Predictive Value (Precision)*: Proportion of positive results that are true positives.

-   *Negative Predictive Value*: Proportion of negative results that are true negatives.

-   *Prevalence*: Proportion of the actual positive cases in the dataset.

-   *Detection Rate*: Proportion of actual positives correctly identified by the model.

-   *Detection Prevalence*:Proportion of the predicted positives out of the total predictions.

-   *Balanced Accuracy*: It's the average of sensitivity and specificity, providing a balanced measure for imbalanced datasets.

Summary: The model has a high overall accuracy (97.24%), but its specificity (23.72%) is low, indicating it struggles to correctly identify the negative class (Yes).

The high sensitivity (99.77%) suggests it's good at identifying the majority class (No), but at the cost of false positives.

Adjusting the model or the decision threshold can help to balance sensitivity and specificity better.

In order to reduce the false negative rate we can take change the threshold at which we classify one class to yes or no. By default we have been using equal priors for all variables, but we can change this to put more weight on the minority class (default = yes). This swill increase the false positive rate, so we need to evaluate for our data what type of errors we prefer depending on the risk of making the wrong classifications.

[ROC Curve]{#rocCurve}: We can use A ROC plot and AUC (Area under the curve) to summarize the overall performance of our model. A higher AUC is desired.

```{r, fig.align='center'}
# Fit the LDA model
LDA.fit <- lda(default ~ income + balance + student, data = Default)

# Get predicted probabilities
LDA.fit.prob <- predict(LDA.fit, newdata = Default[, 2:4])$posterior[, 2]

# Generate ROC curve
roc_curve <- roc(Default$default, LDA.fit.prob)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)

# Add AUC to the plot
auc(roc_curve)
```

The *Receiver Operating Characteristic* (ROC) curve is a graphical representation used to evaluate the performance of a classification model. It plots the True Positive Rate (Sensitivity) against the False Positive Rate (1-Specificity) at various threshold settings.

-   *True Positive Rate* (TPR): Also known as sensitivity, it measures the proportion of actual positives correctly identified by the model.

-   *False Positive Rate* (FPR): It measures the proportion of actual negatives incorrectly identified as positives by the model.

The ROC curve helps visualize the trade-off between sensitivity and specificity for different threshold values. It is also used to compare Models: By comparing the ROC curves of different models, you can assess which model performs better.

*The Area Under the ROC Curve* (AUC) is a single scalar value that summarizes the performance of a classification model. - AUC = 1: Perfect model (perfectly separates positive and negative classes). - AUC = 0.5: Random model (no better than random guessing). - AUC \< 0.5: Worse than random guessing.

Use of AUC: - Model Comparison: A higher AUC value indicates a better-performing model. - Robustness: AUC is insensitive to the threshold chosen for classification, providing a more robust evaluation metric.

In essence, the ROC curve and AUC provide a comprehensive way to evaluate and compare classification models, ensuring you pick the most accurate and reliable one.

Following what we see in the plot we think that a prior of 0.9 and 0.1 can be better, so we adjust our model accordingly:

```{r}
# Fit LDA model with adjusted prior probabilities
LDA.fit <- lda(default ~ income + balance + student, data = Default, prior = c(0.9, 0.1))

# Predict and evaluate
LDA.fit.Classes <- predict(LDA.fit, newdata = Default[, 2:4])$class
confusion_matrix <- caret::confusionMatrix(factor(LDA.fit.Classes), Default$default)

# Print confusion matrix
print('Confusion Matrix')
print(confusion_matrix)

# Extract the confusion matrix values
cm <- confusion_matrix$table

# Calculate False Positive Rate (FPR) and False Negative Rate (FNR)
false_positive_rate <- cm[2, 1] / sum(cm[, 1])
false_negative_rate <- cm[1, 2] / sum(cm[, 2])

# Print the rates
cat('False Positive Rate: ', false_positive_rate, '\n')
cat('False Negative Rate: ', false_negative_rate, '\n')
```

------------------------------------------------------------------------

::: exercise-box
Example with three classes:

As an example we can see Fisher's Iris dataset. It has 3 classes (Iris Species) and uses 4 different parameters to classify the different species: Sepal Length, Sepal Width, Petal Length and Petal Width. If we plot each variable against each other using `pairs()` we can see that we can actually see quite a clear differentiation between the species using these variables.

```{r, fig.align='center', echo=FALSE, fig.height=10, fig.width=10 }
par(mar = c(0, 0, 0, 0) + 0.1, pch = 20, cex = 2)
pairs(iris[, 1:4], col = as.numeric(iris$Species),
      main = "Pairs Plot of Iris Dataset")

# Add a blank plot for the legend
par(mar = c(0, 0, 0, 0))

legend("bottomright", legend = levels(iris$Species), 
       col = 1:3, pch = 19, title = "Species", horiz = TRUE)
```

We are going to fit a model for our iris data and see how well it works predicting the species.

```{r}
LDA.fit = lda( Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, iris)
LDA.fit
```

We see that now we have to LDFs LD1 and LD2.

*Proportion of Trace*: The proportion of trace tells you how much of the total between-class variance each linear discriminant function explains.

In this case:

-   LD1: Explains 99.12% of the variance.

-   LD2: Explains 0.88% of the variance.

So, LD1 is the primary driver for separating the species in the iris dataset, while LD2 adds a minor amount of additional discrimination.

Now we are going to perform the classification:

```{r}
LDA.fit.C = predict(LDA.fit, newdata=iris[,c(1,2,3,4)])$class

head(LDA.fit.C)
```

We can quickly create a confusion matrix using table:

```{r}
table(iris$Species, LDA.fit.C)
```

As we have seen before we can also use the confusion matrix function from the package caret:

```{r}
caret::confusionMatrix(data=LDA.fit.C, reference=iris$Species )
```

Lastly, we can create an LDA plot to view the linear discriminants of the model and visualize how well it separated the three different species in our dataset:

```{r, fig.align='center', fig.width=10}
# Predict the LDA values
lda.pred <- predict(LDA.fit, newdata=iris[,c(1,2,3,4)])$x

# Combine the LDA values with the original dataset
lda.data <- data.frame(lda.pred, Species = iris$Species)

# Define colors for each species
species_colors <- c(setosa = "red", versicolor = "green", virginica = "blue")

# Plot the LDA results using base R plot
plot(lda.data$LD1, lda.data$LD2, col = species_colors[lda.data$Species],
     pch = 19, xlab = "Linear Discriminant 1", ylab = "Linear Discriminant 2",
     main = "LDA of Iris Dataset")

# Add a legend
legend("topright", legend = levels(iris$Species), 
       col = species_colors, pch = 19, title = "Species")

```
:::

### Quadratic Discriminant Analysis (QDA).

So far we have seen how to use discriminant analysis for a normal distribution, with the same variance for each class. If the distribution is still normal but the variance for each class is different, we use *quadratic discriminant analysis* The formula for the discriminant score was quite simple for the linear discriminant analysis because the quadratic terms were cancelling each other, but now, because $\Sigma_k$ are different, we cannot apply that cancellation. $$
\delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + \log{\pi_k} - \frac{1}{2}\log{|\Sigma_k|}
$$ {#eq-quadraticDiscriminantFunction}

Where - $\Sigma_k$ represents the covariance matrix for class (k). It describes the spread and orientation of the data points in class (k). - $\pi_k$: This is the prior probability of class (k). It represents the proportion of data points that belong to class (k) in the training dataset.

Quadratic discriminant analysis is good when the number of features is not high, because you need to estimate the covariance matrices.

::: exercise-box
Example:

We are going to use the iris dataset to perform quadratic Discriminant Analisys. We will split the dataset into training and testing sets:

```{r}
str(iris)
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
train <- iris[sample, ]
test <- iris[!sample, ]

```

and now we use the `qda` function from the MASS package to fit the model over the training data:

```{r}
model <- qda(Species ~ ., data=train)
print(model)

```

*Prior probabilities of group*: These represent the proportions of each Species in the training set. For example, 35.8% of all observations in the training set were of species virginica.

*Group means*: These display the mean values for each predictor variable for each species. As ususal, to use the model to get the fitted values we use `predict`

```{r}
predictions <- predict(model, test)
names(predictions)

```

Predict returns a list with two variables: - *class*: the predicted class - *Posterior*: the posterior probability that an observation belong to each class.

```{r}
head(predictions$class)
head(predictions$posterior)
```

We can use the following code to see what percentage of observations the QDA model correctly predicted:

```{r}
mean(predictions$class==test$Species)
```

And we create a confusion matrix to see the results:

```{r}
table(predictions$class, test$Species)
```
:::

### Naive Bayes

Naive Bayes is best used when the number of features is high because it does not require to calculate the covariance matrices. It assumes that the features are independent in each class, this means your predictor variables are independent, which is almost never true for real data. Despite the strong assumptions that it does, naive Bayes often produces good classification results.

It can be used for mixed feature vectors (quantitative and qualitative). 
::: {.exercise-box} 
Example 
We are going to be working again with the Iris dataset. This time we are going to introduce also a new function `ggpairs()` from the package GGally, this will help us visualize the correlation between the variables before we start our analysis

```{r ggpairs, fig.align='center', fig.width=8, fig.height=8}
GGally::ggpairs(iris[-5], title = "The correlation between the predictors")
```

We are going to split the data for the train and the test:

```{r}
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
train <- iris[sample, ]
test <- iris[!sample, ]
```

We will need the library `naivebayes` for fitting the model:

```{r}
naive.fit <- naivebayes::naive_bayes(Species ~., data=train)
naive.fit
```

Now we can use the model to predict values using the test data subset and create a confusion matrix

```{r}
predictions <- predict(naive.fit, test)

(confusion_matrix<- table(predictions,test$Species))

(accuracy<- sum(diag(confusion_matrix))/sum(confusion_matrix))
```

:::

# Generalized Linear Models (GLM)

Generalized Linear Models (GLMs) are a flexible extension of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. GLMs are used to model a wide range of data types and relationships.

**Common types of GLMs** - Linear Regression: when the response variable is continuous and normally distributed. - Logistic Regression: When the response is binary (e.g. success/failure) - Poisson Regression: For count data (e.g. number of events)

## Poison regression model

We will use the dataset `bikershare` from ISLR2 package. The response is *bikers* that are the number of hourly users in `bikeshare` program in Washington DC.

First we are just going to plot the data in the dataset, without any fitting for the number of riders per hour of the day:

```{r bikeshareScatter, echo=FALSE, fig.align='center', fig.width=6}

plot(jitter(as.numeric(Bikeshare$hr)), jitter(Bikeshare$bikers), 
     xlab = "Hour", 
     ylab = "Number of Bikers", 
     main = "Number of Bikers per Hour",
     pch = 19, # Point character
     col = 'lightblue',
     cex = 0.5
  )
smooth.spline.fit <- smooth.spline(as.numeric(Bikeshare$hr), Bikeshare$bikers, spar=0.5)
lines(smooth.spline.fit, col = "darkblue", lwd = 2)
```

Now if we use boxplots we can see how as the number of bikers increase, so does the standard deviation:

```{r, echo=FALSE, fig.align='center', fig.width=6}
plot(Bikeshare$hr, Bikeshare$bikers, col= 'lightblue',
     xlab = "Hour", 
     ylab = "Number of Bikers", 
     main = "Number of Bikers per Hour")
```

If we use linear model to make predictions and we plot the results as we did before in a scatter plot:

To perform this analysis, first we fit a linear regression model. We are going to show two different ways of doing it:

```{r lmoverbikesharedata1}
lm.fit <- lm(
bikers ~ mnth + hr + workingday + temp + weathersit ,
data = Bikeshare)
summary(lm.fit)
```

In this model the first level of hr (0) and mnth (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines.

We created another second model with numbered values for `mnth` and `hr`.

```{r lmoverbikesharedata2}
contrasts (Bikeshare$hr) = contr.sum (24)
contrasts (Bikeshare$mnth) = contr.sum (12)
lm.fit <- lm(
  bikers ~  mnth + hr + workingday + temp + weathersit ,
data = Bikeshare)
summary(lm.fit)
```

Notice that we used contrast for month and hour. `Bikeshare$hr` and `Bikeshare$mnth` are categorical variables representing hour and month, respectively. Sum-to-Zero contrasts ensure that the coefficients of the categories sum to zero. This makes the interpretation of the regression coefficients easier.

For instance, if you have 24 hours in a day and set up sum-to-zero contrasts (`contr.sum(24)`), the sum of all hour coefficients will be zero.

This helps in comparing each category to the overall mean effect, rather than to a reference category.

By default, categorical variables are typically converted to dummy variables (0/1), which can lead to difficulties in interpreting the results, especially with a large number of categories.

In this second model the coefficient estimate for the last level of `mnth` is not zero: instead, it equals the negative of the sum of the coefficient estimates for all of the other levels. Similarly, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all of the other levels. This indicates that the difference between the mean level and the coefficients of `hr` and `mnth` in the second model will always total to zero.

We use the linear model to predict the values:

```{r bikersharepredlm, echo=FALSE, fig.align='center', fig.width=6}
lm.pred <- predict(lm.fit)
point_colors <- ifelse(lm.pred < 0, "red", "lightblue")
plot(jitter(as.numeric(Bikeshare$hr)), jitter(lm.pred), 
     xlab = "Hour", 
     ylab = "Number of Bikers", 
     main = "Number of Bikers per Hour",
     pch = 19, # Point character
     col = point_colors,
     cex = 0.5
  )
smooth.spline.fit <- smooth.spline(as.numeric(Bikeshare$hr), lm.pred, spar=0.5)
lines(smooth.spline.fit, col = "darkblue", lwd = 2)
```

One of first things that we notice is that there are negative values (red points) predicted, and this is because the linear model does not have any type of constraint for that, so **linear models are not the best choice for counts**, we have a better alternative and this is the *Poisson Regression Model*.

The Probability mass function (PMF) is this for a single variance: $$
P(Y = k) = \frac{e^{-\lambda} \lambda^k}{k!}
$$ {#eq-poissonpmf} where:

-   $P(Y = k)$: Probability of observing ( k ) events.
-   $\lambda$: Average rate (mean number of events) in a given interval.
-   $e$: Euler's number (approximately 2.71828).
-   $k!$: Factorial of $k$ (number of events).

Note that for the Poisson distribution, the mean and the variance are directly related, meaning that when the mean is higher, the variance is also higher. In fact we assume that the variance equals the mean.

This formula calculates the probability of exactly $k$ events occurring in a fixed interval when events happen at a constant mean rate and independently of the time since the last event. The Poisson distribution is useful for modeling the number of events in a specific time.

When we have multiple parameters, the model changes to include the covariates:

$$
\lambda(X_1,\dots,X_p)=e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}
$$ {#eq-poissonpmfcovariates}

Now we are going to use `glm()` function with family Poisson in r to fit this model:

```{r bikershareglm}
glm.fit <- glm(bikers ~  mnth + hr + workingday + temp + weathersit ,
data = Bikeshare, family=poisson)
summary(glm.fit)
```

and the predicted values plot show that we don't have negative counts anymore. We must use the argument type= "response" which tells R to output probabilities of the form $P(Y=1|X)$ as opposed to other information such as the logit.

```{r bikersharepredglm, echo=FALSE, fig.align='center', fig.width=6}

glm.pred <- predict(glm.fit, type = "response")
point_colors <- ifelse(glm.pred < 0, "red", "lightblue")
plot(jitter(as.numeric(Bikeshare$hr)), jitter(glm.pred),
     xlab = "Hour",
     ylab = "Number of Bikers",
     main = "Number of Bikers per Hour",
     pch = 19, # Point character
     col = point_colors,
     cex = 0.5
  )
smooth.spline.fit <- smooth.spline(as.numeric(Bikeshare$hr), glm.pred, spar=0.5)
lines(smooth.spline.fit, col = "darkblue", lwd = 2)
```

In order to visualize the outputs of the models, we plot the response estimates against the coefficients for both the linear model and the Poisson model. But first, it is important to obtain the coefficient estimates associated with the last month and hour. The coefficients for January through November can be obtained directly from the lm.fit object. The coefficient for December must be explicitly computed as the negative sum of all the other months. The linear model results:

```{r mgralph, echo=FALSE, fig.align='center', fig.width=6}
coef.months <- c( coef (lm.fit)[2:12],-sum ( coef (lm.fit)[2:12]))

plot (coef.months , xlab = " Month ", ylab = "Coefficient",
xaxt = "n", col = " blue ", pch = 19, type = "o")

axis (side = 1, at = 1:12, labels = c("J", "F", "M", "A",
"M", "J", "J", "A", "S", "O", "N", "D"))
```

and the glm results:

```{r glmcoeffgraphmonth , echo=FALSE, fig.align='center', fig.width=6}
coef.mnth <- c(coef (glm.fit)[2:12],
-sum ( coef (glm.fit)[2:12]))
plot(coef.mnth , xlab = " Month ", ylab = " Coefficient ",
xaxt = "n", col = " blue ", pch = 19, type = "o")
axis(side = 1, at = 1:12, labels = c("J", "F", "M", "A", "M",
"J", "J", "A", "S", "O", "N", "D"))
```

And for the hours using the linear model:

```{r plothourscoefficients, echo=FALSE, fig.align='center', fig.width=6}
coef.hours <- c(coef(lm.fit)[13:35],
-sum (coef(lm.fit)[13:35]))
plot(coef.hours , xlab = " Hour ", ylab = " Coefficient ",
col = " blue ", pch = 19, type = "o")
```

And the glm poisson model:

```{r hoursplotglm, echo=FALSE, fig.align='center', fig.width=6}
coef.hours <- c(coef(glm.fit)[13:35],-sum (coef(glm.fit)[13:35]))
plot(coef.hours , xlab = " Hour ", ylab = " Coefficient ",
col = " blue ", pch = 19, type = "o")
```

The predictions from the Poisson regression model are correlated with those from the linear model; however, our Poisson model produces non-negative outputs. For Poisson regression the responses at each level of X become more variable with increasing means, where variance=mean.In addition, the mean values of Y at each level of X fall on a curve, not a line. AS a result, at either very low or very high levels of Y, the Poisson regression predictions tend to be larger than those from the linear model.

::: orange-box
**Linear Regression:** Use When: You want to predict a continuous outcome based on one or more predictor numerical variables. Example: Predicting house prices based on size, location, and age.

**Logistic Regression:** Use When: You want to predict a binary outcome (0/1) based on one or more predictor variables. Example: Predicting whether a customer will buy a product (Yes/No) based on their browsing history.

**Discriminant Analysis (LDA/QDA):** Use When: You want to classify observations into predefined categories based on predictor variables. LDA is used when the assumptions of equal covariance matrices are valid (same variance in each class), while QDA is used when they are not.

Example: Classifying iris species based on sepal and petal measurements.

**Naive Bayes** is useful when the number of parameters is large

**Generalized Linear Models (GLMs)**: Use When: You need more flexibility in modeling different types of outcomes (e.g., counts, binary, proportions) and you might have predictors that are not normally distributed.

Example: Using Poisson regression (a type of GLM) to model the number of calls received by a call center per hour.
:::

# Polynomial Regression

Polynomial regression is an extension of linear regression that models the relationship between the independent variable𝑥and the dependent variable𝑦as an𝑛th-degree polynomial. In linear regression, the relationship between $x$ and $y$ is modeled as a straight line: $y=\beta_0+\beta_1X$ In polynomial regression, the relationship is modeled as a polynomial. For example for a single variable $x$: $y=\beta_0+\beta_1x+\beta_2x^2+\dots+\beta_nx^n$. It can capture more complex patterns in the data that linear regression might miss. By adjusting the degree of the polynomial, you can model different types of curves. To choose the degree of polynomial to use, we can use cross-validation for example.

```{r}
# Sample data
set.seed(123)
x <- runif(100, 0, 10)
y <- 1 + 2 * x + 3 * x^2 + rnorm(100, 0, 10)  # True relationship is quadratic

# Fit a polynomial regression model
model <- lm(y ~ poly(x, 2, raw = TRUE))

# Summary of the model
summary(model)

# Plot the data and the polynomial fit
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2, raw = TRUE), se = TRUE) +
  labs(title = "Polynomial Regression Fit", x = "x", y = "y")

```

`poly(x, 2, raw = TRUE)`: Fits a polynomial of degree 2 (quadratic) to the data. High-degree polynomials can overfit the data, capturing noise rather than the underlying relationship. It's important to choose the polynomial degree carefully.

::: {.callout-orange appearance="simple" icon="false"}
The `stat_smooth()` function in `ggplot2` is used to add a smooth line to your plot, which represents a fitted model (like a regression line or a *loess* curve) to your data. It's a great way to visualize trends and patterns in the data.

-   Method: Specifies the smoothing method to be used.Common methods include "lm" for linear models, "glm" for generalized linear models, "loess" (Locally Estimated Scatterplot Smoothing) for local polynomial regression, and "gam" for generalized additive models.

-   Formula: Defines the model formula. For example, y \~ x for simple linear regression or y \~ poly(x, 2) for polynomial regression.

-   Se: Boolean indicating whether to display confidence intervals around the smooth line. By default, it's TRUE.
:::

In this case of polynomial regression we are not interested in the coefficients as we were in linear models, we are more interested in the fitted function values for a value $x_0$

$$
\hat{f}(x_0)= \hat\beta_0+\hat\beta_1x_0+\hat\beta_2x_0^2+\hat\beta_3x_0^3
$$

## standard error

Since $\hat f(x_0)$ is a linear function, we can get a variance at any value $x_0$ this is the *pointwise-variances*. Pointwise means that we are showing what the standard error is at any given point.

## Logisctic regression

For logistic regression, the details are pretty much the same.

$$
Pr(y_i=\hat y_i |x_i) = \frac{exp(\beta_0+\beta_1x_i+\beta_2x_i^2+\dots+\beta_nx_i^n)}{1+exp(\beta_0+\beta_1x_i+\beta_2x_i^2+\dots+\beta_nx_i^n)}
$$ 

To get confidence intervals, we compute the upper and lower bounds on the logit scale, and then invert to get on the probability scale.

![](polynomialRegression.jpeg)
Note that we have inserted these images as jpeg here because we will reproduce them with code later in our r practice. 

## Caveats of polynomial regression

-   Polynomials have notorious tail behavior: At the very end of the data range their values become very erratic. As the input𝑥becomes very large (positive or negative), the value of a polynomial can increase or decrease very quickly. Small changes in the coefficients of higher-degree terms can result in significant changes in the polynomial's behavior, particularly at the tails. This makes polynomials sensitive to the precision of the coefficients. If you plot a high-degree polynomial, you'll notice that the tails can show steep inclines, declines, or wild oscillations compared to the central portion of the plot. In practical terms, this means that polynomials may not always be the best choice for modeling data that extends over a wide range, especially if the behavior at extreme values is important to understand or predict. This makes polynomials bad for extrapolation.

::: exercise-box
Polynomial regression 

In this set of examples for non-linear models we are going to analyze the Wage dataset from ISLR2

This syntax fits a linear model, using the `lm()` function, in order to predict `wage` using a fourth-degree polynomial in `age`: `poly(age, 4)`. The `poly()` command allows us to avoid having to write out a long formula with powers of `age`. The function returns a matrix whose columns are a basis of *orthogonal polynomials*, which essentially means that each column is a linear combination of the variables `age`, $age^2$,  $age^3$ and $age^4$.

```{r}
fit<- lm(wage ~ poly(age,4), data = Wage)
coef(summary(fit))
```

However, we can also use `poly()` to obtain `age`, $age^2$,  $age^3$ and $age^4$ directly, if we prefer. We can do this by using the `raw = TRUE` argument to the `poly()` function.
Later we will see that this does not affect the model in a meaningful way, though the choice of basis clearly affects the coefficient estimates and the $p$-values will be different, but it does not affect the fitted values obtained.

```{r}
fit2 <- lm(wage ~ poly(age, 4, raw = T), data = Wage)
coef(summary(fit2))
```
let's check it out:
```{r, fig.align='center', fig.width=6}
plot(fitted(fit),fitted(fit2))
```
and this shows that the fitted values are the same for both ways of constructing our model.

We could also do the same more compactly, using the `cbind()` function for building a matrix from a collection of vectors; any function call such as `cbind()` inside a formula also serves as a wrapper.

```{r }
fit2b <- lm(wage ~ cbind(age, age^2, age^3, age^4),
    data = Wage)
```


We are usually not interested in the coefficients of the polynomials, but in the function that it produces, so let's make a plot of the fitted function alongside with the standard errors of the fit:

```{r fig.align='center',fig.width=6}
agelims = range(Wage$age)
age.grid = seq(agelims[1], agelims[2])

preds <- predict(fit, newdata=list(age=age.grid),se=TRUE)

se.bands = cbind(preds$fit +2*preds$se, preds$fit -2*preds$se)

plot(Wage$age,Wage$wage,col='darkgrey')
lines(age.grid,preds$fit,lwd=2,col='blue')
matlines(age.grid, se.bands, col = "blue", lty = 2)

```

We see in the summaries above that coefficients are all significant up to the 4th degree, this means that a cubic polynomial would be sufficient, but this comparison is only possible when we have a single variable and we are using a linear model. In the rest of the cases we would use `anova()` as we will see now.

In performing a polynomial regression we must decide on the degree of the polynomial to use. One way to do this is by using hypothesis tests. We now fit models ranging from linear to a degree-5 polynomial and seek to determine the simplest model which is sufficient to explain the relationship between `wage` and `age`. We use the `anova()` function, which performs an  *analysis of variance* (ANOVA, using an F-test) in order to test the null hypothesis that a model $M_1$ is sufficient to explain the data against the alternative hypothesis that a more complex model $M_2$ is required. 
In order to use the `anova()` function, $M_1$ and $M_2$ must be *nested* models: the predictors in $M_1$ must be a subset of the predictors in $M_2$. In this case, we fit five different models and sequentially compare the simpler model to the more complex
model.

```{r chunk9}
fit.1 <- lm(wage ~ age, data = Wage)
fit.2 <- lm(wage ~ poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ poly(age, 3), data = Wage)
fit.4 <- lm(wage ~ poly(age, 4), data = Wage)
fit.5 <- lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```
The $p$-value comparing the linear `Model 1` to the quadratic `Model 2` is essentially zero ($<$$10^{-15}$), indicating that a linear fit is not sufficient. Similarly the $p$-value comparing the quadratic  `Model 2` to the cubic `Model 3` is very low ($0.0017$), so the quadratic fit is also insufficient. The $p$-value comparing the cubic and degree-4 polynomials, `Model 3` and `Model 4`, is approximately $5 \%$ while the degree-5 polynomial `Model 5` seems unnecessary because its $p$-value is $0.37$. Hence, either a cubic or a quartic polynomial appear to provide a reasonable fit to the data, but lower-or higher-order models are not justified.

In this case, instead of using the `anova()` function, we could have obtained these $p$-values more succinctly by exploiting the fact that `poly()` creates orthogonal polynomials.

```{r }
coef(summary(fit.5))
```
Notice that the p-values are the same, and in fact the square of the  $t$-statistics are equal to the F-statistics from the `anova()` function; for example:

```{r chunk11}
(-11.983)^2
```
However, the ANOVA method works whether or not we used orthogonal polynomials; it also works when we have other terms in the model as well. For example, we can use `anova()` to compare these three models:

```{r chunk12}
fit.1 <- lm(wage ~ education + age, data = Wage)
fit.2 <- lm(wage ~ education + poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ education + poly(age, 3), data = Wage)
anova(fit.1, fit.2, fit.3)
```
**Logistic Regression **

Next we consider the task of predicting whether an individual earns more than $250{,}000$ per year. We proceed much as before, except that first we create the appropriate response vector, and then apply the `glm()` function using `family = "binomial"` in order to fit a polynomial logistic regression model.

Note that we again use the wrapper `I()` to create this binary response variable on the fly. The expression `wage > 250` evaluates to a logical variable containing `TRUE`s and `FALSE`s, which `glm()` coerces to binary by setting the `TRUE`s to 1 and the `FALSE`s to 0.

```{r}
fit= glm(I(wage>250) ~ poly(age, 3), data = Wage, family= binomial)
summary(fit)
preds <- predict(fit, newdata = list(age = age.grid), se = T)
```

In this case, due to the nature of glm functions, its loses the orthogonal characteristics so we cannot use the $p$-values alone to see if we need a polynomial of degree 3 or not, what we would need to do is to fit a model with polynomials of different degrees and compare the results among those instead.

We have to manually calculate the standard error bands as we did before. 

```{r}
preds = predict(fit, list(age=age.grid), se=T)
se.bands = preds$fit + cbind(fit=0, lower=-2*preds$se, upper=2*preds$se )
se.bands[1:5,]
```
However, calculating the confidence intervals is slightly more involved than in the linear regression case.

The default prediction type for a `glm()` model is `type = "link"`, which is what we use here.  This means we get predictions for the *logit*, or log-odds: that is, we have fit a model of the form 
\[
\log\left(\frac{\Pr(Y=1|X)}{1-\Pr(Y=1|X)}\right)=X\beta,
\]
 and the predictions given are of the form $X\hat\beta$.  The  standard errors given are also for $X \hat\beta$. In order to obtain confidence intervals for $\Pr(Y=1|X)$, we use the transformation:
\[
\Pr(Y=1|X)=\frac{\exp(X\beta)}{1+\exp(X\beta)}.
\]

So the model gave us the predictions on the logit scale. We want the predictions as a probability. To transform we need to apply the inverse logit mapping

$$p= \frac{e^\eta}{1+e^\eta}$$
we can do this simultaneously for all three columns of 'se.bands`:

```{r, fig.align='center', fig.width=6}
prob.bands <- exp(se.bands)/(1+exp(se.bands))
matplot(age.grid, prob.bands, col= 'blue', lwd= c(2,1,1), lty=c(1,2,2), type="l", ylim =c(0,.1))
points(jitter(Wage$age), I(Wage$wage > 250)/10,pch= "l", cex =0.5)
```
The last part of the code above (`points`) is showing which values are 1 and which ones are 0 for the variable wager>250k. We had to divide this by 10 because we have very little values in the top range and otherwise they would not show. 

Note that we could have directly computed the probabilities by selecting the `type = "response"` option in the `predict()` function.

```{r }
preds <- predict(fit, newdata = list(age = age.grid),
    type = "response", se = T)
```

However, the corresponding confidence intervals would not have been sensible because we would end up with negative probabilities! By calculating the probabilities and the fitted values first for the logit scale and then transforming them, we ensure that they never get below 0. 
:::

## step Functions
In step functions what you do is you cut your continuous variable into discrete sub-ranges. For example for our salary wage over age if we cut at age 35, we create a dummy variable, if age is less than 35 you make it a 1, if not, you make it a 0, and for each cut you create a dummy variable and then you fit all of this into a linear model. 
It is also an easy way to create interactions, for example we can create an interaction between year and age. We can cut year at 2005 and create a dummy variable, then we can multiply that with age and this will create an interaction, and that will fit a different linear model as a function of age for people who worked before 2005 and those after 2005. In R is very easy to create, we use the indicator function `I` for example `I(year<2005)` and that will create 0 and 1 for the true and false values of that logical, and if you want to cut in more than one place, there's a function called `cut` so we can cut age like this `cut(age,c(18,25,40,65,90))` and that will create a factor which divides into those ranges. 

:::{exercise-box}
Step Functions

In order to fit a step function, we use the `cut()` function.

```{r chunk18}
table(cut(Wage$age, 4))
fit <- lm(wage ~ cut(age, 4), data = Wage)
coef(summary(fit))
```
Here `cut()` automatically picked the cutpoints at $33.5$, $49$, and $64.5$ years of age. We could also have specified our own cutpoints directly using the `breaks` option.
The function `cut()` returns an ordered categorical variable; the `lm()` function then creates a set of dummy variables for use in the regression. The `age < 33.5` category is left out, so the intercept coefficient of $94{,}160$ can be interpreted as the average salary for those under $33.5$ years of age, and the other coefficients can be interpreted as the average additional salary for those in the other age groups.
We can produce predictions and plots just as we did in the case of the polynomial fit.

:::

## Piecewise Polynomials and splines

Instead of a single polynomial in X over its whole domain, we can rather use different polynomials in regions defined by knots.This will fit two (or more) different functions into different regions of the data. This may create very separate lines, to avoid that we can create constraints to enforce the different functions to merge at the knot, and if we want to add more constraints, we can enforce continuity at the derivatives, and that will make the functions to behave more like a single polynomial function. The maximum number of constraints you can add is one less than the degree of polynomial. These are splines (the maximum level of continuity) allowed for piecewise polynomials. Adding the same number as the degrees would just create a polynomial function and not different functions.

Splines serve the same function as polynomials but are more flexible. 

### Linear Splines.
A linear spline with knots at k=1...k=n is a piecewise linear polynomial continuous at each knot.
### Cubic Splines
A cubic spline is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot. 
### Natural Splines
A natural cubic spline is a cubic spline with two extra constraints at each end (boundary) of the data, and the constraints make the function extrapolate linearly beyond the boundary knots, this is reducing the standard errors at the ends. 
For fitting cubic splines in R we use the function `bs()` you give it the knots as arguments. For natural splines we use the function `ns()` both in package `splines` 

*knot placement*: one strategy is to decide on the total number of knots K, and then place them uniformly across different quantiles of the variable x. That results in a certain number of parameters. So a cubic spline with k knots gets k+4 parameters or degrees of freedom.
If you use a natural spline with k knots, it only has k degrees of freedom because you get back two degrees of freedom for the two constraints on each of the boundaries. 

### Smoothing spline
There is a special type of spline called the smoothing spline with a knot at every unique value of x. We are not going to get into mathematical details here, you can check in the book, but in R we can use the function `smooth.spline()` and it will fit a smoothing spline. 

:::{exercise-box}
Splines

The library to work with splines is `splines`. Regression
splines can be fit by constructing an appropriate matrix of basis functions.
The `bs()` function generates the entire matrix of basis functions for splines with the specified set of knots. By default, cubic splines are produced.

Fitting `wage` to `age` using a regression spline is simple: 
```{r, fig.align='center', fig.width=6}
library(splines)
fit = lm(wage ~ bs(age,knots=c(25,40,60)),data=Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = T)
plot(Wage$age,Wage$wage,col='darkgrey', ylim =c(0,300))
lines(age.grid, predict(fit, list(age=age.grid)), col = 'blue',lwd=2)
abline(v= c(25,40,60), lty=2, col='red')
lines(age.grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(age.grid, pred$fit - 2 * pred$se, lty = "dashed")
```
The advantage is that they don't vary so much at the boundaries. 

Here we have pre specified knots at ages $25$, $40$, and $60$. This produces a spline with six basis functions. (Recall that a cubic spline with three knots has seven degrees of freedom; these degrees of freedom are used up by an intercept, plus six basis functions). 

We could also use the `df` option to produce a spline with knots at uniform quantiles of the data.

```{r }
dim(bs(Wage$age, knots = c(25, 40, 60)))
dim(bs(Wage$age, df = 6))
attr(bs(Wage$age, df = 6), "knots")
```

In this case `R` chooses knots at ages $33.8, 42.0$, and $51.0$, which correspond to the 25th, 50th, and 75th percentiles of `age`. The function `bs()` also has a `degree` argument, so we can fit splines of any degree, rather than the default degree of 3 (which yields a cubic spline). 

In order to instead fit a natural spline, we use the `ns()` function.
Here we fit a natural spline with four degrees of freedom.

```{r, fig.align='center', fig.width=6}
fit2 <- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 <- predict(fit2, newdata = list(age = age.grid),se = T)
plot(Wage$age, Wage$wage, col = "gray")
lines(age.grid, pred2$fit, col = "red", lwd = 2)
```
As with the `bs()` function, we could instead specify the knots directly using the `knots` option.

**Smoothing splines**
 In order to fit a smoothing spline, we use the `smooth.spline()` function. Here we fit a smoothing spline with 16 degrees of freedom, and as we can see, it overfits a bit. 

```{r, fig.align='center', fig.width=6}
plot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Smoothing Spline")
fit <- smooth.spline(Wage$age, Wage$wage, df = 16)
lines(fit, col = "red", lwd = 2)
```
What we can do instead is use cross validation *leave one out* to decide the smoothing parameter for us automatically:

```{r, fig.align='center', fig.width=6}
plot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = "darkgrey")
 title("Smoothing Spline")

 fit2 <- smooth.spline(Wage$age, Wage$wage, cv = TRUE)

lines(fit, col = "red", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
legend("topright", legend = c("16 DF", "6.8 DF"),
    col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
fit2
```

Notice that in the first call to `smooth.spline()`, we specified `df = 16`. The function then  determines which value of $\lambda$  leads to $16$ degrees of freedom.
 In the second call to `smooth.spline()`, we select the smoothness level by cross-validation. This results in a value of $\lambda$ that yields 6.8 degrees of freedom.
 
 
**Local Regression**

In order to perform local regression, we use the `loess()` function.

```{r , fig.align='center', fig.width=6}
plot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Local Regression")
fit <- loess(wage ~ age, span = .2, data = Wage)
fit2 <- loess(wage ~ age, span = .5, data = Wage)
lines(age.grid, predict(fit, data.frame(age = age.grid)),
    col = "red", lwd = 2)
lines(age.grid, predict(fit2, data.frame(age = age.grid)),
    col = "blue", lwd = 2)
legend("topright", legend = c("Span = 0.2", "Span = 0.5"),
    col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
```

Here we have performed local linear regression using spans of $0.2$ and $0.5$: that is, each neighborhood consists of 20 \% or 50 \% of the observations. The larger the span, the smoother the fit.
The `locfit` library can also be used for fitting local regression models in `R`.



:::

##  Generalized Additive Models (GAM)

Generalized Additive Models (GAMs) are a type of statistical model used to capture non-linear relationships between the dependent variable and one or more predictor variables. Unlike traditional linear models, GAMs allow for more flexibility by using smooth functions to model the relationship between predictors and the response variable.

*Additive Structure*: The model is a sum of smooth functions of the predictor variables.
*Flexibility*: Can capture non-linear patterns without specifying a particular form for the relationship.
*Interpretability*: Each predictor's effect is represented by a smooth function, making it easier to interpret.

$$Y = \beta_0 + f_1(X_1) + f_2(X_2) + \ldots + f_p(X_p) + \epsilon$$
where Y is the dependent variable, 
$\beta_0$ is the intercept,
$f_1(X_1) + f_2(X_2) + \ldots + f_p(X_p)$ are the smooth functions of the predictor variables $X_1,X_2,\dots,X_p$
$\epsilon$ is the error term. 

GAMs are widely used in various fields such as environmental science, economics, and healthcare to model complex relationships in data.


:::{.exercise-box}

Generalized Additive Models (GAM)

So far we have focused on fitting models with mostly single nonlinear terms. 
The `gam` package makes it easy to work with multiple nonlinear terms. In addition, it knows how to plot these functions and their standard errors. 

The `s()` function, which is part of the `gam` library, is used to indicate that we would like to use a smoothing spline. We specify that the function of `year` should have $4$ degrees of freedom, and that the function of `age` will have $5$ degrees of freedom. Since `education` is qualitative, we leave it as is, and it is converted into four dummy variables. We use the `gam()` function in order to fit a GAM using these components.
 All of the terms in are fit simultaneously, taking each other into account to explain the response.


```{r, fig.align='center'}
library(gam)
gam1 = gam(wage ~ s(age,df=5)+s(year,df=4)+education , data=Wage)
par(mfrow=c(1,3))
plot(gam1,se=T, col = "blue")

```
This produces a plot for each of the term on the gam with their standard deviation.

The generic `plot()` function recognizes that `gam.m3` is an object of class `Gam`, and invokes the appropriate `plot.Gam()` method.  Conveniently, even though `gam1` is not of class `Gam` but rather of class `lm`, we can use `plot.Gam()` on it.  :

```{r, fig.align='center'}
par(mfrow=c(1,3))
plot.Gam(gam1, se = TRUE, col = "red")
```

Notice here we had to use `plot.Gam()` rather than the *generic* `plot()` function.

In these plots, the function of `lyear` looks rather linear. We can perform a series of ANOVA tests in order to determine which of these three models is best:  a GAM that excludes `year` ($M_1$),  a GAM that uses a linear function of `lyear` ($M_2$), or  a GAM that uses a spline function of `year` ($M_3$).

```{r}

gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage) #no year
gam.m2 <- gam(wage ~ year + s(age, 5) + education,data = Wage) #linear function year
gam.m3 = gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage) #spline year
anova(gam.m1, gam.m2, gam.m3, test = "F")

```

We find that there is compelling evidence that a GAM with a linear function of `year` is better than a GAM that does not include `year` at all ($p$-value = 0.00014). However, there is no evidence that a non-linear function of `year` is needed (p-value = 0.349).
In other words, based on the results of this ANOVA, $M_2$ is preferred. 

The `summary()` function produces a summary of the gam fit.

```{r }
summary(gam.m3)
```
The "Anova for Parametric Effects" $p$-values clearly demonstrate that `year`, `age`, and `education` are all highly statistically significant, even when only assuming a linear relationship. 
Alternatively, the "Anova for Nonparametric Effects" $p$-values for `year` and `age` correspond to a null hypothesis of a linear relationship versus the alternative of a non-linear relationship. The large $p$-value for `year` reinforces our conclusion from the ANOVA test that a linear function is adequate for this term. However, there is very clear evidence that a non-linear term is required for `age`.


We can make **predictions** using the `predict()` method for the class `Gam`.
 Here we make predictions on the training set.

```{r}
preds <- predict(gam.m2, newdata = Wage)
```

**Logistic Regression GAM**


gam also works for logistic regressions. Here we will code wage as a true of false depending on the value >250k.

In order to fit a logistic regression GAM, we once again use the `I()` function in constructing the binary response variable, and set `family=binomial`.

```{r, fig.align='center'}
gam2 <- gam(
    I(wage > 250) ~ year + s(age, df = 5) + education,
    family = binomial, data = Wage
  )
par(mfrow = c(1, 3))
plot(gam2, se = T, col = "green")
```
and what we are seeing in these plots are the contributions to the logit of the probability in the separate functions. It is easy to see that there are no high earners in the `< HS` category:

It is easy to see that there are no high earners in the `< HS` category:

```{r chunk35}
table(Wage$education, I(Wage$wage > 250))
```
Hence, we fit a logistic regression GAM using all but this category. This provides more sensible results.

```{r, fig.align='center'}
gam.lr.s <- gam(
    I(wage > 250) ~ year + s(age, df = 5) + education,
    family = binomial, data = Wage,
    subset = (education != "1. < HS Grad")
  )
par(mfrow = c(1, 3))
plot(gam.lr.s, se = T, col = "green")
```


`gam` can also plot models generated by other packages
```{r, fig.align='center'}
par(mfrow=c(1,3))
lm1=lm(wage ~ns(age, df=4)+ns(year,df=4)+education,data=Wage)
plot.Gam(lm1,se=T)
```

**local regression**
We can also use local regression fits as building blocks in a GAM, using the `lo()` function.

```{r , fig.align='center'}
par(mfrow=c(1,3))
gam.lo <- gam(
    wage ~ s(year, df = 4) + lo(age, span = 0.7) + education,
    data = Wage
  )
plot(gam.lo, se = TRUE, col = "green")
```

Here we have used local regression for the `age` term, with a span of
$0.7$.
We can also use the `lo()` function to create interactions before calling the `gam()` function. For example,

```{r }
gam.lo.i <- gam(wage ~ lo(year, age, span = 0.5) + education,
    data = Wage)
```

fits a two-term model, in which the first term is an interaction between `year` and `age`, fit by a local regression surface. We can plot the resulting two-dimensional surface if we first install the `akima` package.

```{r, fig.align='center'}
require(akima)
par(mfrow=c(1,2))
plot(gam.lo.i)
```


:::

# Survival Analysis

Survival analysis concerns a special kind of outcome variable: the time until an event occurs. For example, suppose that we have conducted a five-year medical study, in which patients have been treated for cancer. We would like to fit a model to predict patient survival time, using features such a baseline measurements or type of treatment. Sounds like a regression problem, but there is an important complication: some of the patients have survived until the end of the study. Such a patient's survival time is said to be *censored*. We do not want to discard this subset of surviving patients, since the fact that they survived at least 5 years amounts to valuable information.

The applications of survival analysis extend far beyond medicine. For example, consider a company that wishes to model churn, the event when customers cancel subscription to a service. The company might collect data on customers over some period of time, in order to predict each customer's time to cancellation.

For each individual, we suppose that there is a true *failure* or *event* time T, as well as a true censoring time C. The survival time represents the time at which the event of interest occurs. The censoring is the time at which censoring occurs, for example the time at which the patient drops out of the study or the study ends. For each observation we compute the min of those times, either the event or the censoring time. If the event occurs before censoring, then we observe the true survival time T, if censoring occurs before the event, then we observe C. We observe a status indicator $\delta = 1\ if\  T\leq C$ $\delta = 0\ if\  T > C$ so in our dataset we observe n pairs (Y,$\delta$) where Y is the time to T or C.

Suppose that a number of patients drop out of a cancer study early because they are very sick. An Analysis that does not take into consideration the reason why the patients dropped out will likely overestimate the true average survival time. Similarly, suppose that males are more likely to drop out of the study than females, then a comparison of male and female survival times may wrongly suggest that males survive longer than females.

In general, we need to assume that, conditional on the features, the event time T is independent of the censoring time C, and the above examples violate that assumption. There is no way of statistically checking if this assumption is right, you will have to think about those variables yourself and find out the reasons why observations are censored.

The **survival curve** is the probability that a true survival time $T$ is higher than a specified time $t$. $$
S(t)=Pr(T>t)
$$ {#eq-survivalCurve}

This decreasing function quantifies the probability of surviving past time $t$ For example, suppose that a company is interested in modeling customer churn. Let $T$ represent the time that a customer cancels a subscription to the service, then $S(t)$ represents the probability that a customer cancels later than time $t$. The larger the value of $S(t)$, the less likely that the customer will cancel before time $t$.

Let's consider the `BrainCancer` dataset, which contains the survival times for patients with primary brain tumors undergoing treatments. The predictors are `gtv` (gross tumor volume), `sex`, `diagnosis` (type of cancer), `loc` (location of the tumor), `ki` (Karnofsky index), and `stereo` (stereotactic method). 53 of the 88 patients were still alive at the end of the study.

Suppose we'd like to estimate the probability of surviving past 20 months. $S(20)=Pr(T>20)$. The first approach that would come to our minds would be to just count the proportion of patients that survived past 20 months, and that is 48/88 = 55%. However, this is not right, 17 of the 40 patients who did not survive to 20 months were actually censored, and this way of doing the analysis implicitly assumes they died, hence this probability is an underestimate.

## The Kaplan-Meier Estimate

Suppose we have 5 observations (patients). number 2 and 4 censored and we have the time to event for the rest:

```{r ,echo=FALSE, fig.align='center'}
# Sample data
time <- c(2, 3, 4, 5, 6)
status <- c(1, 0, 1, 0, 1)  # 1 = event (e.g., death), 0 = censored
patient <- c(1, 2, 3, 4, 5)

# Create a data frame
data <- data.frame(time, status, patient)
# Plot the time vs patient
plot(data$time, 1:nrow(data), type = "n", xlab = "Time", ylab = "Patient", yaxt = "n", xlim=c(0,6), ylim=c(0,6))

# Add patient labels
axis(2, at = 1:nrow(data), labels = data$patient)

# Add points for events and censored data
points(data$time[data$status == 1], data$patient[data$status == 1], pch = 19, col = "red")  # Events
points(data$time[data$status == 0], data$patient[data$status == 0], pch = 1, col = "blue")  # Censored

for (i in 1:nrow(data)) { lines(c(0, data$time[i]), c(data$patient[i], data$patient[i]), col = "gray", lty = 2) }
```

when we are observing the first patient's T, 4 out of 5 patients survived past that point in time, so the probability of surviving past t(2) is 4/5

Then we move to the next T (skip the censored number 2) and we have that 2 out of the 3 patients remaining survived past that point. So the probability of surviving past t(4) is $\frac{4}{5} \times \frac{2}{3}$ so the probability of surviving past t(4) is the probability of surviving t(4) given that you have survived pass the previous T.

The last point, patient 5 in t(6) is the last failure, and no one has survived pass that point, so we have $\frac{4}{5} \times \frac{2}{3} \times 0$

So we see how the role of the censored observations is to participate in the denominator up to the point that they dropped out of the study.

The kaplan-Meier curve for this simple experiment would look like this:

```{r, fig.align='center', fig.width=6}
fit <- survival::survfit(Surv(time, status) ~ 1, data = data)
survminer::ggsurvplot(fit, data = data, xlab = "Time", ylab = "Survival Probability", title = "Kaplan-Meier Survival Estimate", censor.shape = 124, censor.size = 4, risk.table = TRUE, risk.table.col = "strata", ggtheme = theme_minimal())
```

Now that we have seen this with a simple example, let's run it over the BrainCancer dataset. Each point in the solid step-like curve shows the estimated probability of surviving past the time indicated on the horizontal axis.

Before begining the analysis, it is important to know how the `status` variable has been coded. Most software, including R, uses the convention that `status =1` indicates an uncensored observation, and `status =0` indicates a censored observations. But some scientist might use the opposite coding.

To begin the analysis we recreate the Kaplan-Meier survival curve using the `survfit()` function from `survival` library. By including `~1` we indicate that the survival analysis is performed without considering any covariates or grouping. The model will estimate a single survival curve for all subjects combined. This allows us to understand the general survival pattern across the entire dataset.

```{r, fig.align='center', fig.width=6}
library(survival)

BrainCancer= ISLR2::BrainCancer
fit.surv <- survfit(Surv(time, status) ~ 1, data = BrainCancer)
plot(fit.surv, xlab="Months", ylab= "Estimated Pr survival")
```

The dotted lines indicates the confidence interval for 2 standard errors. The estimated probability of survival past 20 months is 71%, which is higher than the 55% that we estimated just but counting the amount of survivors.

## Proportional Hazards Model: Log-Rank Test

Now we are going to see how to compare two samples. For example, for our BrainCancer dataset, let's compare females and males. If we plot them we can appreciate a difference until month 50 but is this difference significant?

```{r}
fit.sex <- survfit(Surv(time, status) ~ sex, data= BrainCancer)

plot(fit.sex, xlab = "Months",
    ylab = "Estimated Probability of Survival", col = c(2,4))

legend("bottomleft", levels(BrainCancer$sex), col = c(2,4), lty = 1)

```

The proportional Hazard Model allows us to compare two survival samples. It is the equivalent of a t.test for survival data. Being $d_1<d_2<\dots,d_k$ unique death times among non-censored patients, $r_k$ is the number of patients at risk at time $d_k$ and $q_k$ is the number of patients who died at time $d_k$. We divide our patients in two groups males and females, and we will call their variables $q_{1k}$ and $q_{2k}$ for the number of male and female patients who died and $r_{1k}$ and $r_{2k}$ are the number of patients at risk in each group. So at a specific time $d_k$ we have:

```{r echo=FALSE}
library(kableExtra)
data <- data.frame(
  Group1 = c("$q_{1k}$", "$r_{1k} - q_{1k}$", "$r_{1k}$"),
  Group2 = c("$q_{2k}$", "$r_{2k} - q_{2k}$", "$r_{2k}$"),
  Total = c("$q_{k}$", "$r_{k} - q_{k}$", "$r_{k}$") )
row.names(data) <- c("Died", "Survived", "Total")
kable(data, format = "html", escape = FALSE)%>% kable_styling(full_width = FALSE, position = "center")
```

To test $H_0$, our null hypothesis that the survival rates are the same for both genders, we construct a test statistic of the form:

$$
W=\frac{X-E(X)}{\sqrt{Var(X)}}
$$ where E(X) is the expectation and Var(X) is the variance of X under the null hypothesis. $X=\sum^K_{k=1}q_{1k}$

The resulting formula for the log-rank test statistic is:

$$
W =\frac{\sum^K_{k=1}(q_{1k}-E(q_{1k}))}{\sqrt{\sum^K_{k=1}Var(q_{1k})}}
$$ {#eq-logrankTestStatistic}

We are not going to develop this formula further in this page, but the full definition can be found on the book. When the sample size is large, the *log-rank test statistic W* has approximately a standard normal distribution.

This can be used to compute a `p-value` for the null hypothesis that there is no difference between survival curves in the two groups.

Instead of calculating it manually we will use R with the function `survdiff()`:

```{r}
logrank.test <- survdiff(Surv(time, status) ~ sex, data =BrainCancer)
logrank.test
```

We use a two sided test because we want to know if the male survival rate is less or more than those of females, we are not interested only in if they are higher. We see that our $p$-value for a two-sided test is 0.2, which does not allow us to reject the null hypothesis that there is a difference. This could be just because the dataset is too small. Even though the number of observations are 88, the survival information is extracted from the number of failures, so the censored are not really adding so much information.

The `survminer` package draws survival curves using ggplot, and can provide log-rank $p$-values:

```{r, fig.align='center', fig.width= 8, fig.height=6}
library(survminer)
fit.sex <- survfit(Surv(time, status) ~ sex, data= BrainCancer)
survminer::ggsurvplot(fit.sex, data = BrainCancer,
           pval = TRUE,
           conf.int = TRUE,
           risk.table = TRUE,
           legend.title = "Sex",
           legend.labs = c("Female", "Male"))

```

## Regression Models with a Survival Response Cox Model.

We now consider the task of fitting a regression model to survival data. We wish to predict the true survival time T. Because of the censoring, we cannot really just fit a linear or log model. To overcome this difficulty, we instead make use of a sequential construction, similar to the idea used for the Kaplan-Meier survival curve.

**Proportional Hazards Model (Cox Model)** This is a statistical technique used to explore the relationship between the survival time of subjects and one or more predictor variables.

We start by defining the *Hazard function* this is the risk of the event happening at a particular time, given the fact that the subject has survived up to that time. Why do we care about the hazard function? It turns out that a key approach for modeling survival data as a function of covariates relies heavily on the hazard function.

The model assumes that the hazard rate for an individual is a baseline hazard, shared by all individuals, modified by a set of covariates (predictors). The hazard at time t for an individual is given by $$
h(t | X) = h_0(t) \exp\left( \sum^p_{j=1}x_{ij}\beta_j \right)=  h_0(t) \exp(\beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p)
$$ {#eq-hazardFunction}

Where $h_0(t)$ is an unspecified function, known as the *baseline hazard* The quantity $exp\left( \sum^p_{j=1}x_{ij}\beta_j \right)$ is called the *relative risk*

There is a deeper explanation of this method on the book that we skip here.

There is no intercept in the proportional hazards model because the intercept is absorbed into the baseline hazard. We assume that there are no tied failure times (only one individual Fails at a given time). In the case this is not correct and there are ties, the model is more complicated to calculate and a number of computational approximations must be used. Apart from estimating the coefficients, we may also wish to estimate the baseline hazard, so we can estimate the survival curve. These are implemented in the `survival` package in R.

If we have a single predictor (sex) with a binary outcome (0,1) or (male,female). To test whether there is a difference between the survival tiems of the observations in the two groups we can consider two different approaches:

1- Fit a Cox proportional hazards model and test the null hypothesis that $\beta =0$ 2- Perform a log-rank test to compare the two groups. When taking approach 1, there are a number of possible ways to test $H_0$. One way is known as *score test* and it turn out that in the case of a single binary covariate, the score test for $H_0:\beta=0$ in Cox's proportional hazards model is exactly equal to the log-rank test.

Let's fix Cox proportional hazards models using the `coxph()` function. To begin, we consider a model that uses `sex` as the only predictor in the BrainCancer dataset.

```{r}
fit.cox<- survival::coxph(Surv(time, status) ~ sex, data=BrainCancer)
summary(fit.cox)
```

As we saw with the log-rank test, there is no evidence to suggest that there is a survival difference by sex.

Now we fit all the predictors:

```{r}
fit.cox<- survival::coxph(Surv(time, status) ~ sex + diagnosis + loc + ki + gtv +
   stereo, data = BrainCancer)
fit.cox
```

**Interpreting the results:**

As we mentioned before, there is no intersect because it has been absorbed into the baseline hazard function $h_0(t)$.

-   Call: This shows the formula used for the model.

-   n: Number of observations used in the analysis (87).

-   number of events: Number of observed events (35), such as deaths or failures.

-   coef: The regression coefficients for each predictor.

-   exp(coef): The hazard ratios (HR), which indicate the effect size of each predictor.

-   se(coef): Standard errors of the coefficients.

-   z: z-statistic for testing the null hypothesis that the coefficient is zero.

-   Pr(\>\|z\|): $p$-values for the z-tests, indicating whether the coefficients are significantly different from zero.

-   Likelihood ratio test, Wald test, Score (log-rank) test: These tests assess the overall significance of the model. The low $p$-values (\< 0.001) indicate that the predictors collectively have a significant effect on survival.

In our specific result, the `diagnosis` variable has been coded so that the baseline corresponds to meningioma. Results indicate that the risk associated with HG glioma is more than eight times (i.e. $e^{2.15} = 8.62$) the risk associated with meningioma. In addition, larger values of the Karnofsky index (ki) are associated with lower risk. (negative coefficient).

We need to remember, that like in linear models, if some of the variables are correlated, a variable could show as non-significant even if it is, because the other correlated variables are absorbing part of its influence.

It is possible to plot estimated survival curves for each diagnosis category, adjusting for the other predictors. To make these plots, set the values of the other predictors equal to the mean for quantitative variables, and the modal value for factors.

```{r, fig.align='center', fig.width=7}

with(BrainCancer,{
  modaldata <<- data.frame(
       diagnosis = levels(diagnosis),
       sex = rep("Female", 4),
       loc = rep("Supratentorial", 4),
       ki = rep(mean(ki), 4),
       gtv = rep(mean(gtv), 4),
       stereo = rep("SRT", 4)
       )})
modaldata

survplots <- survfit(fit.cox, newdata = modaldata)
  plot(survplots, xlab = "Months",
      ylab = "Survival Probability", col = 2:5)
  legend("bottomleft", levels(BrainCancer$diagnosis), col = 2:5, lty = 1)

```

::: exercise-box
We are using now the Publication dataset from ISLR2. The time is time until publication and we are interested in knowing if having a positive or negative result versus a null result resulted in faster publication time. A censored time here would be that either the paper was never published or was published after this studied finished. We will start by plotting the survival curve.

```{r, fig.align='center', fig.width=7}

fit.posres <- survfit(Surv(time, status) ~ posres, data = Publication)
plot(fit.posres, xlab ="Months", ylab= "Pr. of not being published", col =3:4)
legend("topright", c("Negative Result", "Positive Result"), col=3:4, lty =1)
```

we can fit a cox model:

```{r}
(fit.pub <- coxph(Surv(time,status) ~ posres, data=Publication))

```

The $p$-value is not significant. We can do a log-rank test:

```{r}
(logrank.test <- survdiff(Surv(time,status) ~ posres, data=Publication))
```

and again we see not significant difference.

However, the results change dramatically when we include other predictors in the model. Here we have excluded only the funding mechanism variable

```{r}
(fit.pub <- coxph(Surv(time,status)~ . -mech, data=Publication))
```

We have now some significant variables, including `posres` result. This often happens when we include other confounding factors.
:::

::: exercise-box
In this section we will simulate survival data using the `sim.survdata()` function, which is part of the `coxed` library. Our simulated data will represent the observed wait times (in seconds) for 2000 customers who have phoned a call center. In this context, censoring occurs if a customers hangs up before his or her call is anwered.

There are three covariates: `operators` (the number of call center operators available) `center` and `time` of day (morning, afternoon, evening). We generate data for these covariates so that all possibilities are equally likely, for instance, morning, afternoon and evening calls are equally likely, and the number of operators (from 5 to 15) is equally likely.

```{r}
set.seed(4)
N <-2000
Operators <- sample(5:15, N, replace=T)
Center <-sample(c("A","B","C"), N, replace=T )
Time <- sample(c("Morn.","After.","Even."), N, replace=T)
X <- model.matrix( ~ Operators + Center + Time)[,-1]

X[1:5,]

```

Next, we specify the coefficients and the hazard function:

```{r}
true.beta <- c(0.04, -0.3,0,0.2,-0.2)
h.fn <- function(x) return(0.00001 * x) #function to create the HAzard.
```

Here, we have set the coefficient associated with `Operators` to equal $0.04$; in other words, each additional operator leads to a $e^{0.04}=1.041$-fold increase in the "risk" that the call will be answered, given the `Center` and `Time` covariates. This makes sense: the greater the number of operators at hand, the shorter the wait time.

The coefficient associated with `Center = B` is $-0.3$, and `Center = A` is treated as the baseline. This means that the risk of a call being answered at Center B is $0.74$ times the risk that it will be answered at Center A; in other words, the wait times are a bit longer at Center B.

We are now ready to generate data under the Cox proportional hazards model. The `sim.survdata()` function allows us to specify the maximum possible failure time, which in this case corresponds to the longest possible wait time for a customer; we set this to equal $1{,}000$ seconds.

```{r chunk19}
library(coxed)
queuing <- sim.survdata(N = N, T = 1000, X = X,
    beta = true.beta, hazard.fun = h.fn)
names(queuing)
```

The "observed" data is stored in `queuing$data`, with `y` corresponding to the event time and `failed` an indicator of whether the call was answered (`failed = T`) or the customer hung up before the call was answered (`failed = F`). We see that almost $90\%$ of calls were answered.

```{r chunk20}
head(queuing$data)
mean(queuing$data$failed)
```

We now plot Kaplan-Meier survival curves. First, we stratify by `Center`.

```{r ,fig.align='center',fig.width=7}
fit.Center <- survfit(Surv(y, failed) ~ Center,
    data = queuing$data)
plot(fit.Center, xlab = "Seconds",
    ylab = "Probability of Still Being on Hold",
    col = c(2, 4, 5))
legend("topright",
     c("Call Center A", "Call Center B", "Call Center C"),
     col = c(2, 4, 5), lty = 1)
```

Next, we stratify by `Time`.

```{r ,fig.align='center',fig.width=7}
fit.Time <- survfit(Surv(y, failed) ~ Time,
   data = queuing$data)
plot(fit.Time, xlab = "Seconds",
    ylab = "Probability of Still Being on Hold",
    col = c(2, 4, 5))
legend("topright", c("Morning", "Afternoon", "Evening"),
    col = c(5, 2, 4), lty = 1)
```

It seems that calls at Call Center B take longer to be answered than calls at Centers A and C. Similarly, it appears that wait times are longest in the morning and shortest in the evening hours. We can use a log-rank test to determine whether these differences are statistically significant.

```{r }
survdiff(Surv(y, failed) ~ Center, data = queuing$data)
survdiff(Surv(y, failed) ~ Time, data = queuing$data)
```

We find that differences between centers are highly significant, as are differences between times of day.

Finally, we fit Cox's proportional hazards model to the data.

```{r}
fit.queuing <- coxph(Surv(y, failed) ~ .,
    data = queuing$data)
fit.queuing
```

The $p$-values for `Center = B`, `Time = Even.` and `Time = Morn.` are very small. It is also clear that the hazard --- that is, the instantaneous risk that a call will be answered --- increases with the number of operators. Since we generated the data ourselves, we know that the true coefficients for `Operators`, `Center = B`, `Center = C`, `Time = Even.` and `Time = Morn.` are $0.04$, $-0.3$, $0$, $0.2$, and $-0.2$, respectively. The coefficient estimates resulting from the Cox model are fairly accurate.
:::

## Harrel's concordance index (the C-index).

This is a method for assessing a fitted survival model on a test set. For each observation, we calculate the estimated risk score, so with the coefficients we got from the model we calculate the estimated risk score, called eta hat of i $\hat\eta_i = \hat\beta_1x_{i1}+\dots+\hat\beta_px{ip}$ Then for each pairs of observations, we compute the proportion that we got right from the model, this is, if they had a lower risk, they lived longer than their counterpart in the pair.

$$
C = \frac{\sum_{i<j} \delta_{ij} \cdot \mathbb{I}(\hat{T}_i < \hat{T}_j)}{\sum_{i<j} \delta_{ij}}
$$ {#eq-Cindex}

When we are comparing a pair that has one censored observation, we cannot really say if they lived longer or not, so we only count up over those that we can compare.

If we get a c-index of 0.733, roughly speaking that means that given two random observations, the model can predict with 73.3% accuracy which one will survive longer.

::: {.callout-orange appearance="simple" icon="false"}
::: centered-text
**Other considerations of survival analysis**
:::

-   There is not a single type of censoring, there is right censoring, left censoring and interval censoring.

    -   *Right censoring*: This occurs when the event of interest has not happened by the end of the study period or when a subject leaves the study before experiencing the event. Example: If a patient is still alive at the end of the study or drops out, the exact survival time is unknown but is known to be greater than the last observed time.
    -   *Left censoring*: This happens when the event of interest has already occurred before the start of the study or before a subject's entry into the study. Example: If you're studying the time to a certain disease and some individuals already had the disease before the study began, their exact time to event is unknown but is known to be less than the first observed time.
    -   *Interval censoring*: This occurs when the event of interest is known to have occurred within a specific time interval, but the exact time is unknown. Example: If follow-up visits are scheduled annually and a patient was healthy at their last visit but found to have developed a disease at the next visit, the exact time of disease onset is unknown but is known to have occurred within that year.

-   There are condiderations to be made about the time scale, for example, use if calendar time or patient's age

-   Time-dependent covariates: we measure certain predictors on the same patient over time
:::
